export const jsonForSmallDataset = [
  {
    title: "Data science",
    originalContent:
      'Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data. \nData science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.\nData science is "a concept to unify statistics, data analysis, informatics, and their related methods" to "understand and analyze actual phenomena" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a "fourth paradigm" of science (empirical, theoretical, computational, and now data-driven) and asserted that "everything about science is changing because of the impact of information technology" and the data deluge.\nA data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.\n\n\n== Foundations ==\nData science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business. Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data. In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.\n\n\n=== Relationship to statistics ===\nMany statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.\nStanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.\n\n\n== Etymology ==\n\n\n=== Early usage ===\nIn 1962, John Tukey described a field he called "data analysis", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term "data science" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.\nThe term "data science" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science. In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data. In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.\nDuring the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included "knowledge discovery" and "data mining".\n\n\n=== Modern usage ===\nIn 2012, technologists Thomas H. Davenport and DJ Patil declared "Data Scientist: The Sexiest Job of the 21st Century", a catchphrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe. A decade later, they reaffirmed it, stating that "the job is more in demand than ever with employers".\nThe modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name. "Data science" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched the Data Science Journal. In 2003, Columbia University launched The Journal of Data Science. In 2014, the American Statistical Association\'s Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.\nThe professional title of "data scientist" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report "Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century", it referred broadly to any key role in managing a digital data collection.\nThere is still no consensus on the definition of data science, and it is considered by some to be a buzzword. Big data is a related marketing term. Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.\n\n\n== Data science and data analysis ==\n\nData science and data analysis are both important disciplines in the field of data management and analysis, but they differ in several key ways. While both fields involve working with data, data science is more of an interdisciplinary field that involves the application of statistical, computational, and machine learning methods to extract insights from data and make predictions, while data analysis is more focused on the examination and interpretation of data to identify patterns and trends.\nData analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning, data visualization, and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data. For example, a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies.\nData science, on the other hand, is a more complex and iterative process that involves working with larger, more complex datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data-driven decisions. In addition to statistical analysis, data science often involves tasks such as data preprocessing, feature engineering, and model selection. For instance, a data scientist might develop a recommendation system for an e-commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences.\nWhile data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions. Data scientists are often responsible for collecting and cleaning data, selecting appropriate analytical techniques, and deploying models in real-world scenarios. They work at the intersection of mathematics, computer science, and domain expertise to solve complex problems and uncover hidden patterns in large datasets.\nDespite these differences, data science and data analysis are closely related fields and often require similar skill sets. Both fields require a solid foundation in statistics, programming, and data visualization, as well as the ability to communicate findings effectively to both technical and non-technical audiences. Both fields benefit from critical thinking and domain knowledge, as understanding the context and nuances of the data is essential for accurate analysis and modeling.\nIn summary, data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis. Data analysis focuses on extracting insights and drawing conclusions from structured data, while data science involves a more comprehensive approach that combines statistical analysis, computational methods, and machine learning to extract insights, build predictive models, and drive data-driven decision-making. Both fields use data to understand patterns, make informed decisions, and solve complex problems across various domains.\n\n\n== Data Science as an Academic Discipline ==\nAs illustrated in the previous sections, there is substantially some considerable differences between data science, data analysis and statistics. Consequently, just like statistics grew into an independent field from applied mathematics, similarly data science has emerged as a independent field and has gained traction over the recent years. The unique demand for professional skills on computerized data analysis skills has exploded due to the increasing amounts of data emanating from a variety of independent sources. Whereas some of these highly sought skills can be provided by statisticians, the lack of high algorithmic writing skills makes them less preferred than trained data scientists who provide unique expertise on skills such as NoSQL, Apache Hadoop, Cloud Computing platforms and use of complex networks. This paradigm shift has seen various institution craft academic programmes to prepare skilled labor for the market. Some of the institutions offering degree programmes in data science include Stanford University, Harvard University, University of Oxford, ETH Zurich, Meru University[1] among many others.\n\n\n== Cloud computing for data science ==\n\nCloud computing can offer access to large amounts of computational power and storage. In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.\nSome distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reducing processing times.\n\n\n== Ethical consideration in data science ==\nData science involve collecting, processing, and analyzing data which often including personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts \nMachine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.\n\n\n== See also ==\nPython (programming language)\nR (programming language)\nData engineering\nBig data\nMachine learning\n\n\n== References ==',
  },
  {
    title: "Relativity",
    originalContent:
      "Relativity may refer to:\n\n\n== Physics ==\nGalilean relativity, Galileo's conception of relativity\nNumerical relativity, a subfield of computational physics that aims to establish numerical solutions to Einstein's field equations in general relativity\nPrinciple of relativity, used in Einstein's theories and derived from Galileo's principle\nTheory of relativity, a general treatment that refers to both special relativity and general relativity\nGeneral relativity, Albert Einstein's theory of gravitation\nSpecial relativity, a theory formulated by Albert Einstein, Henri Poincaré, and Hendrik Lorentz\nRelativity: The Special and the General Theory, a 1920 book by Albert Einstein\n\n\n== Social sciences ==\nLinguistic relativity\nCultural relativity\nMoral relativity\n\n\n== Arts and entertainment ==\n\n\n=== Music ===\nRelativity Music Group, a Universal subsidiary record label for releasing film soundtracks\nRelativity Records, an American record label\nRelativity (band), a Scots-Irish traditional music quartet 1985–1987\nRelativity (Emarosa album), 2008\nRelativity (Indecent Obsession album), 1993\nRelativity (Walt Dickerson album) or the title song, 1962\nRelativity, an EP by Grafton Primary, 2007\n\n\n=== Television ===\nRelativity (TV series), a 1996–1997 American drama series\n\"Relativity\" (Farscape), an episode\n\"Relativity\" (Star Trek: Voyager), an episode\n\n\n=== Other ===\nRelativity (M. C. Escher), a 1953 lithograph print by M. C. Escher\nRelativity Media, an American film production company\n\n\n== Business ==\nRelativity Space, an American aerospace manufacturing company\n\n\n== See also ==\n\nRelative (disambiguation)\nRelativism, a family of philosophical, religious, and social views",
  },
  {
    title: "RNA",
    originalContent:
      "Ribonucleic acid (RNA) is a polymeric molecule that is essential for most biological functions, either by performing the function itself (non-coding RNA) or by forming a template for the production of proteins (messenger RNA). RNA and deoxyribonucleic acid (DNA) are nucleic acids. The nucleic acids constitute one of the four major macromolecules essential for all known forms of life. RNA is assembled as a chain of nucleotides. Cellular organisms use messenger RNA (mRNA) to convey genetic information (using the nitrogenous bases of guanine, uracil, adenine, and cytosine, denoted by the letters G, U, A, and C) that directs synthesis of specific proteins. Many viruses encode their genetic information using an RNA genome.\nSome RNA molecules play an active role within cells by catalyzing biological reactions, controlling gene expression, or sensing and communicating responses to cellular signals. One of these active processes is protein synthesis, a universal function in which RNA molecules direct the synthesis of proteins on ribosomes. This process uses transfer RNA (tRNA) molecules to deliver amino acids to the ribosome, where ribosomal RNA (rRNA) then links amino acids together to form coded proteins.\nIt has become widely accepted in science that early in the history of life on Earth, prior to the evolution of DNA and possibly of protein-based enzymes as well, an \"RNA world\" existed in which RNA served as both living organisms' storage method for genetic information—a role fulfilled today by DNA, except in the case of RNA viruses—and potentially performed catalytic functions in cells—a function performed today by protein enzymes, with the notable and important exception of the ribosome, which is a ribozyme.\n\n\n== Chemical structure of RNA ==\n\n\n=== Basic chemical composition ===\n\nEach nucleotide in RNA contains a ribose sugar, with carbons numbered 1' through 5'. A base is attached to the 1' position, in general, adenine (A), cytosine (C), guanine (G), or uracil (U). Adenine and guanine are purines, and cytosine and uracil are pyrimidines. A phosphate group is attached to the 3' position of one ribose and the 5' position of the next. The phosphate groups have a negative charge each, making RNA a charged molecule (polyanion). The bases form hydrogen bonds between cytosine and guanine, between adenine and uracil and between guanine and uracil. However, other interactions are possible, such as a group of adenine bases binding to each other in a bulge,\nor the GNRA tetraloop that has a guanine–adenine base-pair.\n\n\n=== Differences between DNA and RNA ===\n\nLike DNA, most biologically active RNAs, including mRNA, tRNA, rRNA, snRNAs, and other non-coding RNAs, contain self-complementary sequences that allow parts of the RNA to fold and pair with itself to form double helices. Analysis of these RNAs has revealed that they are highly structured. Unlike DNA, their structures do not consist of long double helices, but rather collections of short helices packed together into structures akin to proteins.\nIn this fashion, RNAs can achieve chemical catalysis (like enzymes). For instance, determination of the structure of the ribosome—an RNA-protein complex that catalyzes the assembly of proteins—revealed that its active site is composed entirely of RNA.\n\nAn important structural component of RNA that distinguishes it from DNA is the presence of a hydroxyl group at the 2' position of the ribose sugar. The presence of this functional group causes the helix to mostly take the A-form geometry, although in single strand dinucleotide contexts, RNA can rarely also adopt the B-form most commonly observed in DNA.  The A-form geometry results in a very deep and narrow major groove and a shallow and wide minor groove. A second consequence of the presence of the 2'-hydroxyl group is that in conformationally flexible regions of an RNA molecule (that is, not involved in formation of a double helix), it can chemically attack the adjacent phosphodiester bond to cleave the backbone.\n\n\n=== Secondary and tertiary structures ===\nThe functional form of single-stranded RNA molecules, just like proteins, frequently requires a specific spatial tertiary structure. The scaffold for this structure is provided by secondary structural elements that are hydrogen bonds within the molecule. This leads to several recognizable \"domains\" of secondary structure like hairpin loops, bulges, and internal loops. In order to create, i.e., design, RNA for any given secondary structure, two or three bases would not be enough, but four bases are enough. This is likely why nature has \"chosen\" a four base alphabet: fewer than four would not allow the creation of all structures, while more than four bases are not necessary to do so. Since RNA is charged, metal ions such as Mg2+ are needed to stabilise many secondary and tertiary structures.\nThe naturally occurring enantiomer of RNA is D-RNA composed of D-ribonucleotides. All chirality centers are located in the D-ribose. By the use of L-ribose or rather L-ribonucleotides, L-RNA can be synthesized. L-RNA is much more stable against degradation by RNase.\nLike other structured biopolymers such as proteins, one can define topology of a folded RNA molecule. This is often done based on arrangement of intra-chain contacts within a folded RNA, termed as circuit topology.\n\n\n=== Chemical modifications ===\n\nRNA is transcribed with only four bases (adenine, cytosine, guanine and uracil), but these bases and attached sugars can be modified in numerous ways as the RNAs mature. Pseudouridine (Ψ), in which the linkage between uracil and ribose is changed from a C–N bond to a C–C bond, and ribothymidine (T) are found in various places (the most notable ones being in the TΨC loop of tRNA). Another notable modified base is hypoxanthine, a deaminated adenine base whose nucleoside is called inosine (I). Inosine plays a key role in the wobble hypothesis of the genetic code.\nThere are more than 100 other naturally occurring modified nucleosides. The greatest structural diversity of modifications can be found in tRNA, while pseudouridine and nucleosides with 2'-O-methylribose often present in rRNA are the most common. The specific roles of many of these modifications in RNA are not fully understood. However, it is notable that, in ribosomal RNA, many of the post-transcriptional modifications occur in highly functional regions, such as the peptidyl transferase center and the subunit interface, implying that they are important for normal function.\n\n\n== Types of RNA ==\n\nMessenger RNA (mRNA) is the type of RNA that carries information from DNA to the ribosome, the sites of protein synthesis (translation) in the cell cytoplasm. The coding sequence of the mRNA determines the amino acid sequence in the protein that is produced. However, many RNAs do not code for protein (about 97% of the transcriptional output is non-protein-coding in eukaryotes).\nThese so-called non-coding RNAs (\"ncRNA\") can be encoded by their own genes (RNA genes), but can also derive from mRNA introns. The most prominent examples of non-coding RNAs are transfer RNA (tRNA) and ribosomal RNA (rRNA), both of which are involved in the process of translation. There are also non-coding RNAs involved in gene regulation, RNA processing and other roles. Certain RNAs are able to catalyse chemical reactions such as cutting and ligating other RNA molecules, and the catalysis of peptide bond formation in the ribosome; these are known as ribozymes.\nAccording to the length of RNA chain, RNA includes small RNA and long RNA. Usually, small RNAs are shorter than 200 nt in length, and long RNAs are greater than 200 nt long. Long RNAs, also called large RNAs, mainly include long non-coding RNA (lncRNA) and mRNA. Small RNAs mainly include 5.8S ribosomal RNA (rRNA), 5S rRNA, transfer RNA (tRNA), microRNA (miRNA), small interfering RNA (siRNA), small nucleolar RNA (snoRNAs), Piwi-interacting RNA (piRNA), tRNA-derived small RNA (tsRNA) and small rDNA-derived RNA (srRNA).\nThere are certain exceptions as in the case of the 5S rRNA of the members of the genus Halococcus (Archaea), which have an insertion, thus increasing its size.\n\n\n=== RNAs involved in protein synthesis ===\nMessenger RNA (mRNA) carries information about a protein sequence to the ribosomes, the protein synthesis factories in the cell. It is coded so that every three nucleotides (a codon) corresponds to one amino acid. In eukaryotic cells, once precursor mRNA (pre-mRNA) has been transcribed from DNA, it is processed to mature mRNA. This removes its introns—non-coding sections of the pre-mRNA. The mRNA is then exported from the nucleus to the cytoplasm, where it is bound to ribosomes and translated into its corresponding protein form with the help of tRNA. In prokaryotic cells, which do not have nucleus and cytoplasm compartments, mRNA can bind to ribosomes while it is being transcribed from DNA. After a certain amount of time, the message degrades into its component nucleotides with the assistance of ribonucleases.\nTransfer RNA (tRNA) is a small RNA chain of about 80 nucleotides that transfers a specific amino acid to a growing polypeptide chain at the ribosomal site of protein synthesis during translation. It has sites for amino acid attachment and an anticodon region for codon recognition that binds to a specific sequence on the messenger RNA chain through hydrogen bonding.\n\nRibosomal RNA (rRNA) is the catalytic component of the ribosomes. The rRNA is the component of the ribosome that hosts translation. Eukaryotic ribosomes contain four different rRNA molecules: 18S, 5.8S, 28S and 5S rRNA. Three of the rRNA molecules are synthesized in the nucleolus, and one is synthesized elsewhere. In the cytoplasm, ribosomal RNA and protein combine to form a nucleoprotein called a ribosome. The ribosome binds mRNA and carries out protein synthesis. Several ribosomes may be attached to a single mRNA at any time. Nearly all the RNA found in a typical eukaryotic cell is rRNA.\nTransfer-messenger RNA (tmRNA) is found in many bacteria and plastids. It tags proteins encoded by mRNAs that lack stop codons for degradation and prevents the ribosome from stalling.\n\n\n=== Regulatory RNA ===\nThe earliest known regulators of gene expression were proteins known as repressors and activators – regulators with specific short binding sites within enhancer regions near the genes to be regulated.  Later studies have shown that RNAs also regulate genes. There are several kinds of RNA-dependent processes in eukaryotes regulating the expression of genes at various points, such as RNAi repressing genes post-transcriptionally,  long non-coding RNAs shutting down blocks of chromatin epigenetically, and enhancer RNAs inducing increased gene expression. Bacteria and archaea  have also been shown to use regulatory RNA systems such as bacterial small RNAs and CRISPR.  Fire and Mello were awarded the 2006 Nobel Prize in Physiology or Medicine for discovering microRNAs  (miRNAs), specific short RNA molecules that can base-pair with mRNAs.\n\n\n==== MicroRNA (miRNA) and small interfering RNA (siRNA) ====\n\nPost-transcriptional expression levels of many genes can be controlled by RNA interference, in which miRNAs, specific short RNA molecules, pair with mRNA regions and target them for degradation.  This antisense-based process involves steps that first process the RNA so that it can base-pair with a region of its target mRNAs. Once the base pairing occurs, other proteins direct the mRNA to be destroyed by nucleases.\n\n\n==== Long non-coding RNAs ====\n\nNext to be linked to regulation were Xist and other long noncoding RNAs associated with X chromosome inactivation.  Their roles, at first mysterious, were shown by Jeannie T. Lee and others to be the silencing of blocks of chromatin via recruitment of Polycomb complex so that messenger RNA could not be transcribed from them. Additional lncRNAs, currently defined as RNAs of more than 200 base pairs that do not appear to have coding potential, have been found associated with regulation of stem cell pluripotency and cell division.\n\n\n==== Enhancer RNAs ====\n\nThe third major group of regulatory RNAs is called enhancer RNAs.  It is not clear at present whether they are a unique category of RNAs of various lengths or constitute a distinct subset of lncRNAs.  In any case, they are transcribed from enhancers, which are known regulatory sites in the DNA near genes they regulate.  They up-regulate the transcription of the gene(s) under control of the enhancer from which they are transcribed.\n\n\n=== Small RNA in prokaryotes ===\n\n\n==== Small RNA ====\nAt first, regulatory RNA was thought to be a eukaryotic phenomenon, a part of the explanation for why so much more transcription in higher organisms was seen than had been predicted. But as soon as researchers began to look for possible RNA regulators in bacteria, they turned up there as well, termed as small RNA (sRNA).  Currently, the ubiquitous nature of systems of RNA regulation of genes has been discussed as support for the RNA World theory. There are indications that the enterobacterial sRNAs are involved in various cellular processes and seem to have significant role in stress responses such as membrane stress, starvation stress, phosphosugar stress and DNA damage. Also, it has been suggested that sRNAs have been evolved to have important role in stress responses because of their kinetic properties that allow for rapid response and stabilisation of the physiological state. Bacterial small RNAs generally act via antisense pairing with mRNA to down-regulate its translation, either by affecting stability or affecting cis-binding ability. Riboswitches have also been discovered. They are cis-acting regulatory RNA sequences acting allosterically.  They change shape when they bind metabolites so that they gain or lose the ability to bind chromatin to regulate expression of genes.\n\n\n==== CRISPR RNA ====\nArchaea also have systems of regulatory RNA.  The CRISPR system, recently being used to edit DNA in situ, acts via regulatory RNAs in archaea and bacteria to provide protection against virus invaders.\n\n\n== RNA synthesis and processing ==\n\n\n=== Synthesis ===\nSynthesis of RNA typically occurs in the cell nucleus and is usually catalyzed by an enzyme—RNA polymerase—using DNA as a template, a process known as transcription. Initiation of transcription begins with the binding of the enzyme to a promoter sequence in the DNA (usually found \"upstream\" of a gene). The DNA double helix is unwound by the helicase activity of the enzyme. The enzyme then progresses along the template strand in the 3’ to 5’ direction, synthesizing a complementary RNA molecule with elongation occurring in the 5’ to 3’ direction. The DNA sequence also dictates where termination of RNA synthesis will occur.\nPrimary transcript RNAs are often modified by enzymes after transcription. For example, a poly(A) tail and a 5' cap are added to eukaryotic pre-mRNA and introns are removed by the spliceosome.\nThere are also a number of RNA-dependent RNA polymerases that use RNA as their template for synthesis of a new strand of RNA. For instance, a number of RNA viruses (such as poliovirus) use this type of enzyme to replicate their genetic material. Also, RNA-dependent RNA polymerase is part of the RNA interference pathway in many organisms.\n\n\n=== RNA processing ===\n\nMany RNAs are involved in modifying other RNAs.\nIntrons are spliced out of pre-mRNA by spliceosomes, which contain several small nuclear RNAs (snRNA), or the introns can be ribozymes that are spliced by themselves.\nRNA can also be altered by having its nucleotides modified to  nucleotides other than A, C, G and U.\nIn eukaryotes, modifications of RNA nucleotides are in general directed by small nucleolar RNAs (snoRNA; 60–300 nt), found in the nucleolus and cajal bodies. snoRNAs associate with enzymes and guide them to a spot on an RNA by basepairing to that RNA. These enzymes then perform the nucleotide modification. rRNAs and tRNAs are extensively modified, but snRNAs and mRNAs can also be the target of base modification. RNA can also be methylated.\n\n\n== RNA in genetics ==\n\n\n=== RNA genomes ===\nLike DNA, RNA can carry genetic information. RNA viruses have genomes composed of RNA that encodes a number of proteins. The viral genome is replicated by some of those proteins, while other proteins protect the genome as the virus particle moves to a new host cell. Viroids are another group of pathogens, but they consist only of RNA, do not encode any protein and are replicated by a host plant cell's polymerase.\n\n\n=== Reverse transcription ===\nReverse transcribing viruses replicate their genomes by reverse transcribing DNA copies from their RNA; these DNA copies are then transcribed to new RNA. Retrotransposons also spread by copying DNA and RNA from one another, and telomerase contains an RNA that is used as template for building the ends of eukaryotic chromosomes.\n\n\n=== Double-stranded RNA ===\n\nDouble-stranded RNA (dsRNA) is RNA with two complementary strands, similar to the DNA found in all cells, but with the replacement of thymine by uracil and the adding of one oxygen atom. dsRNA forms the genetic material of some viruses (double-stranded RNA viruses). Double-stranded RNA, such as viral RNA or siRNA, can trigger RNA interference in eukaryotes, as well as interferon response in vertebrates. In eukaryotes, double-stranded RNA (dsRNA) plays a role in the activation of the innate immune system against viral infections.\n\n\n=== Circular RNA ===\n\nIn the late 1970s, it was shown that there is a single stranded covalently closed, i.e. circular form of RNA expressed throughout the animal and plant kingdom (see circRNA). circRNAs are thought to arise via a \"back-splice\" reaction where the spliceosome joins a upstream 3' acceptor to a downstream 5' donor splice site. So far the function of circRNAs is largely unknown, although for few examples a microRNA sponging activity has been demonstrated.\n\n\n== Key discoveries in RNA biology ==\n\nResearch on RNA has led to many important biological discoveries and numerous Nobel Prizes. Nucleic acids were discovered in 1868 by Friedrich Miescher, who called the material 'nuclein' since it was found in the nucleus. It was later discovered that prokaryotic cells, which do not have a nucleus, also contain nucleic acids. The role of RNA in protein synthesis was suspected already in 1939. Severo Ochoa won the 1959 Nobel Prize in Medicine (shared with Arthur Kornberg) after he discovered an enzyme that can synthesize RNA in the laboratory. However, the enzyme discovered by Ochoa (polynucleotide phosphorylase) was later shown to be responsible for RNA degradation, not RNA synthesis. In 1956 Alex Rich and David Davies hybridized two separate strands of RNA to form the first crystal of RNA whose structure could be determined by X-ray crystallography.\nThe sequence of the 77 nucleotides of a yeast tRNA was found by Robert W. Holley in 1965, winning Holley the 1968 Nobel Prize in Medicine (shared with Har Gobind Khorana and Marshall Nirenberg).\nIn the early 1970s, retroviruses and reverse transcriptase were discovered, showing for the first time that enzymes could copy RNA into DNA (the opposite of the usual route for transmission of genetic information). For this work, David Baltimore, Renato Dulbecco and Howard Temin were awarded a Nobel Prize in 1975.\nIn 1976, Walter Fiers and his team determined the first complete nucleotide sequence of an RNA virus genome, that of bacteriophage MS2.\nIn 1977, introns and RNA splicing were discovered in both mammalian viruses and in cellular genes, resulting in a 1993 Nobel to Philip Sharp and Richard Roberts.\nCatalytic RNA molecules (ribozymes) were discovered in the early 1980s, leading to a 1989 Nobel award to Thomas Cech and Sidney Altman. In 1990, it was found in Petunia that introduced genes can silence similar genes of the plant's own, now known to be a result of RNA interference.\nAt about the same time, 22 nt long RNAs, now called microRNAs, were found to have a role in the development of C. elegans.\nStudies on RNA interference earned a Nobel Prize for Andrew Fire and Craig Mello in 2006, and another Nobel for studies on the transcription of RNA to Roger Kornberg in the same year. The discovery of gene regulatory RNAs has led to attempts to develop drugs made of RNA, such as siRNA, to silence genes. Adding to the Nobel prizes for research on RNA, in 2009 it was awarded for the elucidation of the atomic structure of the ribosome to Venki Ramakrishnan, Thomas A. Steitz, and Ada Yonath. In 2023 the Nobel Prize in Physiology or Medicine was awarded to Katalin Karikó and Drew Weissman for their discoveries concerning modified nucleosides that enabled the development of effective mRNA vaccines against COVID-19.\n\n\n=== Relevance for prebiotic chemistry and abiogenesis ===\nIn 1968, Carl Woese hypothesized that RNA might be catalytic and suggested that the earliest forms of life (self-replicating molecules) could have relied on RNA both to carry genetic information and to catalyze biochemical reactions—an RNA world. In May 2022, scientists discovered that RNA can form spontaneously on prebiotic basalt lava glass, presumed to have been abundant on the early Earth.\nIn March 2015, DNA and RNA nucleobases, including uracil, cytosine and thymine, were reportedly formed in the laboratory under outer space conditions, using starter chemicals such as pyrimidine, an organic compound commonly found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), is one of the most carbon-rich compounds found in the universe and may have been formed in red giants or in interstellar dust and gas clouds. In July 2022, astronomers reported massive amounts of prebiotic molecules, including possible RNA precursors, in the galactic center of the Milky Way Galaxy.\n\n\n== Medical applications ==\nRNA, initially deemed unsuitable for therapeutics due to its short half-life, has been made useful through advances in stabilization. Therapeutic applications arise as RNA folds into complex conformations and binds proteins, nucleic acids, and small molecules to form catalytic centers. RNA-based vaccines are thought to be easier to produce than traditional vaccines derived from killed or altered pathogens, because it can take months or years to grow and study a pathogen and determine which molecular parts to extract, inactivate, and use in a vaccine. Small molecules with conventional therapeutic properties can target RNA and DNA structures, thereby treating novel diseases. However, research is scarce on small molecules targeting RNA and approved drugs for human illness. Ribavirin, branaplam, and ataluren are currently available medications that stabilize double-stranded RNA structures and control splicing in a variety of disorders. \nProtein-coding mRNAs have emerged as new therapeutic candidates, with RNA replacement being particularly beneficial for brief but torrential protein expression. In vitro transcribed mRNAs (IVT-mRNA) have been used to deliver proteins for bone regeneration, pluripotency, and heart function in animal models. SiRNAs, short RNA molecules, play a crucial role in innate defense against viruses and chromatin structure. They can be artificially introduced to silence specific genes, making them valuable for gene function studies, therapeutic target validation, and drug development.\nmRNA vaccines have emerged as an important new class of vaccines, using mRNA to manufacture proteins which provoke an immune response. Their first successful large-scale application came in the form of COVID-19 vaccines during the COVID-19 pandemic.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nRNA World website Link collection (structures, sequences, tools, journals)\nNucleic Acid Database Images of DNA, RNA, and complexes.\nAnna Marie Pyle's Seminar: RNA Structure, Function, and Recognition Archived 2018-06-21 at the Wayback Machine",
  },
  {
    title: "Behavior",
    originalContent:
      'Behavior (American English) or behaviour (British English) is the range of actions and mannerisms made by individuals, organisms, systems or artificial entities in some environment. These systems can include other systems or organisms as well as the inanimate physical environment. It is the computed response of the system or organism to various stimuli or inputs, whether internal or external, conscious or subconscious, overt or covert, and voluntary or involuntary. While some behavior is produced in response to an organism\'s environment (extrinsic motivation), behavior can also be the product of intrinsic motivation, also referred to as "agency" or "free will".\nTaking a behavior informatics perspective, a behavior consists of actor, operation, interactions, and their properties. This can be represented as a behavior vector.\n\n\n== Models ==\n\n\n=== Biology ===\n\nAlthough disagreement exists as to how to precisely define behavior in a biological context, one common interpretation based on a meta-analysis of scientific literature states that "behavior is the internally coordinated responses (actions or inactions) of whole living organisms (individuals or groups) to internal or external stimuli".\nA broader definition of behavior, applicable to plants and other organisms, is similar to the concept of phenotypic plasticity. It describes behavior as a response to an event or environment change during the course of the lifetime of an individual, differing from other physiological or biochemical changes that occur more rapidly, and excluding changes that are a result of development (ontogeny).\nBehaviors can be either innate or learned from the environment.\nBehaviour can be regarded as any action of an organism that changes its relationship to its environment. Behavior provides outputs from the organism to the environment.\n\n\n=== Human behavior ===\n\nThe endocrine system and the nervous system likely influence human behavior. Complexity in the behavior of an organism may be correlated to the complexity of its nervous system. Generally, organisms with more complex nervous systems have a greater capacity to learn new responses and thus adjust their behavior.\n\n\n=== Animal behavior ===\n\nEthology is the scientific and objective study of animal behavior, usually with a focus on behavior under natural conditions, and viewing behavior as an evolutionarily adaptive trait. Behaviorism is a term that also describes the scientific and objective study of animal behavior, usually referring to measured responses to stimuli or trained behavioral responses in a laboratory context, without a particular emphasis on evolutionary adaptivity.\n\n\n== Consumer behavior ==\n\n\n=== Consumers behavior ===\nConsumer behavior involves the processes consumers go through, and reactions they have towards products or services. It has to do with consumption, and the processes consumers go through around purchasing and consuming goods and services. Consumers recognize needs or wants, and go through a process to satisfy these needs. Consumer behavior is the process they go through as customers, which includes types of products purchased, amount spent, frequency of purchases and what influences them to make the purchase decision or not.\nCircumstances that influence consumer behaviour are varied, with contributions from both internal and external factors.  Internal factors include attitudes, needs, motives, preferences and perceptual processes, whilst external factors include marketing activities, social and economic factors, and cultural aspects.  Doctor Lars Perner of the University of Southern California claims that there are also physical factors that influence consumer behavior, for example, if a consumer is hungry, then this physical feeling of hunger will influence them so that they go and purchase a sandwich to satisfy the hunger.\n\n\n=== Consumer decision making ===\nLars Perner presents a model that outlines the decision-making process involved in consumer behaviour. The process initiates with the identification of a problem, wherein the consumer acknowledges an unsatisfied need or desire. Subsequently, the consumer proceeds to seek information, whereas for low-involvement products, the search tends to rely on internal resources, retrieving alternatives from memory. Conversely, for high-involvement products, the search is typically more extensive, involving activities like reviewing reports, reading reviews, or seeking recommendations from friends.\nThe consumer will then evaluate his or her alternatives, comparing price, and quality, doing trade-offs between products, and narrowing down the choice by eliminating the less appealing products until there is one left. After this has been identified, the consumer will purchase the product.\nFinally, the consumer will evaluate the purchase decision, and the purchased product, bringing in factors such as value for money, quality of goods, and purchase experience. However, this logical process does not always happen this way, people are emotional and irrational creatures. People make decisions with emotion and then justify them with logic according to Robert Cialdini Ph.D. Psychology.\n\n\n=== How the 4P\'s influence consumer behavior ===\nThe Marketing mix (4 P\'s) are a marketing tool and stand for Price, Promotion, Product, and Placement.\nDue to the significant impact of business-to-consumer marketing on consumer behavior, the four elements of the marketing mix, known as the 4 P\'s (product, price, place, and promotion), exert a notable influence on consumer behavior. The price of a good or service is largely determined by the market, as businesses will set their prices to be similar to that of other businesses so as to remain competitive whilst making a profit.  When market prices for a product are high, it will cause consumers to purchase less and use purchased goods for longer periods of time, meaning they are purchasing the product less often. Alternatively, when market prices for a product are low, consumers are more likely to purchase more of the product, and more often.\nThe way that promotion influences consumer behavior has changed over time. In the past, large promotional campaigns and heavy advertising would convert into sales for a business, but nowadays businesses can have success on products with little or no advertising. This is due to the Internet and in particular social media. They rely on word of mouth from consumers using social media, and as products trend online, so sales increase as products effectively promote themselves.  Thus, promotion by businesses does not necessarily result in consumer behavior trending towards purchasing products.\nThe way that product influences consumer behavior is through consumer willingness to pay, and consumer preferences. This means that even if a company were to have a long history of products in the market, consumers will still pick a cheaper product over the company in question\'s product if it means they will pay less for something that is very similar. This is due to consumer willingness to pay, or their willingness to part with the money they have earned. The product also influences consumer behavior through customer preferences. For example, take Pepsi vs Coca-Cola, a Pepsi-drinker is less likely to purchase Coca-Cola, even if it is cheaper and more convenient. This is due to the preference of the consumer, and no matter how hard the opposing company tries they will not be able to force the customer to change their mind.\nProduct placement in the modern era has little influence on consumer behavior, due to the availability of goods online.  If a customer can purchase a good from the comfort of their home instead of purchasing in-store, then the placement of products is not going to influence their purchase decision.\n\n\n== In management ==\nBehavior outside of psychology includes\n\n\n=== Organizational ===\nIn management, behaviors are associated with desired or undesired focuses. Managers generally note what the desired outcome is, but behavioral patterns can take over. These patterns are the reference to how often the desired behavior actually occurs. Before a behavior actually occurs, antecedents focus on the stimuli that influence the behavior that is about to happen. After the behavior occurs, consequences fall into place. Consequences consist of rewards or punishments.\n\n\n=== Social behavior ===\nSocial behavior is behavior among two or more organisms within the same species, and encompasses any behavior in which one member affects the other. This is due to an interaction among those members. Social behavior can be seen as similar to an exchange of goods, with the expectation that when one gives, one will receive the same. This behavior can be affected by both the qualities of the individual and the environmental (situational) factors. Therefore, social behavior arises as a result of an interaction between the two—the organism and its environment. This means that, in regards to humans, social behavior can be determined by both the individual characteristics of the person, and the situation they are in.\n\n\n== Behavior informatics ==\nBehavior informatics also called behavior computing, explores behavior intelligence and behavior insights from the informatics and computing perspectives.\nDifferent from applied behavior analysis from the psychological perspective, BI builds computational theories, systems and tools to qualitatively and quantitatively model, represent, analyze, and manage behaviors of individuals, groups and/or organizations.\n\n\n== Health ==\n\nHealth behavior refers to a person\'s beliefs and actions regarding their health and well-being. Health behaviors are direct factors in maintaining a healthy lifestyle. Health behaviors are influenced by the social, cultural, and physical environments in which we live. They are shaped by individual choices and external constraints. Positive behaviors help promote health and prevent disease, while the opposite is true for risk behaviors. Health behaviors are early indicators of population health. Because of the time lag that often occurs between certain behaviors and the development of disease, these indicators may foreshadow the future burdens and benefits of health-risk and health-promoting behaviors.\n\n\n=== Correlates ===\nA variety of studies have examined the relationship between health behaviors and health outcomes (e.g., Blaxter 1990) and have demonstrated their role in both morbidity and mortality.\nThese studies have identified seven features of lifestyle which were associated with lower morbidity and higher subsequent long-term survival (Belloc and Breslow 1972):\n\nAvoiding snacks\nEating breakfast regularly\nExercising regularly\nMaintaining a desirable body weight\nModerate alcohol intake\nNot smoking\nSleeping 7–8hrs per night\nHealth behaviors impact upon individuals\' quality of life, by delaying the onset of chronic disease and extending active lifespan. Smoking, alcohol consumption, diet, gaps in primary care services and low screening uptake are all significant determinants of poor health, and changing such behaviors should lead to improved health.\nFor example, in US, Healthy People 2000, United States Department of Health and Human Services, lists increased physical activity, changes in nutrition and reductions in tobacco, alcohol and drug use as important for health promotion and disease prevention.\n\n\n=== Treatment approach ===\nAny interventions done are matched with the needs of each individual in an ethical and respected manner. Health belief model encourages increasing individuals\' perceived susceptibility to negative health outcomes and making individuals aware of the severity of such negative health behavior outcomes. E.g. through health promotion messages. In addition, the health belief model suggests the need to focus on the benefits of health behaviors and the fact that barriers to action are easily overcome. The theory of planned behavior suggests using persuasive messages for tackling behavioral beliefs to increase the readiness to perform a behavior, called intentions. The theory of planned behavior advocates the need to tackle normative beliefs and control beliefs in any attempt to change behavior. Challenging the normative beliefs is not enough but to follow through the intention with self-efficacy from individual\'s mastery in problem solving and task completion is important to bring about a positive change. Self efficacy is often cemented through standard persuasive techniques.\n\n\n== See also ==\n\n\n== References ==\n\nGeneral\nCao, L. (2014). Behavior Informatics: A New Perspective. IEEE Intelligent Systems (Trends and Controversies), 29(4): 62–80.\nClemons, E. K. (2008). "How Information Changes Consumer Behavior and How Consumer Behavior Determines Corporate Strategy". Journal of Management Information Systems. 25 (2): 13–40. doi:10.2753/mis0742-1222250202. S2CID 16370526.\nDowhan, D (2013). "Hitting Your Target". Marketing Insights. 35 (2): 32–38.\nPerner, L. (2008), Consumer behavior. University of Southern California, Marshall School of Business. Retrieved from http://www.consumerpsychologist.com/intro_Consumer_Behavior.html\nSzwacka-Mokrzycka, J (2015). "TRENDS IN CONSUMER behavior CHANGES. OVERVIEW OF CONCEPTS". Acta Scientiarum Polonorum. Oeconomia. 14 (3): 149–156.\n\n\n== Further reading ==\nBateson, P. (2017) behavior, Development and Evolution. Open Book Publishers, Cambridge. ISBN 978-1-78374-250-9.\nPlomin, Robert; DeFries, John C.; Knopik, Valerie S.; Neiderhiser, Jenae M. (24 September 2012). Behavioral Genetics. Shaun Purcell (Appendix: Statistical Methods in Behavioral Genetics). Worth Publishers. ISBN 978-1-4292-4215-8. Retrieved 4 September 2013.\nFlint, Jonathan; Greenspan, Ralph J.; Kendler, Kenneth S. (28 January 2010). How Genes Influence Behavior. Oxford University Press. ISBN 978-0-19-955990-9.\n\n\n== External links ==\n\nWhat is behavior? Baby don\'t ask me, don\'t ask me, no more at Earthling Nature.\nbehaviorinformatics.org\nLinks to review articles by Eric Turkheimer and co-authors on behavior research\nLinks to IJCAI2013 tutorial on behavior informatics and computing',
  },
  {
    title: "Neural network",
    originalContent:
      'A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural network.\n\nIn neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses.\nIn machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems.\n\n\n== In biology ==\n\nIn the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses.\nEach neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead.\nPopulations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks, and many of these together form brains and nervous systems.\nSignals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, where they cause contraction and thereby motion.\n\n\n== In machine learning ==\n\nIn machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software.\nNeurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the hidden layers) to the final layer (the output layer).\nThe "signal" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.\nNeural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI.\n\n\n== History ==\n\nThe theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.\nArtificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957,\nartificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts.\n\n\n== See also ==\nEmergence\nBiological cybernetics\nBiologically-inspired computing\n\n\n== References ==',
  },
  {
    title: "Decision tree",
    originalContent:
      'A decision tree  is a decision support recursive partitioning structure that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\nDecision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.\n\n\n== Overview ==\nA decision tree is a flowchart-like structure in which each internal node represents a "test" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.\nIn decision analysis, a decision tree and the closely related influence diagram are used as a visual and analytical decision support tool, where the expected values (or expected utility) of competing alternatives are calculated.\nA decision tree consists of three types of nodes:\n\nDecision nodes – typically represented by squares\nChance nodes – typically represented by circles\nEnd nodes – typically represented by triangles\nDecision trees are commonly used in operations research and operations management. If, in practice, decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm. Another use of decision trees is as a descriptive means for calculating conditional probabilities.\nDecision trees, influence diagrams, utility functions, and other decision analysis tools and methods are taught to undergraduate students in schools of business, health economics, and public health, and are examples of operations research or management science methods. These tools are also used to predict decisions of householders in normal and emergency scenarios.\n\n\n== Decision-tree building blocks ==\n\n\n=== Decision-tree elements ===\n\nDrawn from left to right, a decision tree has only burst nodes (splitting paths) but no sink nodes (converging paths). So used manually they can grow very big and are then often hard to draw fully by hand. Traditionally, decision trees have been created manually – as the aside example shows – although increasingly, specialized software is employed.\n\n\n=== Decision rules ===\nThe decision tree can be linearized into decision rules, where the outcome is the contents of the leaf node, and the conditions along the path form a conjunction in the if clause. In general, the rules have the form:\n\nif condition1 and condition2 and condition3 then outcome.\nDecision rules can be generated by constructing association rules with the target variable on the right. They can also denote temporal or causal relations.\n\n\n=== Decision tree using flowchart symbols ===\nCommonly a decision tree is drawn using flowchart symbols as it is easier for many to read and understand. Note there is a conceptual error in the "Proceed" calculation of the tree shown below; the error relates to the calculation of "costs" awarded in a legal action.\n\n\n=== Analysis example ===\nAnalysis can take into account the decision maker\'s (e.g., the company\'s) preference or utility function, for example:\n\nThe basic interpretation in this situation is that the company prefers B\'s risk and payoffs under realistic risk preference coefficients (greater than $400K—in that range of risk aversion, the company would need to model a third strategy, "Neither A nor B").\nAnother example, commonly used in operations research courses, is the distribution of lifeguards on beaches (a.k.a. the "Life\'s a Beach" example). The example describes two beaches with lifeguards to be distributed on each beach. There is maximum budget B that can be distributed among the two beaches (in total), and using a marginal returns table, analysts can decide how many lifeguards to allocate to each beach.\n\nIn this example, a decision tree can be drawn to illustrate the principles of diminishing returns on beach #1.\n\nThe decision tree illustrates that when sequentially distributing lifeguards, placing a first lifeguard on beach #1 would be optimal if there is only the budget for 1 lifeguard. But if there is a budget for two guards, then placing both on beach #2 would prevent more overall drownings.\n\n\n=== Influence diagram ===\nMuch of the information in a decision tree can be represented more compactly as an influence diagram, focusing attention on the issues and relationships between events.\n\n\n== Association rule induction ==\n\nDecision trees can also be seen as generative models of induction rules from empirical data. An optimal decision tree is then defined as a tree that accounts for most of the data, while minimizing the number of levels (or "questions"). Several algorithms to generate such optimal trees have been devised, such as ID3/4/5, CLS, ASSISTANT, and CART.\n\n\n== Advantages and disadvantages ==\nAmong decision support tools, decision trees (and influence diagrams) have several advantages. Decision trees:\n\nAre simple to understand and interpret. People are able to understand decision tree models after a brief explanation.\nHave value even with little hard data. Important insights can be generated based on experts describing a situation (its alternatives, probabilities, and costs) and their preferences for outcomes.\nHelp determine worst, best, and expected values for different scenarios.\nUse a white box model. If a given result is provided by a model.\nCan be combined with other decision techniques.\nThe action of more than one decision-maker can be considered.\nDisadvantages of decision trees:\n\nThey are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree.\nThey are often relatively inaccurate.  Many other predictors perform better with similar data.  This can be remedied by replacing a single decision tree with a random forest of decision trees, but a random forest is not as easy to interpret as a single decision tree.\nFor data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of those attributes with more levels.\nCalculations can get very complex, particularly if many values are uncertain and/or if many outcomes are linked.\n\n\n== Optimizing a decision tree ==\nA few things should be considered when improving the accuracy of the decision tree classifier. The following are some possible optimizations to consider when looking to make sure the decision tree model produced makes the correct decision or classification. Note that these things are not the only things to consider but only some.\nIncreasing the number of levels of the tree\nThe accuracy of the decision tree can change based on the depth of the decision tree. In many cases, the tree’s leaves are pure nodes. When a node is pure, it means that all the data in that node belongs to a single class. For example, if the classes in the data set are Cancer and Non-Cancer a leaf node would be considered pure when all the sample data in a leaf node is part of only one class, either cancer or non-cancer. It is important to note that a deeper tree is not always better when optimizing the decision tree. A deeper tree can influence the runtime in a negative way. If a certain classification algorithm is being used, then a deeper tree could mean the runtime of this classification algorithm is significantly slower. There is also the possibility that the actual algorithm building the decision tree will get significantly slower as the tree gets deeper. If the tree-building algorithm being used splits pure nodes, then a decrease in the overall accuracy of the tree classifier could be experienced. Occasionally, going deeper in the tree can cause an accuracy decrease in general, so it is very important to test modifying the depth of the decision tree and selecting the depth that produces the best results. To summarize, observe the points below, we will define the number D as the depth of the tree.\nPossible advantages of increasing the number D:\n\nAccuracy of the decision-tree classification model increases.\nPossible disadvantages of increasing D\n\n Runtime issues\nDecrease in accuracy in general\nPure node splits while going deeper can cause issues.\nThe ability to test the differences in classification results when changing D is imperative. We must be able to easily change and test the variables that could affect the accuracy and reliability of the decision tree-model.\nThe choice of node-splitting functions\nThe node splitting function used can have an impact on improving the accuracy of the decision tree. For example, using the information-gain function may yield better results than using the phi function. The phi function is known as a measure of “goodness” of a candidate split  at a node in the decision tree. The information gain function is known as a measure of the “reduction in entropy”. In the following, we will build two decision trees. One decision tree will be built using the phi function to split the nodes and one decision tree will be built using the information gain function to split the nodes.\nThe main advantages and disadvantages of information gain and  phi function\n\nOne major drawback of information gain is that the feature that is chosen as the next node in the tree tends to have more unique values.\nAn advantage of information gain is that it tends to choose the most impactful features that are close to the root of the tree. It is a very good measure for deciding the relevance of some features.\nThe phi function is also a good measure for deciding the relevance of some features based on "goodness".\nThis is the information gain function formula. The formula states the information gain is a function of the entropy of a node of the decision tree minus the entropy of a candidate split at node t of a decision tree.\n\n  \n    \n      \n        \n          I\n          \n            \n              gain\n            \n          \n        \n        (\n        s\n        )\n        =\n        H\n        (\n        t\n        )\n        −\n        H\n        (\n        s\n        ,\n        t\n        )\n      \n    \n    {\\displaystyle I_{\\textrm {gain}}(s)=H(t)-H(s,t)}\n  \n\nThis is the phi function formula. The phi function is maximized when the chosen feature splits the samples in a way that produces homogenous splits and have around the same number of samples in each split.\n\n  \n    \n      \n        Φ\n        (\n        s\n        ,\n        t\n        )\n        =\n        (\n        2\n        ∗\n        \n          P\n          \n            L\n          \n        \n        ∗\n        \n          P\n          \n            R\n          \n        \n        )\n        ∗\n        Q\n        (\n        s\n        \n          |\n        \n        t\n        )\n      \n    \n    {\\displaystyle \\Phi (s,t)=(2*P_{L}*P_{R})*Q(s|t)}\n  \n\nWe will set D, which is the depth of the decision tree we are building, to three (D = 3). We also have the following data set of cancer and non-cancer samples and the mutation features that the samples either have or do not have. If a sample has a feature mutation then the sample is positive for that mutation, and it will be represented by one. If a sample does not have a feature mutation then the sample is negative for that mutation, and it will be represented by zero.\nTo summarize, C stands for cancer and NC stands for non-cancer. The letter M stands for mutation, and if a sample has a particular mutation it will show up in the table as a one and otherwise zero.\n\nNow, we can use the formulas to calculate the phi function values and information gain values for each M in the dataset. Once all the values are calculated the tree can be produced. The first thing to be done is to select the root node. In information gain and the phi function we consider the optimal split to be the mutation that produces the highest value for information gain or the phi function. Now assume that M1  has the highest phi function value and M4 has the highest information gain value. The M1 mutation will be the root of our phi function tree and M4 will be the root of our information gain tree. You can observe the root nodes below \n\nNow, once we have chosen the root node we can split the samples into two groups based on whether a sample is positive or negative for the root node mutation. The groups will be called group A and group B. For example, if we use M1 to split the samples in the root node we get NC2 and C2 samples in group A and the rest of the samples NC4, NC3, NC1, C1 in group B.\nDisregarding the mutation chosen for the root node, proceed to place the next best features that have the highest values for information gain or the phi function in the left or right child nodes of the decision tree. Once we choose the root node and the two child nodes for the tree of depth = 3 we can just add the leaves. The leaves will represent the final classification decision the model has produced based on the mutations a sample either has or does not have. The left tree is the decision tree we obtain from using information gain to split the nodes and the right tree is what we obtain from using the phi function to split the nodes.\n\nNow assume the classification results from both trees are given using a confusion matrix.\nInformation gain confusion matrix:\n\nPhi function confusion matrix:\n\nThe tree using information gain has  the same results when using the phi function when calculating the accuracy. When we classify the samples based on the model using information gain we get one true positive, one false positive, zero false negatives, and four true negatives. For the model using the phi function we get two true positives, zero false positives, one false negative, and three true negatives. The next step is to evaluate the effectiveness of the decision tree using some key metrics that will be discussed in the evaluating a decision tree section below. The metrics that will be discussed below can help determine the next steps to be taken when optimizing the decision tree.\nOther techniques\nThe above information is not where it ends for building and optimizing a decision tree. There are many techniques for improving the decision tree classification models we build. One of the techniques is making our decision tree model from a bootstrapped dataset. The bootstrapped dataset helps remove the bias that occurs when building a decision tree model with the same data the model is tested with. The ability to leverage the power of random forests can also help significantly improve the overall accuracy of the model being built. This method generates many decisions from many decision trees and tallies up the votes from each decision tree to make the final classification. There are many techniques, but the main objective is to test building your decision tree model in different ways to make sure it reaches the highest performance level possible.\n\n\n== Evaluating a decision tree ==\nIt is important to know the measurements used to evaluate decision trees. The main metrics used are accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate. All these measurements are derived from the number of true positives, false positives, True negatives, and false negatives obtained when running a set of samples through the decision tree classification model. Also, a confusion matrix can be made to display these results. All these main metrics tell something different about the strengths and weaknesses of the classification model built based on your decision tree. For example, a low sensitivity with high specificity could indicate the classification model built from the decision tree does not do well identifying cancer samples over non-cancer samples.\nLet us take the confusion matrix below. The confusion matrix shows us the decision tree model classifier built gave 11 true positives, 1 false positive, 45 false negatives, and 105 true negatives.\n\nWe will now calculate the values accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate.\nAccuracy:\n\n  \n    \n      \n        A\n        c\n        c\n        u\n        r\n        a\n        c\n        y\n        =\n        (\n        T\n        P\n        +\n        T\n        N\n        )\n        \n          /\n        \n        (\n        T\n        P\n        +\n        T\n        N\n        +\n        F\n        P\n        +\n        F\n        N\n        )\n      \n    \n    {\\displaystyle Accuracy=(TP+TN)/(TP+TN+FP+FN)}\n  \n\n  \n    \n      \n        (\n        11\n        +\n        105\n        )\n        ÷\n        162\n        =\n        71.60\n        %\n      \n    \n    {\\displaystyle (11+105)\\div 162=71.60\\%}\n  \n\nSensitivity (TPR – true positive rate):\n\n  \n    \n      \n        T\n        P\n        R\n        =\n        T\n        P\n        \n          /\n        \n        (\n        T\n        P\n        +\n        F\n        N\n        )\n      \n    \n    {\\displaystyle TPR=TP/(TP+FN)}\n  \n\n  \n    \n      \n        (\n        11\n        )\n        ÷\n        (\n        11\n        +\n        45\n        )\n        =\n        19.64\n        %\n      \n    \n    {\\displaystyle (11)\\div (11+45)=19.64\\%}\n  \n\nSpecificity (TNR – true negative rate):\n\n  \n    \n      \n        T\n        N\n        R\n        =\n        T\n        N\n        \n          /\n        \n        (\n        T\n        N\n        +\n        F\n        P\n        )\n      \n    \n    {\\displaystyle TNR=TN/(TN+FP)}\n  \n\n  \n    \n      \n        105\n        ÷\n        (\n        105\n        +\n        1\n        )\n        =\n        99.06\n        %\n      \n    \n    {\\displaystyle 105\\div (105+1)=99.06\\%}\n  \n\nPrecision (PPV – positive predictive value):\n\n  \n    \n      \n        P\n        P\n        V\n        =\n        T\n        P\n        \n          /\n        \n        (\n        T\n        P\n        +\n        F\n        P\n        )\n      \n    \n    {\\displaystyle PPV=TP/(TP+FP)}\n  \n\n  \n    \n      \n        11\n        \n          /\n        \n        (\n        11\n        +\n        1\n        )\n        =\n        91.66\n        %\n      \n    \n    {\\displaystyle 11/(11+1)=91.66\\%}\n  \n\nMiss Rate (FNR – false negative rate):\n\n  \n    \n      \n        F\n        N\n        R\n        =\n        F\n        N\n        \n          /\n        \n        (\n        F\n        N\n        +\n        T\n        P\n        )\n      \n    \n    {\\displaystyle FNR=FN/(FN+TP)}\n  \n\n  \n    \n      \n        45\n        ÷\n        (\n        45\n        +\n        11\n        )\n        =\n        80.35\n        %\n      \n    \n    {\\displaystyle 45\\div (45+11)=80.35\\%}\n  \n\nFalse discovery rate (FDR):\n\n  \n    \n      \n        F\n        D\n        R\n        =\n        F\n        P\n        \n          /\n        \n        (\n        F\n        P\n        +\n        T\n        P\n        )\n      \n    \n    {\\displaystyle FDR=FP/(FP+TP)}\n  \n\n  \n    \n      \n        1\n        ÷\n        (\n        1\n        +\n        11\n        )\n        =\n        8.30\n        %\n      \n    \n    {\\displaystyle 1\\div (1+11)=8.30\\%}\n  \n\nFalse omission rate (FOR):\n\n  \n    \n      \n        F\n        O\n        R\n        =\n        F\n        N\n        \n          /\n        \n        (\n        F\n        N\n        +\n        T\n        N\n        )\n      \n    \n    {\\displaystyle FOR=FN/(FN+TN)}\n  \n\n  \n    \n      \n        45\n        ÷\n        (\n        45\n        +\n        105\n        )\n        =\n        30.00\n        %\n      \n    \n    {\\displaystyle 45\\div (45+105)=30.00\\%}\n  \n\nOnce we have calculated the key metrics we can make some initial conclusions on the performance of the decision tree model built. The accuracy that we calculated was 71.60%. The accuracy value is good to start but we would like to get our models as accurate as possible while maintaining the overall performance. The sensitivity value of 19.64% means that out of everyone who was actually positive for cancer tested positive. If we look at the specificity value of 99.06% we know that out of all the samples that were negative for cancer actually tested negative. When it comes to sensitivity and specificity it is important to have a balance between the two values, so if we can decrease our specificity to increase the sensitivity that would prove to be beneficial. These are just a few examples on how to use these values and the meanings behind them to evaluate the decision tree model and improve upon the next iteration.\n\n\n== See also ==\nBehavior tree (artificial intelligence, robotics and control) – Mathematical model of plan execution\nBoosting (machine learning) – Method in machine learning\nDecision cycle – Sequence of steps for decision-making\nDecision list\nDecision matrix\nDecision table – Table specifying actions based on conditions\nDecision tree model – Model of computational complexity of computation\nDesign rationale – explicit documentation of the reasons behind decisions made when designing a system or artifactPages displaying wikidata descriptions as a fallback\nDRAKON – Algorithm mapping tool\nMarkov chain – Random process independent of past history\nRandom forest – Tree-based ensemble machine learning method\nOrdinal priority approach – Multiple-criteria decision analysis method\nOdds algorithm – Method of computing optimal strategies for last-success problems\nTopological combinatorics\nTruth table – Mathematical table used in logic\n\n\n== References ==\n\n\n== External links ==\n\nExtensive Decision Tree tutorials and examples\nGallery of example decision trees\nGradient Boosted Decision Trees',
  },
  {
    title: "Chatbot",
    originalContent:
      'A chatbot (originally chatterbot) is a software application or web interface designed to have textual or spoken conversations. Modern chatbots are typically online and use generative artificial intelligence systems that are capable of maintaining a conversation with a user in natural language and simulating the way a human would behave as a conversational partner. Such chatbots often use deep learning and natural language processing, but simpler chatbots have existed for decades.\nAlthough chatbots have existed since the late 1960s, the field gained widespread attention in the early 2020s due to the popularity of OpenAI\'s ChatGPT, followed by alternatives such as Microsoft\'s Copilot and Google\'s Gemini. Such examples reflect the recent practice of basing such products upon broad foundational large language models, such as GPT-4 or the Gemini language model, that get fine-tuned so as to target specific tasks or applications (i.e., simulating human conversation, in the case of chatbots). Chatbots can also be designed or customized to further target even more specific situations and/or particular subject-matter domains.\nA major area where chatbots have long been used is in customer service and support, with various sorts of virtual assistants. Companies spanning a wide range of industries have begun using the latest generative artificial intelligence technologies to power more advanced developments in such areas.\n\n\n== History ==\n\n\n=== Turing test ===\nIn 1950, Alan Turing\'s famous article "Computing Machinery and Intelligence" was published, which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge to the extent that the judge is unable to distinguish reliably—on the basis of the conversational content alone—between the program and a real human.\n\n\n=== Eliza ===\nThe notoriety of Turing\'s proposed test stimulated great interest in Joseph Weizenbaum\'s program ELIZA, published in 1966, which seemed to be able to fool users into believing that they were conversing with a real human. However Weizenbaum himself did not claim that ELIZA was genuinely intelligent, and the introduction to his paper presented it more as a debunking exercise:In artificial intelligence, machines are made to behave in wondrous ways, often sufficient to dazzle even the most experienced observer. But once a particular program is unmasked, once its inner workings are explained, its magic crumbles away; it stands revealed as a mere collection of procedures. The observer says to himself "I could have written that". With that thought, he moves the program in question from the shelf marked "intelligent", to that reserved for curios. The object of this paper is to cause just such a re-evaluation of the program about to be "explained". Few programs ever needed it more.ELIZA\'s key method of operation involves the recognition of clue words or phrases in the input, and the output of the corresponding pre-prepared or pre-programmed responses that can move the conversation forward in an apparently meaningful way (e.g. by responding to any input that contains the word \'MOTHER\' with \'TELL ME MORE ABOUT YOUR FAMILY\'). Thus an illusion of understanding is generated, even though the processing involved has been merely superficial. ELIZA showed that such an illusion is surprisingly easy to generate because human judges are ready to give the benefit of the doubt when conversational responses are capable of being interpreted as "intelligent".\nInterface designers have come to appreciate that humans\' readiness to interpret computer output as genuinely conversational—even when it is actually based on rather simple pattern-matching—can be exploited for useful purposes. Most people prefer to engage with programs that are human-like, and this gives chatbot-style techniques a potentially useful role in interactive systems that need to elicit information from users, as long as that information is relatively straightforward and falls into predictable categories. Thus, for example, online help systems can usefully employ chatbot techniques to identify the area of help that users require, potentially providing a "friendlier" interface than a more formal search or menu system. This sort of usage holds the prospect of moving chatbot technology from Weizenbaum\'s "shelf ... reserved for curios" to that marked "genuinely useful computational methods".\n\n\n=== Early chatbots ===\nAmong the most notable early chatbots are ELIZA (1966) and PARRY (1972). More recent notable programs include A.L.I.C.E., Jabberwacky and D.U.D.E (Agence Nationale de la Recherche and CNRS 2006). While ELIZA and PARRY were used exclusively to simulate typed conversation, many chatbots now include other functional features, such as games and web searching abilities. In 1984, a book called The Policeman\'s Beard is Half Constructed was published, allegedly written by the chatbot Racter (though the program as released would not have been capable of doing so).\nFrom 1978 to some time after 1983, the CYRUS project led by Janet Kolodner constructed a chatbot simulating Cyrus Vance (57th United States Secretary of State). It used case-based reasoning, and updated its database daily by parsing wire news from United Press International. The program was unable to process the news items subsequent to the surprise resignation of Cyrus Vance in April 1980, and the team constructed another chatbot simulating his successor, Edmund Muskie.\nOne pertinent field of AI research is natural-language processing. Usually, weak AI fields employ specialized software or programming languages created specifically for the narrow function required. For example, A.L.I.C.E. uses a markup language called AIML, which is specific to its function as a conversational agent, and has since been adopted by various other developers of, so-called, Alicebots. Nevertheless, A.L.I.C.E. is still purely based on pattern matching techniques without any reasoning capabilities, the same technique ELIZA was using back in 1966. This is not strong AI, which would require sapience and logical reasoning abilities.\nJabberwacky learns new responses and context based on real-time user interactions, rather than being driven from a static database. Some more recent chatbots also combine real-time learning with evolutionary algorithms that optimize their ability to communicate based on each conversation held.\nChatbot competitions focus on the Turing test or more specific goals. Two such annual contests are the Loebner Prize and The Chatterbox Challenge (the latter has been offline since 2015, however, materials can still be found from web archives).\nDBpedia created a chatbot during the GSoC of 2017. It can communicate through Facebook Messenger (see Master of Code Global article).\n\n\n=== Modern chatbots based on large language models ===\nModern chatbots like ChatGPT are often based on large language models called generative pre-trained transformers (GPT). They are based on a deep learning architecture called the transformer, which contains artificial neural networks. They learn how to generate text by being trained on a large text corpus, which provides a solid foundation for the model to perform well on downstream tasks with limited amounts of task-specific data. Despite criticism of its accuracy and tendency to "hallucinate"—that is, to confidently output false information and even cite non-existent sources—ChatGPT has gained attention for its detailed responses and historical knowledge. Another example is BioGPT, developed by Microsoft, which focuses on answering biomedical questions. In November 2023, Amazon announced a new chatbot, called Q, for people to use at work.\n\n\n== Application ==\n\n\n=== Messaging apps ===\nMany companies\' chatbots run on messaging apps or simply via SMS. They are used for B2C customer service, sales and marketing.\nIn 2016, Facebook Messenger allowed developers to place chatbots on their platform. There were 30,000 bots created for Messenger in the first six months, rising to 100,000 by September 2017.\nSince September 2017, this has also been as part of a pilot program on WhatsApp. Airlines KLM and Aeroméxico both announced their participation in the testing; both airlines had previously launched customer services on the Facebook Messenger platform. \nThe bots usually appear as one of the user\'s contacts, but can sometimes act as participants in a group chat.\nMany banks, insurers, media companies, e-commerce companies, airlines, hotel chains, retailers, health care providers, government entities, and restaurant chains have used chatbots to answer simple questions, increase customer engagement, for promotion, and to offer additional ways to order from them. Chatbots are also used in market research to collect short survey responses.\nA 2017 study showed 4% of companies used chatbots. According to a 2016 study, 80% of businesses said they intended to have one by 2020.\n\n\n==== As part of company apps and websites ====\nPrevious generations of chatbots were present on company websites, e.g. Ask Jenn from Alaska Airlines which debuted in 2008 or Expedia\'s virtual customer service agent which launched in 2011. The newer generation of chatbots includes IBM Watson-powered "Rocky", introduced in February 2017 by the New York City-based e-commerce company Rare Carat to provide information to prospective diamond buyers.\n\n\n==== Chatbot sequences ====\nUsed by marketers to script sequences of messages, very similar to an autoresponder sequence. Such sequences can be triggered by user opt-in or the use of keywords within user interactions. After a trigger occurs a sequence of messages is delivered until the next anticipated user response. Each user response is used in the decision tree to help the chatbot navigate the response sequences to deliver the correct response message.\n\n\n=== Company internal platforms ===\nCompanies have used chatbots for customer support, human resources, or in Internet-of-Things (IoT) projects. Overstock.com, for one, has reportedly launched a chatbot named Mila to attempt to automate certain processes when customer service employees request sick leave. Other large companies such as Lloyds Banking Group, Royal Bank of Scotland, Renault and Citroën are now using chatbots instead of call centres with humans to provide a first point of contact. In large companies, like in hospitals and aviation organizations, chatbots are also used to share information within organizations, and to assist and replace service desks.\n\n\n=== Customer service ===\nChatbots have been proposed as a replacement for customer service departments. \nDeep learning techniques can be incorporated into chatbot applications to allow them to map conversations between users and customer service agents, especially in social media.\nIn 2019, Gartner predicted that by 2021, 15% of all customer service interactions globally will be handled completely by AI.  A study by Juniper Research in 2019 estimates retail sales resulting from chatbot-based interactions will reach $112 billion by 2023.\nIn 2016, Russia-based Tochka Bank launched a chatbot on Facebook for a range of financial services, including a possibility of making payments. In July 2016, Barclays Africa also launched a Facebook chatbot.\nIn 2023, US-based National Eating Disorders Association replaced its human helpline staff with a chatbot but had to take it offline after users reported receiving harmful advice from it.\n\n\n=== Healthcare ===\nChatbots are also appearing in the healthcare industry. A study suggested that physicians in the United States believed that chatbots would be most beneficial for scheduling doctor appointments, locating health clinics, or providing medication information.\nChatGPT is able to answer user queries related to health promotion and disease prevention such as screening and vaccination. WhatsApp has teamed up with the World Health Organization (WHO) to make a chatbot service that answers users\' questions on COVID-19.\nIn 2020, the Government of India launched a chatbot called MyGov Corona Helpdesk, that worked through WhatsApp and helped people access information about the Coronavirus (COVID-19) pandemic.\nCertain patient groups are still reluctant to use chatbots. A mixed-methods 2019 study showed that people are still hesitant to use chatbots for their healthcare due to poor understanding of the technological complexity, the lack of empathy, and concerns about cyber-security. The analysis showed that while 6% had heard of a health chatbot and 3% had experience of using it, 67% perceived themselves as likely to use one within 12 months. The majority of participants would use a health chatbot for seeking general health information (78%), booking a medical appointment (78%), and looking for local health services (80%). However, a health chatbot was perceived as less suitable for seeking results of medical tests and seeking specialist advice such as sexual health.\nThe analysis of attitudinal variables showed that most participants reported their preference for discussing their health with doctors (73%) and having access to reliable and accurate health information (93%). While 80% were curious about new technologies that could improve their health, 66% reported only seeking a doctor when experiencing a health problem and 65% thought that a chatbot was a good idea. 30% reported dislike about talking to computers, 41% felt it would be strange to discuss health matters with a chatbot and about half were unsure if they could trust the advice given by a chatbot. Therefore, perceived trustworthiness, individual attitudes towards bots, and dislike for talking to computers are the main barriers to health chatbots.\n\n\n=== Politics ===\n\nIn New Zealand, the chatbot SAM – short for Semantic Analysis Machine – has been developed by Nick Gerritsen of Touchtech. It is designed to share its political thoughts, for example on topics such as climate change, healthcare and education, etc. It talks to people through Facebook Messenger.\nIn 2022, the chatbot "Leader Lars" or "Leder Lars" was nominated for The Synthetic Party to run in the Danish parliamentary election, and was built by the artist collective Computer Lars. Leader Lars differed from earlier virtual politicians by leading a political party and by not pretending to be an objective candidate. This chatbot engaged in critical discussions on politics with users from around the world.\nIn India, the state government has launched a chatbot for its Aaple Sarkar platform, which provides conversational access to information regarding public services managed.\n\n\n=== Toys ===\nChatbots have also been incorporated into devices not primarily meant for computing, such as toys.\nHello Barbie is an Internet-connected version of the doll that uses a chatbot provided by the company ToyTalk, which previously used the chatbot for a range of smartphone-based characters for children. These characters\' behaviors are constrained by a set of rules that in effect emulate a particular character and produce a storyline.\nThe My Friend Cayla doll was marketed as a line of 18-inch (46 cm) dolls which uses speech recognition technology in conjunction with an Android or iOS mobile app to recognize the child\'s speech and have a conversation. Like the Hello Barbie doll, it attracted controversy due to vulnerabilities with the doll\'s Bluetooth stack and its use of data collected from the child\'s speech.\nIBM\'s Watson computer has been used as the basis for chatbot-based educational toys for companies such as CogniToys, intended to interact with children for educational purposes.\n\n\n=== Malicious use ===\nMalicious chatbots are frequently used to fill chat rooms with spam and advertisements by mimicking human behavior and conversations or to entice people into revealing personal information, such as bank account numbers. They were commonly found on Yahoo! Messenger, Windows Live Messenger, AOL Instant Messenger and other instant messaging protocols. There has also been a published report of a chatbot used in a fake personal ad on a dating service\'s website.\nTay, an AI chatbot designed to learn from previous interaction, caused major controversy due to it being targeted by internet trolls on Twitter. Soon after its launch, the bot was exploited, and with its "repeat after me" capability, it started releasing racist, sexist, and controversial responses to Twitter users. This suggests that although the bot learned effectively from experience, adequate protection was not put in place to prevent misuse.\nIf a text-sending algorithm can pass itself off as a human instead of a chatbot, its message would be more credible. Therefore, human-seeming chatbots with well-crafted online identities could start scattering fake news that seems plausible, for instance making false claims during an election. With enough chatbots, it might be even possible to achieve artificial social proof.\n\n\n=== Data security ===\nData security is one of the major concerns of chatbot technologies. Security threats and system vulnerabilities are weaknesses that are often exploited by malicious users. Storage of user data and past communication, that is highly valuable for training and development of chatbots, can also give rise to security threats. Chatbots operating on third-party networks may be subject to various security issues if owners of the third-party applications have policies regarding user data that differ from those of the chatbot. Security threats can be reduced or prevented by incorporating protective mechanisms. User authentication, chat End-to-end encryption, and self-destructing messages are some effective solutions to resist potential security threats.\n\n\n== Limitations of chatbots ==\n\nChatbots have difficulty managing non-linear conversations that must go back and forth on a topic with a user.\nLarge language models are more versatile, but require a large amount of conversational data to train. These modeles generate new responses word by word based on user input, are usually trained on a large dataset of natural-language phrases. They sometimes provide plausible-sounding but incorrect or nonsensical answers. They can make up names, dates, historical events, and even simple math problems. When large language models produce coherent-sounding but inaccurate or fabricated content, this is referred to as "hallucinations". When humans use and apply chatbot content contaminated with hallucinations, this results in "botshit". Given the increasing adoption and use of chatbots for generating content, there are concerns that this technology will significantly reduce the cost it takes humans to generate misinformation.\n\n\n== Impact on jobs ==\nChatbots and technology in general used to automate repetitive tasks. But advanced chatbots like ChatGPT are also targeting high-paying, creative, and knowledge-based jobs, raising concerns about workforce disruption and quality trade-offs in favor of cost-cutting.\nChatbots are increasingly used by small and medium enterprises, to handle customer interactions efficiently, reducing reliance on large call centers and lowering operational costs.\nPrompt engineering, the task of designing and refining prompts (inputs) leading to desired AI-generated responses has quickly gained significant demand with the advent of large language models, although the viability of this job is questioned due to new techniques for automating prompt engineering.\n\n\n== Impact on the environment ==\n\nGenerative AI uses a high amount of electric power. Due to reliance on fossil fuels in its generation, this increases air pollution, water pollution, and greenhouse gas emissions. In 2023, a question to ChatGPT consumed on average 10 times as much energy as a Google search. Data centres in general, and those used for AI tasks specifically, consume significant amounts of water for cooling.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nGertner, Jon. (2023) "Wikipedia\'s Moment of Truth: Can the online encyclopedia help teach A.I. chatbots to get their facts right — without destroying itself in the process?" New York Times Magazine (18 July 2023) online\nSearle, John (1980), "Minds, Brains and Programs", Behavioral and Brain Sciences, 3 (3): 417–457, doi:10.1017/S0140525X00005756, S2CID 55303721\nShevat, Amir (2017). Designing bots: Creating conversational experiences (First ed.). Sebastopol, CA: O\'Reilly Media. ISBN 978-1-4919-7482-7. OCLC 962125282.\nVincent, James, "Horny Robot Baby Voice: James Vincent on AI chatbots", London Review of Books, vol. 46, no. 19 (10 October 2024), pp. 29–32. "[AI chatbot] programs are made possible by new technologies but rely on the timelelss human tendency to anthropomorphise." (p. 29.)\n\n\n== External links ==\n Media related to Chatbots at Wikimedia Commons\n Conversational bots at Wikibooks',
  },
  {
    title: "Unsupervised learning",
    originalContent:
      "Unsupervised learning is a framework in machine learning where, in contrast to supervised learning, algorithms learn patterns exclusively from unlabeled data. Other frameworks in the spectrum of supervisions include weak- or semi-supervision, where a small portion of the data is tagged, and self-supervision. Some researchers consider self-supervised learning a form of unsupervised learning.\nConceptually, unsupervised learning divides into the aspects of data, training, algorithm, and downstream applications. Typically, the dataset is harvested cheaply \"in the wild\", such as massive text corpus obtained by web crawling, with only minor filtering (such as Common Crawl). This compares favorably to supervised learning, where the dataset (such as the ImageNet1000) is typically constructed manually, which is much more expensive.\nThere were algorithms designed specifically for unsupervised learning, such as clustering algorithms like k-means, dimensionality reduction techniques like principal component analysis (PCA), Boltzmann machine learning, and autoencoders. After the rise of deep learning, most large-scale unsupervised learning have been done by training general-purpose neural network architectures by gradient descent, adapted to performing unsupervised learning by designing an appropriate training procedure.\nSometimes a trained model can be used as-is, but more often they are modified for downstream applications. For example, the generative pretraining method trains a model to generate a textual dataset, before finetuning it for other applications, such as text classification. As another example, autoencoders are trained to good features, which can then be used as a module for other models, such as in a latent diffusion model.\n\n\n== Tasks ==\nTasks are often categorized as discriminative (recognition) or generative (imagination).  Often but not always, discriminative tasks use supervised methods and generative tasks use unsupervised (see Venn diagram); however, the separation is very hazy.  For example, object recognition favors supervised learning but unsupervised learning can also cluster objects into groups.  Furthermore, as progress marches onward some tasks employ both methods, and some tasks swing from one to another.  For example, image recognition started off as heavily supervised, but became hybrid by employing unsupervised pre-training, and then moved towards supervision again with the advent of dropout, ReLU, and adaptive learning rates.\nA typical generative task is as follows. At each step, a datapoint is sampled from the dataset, and part of the data is removed, and the model must infer the removed part. This is particularly clear for the denoising autoencoders and BERT.\n\n\n== Neural network architectures ==\n\n\n=== Training ===\nDuring the learning phase, an unsupervised network tries to mimic the data it's given and uses the error in its mimicked output to correct itself (i.e. correct its weights and biases). Sometimes the error is expressed as a low probability that the erroneous output occurs, or it might be expressed as an unstable high energy state in the network.\nIn contrast to supervised methods' dominant use of backpropagation, unsupervised learning also employs other methods  including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations. See the table below for more details.\n\n\n=== Energy ===\nAn energy function is a macroscopic measure of a network's activation state.  In Boltzmann machines, it plays the role of the Cost function.  This analogy with physics is inspired by Ludwig Boltzmann's analysis of a gas' macroscopic energy from the microscopic probabilities of particle motion \n  \n    \n      \n        p\n        ∝\n        \n          e\n          \n            −\n            E\n            \n              /\n            \n            k\n            T\n          \n        \n      \n    \n    {\\displaystyle p\\propto e^{-E/kT}}\n  \n, where k is the Boltzmann constant and T is temperature. In the RBM network the relation is \n  \n    \n      \n        p\n        =\n        \n          e\n          \n            −\n            E\n          \n        \n        \n          /\n        \n        Z\n      \n    \n    {\\displaystyle p=e^{-E}/Z}\n  \n, where \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n and \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n vary over every possible activation pattern and \n  \n    \n      \n        \n          \n            Z\n            =\n            \n              ∑\n              \n                \n                  \n                    All Patterns\n                  \n                \n              \n            \n            \n              e\n              \n                −\n                E\n                (\n                \n                  pattern\n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle {Z=\\sum _{\\scriptscriptstyle {\\text{All Patterns}}}e^{-E({\\text{pattern}})}}}\n  \n. To be more precise, \n  \n    \n      \n        p\n        (\n        a\n        )\n        =\n        \n          e\n          \n            −\n            E\n            (\n            a\n            )\n          \n        \n        \n          /\n        \n        Z\n      \n    \n    {\\displaystyle p(a)=e^{-E(a)}/Z}\n  \n, where \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is an activation pattern of all neurons (visible and hidden). Hence, some early neural networks bear the name Boltzmann Machine.  Paul Smolensky calls \n  \n    \n      \n        −\n        E\n        \n      \n    \n    {\\displaystyle -E\\,}\n  \n the Harmony. A network seeks low energy which is high Harmony.\n\n\n=== Networks ===\nThis table shows connection diagrams of various unsupervised networks, the details of which will be given in the section Comparison of Networks.  Circles are neurons and edges between them are connection weights.  As network design changes, features are added on to enable new capabilities or removed to make learning faster.  For instance, neurons change between deterministic (Hopfield) and stochastic (Boltzmann) to allow robust output, weights are removed within a layer (RBM) to hasten learning, or connections are allowed to become asymmetric (Helmholtz).\n\nOf the networks bearing people's names, only Hopfield worked directly with neural networks.  Boltzmann and Helmholtz came before artificial neural networks, but their work in physics and physiology inspired the analytical methods that were used.\n\n\n=== History ===\n\n\n=== Specific Networks ===\nHere, we highlight some characteristics of select networks.  The details of each are given in the comparison table below. \n\nHopfield Network\nFerromagnetism inspired Hopfield networks.  A neuron correspond to an iron domain with binary magnetic moments Up and Down, and neural connections correspond to the domain's influence on each other.  Symmetric connections enable a global energy formulation. During inference the network updates each state using the standard activation step function. Symmetric weights and the right energy functions guarantees convergence to a stable activation pattern.  Asymmetric weights are difficult to analyze.  Hopfield nets are used as Content Addressable Memories (CAM).\n\nBoltzmann Machine\nThese are stochastic Hopfield nets. Their state value is sampled from this pdf as follows: suppose a binary neuron fires with the Bernoulli probability p(1) = 1/3 and rests with p(0) = 2/3. One samples from it by taking a uniformly distributed random number y, and plugging it into the inverted cumulative distribution function, which in this case is the step function thresholded at 2/3. The inverse function = { 0 if x <= 2/3, 1 if x > 2/3 }.\n\nSigmoid Belief Net\nIntroduced by Radford Neal in 1992, this network applies ideas from probabilistic graphical models to neural networks.  A key difference is that nodes in graphical models have pre-assigned meanings, whereas Belief Net neurons' features are determined after training. The network is a sparsely connected directed acyclic graph composed of binary stochastic neurons.  The learning rule comes from Maximum Likelihood on p(X):  Δwij \n  \n    \n      \n        ∝\n      \n    \n    {\\displaystyle \\propto }\n  \n sj * (si - pi), where pi = 1 / ( 1 + eweighted inputs into neuron i ).  sj's are activations from an unbiased sample of the posterior distribution and this is problematic due to the Explaining Away problem raised by Judea Perl.  Variational Bayesian methods uses a surrogate posterior and blatantly disregard this complexity.\n\nDeep Belief Network\nIntroduced by Hinton, this network is a hybrid of RBM and Sigmoid Belief Network.  The top 2 layers is an RBM and the second layer downwards form a sigmoid belief network.  One trains it by the stacked RBM method and then throw away the recognition weights below the top RBM.  As of 2009, 3-4 layers seems to be the optimal depth.\n\nHelmholtz machine\nThese are early inspirations for the Variational Auto Encoders. Its 2 networks combined into one—forward weights operates recognition and backward weights implements imagination. It is perhaps the first network to do both. Helmholtz did not work in machine learning but he inspired the view of \"statistical inference engine whose function is to infer probable causes of sensory input\". the stochastic binary neuron outputs a probability that its state is 0 or 1. The data input is normally not considered a layer, but in the Helmholtz machine generation mode, the data layer receives input from the middle layer and has separate weights for this purpose, so it is considered a layer. Hence this network has 3 layers.\n\nVariational autoencoder\nThese are inspired by Helmholtz machines and combines probability network with neural networks. An Autoencoder is a 3-layer CAM network, where the middle layer is supposed to be some internal representation of input patterns. The encoder neural network is a probability distribution qφ(z given x) and the decoder network is pθ(x given z). The weights are named phi & theta rather than W and V as in Helmholtz—a cosmetic difference. These 2 networks here can be fully connected, or use another NN scheme.\n\n\n=== Comparison of networks ===\n\n\n=== Hebbian Learning, ART, SOM ===\nThe classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\nAmong neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.\n\n\n== Probabilistic methods ==\nTwo of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships. Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.\nA central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution .\n\n\n=== Approaches ===\nSome of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Approaches for learning latent variable models. Each approach uses several methods as follows:\n\nClustering methods include: hierarchical clustering, k-means, mixture models, model-based clustering, DBSCAN, and OPTICS algorithm\nAnomaly detection methods include: Local Outlier Factor, and Isolation Forest\nApproaches for learning latent variable models such as Expectation–maximization algorithm (EM), Method of moments, and Blind signal separation techniques (Principal component analysis, Independent component analysis, Non-negative matrix factorization, Singular value decomposition)\n\n\n=== Method of moments ===\nOne of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.\nIn particular, the method of moments is shown to be effective in learning the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.\nThe Expectation–maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.\n\n\n== See also ==\nAutomated machine learning\nCluster analysis\nModel-based clustering\nAnomaly detection\nExpectation–maximization algorithm\nGenerative topographic map\nMeta-learning (computer science)\nMultivariate analysis\nRadial basis function network\nWeak supervision\n\n\n== References ==\n\n\n== Further reading ==",
  },
  {
    title: "Swarm intelligence",
    originalContent:
      'Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.\nSI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of "intelligent" global behavior, unknown to the individual agents. Examples of swarm intelligence in natural systems include ant colonies, bee colonies, bird flocking, hawks hunting, animal herding, bacterial growth, fish schooling and microbial intelligence.\nThe application of swarm principles to robots is called swarm robotics while swarm intelligence refers to the more general set of algorithms. Swarm prediction has been used in the context of forecasting problems. Similar approaches to those proposed for swarm robotics are considered for genetically modified organisms in synthetic collective intelligence.\n\n\n== Models of swarm behavior ==\n\n\n=== Boids (Reynolds 1987) ===\n\nBoids is an artificial life program, developed by Craig Reynolds in 1986, which simulates flocking. It was published in 1987 in the proceedings of the ACM SIGGRAPH conference.\nThe name "boid" corresponds to a shortened version of "bird-oid object", which refers to a bird-like object.\nAs with most artificial life simulations, Boids is an example of emergent behavior; that is, the complexity of Boids arises from the interaction of individual agents (the boids, in this case) adhering to a set of simple rules.  The rules applied in the simplest Boids world are as follows:\n\nseparation: steer to avoid crowding local flockmates\nalignment: steer towards the average heading of local flockmates\ncohesion: steer to move toward the average position (center of mass) of local flockmates\nMore complex rules can be added, such as obstacle avoidance and goal seeking.\n\n\n=== Self-propelled particles (Vicsek et al.  1995) ===\n\nSelf-propelled particles (SPP), also referred to as the Vicsek model, was introduced in 1995 by Vicsek et al. as a special case of the boids model introduced in 1986 by Reynolds. A swarm is modelled in SPP by a collection of particles that move with a constant speed but respond to a random perturbation by adopting at each time increment the average direction of motion of the other particles in their local neighbourhood. SPP models predict that swarming animals share certain properties at the group level, regardless of the type of animals in the swarm. Swarming systems give rise to emergent behaviours which occur at many different scales, some of which are turning out to be both universal and robust. It has become a challenge in theoretical physics to find minimal statistical models that capture these behaviours.\n\n\n== Metaheuristics ==\n\nEvolutionary algorithms (EA), particle swarm optimization (PSO), differential evolution (DE), ant colony optimization (ACO) and their variants dominate the field of nature-inspired metaheuristics. This list includes algorithms published up to circa the year 2000. A large number of more recent metaphor-inspired metaheuristics have started to attract criticism in the research community for hiding their lack of novelty behind an elaborate metaphor. For algorithms published since that time, see List of metaphor-based metaheuristics.\nMetaheuristics lack a confidence in a solution. When appropriate parameters are determined, and when sufficient convergence stage is achieved, they often find a solution that is optimal, or near close to optimum – nevertheless, if one does not know optimal solution in advance, a quality of a solution is not known. In spite of this obvious drawback it has been shown that these types of algorithms work well in practice, and have been extensively researched, and developed.  On the other hand, it is possible to avoid this drawback by calculating solution quality for a special case where such calculation is possible, and after such run it is known that every solution that is at least as good as the solution a special case had, has at least a solution confidence a special case had. One such instance is Ant-inspired Monte Carlo algorithm for Minimum Feedback Arc Set where this has been achieved probabilistically via hybridization of Monte Carlo algorithm with Ant Colony Optimization technique.\n\n\n=== Ant colony optimization (Dorigo 1992) ===\n\nAnt colony optimization (ACO), introduced by Dorigo in his doctoral dissertation, is a class of optimization algorithms modeled on the actions of an ant colony. ACO is a probabilistic technique useful in problems that deal with finding better paths through graphs. Artificial \'ants\'—simulation agents—locate optimal solutions by moving through a parameter space representing all possible solutions. Natural ants lay down pheromones directing each other to resources while exploring their environment. The simulated \'ants\' similarly record their positions and the quality of their solutions, so that in later simulation iterations more ants locate for better solutions.\n\n\n=== Particle swarm optimization (Kennedy, Eberhart & Shi 1995) ===\n\nParticle swarm optimization (PSO) is a global optimization algorithm for dealing with problems in which a best solution can be represented as a point or surface in an n-dimensional space.  Hypotheses are plotted in this space and seeded with an initial velocity, as well as a communication channel between the particles.  Particles then move through the solution space, and are evaluated according to some fitness criterion after each timestep. Over time, particles are accelerated towards those particles within their communication grouping which have better fitness values. The main advantage of such an approach over other global minimization strategies such as simulated annealing is that the large number of members that make up the particle swarm make the technique impressively resilient to the problem of local minima.\n\n\n=== Artificial bee colony algorithm (Karaboga 2005) ===\n\nKaraboga introduced ABC metaheuristic in 2005 as an answer to optimize numerical problems. Inspired by honey bee foraging behavior, Karaboga\'s model had three components. The employed, onlooker, and scout. In practice, the artificial scout bee would expose all food source positions (solutions) good or bad. The employed bee would search for the shortest route to each position to extract the food amount (quality) of the source. If the food was depleted from the source, the employed bee would become a scout and randomly search for other food sources. Each source that became abandoned created negative feedback meaning, the answers found were poor solutions. The onlooker bees wait for employed bees to either abandon a source or give information that the source has a large quantity of food and is worth sending additional resources to. The more an onlooker bee is recruited, the more positive the feedback is meaning that the answer is likely a good solution.\n\n\n=== Artificial Swarm Intelligence (2015) ===\nArtificial Swarm Intelligence (ASI) is method of amplifying the collective intelligence of networked human groups using control algorithms modeled after natural swarms. Sometimes referred to as Human Swarming or Swarm AI, the technology connects groups of human participants into real-time systems that deliberate and converge on solutions as dynamic swarms when simultaneously presented with a question  ASI has been used for a wide range of applications, from enabling business teams to generate highly accurate financial forecasts to enabling sports fans to outperform Vegas betting markets. ASI has also been used to enable groups of doctors to generate diagnoses with significantly higher accuracy than traditional methods. ASI has been used by the Food and Agriculture Organization (FAO) of the United Nations to help forecast famines in hotspots around the world.\n\n\n== Applications ==\nSwarm Intelligence-based techniques can be used in a number of applications.  The U.S. military is investigating swarm techniques for controlling unmanned vehicles. The European Space Agency is thinking about an orbital swarm for self-assembly and interferometry. NASA is investigating the use of swarm technology for planetary mapping.  A 1992 paper by M. Anthony Lewis and George A. Bekey discusses the possibility of using swarm intelligence to control nanobots within the body for the purpose of killing cancer tumors. Conversely al-Rifaie and Aber have used stochastic diffusion search to help locate tumours. Swarm intelligence (SI) is increasingly applied in Internet of Things (IoT) systems, and by association to Intent-Based Networking (IBN), due to its ability to handle complex, distributed tasks through decentralized, self-organizing algorithms. Swarm intelligence has also been applied for data mining and cluster analysis. Ant-based models are further subject of modern management theory.\n\n\n=== Ant-based routing ===\nThe use of swarm intelligence in telecommunication networks has also been researched, in the form of ant-based routing. This was pioneered separately by Dorigo et al. and Hewlett-Packard in the mid-1990s, with a number of variants existing. Basically, this uses a probabilistic routing table rewarding/reinforcing the route successfully traversed by each "ant" (a small control packet) which flood the network. Reinforcement of the route in the forwards, reverse direction and both simultaneously have been researched: backwards reinforcement requires a symmetric network and couples the two directions together; forwards reinforcement rewards a route before the outcome is known (but then one would pay for the cinema before one knows how good the film is). As the system behaves stochastically and is therefore lacking repeatability, there are large hurdles to commercial deployment. Mobile media and new technologies have the potential to change the threshold for collective action due to swarm intelligence (Rheingold: 2002, P175).\nThe location of transmission infrastructure for wireless communication networks is an important engineering problem involving competing objectives. A minimal selection of locations (or sites) are required subject to providing adequate area coverage for users. A very different, ant-inspired swarm intelligence algorithm, stochastic diffusion search (SDS), has been successfully used to provide a general model for this problem, related to circle packing and set covering. It has been shown that the SDS can be applied to identify suitable solutions even for large problem instances.\nAirlines have also used ant-based routing in assigning aircraft arrivals to airport gates. At Southwest Airlines a software program uses swarm theory, or swarm intelligence—the idea that a colony of ants works better than one alone. Each pilot acts like an ant searching for the best airport gate. "The pilot learns from his experience what\'s the best for him, and it turns out that that\'s the best solution for the airline," Douglas A. Lawson explains. As a result, the "colony" of pilots always go to gates they can arrive at and depart from quickly. The program can even alert a pilot of plane back-ups before they happen. "We can anticipate that it\'s going to happen, so we\'ll have a gate available," Lawson says.\n\n\n=== Crowd simulation ===\nArtists are using swarm technology as a means of creating complex interactive systems or simulating crowds.\n\n\n==== Instances ====\nThe Lord of the Rings film trilogy made use of similar technology, known as Massive (software), during battle scenes. Swarm technology is particularly attractive because it is cheap, robust, and simple.\nStanley and Stella in: Breaking the Ice was the first movie to make use of swarm technology for rendering, realistically depicting the movements of groups of fish and birds using the Boids system.\nTim Burton\'s Batman Returns also made use of swarm technology for showing the movements of a group of bats.\n\nAirlines have used swarm theory to simulate passengers boarding a plane. Southwest Airlines researcher Douglas A. Lawson used an ant-based computer simulation employing only six interaction rules to evaluate boarding times using various boarding methods.(Miller, 2010, xii-xviii).\n\n\n=== Human swarming ===\nNetworks of distributed users can be organized into "human swarms" through the implementation of real-time closed-loop control systems. Developed by Louis Rosenberg in 2015, human swarming, also called artificial swarm intelligence, allows the collective intelligence of interconnected groups of people online to be harnessed. The collective intelligence of the group often exceeds the abilities of any one member of the group. \nStanford University School of Medicine published in 2018 a study showing that groups of human doctors, when connected together by real-time swarming algorithms, could diagnose medical conditions with substantially higher accuracy than individual doctors or groups of doctors working together using traditional crowd-sourcing methods.  In one such study, swarms of human radiologists connected together were tasked with diagnosing chest x-rays and demonstrated a 33% reduction in diagnostic errors as compared to the traditional human methods, and a 22% improvement over traditional machine-learning. \nThe University of California San Francisco (UCSF) School of Medicine released a preprint in 2021 about the diagnosis of MRI images by small groups of collaborating doctors. The study showed a 23% increase in diagnostic accuracy when using Artificial Swarm Intelligence (ASI) technology compared to majority voting.\n\n\n=== Swarm grammars ===\nSwarm grammars are swarms of stochastic grammars that can be evolved to describe complex properties such as found in art and architecture. These grammars interact as agents behaving according to rules of swarm intelligence. Such behavior can also suggest deep learning algorithms, in particular when mapping of such swarms to neural circuits is considered.\n\n\n=== Swarmic art ===\nIn a series of works, al-Rifaie et al. have successfully used two swarm intelligence algorithms—one mimicking the behaviour of one species of ants (Leptothorax acervorum) foraging (stochastic diffusion search, SDS) and the other algorithm mimicking the behaviour of birds flocking (particle swarm optimization, PSO)—to describe a novel integration strategy exploiting the local search properties of the PSO with global SDS behaviour. The resulting hybrid algorithm is used to sketch novel drawings of an input image, exploiting an artistic tension between the local behaviour of the \'birds flocking\'—as they seek to follow the input sketch—and the global behaviour of the "ants foraging"—as they seek to encourage the flock to explore novel regions of the canvas. The "creativity" of this hybrid swarm system has been analysed under the philosophical light of the "rhizome" in the context of Deleuze\'s "Orchid and Wasp" metaphor.\nA more recent work of al-Rifaie et al., "Swarmic Sketches and Attention Mechanism", introduces a novel approach deploying the mechanism of \'attention\' by adapting SDS to selectively attend to detailed areas of a digital canvas. Once the attention of the swarm is drawn to a certain line within the canvas, the capability of PSO is used to produce a \'swarmic sketch\' of the attended line. The swarms move throughout the digital canvas in an attempt to satisfy their dynamic roles—attention to areas with more details—associated with them via their fitness function. Having associated the rendering process with the concepts of attention, the performance of the participating swarms creates a unique, non-identical sketch each time the \'artist\' swarms embark on interpreting the input line drawings. In other works, while PSO is responsible for the sketching process, SDS controls the attention of the swarm.\nIn a similar work, "Swarmic Paintings and Colour Attention", non-photorealistic images are produced using SDS algorithm which, in the context of this work, is responsible for colour attention.\nThe "computational creativity" of the above-mentioned systems are discussed in through the two prerequisites of creativity (i.e. freedom and constraints) within the swarm intelligence\'s two infamous phases of exploration and exploitation.\nMichael Theodore and Nikolaus Correll use swarm intelligent art installation to explore what it takes to have engineered systems to appear lifelike.\n\n\n== Notable researchers ==\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nBonabeau, Eric; Dorigo, Marco; Theraulaz, Guy (1999). Swarm Intelligence: From Natural to Artificial Systems. Oup USA. ISBN 978-0-19-513159-8.\nKennedy, James; Eberhart, Russell C. (2001-04-09). Swarm Intelligence. Morgan Kaufmann. ISBN 978-1-55860-595-4.\nEngelbrecht, Andries (2005-12-16). Fundamentals of Computational Swarm Intelligence. Wiley & Sons. ISBN 978-0-470-09191-3.\n\n\n== External links ==\n\nMarco Dorigo and Mauro Birattari (2007). "Swarm intelligence" in Scholarpedia\nAntoinette Brown.  Swarm Intelligence',
  },
  {
    title: "Evolutionary computation",
    originalContent:
      "In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.\nIn evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes as well as, depending on the method, mixing parental information. In biological terminology, a population of solutions is subjected to natural selection (or artificial selection), mutation and possibly recombination. As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm.\nEvolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures. Evolutionary computation is also sometimes used in evolutionary biology as an in silico experimental procedure to study common aspects of general evolutionary processes.\n\n\n== History ==\nThe concept of mimicking evolutionary processes to solve problems originates before the advent of computers, such as when Alan Turing proposed a method of genetic search in 1948 . Turing's B-type u-machines resemble primitive neural networks, and connections between neurons were learnt via a sort of genetic algorithm. His P-type u-machines resemble a method for reinforcement learning, where pleasure and pain signals direct the machine to learn certain behaviors. However, Turing's paper went unpublished until 1968, and he died in 1954, so this early work had little to no effect on the field of evolutionary computation that was to develop.\nEvolutionary computing as a field began in earnest in the 1950s and 1960s. There were several independent attempts to use the process of evolution in computing at this time, which developed separately for roughly 15 years. Three branches emerged in different places to attain this goal: evolution strategies, evolutionary programming, and genetic algorithms. A fourth branch, genetic programming, eventually emerged in the early 1990s. These approaches differ in the method of selection, the permitted mutations, and the representation of genetic data. By the 1990s, the distinctions between the historic branches had begun to blur, and the term 'evolutionary computing' was  coined in 1991 to denote a field that exists over all four paradigms.\nIn 1962, Lawrence J. Fogel initiated the research of Evolutionary Programming in the United States, which was considered an artificial intelligence endeavor. In this system, finite state machines are used to solve a prediction problem: these machines would be mutated (adding or deleting states, or changing the state transition rules), and the best of these mutated machines would be evolved further in future generations. The final finite state machine may be used to generate predictions when needed. The evolutionary programming method was successfully applied to prediction problems, system identification, and automatic control. It was eventually extended to handle time series data and to model the evolution of gaming strategies.\nIn 1964, Ingo Rechenberg and Hans-Paul Schwefel introduce the paradigm of evolution strategies in Germany. Since traditional gradient descent techniques produce results that may get stuck in local minima, Rechenberg and Schwefel proposed that random mutations (applied to all parameters of some solution vector) may be used to escape these minima. Child solutions were generated from parent solutions, and the more successful of the two was kept for future generations. This technique was first used by the two to successfully solve optimization problems in fluid dynamics. Initially, this optimization technique was performed without computers, instead relying on dice to determine random mutations. By 1965, the calculations were performed wholly by machine.\nJohn Henry Holland introduced genetic algorithms in the 1960s, and it was further developed at the University of Michigan in the 1970s. While the other approaches were focused on solving problems, Holland primarily aimed to use genetic algorithms to study adaptation and determine how it may be simulated. Populations of chromosomes, represented as bit strings, were transformed by an artificial selection process, selecting for specific 'allele' bits in the bit string. Among other mutation methods, interactions between chromosomes were used to simulate the recombination of DNA between different organisms. While previous methods only tracked a single optimal organism at a time (having children compete with parents), Holland's genetic algorithms tracked large populations (having many organisms compete each generation).\nBy the 1990s, a new approach to evolutionary computation that came to be called genetic programming emerged, advocated for by John Koza among others. In this class of algorithms, the subject of evolution was itself a program written in a high-level programming language (there had been some previous attempts as early as 1958 to use machine code, but they met with little success). For Koza, the programs were Lisp S-expressions, which can be thought of as trees of sub-expressions. This representation permits programs to swap subtrees, representing a sort of genetic mixing. Programs are scored based on how well they complete a certain task, and the score is used for artificial selection. Sequence induction, pattern recognition, and planning were all successful applications of the genetic programming paradigm.\nMany other figures played a role in the history of evolutionary computing, although their work did not always fit into one of the major historical branches of the field. The earliest computational simulations of evolution using evolutionary algorithms and artificial life techniques were performed by Nils Aall Barricelli in 1953, with first results published in 1954. Another pioneer in the 1950s was Alex Fraser, who published a series of papers on simulation of artificial selection. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimize the design of systems.\n\n\n== Techniques ==\nEvolutionary computing techniques mostly involve metaheuristic optimization algorithms. Broadly speaking, the field includes:\n\nAgent-based modeling\nAnt colony optimization\nArtificial immune systems\nArtificial life (also see digital organism)\nCultural algorithms\nCoevolutionary algorithm\nDifferential evolution\nDual-phase evolution\nEstimation of distribution algorithm\nEvolutionary algorithm\nEvolutionary programming\nEvolution strategy\nGene expression programming\nGenetic algorithm\nGenetic programming\nGrammatical evolution\nLearnable evolution model\nLearning classifier system\nMemetic algorithms\nNeuroevolution\nParticle swarm optimization\nBeetle antennae search\nSelf-organization such as self-organizing maps, competitive learning\nSwarm intelligence\nA thorough catalogue with many other recently proposed algorithms has been published in the Evolutionary Computation Bestiary. It is important to note that many recent algorithms, however, have poor experimental validation.\n\n\n== Evolutionary algorithms ==\n\nEvolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction, mutation, recombination, natural selection and survival of the fittest. Candidate solutions to the optimization problem play the role of individuals in a population, and the cost function determines the environment within which the solutions \"live\" (see also fitness function). Evolution of the population then takes place after the repeated application of the above operators.\nIn this process, there are two main forces that form the basis of evolutionary systems:  Recombination (e.g. crossover) and mutation create the necessary diversity and thereby facilitate novelty, while selection acts as a force increasing quality.\nMany aspects of such an evolutionary process are stochastic. Changed pieces of information due to recombination and mutation are randomly chosen. On the other hand, selection operators can be either deterministic, or stochastic. In the latter case, individuals with a higher fitness have a higher chance to be selected than individuals with a lower fitness, but typically even the weak individuals have a chance to become a parent or to survive.\n\n\n== Evolutionary algorithms and biology ==\n\nGenetic algorithms deliver methods to model biological systems and systems biology that are linked to the theory of dynamical systems, since they are used to predict the future states of the system. This is just a vivid (but perhaps misleading) way of drawing attention to the orderly, well-controlled and highly structured character of development in biology.\nHowever, the use of algorithms and informatics, in particular of computational theory, beyond the analogy to dynamical systems, is also relevant to understand evolution itself.\nThis view has the merit of recognizing that there is no central control of development; organisms develop as a result of local interactions within and between cells. The most promising ideas about program-development parallels seem to us to be ones that point to an apparently close analogy between processes within cells, and the low-level operation of modern computers. Thus, biological systems are like computational machines that process input information to compute next states, such that biological systems are closer to a computation than classical dynamical system.\nFurthermore, following concepts from computational theory, micro processes in biological organisms are fundamentally incomplete and undecidable (completeness (logic)), implying that “there is more than a crude metaphor behind the analogy between cells and computers.\nThe analogy to computation extends also to the relationship between inheritance systems and biological structure, which is often thought to reveal one of the most pressing problems in explaining the origins of life.\nEvolutionary automata, a generalization of Evolutionary Turing machines, have been introduced in order to investigate more precisely properties of biological and evolutionary computation. In particular, they allow to obtain new results on expressiveness of evolutionary computation. This confirms the initial result about undecidability of natural evolution and evolutionary algorithms and processes. Evolutionary finite automata, the simplest subclass of Evolutionary automata working in terminal mode can accept arbitrary languages over a given alphabet, including non-recursively enumerable (e.g., diagonalization language) and recursively enumerable but not recursive languages (e.g., language of the universal Turing machine).\n\n\n== Notable practitioners ==\nThe list of active researchers is naturally dynamic and non-exhaustive. A network analysis of the community was published in 2007.\n\nKalyanmoy Deb\nKenneth A De Jong\nPeter J. Fleming\nDavid B. Fogel\nStephanie Forrest\nDavid E. Goldberg\nJohn Henry Holland\nTheo Jansen\nJohn Koza\nZbigniew Michalewicz\nMelanie Mitchell\nPeter Nordin\nRiccardo Poli\nIngo Rechenberg\nHans-Paul Schwefel\n\n\n== Journals ==\nWhile articles on or using evolutionary computation permeate the literature, several journals are dedicated to evolutionary computation:\n\nEvolutionary Computation (journal) (founded 1993)\nArtificial Life (journal) (founded 1993)\nIEEE Transactions on Evolutionary Computation (founded 1997)\nGenetic Programming and Evolvable Machines (founded 2000)\nSwarm Intelligence (founded 2007)\nEvolutionary Intelligence (founded 2008)\nJournal of Artificial Evolution and Applications  (2008-2010)\nMemetic Computing (founded 2009)\nInternational Journal of Applied Evolutionary Computation (founded 2010)\nSwarm and Evolutionary Computation (founded 2011)\nInternational Journal of Swarm Intelligence and Evolutionary Computation (founded 2012)\n\n\n== Conferences ==\nThe main conferences in the evolutionary computation area include \n\nACM Genetic and Evolutionary Computation Conference (GECCO),\nIEEE Congress on Evolutionary Computation (CEC),\nEvoStar, which comprises four conferences: EuroGP, EvoApplications, EvoCOP and EvoMUSART,\nParallel Problem Solving from Nature (PPSN).\n\n\n== See also ==\n\n\n== External links ==\nArticle in the Stanford Encyclopedia of Philosophy about Biological Information (English)\n\n\n== Bibliography ==\nTh. Bäck, D.B. Fogel, and Z. Michalewicz (Editors), Handbook of Evolutionary Computation, 1997, ISBN 0750303921\nTh. Bäck and H.-P. Schwefel. An overview of evolutionary algorithms for parameter optimization. Archived July 12, 2018, at the Wayback Machine Evolutionary Computation, 1(1):1–23, 1993.\nW. Banzhaf, P. Nordin, R.E. Keller, and F.D. Francone. Genetic Programming — An Introduction. Morgan Kaufmann, 1998.\nS. Cagnoni, et al., Real-World Applications of Evolutionary Computing, Springer-Verlag Lecture Notes in Computer Science, Berlin, 2000.\nR. Chiong, Th. Weise, Z. Michalewicz (Editors), Variants of Evolutionary Algorithms for Real-World Applications, Springer, 2012, ISBN 3642234232\nK. A. De Jong, Evolutionary computation: a unified approach. MIT Press, Cambridge MA, 2006\nA. E. Eiben and J.E. Smith, From evolutionary computation to the evolution of things, Nature, 521:476-482, doi:10.1038/nature14544, 2015\nA. E. Eiben and J.E. Smith, Introduction to Evolutionary Computing, Springer, First edition, 2003; Second edition, 2015\nD. B. Fogel. Evolutionary Computation. Toward a New Philosophy of Machine Intelligence. IEEE Press, Piscataway, NJ, 1995.\nL. J. Fogel, A. J. Owens, and M. J. Walsh. Artificial Intelligence through Simulated Evolution. New York: John Wiley, 1966.\nD. E. Goldberg. Genetic algorithms in search, optimization and machine learning. Addison Wesley, 1989.\nJ. H. Holland. Adaptation in natural and artificial systems. University of Michigan Press, Ann Arbor, 1975.\nP. Hingston, L. Barone, and Z. Michalewicz (Editors), Design by Evolution, Natural Computing Series, 2008, Springer, ISBN 3540741097\nJ. R. Koza. Genetic Programming: On the Programming of Computers by means of Natural Evolution. MIT Press, Massachusetts, 1992.\nF.J. Lobo, C.F. Lima, Z. Michalewicz (Editors), Parameter Setting in Evolutionary Algorithms, Springer, 2010, ISBN 3642088929\nZ. Michalewicz, Genetic Algorithms + Data Structures – Evolution Programs, 1996, Springer, ISBN 3540606769\nZ. Michalewicz and D.B. Fogel, How to Solve It: Modern Heuristics, Springer, 2004, ISBN 978-3-540-22494-5\nI. Rechenberg. Evolutionstrategie: Optimierung Technischer Systeme nach Prinzipien des Biologischen Evolution. Fromman-Hozlboog Verlag, Stuttgart, 1973. (in German)\nH.-P. Schwefel. Numerical Optimization of Computer Models. John Wiley & Sons, New-York, 1981. 1995 – 2nd edition.\nD. Simon. Evolutionary Optimization Algorithms Archived March 10, 2014, at the Wayback Machine. Wiley, 2013.\nM. Sipper; W. Fu; K. Ahuja; J. H. Moore (2018). \"Investigating the parameter space of evolutionary algorithms\". BioData Mining. 11: 2. doi:10.1186/s13040-018-0164-x. PMC 5816380. PMID 29467825.\nY. Zhang; S. Li. (2017). \"PSA: A novel optimization algorithm based on survival rules of porcellio scaber\". arXiv:1709.09840 [cs.NE].\n\n\n== References ==",
  },
  {
    title: "Black hole thermodynamics",
    originalContent:
      'In physics, black hole thermodynamics is the area of study that seeks to reconcile the laws of thermodynamics with the existence of black hole event horizons.  As the study of the statistical mechanics of black-body radiation led to the development of the theory of quantum mechanics, the effort to understand the statistical mechanics of black holes has had a deep impact upon the understanding of quantum gravity, leading to the formulation of the holographic principle.\n\n\n== Overview ==\nThe second law of thermodynamics requires that black holes have entropy. If black holes carried no entropy, it would be possible to violate the second law by throwing mass into the black hole. The increase of the entropy of the black hole more than compensates for the decrease of the entropy carried by the object that was swallowed.\nIn 1972, Jacob Bekenstein conjectured that black holes should have an entropy proportional to the area of the event horizon, where by the same year, he proposed no-hair theorems.\nIn 1973 Bekenstein suggested \n  \n    \n      \n        \n          \n            \n              ln\n              ⁡\n              \n                2\n              \n            \n            \n              0.8\n              π\n            \n          \n        \n        ≈\n        0.276\n      \n    \n    {\\displaystyle {\\frac {\\ln {2}}{0.8\\pi }}\\approx 0.276}\n  \n as the constant of proportionality, asserting that if the constant was not exactly this, it must be very close to it. The next year, in 1974, Stephen Hawking showed that black holes emit thermal Hawking radiation corresponding to a certain temperature (Hawking temperature). Using the thermodynamic relationship between energy, temperature and entropy, Hawking was able to confirm Bekenstein\'s conjecture and fix the constant of proportionality at \n  \n    \n      \n        1\n        \n          /\n        \n        4\n      \n    \n    {\\displaystyle 1/4}\n  \n:\n\n  \n    \n      \n        \n          S\n          \n            BH\n          \n        \n        =\n        \n          \n            \n              \n                k\n                \n                  B\n                \n              \n              A\n            \n            \n              4\n              \n                ℓ\n                \n                  P\n                \n                \n                  2\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle S_{\\text{BH}}={\\frac {k_{\\text{B}}A}{4\\ell _{\\text{P}}^{2}}},}\n  \n\nwhere \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n is the area of the event horizon, \n  \n    \n      \n        \n          k\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle k_{\\text{B}}}\n  \n is the Boltzmann constant, and \n  \n    \n      \n        \n          ℓ\n          \n            P\n          \n        \n        =\n        \n          \n            G\n            ℏ\n            \n              /\n            \n            \n              c\n              \n                3\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{P}}={\\sqrt {G\\hbar /c^{3}}}}\n  \n is the Planck length. \nThis is often referred to as the Bekenstein–Hawking formula. The subscript BH either stands for "black hole" or "Bekenstein–Hawking". The black hole entropy is proportional to the area of its event horizon \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n. The fact that the black hole entropy is also the maximal entropy that can be obtained by the Bekenstein bound (wherein the Bekenstein bound becomes an equality) was the main observation that led to the holographic principle. This area relationship was generalized to arbitrary regions via the Ryu–Takayanagi formula, which relates the entanglement entropy of a boundary conformal field theory to a specific surface in its dual gravitational theory.\nAlthough Hawking\'s calculations gave further thermodynamic evidence for black hole entropy, until 1995 no one was able to make a controlled calculation of black hole entropy based on statistical mechanics, which associates entropy with a large number of microstates.  In fact, so called "no-hair" theorems appeared to suggest that black holes could have only a single microstate.  The situation changed in 1995 when Andrew Strominger and Cumrun Vafa calculated the right Bekenstein–Hawking entropy of a supersymmetric black hole in string theory, using methods based on D-branes and string duality. Their calculation was followed by many similar computations of entropy of large classes of other extremal and near-extremal black holes, and the result always agreed with the Bekenstein–Hawking formula. However, for the Schwarzschild black hole, viewed as the most far-from-extremal black hole, the relationship between micro- and macrostates has not been characterized. Efforts to develop an adequate answer within the framework of string theory continue.\nIn loop quantum gravity (LQG) it is possible to associate a geometrical interpretation with the microstates: these are the quantum geometries of the horizon. LQG offers a geometric explanation of the finiteness of the entropy and of the proportionality of the area of the horizon.  It is possible to derive, from the covariant formulation of full quantum theory (spinfoam) the correct relation between energy and area (1st law), the Unruh temperature and the distribution that yields Hawking entropy. The calculation makes use of the notion of dynamical horizon and is done for non-extremal black holes. There seems to be also discussed the calculation of Bekenstein–Hawking entropy from the point of view of loop quantum gravity. The current accepted microstate ensemble for black holes is the microcanonical ensemble. The partition function for black holes results in a negative heat capacity. In canonical ensembles, there is limitation for a positive heat capacity, whereas microcanonical ensembles can exist at a negative heat capacity.\n\n\n== The laws of black hole mechanics ==\nThe four laws of black hole mechanics are physical properties that black holes are believed to satisfy. The laws, analogous to the laws of thermodynamics, were discovered by Jacob Bekenstein, Brandon Carter, and James Bardeen. Further considerations were made by Stephen Hawking.\n\n\n=== Statement of the laws ===\nThe laws of black hole mechanics are expressed in geometrized units.\n\n\n==== The zeroth law ====\nThe horizon has constant surface gravity for a stationary black hole.\n\n\n==== The first law ====\nFor perturbations of stationary black holes, the change of energy is related to change of area, angular momentum, and electric charge by\n\n  \n    \n      \n        d\n        E\n        =\n        \n          \n            κ\n            \n              8\n              π\n            \n          \n        \n        \n        d\n        A\n        +\n        Ω\n        \n        d\n        J\n        +\n        Φ\n        \n        d\n        Q\n        ,\n      \n    \n    {\\displaystyle dE={\\frac {\\kappa }{8\\pi }}\\,dA+\\Omega \\,dJ+\\Phi \\,dQ,}\n  \n\nwhere \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is the energy, \n  \n    \n      \n        κ\n      \n    \n    {\\displaystyle \\kappa }\n  \n is the surface gravity, \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n is the horizon area, \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n is the angular velocity, \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  \n is the angular momentum, \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n is the electrostatic potential and \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n is the electric charge.\n\n\n==== The second law ====\nThe horizon area is, assuming the weak energy condition, a non-decreasing function of time:\n\n  \n    \n      \n        \n          \n            \n              d\n              A\n            \n            \n              d\n              t\n            \n          \n        \n        ≥\n        0.\n      \n    \n    {\\displaystyle {\\frac {dA}{dt}}\\geq 0.}\n  \n\nThis "law" was superseded by Hawking\'s discovery that black holes radiate, which causes both the black hole\'s mass and the area of its horizon to decrease over time.\n\n\n==== The third law ====\nIt is not possible to form a black hole with vanishing surface gravity. That is, \n  \n    \n      \n        κ\n        =\n        0\n      \n    \n    {\\displaystyle \\kappa =0}\n  \n cannot be achieved.\n\n\n=== Discussion of the laws ===\n\n\n==== The zeroth law ====\nThe zeroth law is analogous to the zeroth law of thermodynamics, which states that the temperature is constant throughout a body in thermal equilibrium.  It suggests that the surface gravity is analogous to temperature. T constant for thermal equilibrium for a normal system is analogous to \n  \n    \n      \n        κ\n      \n    \n    {\\displaystyle \\kappa }\n  \n constant over the horizon of a stationary black hole.\n\n\n==== The first law ====\nThe left side, \n  \n    \n      \n        d\n        E\n      \n    \n    {\\displaystyle dE}\n  \n, is the change in energy (proportional to mass).  Although the first term does not have an immediately obvious physical interpretation, the second and third terms on the right side represent changes in energy due to rotation and electromagnetism. Analogously, the first law of thermodynamics is a statement of energy conservation, which contains on its right side the term \n  \n    \n      \n        T\n        d\n        S\n      \n    \n    {\\displaystyle TdS}\n  \n.\n\n\n==== The second law ====\nThe second law is the statement of Hawking\'s area theorem.  Analogously, the second law of thermodynamics states that the change in entropy in an isolated system will be greater than or equal to 0 for a spontaneous process, suggesting a link between entropy and the area of a black hole horizon. However, this version violates the second law of thermodynamics by matter losing (its) entropy as it falls in, giving a decrease in entropy. However, generalizing the second law as the sum of black hole entropy and outside entropy, shows that the second law of thermodynamics is not violated in a system including the universe beyond the horizon.\nThe generalized second law of thermodynamics (GSL) was needed to present the second law of thermodynamics as valid. This is because the second law of thermodynamics, as a result of the disappearance of entropy near the exterior of black holes, is not useful. The GSL allows for the application of the law because now the measurement of interior, common entropy is possible. The validity of the GSL can be established by studying an example, such as looking at a system having entropy that falls into a bigger, non-moving black hole, and establishing upper and lower entropy bounds for the increase in the black hole entropy and entropy of the system, respectively. One should also note that the GSL will hold for theories of gravity such as Einstein gravity, Lovelock gravity, or Braneworld gravity, because the conditions to use GSL for these can be met.\nHowever, on the topic of black hole formation, the question becomes whether or not the generalized second law of thermodynamics will be valid, and if it is, it will have been proved valid for all situations. Because a black hole formation is not stationary, but instead moving, proving that the GSL holds is difficult. Proving the GSL is generally valid would require using quantum-statistical mechanics, because the GSL is both a quantum and statistical law. This discipline does not exist so the GSL can be assumed to be useful in general, as well as for prediction. For example, one can use the GSL to predict that, for a cold, non-rotating assembly of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n nucleons, \n  \n    \n      \n        \n          S\n          \n            B\n            H\n          \n        \n        −\n        S\n        >\n        0\n      \n    \n    {\\displaystyle S_{BH}-S>0}\n  \n, where \n  \n    \n      \n        \n          S\n          \n            B\n            H\n          \n        \n      \n    \n    {\\displaystyle S_{BH}}\n  \n is the entropy of a black hole and \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is the sum of the ordinary entropy.\n\n\n==== The third law ====\nThe third law of black hole thermodynamics is controversial. Specific counterexamples called \t\nextremal black holes fail to obey the rule. The classical \nthird law of thermodynamics, known as the  Nernst theorem, which says the entropy of a system must go to zero as the temperature goes to absolute zero is also not a universal law.\nHowever the systems that fail the classical third law have not been realized in practice, leading to the suggestion that the extremal black holes may not represent the physics of black holes generally.\nA weaker form of the classical third law known as the "unattainability principle" states that an infinite number of steps are required to put a system in to its ground state. This form of the third law does have an analog in black hole physics.: 10 \n\n\n=== Interpretation of the laws ===\nThe four laws of black hole mechanics suggest that one should identify the surface gravity of a black hole with temperature and the area of the event horizon with entropy, at least up to some multiplicative constants.  If one only considers black holes classically, then they have zero temperature and, by the no-hair theorem, zero entropy, and the laws of black hole mechanics remain an analogy.  However, when quantum-mechanical effects are taken into account, one finds that black holes emit thermal radiation (Hawking radiation) at a temperature\n\n  \n    \n      \n        \n          T\n          \n            H\n          \n        \n        =\n        \n          \n            κ\n            \n              2\n              π\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle T_{\\text{H}}={\\frac {\\kappa }{2\\pi }}.}\n  \n\nFrom the first law of black hole mechanics, this determines the multiplicative constant of the Bekenstein–Hawking entropy, which is (in geometrized units)\n\n  \n    \n      \n        \n          S\n          \n            BH\n          \n        \n        =\n        \n          \n            A\n            4\n          \n        \n        .\n      \n    \n    {\\displaystyle S_{\\text{BH}}={\\frac {A}{4}}.}\n  \n\nwhich is the entropy of the black hole in Einstein\'s general relativity. Quantum field theory in curved spacetime can be utilized to calculate the entropy for a black hole in any covariant theory for gravity, known as the Wald entropy.\n\n\n== Critique ==\nWhile black hole thermodynamics (BHT) has been regarded as one of the deepest clues to a quantum theory of gravity, there remain a philosophical criticism that "the analogy is not nearly as good as is commonly supposed", that it “is often based on a kind of caricature of thermodynamics” and "it’s unclear what the systems in BHT are supposed to be".\nThese criticisms where reexamined in detail, ending with the opposite conclusion, "stationary black holes are not analogous to thermodynamic systems: they are thermodynamic systems, in the fullest sense."\n\n\n== Beyond black holes ==\nGary Gibbons and Hawking have shown that black hole thermodynamics is more general than black holes—that cosmological event horizons also have an entropy and temperature.\nMore fundamentally, Gerard \'t Hooft and Leonard Susskind used the laws of black hole thermodynamics to argue for a general holographic principle of nature, which asserts that consistent theories of gravity and quantum mechanics must be lower-dimensional.  Though not yet fully understood in general, the holographic principle is central to theories like the AdS/CFT correspondence.\nThere are also connections between black hole entropy and fluid surface tension.\n\n\n== See also ==\nJoseph Polchinski\nRobert Wald\n\n\n== Notes ==\n\n\n== Citations ==\n\n\n== Bibliography ==\nBardeen, J. M.; Carter, B.; Hawking, S. W. (1973). "The four laws of black hole mechanics". Communications in Mathematical Physics. 31 (2): 161–170. Bibcode:1973CMaPh..31..161B. doi:10.1007/BF01645742. S2CID 54690354.\nBekenstein, Jacob D. (April 1973). "Black holes and entropy". Physical Review D. 7 (8): 2333–2346. Bibcode:1973PhRvD...7.2333B. doi:10.1103/PhysRevD.7.2333. S2CID 122636624.\nHawking, Stephen W. (1974). "Black hole explosions?". Nature. 248 (5443): 30–31. Bibcode:1974Natur.248...30H. doi:10.1038/248030a0. S2CID 4290107.\nHawking, Stephen W. (1975). "Particle creation by black holes". Communications in Mathematical Physics. 43 (3): 199–220. Bibcode:1975CMaPh..43..199H. doi:10.1007/BF02345020. S2CID 55539246.\nHawking, S. W.; Ellis, G. F. R. (1973). The Large Scale Structure of Space–Time. New York: Cambridge University Press. ISBN 978-0-521-09906-6.\nHawking, Stephen W. (1994). "The Nature of Space and Time". arXiv:hep-th/9409195.\n\'t Hooft, Gerardus (1985). "On the quantum structure of a black hole" (PDF). Nuclear Physics B. 256: 727–745. Bibcode:1985NuPhB.256..727T. doi:10.1016/0550-3213(85)90418-3. Archived from the original (PDF) on 2011-09-26.\nPage, Don (2005). "Hawking Radiation and Black Hole Thermodynamics". New Journal of Physics. 7 (1): 203. arXiv:hep-th/0409024. Bibcode:2005NJPh....7..203P. doi:10.1088/1367-2630/7/1/203. S2CID 119047329.\n\n\n== External links ==\nBekenstein-Hawking entropy on Scholarpedia\nBlack Hole Thermodynamics\nBlack hole entropy on arxiv.org',
  },
  {
    title: "Electromagnetism",
    originalContent:
      "In physics, electromagnetism is an interaction that occurs between particles with electric charge via electromagnetic fields. The electromagnetic force is one of the four fundamental forces of nature. It is the dominant force in the interactions of atoms and molecules. Electromagnetism can be thought of as a combination of electrostatics and magnetism, which are distinct but closely intertwined phenomena. Electromagnetic forces occur between any two charged particles. Electric forces cause an attraction between particles with opposite charges and repulsion between particles with the same charge, while magnetism is an interaction that occurs between charged particles in relative motion. These two forces are described in terms of electromagnetic fields. Macroscopic charged objects are described in terms of Coulomb's law for electricity and Ampère's force law for magnetism; the Lorentz force describes microscopic charged particles.\nThe electromagnetic force is responsible for many of the chemical and physical phenomena observed in daily life. The electrostatic attraction between atomic nuclei and their electrons holds atoms together. Electric forces also allow different atoms to combine into molecules, including the macromolecules such as proteins that form the basis of life. Meanwhile, magnetic interactions between the spin and angular momentum magnetic moments of electrons also play a role in chemical reactivity; such relationships are studied in spin chemistry. Electromagnetism also plays several crucial roles in modern technology: electrical energy production, transformation and distribution; light, heat, and sound production and detection; fiber optic and wireless communication; sensors; computation; electrolysis; electroplating; and mechanical motors and actuators.\nElectromagnetism has been studied since ancient times. Many ancient civilizations, including the Greeks and the Mayans, created wide-ranging theories to explain lightning, static electricity, and the attraction between magnetized pieces of iron ore. However, it was not until the late 18th century that scientists began to develop a mathematical basis for understanding the nature of electromagnetic interactions. In the 18th and 19th centuries, prominent scientists and mathematicians such as Coulomb, Gauss and Faraday developed namesake laws which helped to explain the formation and interaction of electromagnetic fields. This process culminated in the 1860s with the discovery of Maxwell's equations, a set of four partial differential equations which provide a complete description of classical electromagnetic fields. Maxwell's equations provided a sound mathematical basis for the relationships between electricity and magnetism that scientists had been exploring for centuries, and predicted the existence of self-sustaining electromagnetic waves. Maxwell postulated that such waves make up visible light, which was later shown to be true. Gamma-rays, x-rays, ultraviolet, visible, infrared radiation, microwaves and radio waves were all determined to be electromagnetic radiation differing only in their range of frequencies.\nIn the modern era, scientists continue to refine the theory of electromagnetism to account for the effects of modern physics, including quantum mechanics and relativity. The theoretical implications of electromagnetism, particularly the requirement that observations remain consistent when viewed from various moving frames of reference (relativistic electromagnetism) and the establishment of the speed of light based on properties of the medium of propagation (permeability and permittivity), helped inspire Einstein's theory of special relativity in 1905. Quantum electrodynamics (QED) modifies Maxwell's equations to be consistent with the quantized nature of matter. In QED, changes in the electromagnetic field are expressed in terms of discrete excitations, particles known as photons, the quanta of light.\n\n\n== History ==\n\n\n=== Ancient world ===\nInvestigation into electromagnetic phenomena began about 5,000 years ago. There is evidence that the ancient Chinese, Mayan, and potentially even Egyptian civilizations knew that the naturally magnetic mineral magnetite had attractive properties, and many incorporated it into their art and architecture. Ancient people were also aware of lightning and static electricity, although they had no idea of the mechanisms behind these phenomena. The Greek philosopher Thales of Miletus discovered around 600 B.C.E. that amber could acquire an electric charge when it was rubbed with cloth, which allowed it to pick up light objects such as pieces of straw. Thales also experimented with the ability of magnetic rocks to attract one other, and hypothesized that this phenomenon might be connected to the attractive power of amber, foreshadowing the deep connections between electricity and magnetism that would be discovered over 2,000 years later. Despite all this investigation, ancient civilizations had no understanding of the mathematical basis of electromagnetism, and often analyzed its impacts through the lens of religion rather than science (lightning, for instance, was considered to be a creation of the gods in many cultures).\n\n\n=== 19th century ===\n\nElectricity and magnetism were originally considered to be two separate forces. This view changed with the publication of James Clerk Maxwell's 1873 A Treatise on Electricity and Magnetism in which the interactions of positive and negative charges were shown to be mediated by one force. There are four main effects resulting from these interactions, all of which have been clearly demonstrated by experiments:\n\nElectric charges attract or repel one another with a force inversely proportional to the square of the distance between them: opposite charges attract, like charges repel.\nMagnetic poles (or states of polarization at individual points) attract or repel one another in a manner similar to positive and negative charges and always exist as pairs: every north pole is yoked to a south pole.\nAn electric current inside a wire creates a corresponding circumferential magnetic field outside the wire. Its direction (clockwise or counter-clockwise) depends on the direction of the current in the wire.\nA current is induced in a loop of wire when it is moved toward or away from a magnetic field, or a magnet is moved towards or away from it; the direction of current depends on that of the movement.\nIn April 1820, Hans Christian Ørsted observed that an electrical current in a wire caused a nearby compass needle to move. At the time of discovery, Ørsted did not suggest any satisfactory explanation of the phenomenon, nor did he try to represent the phenomenon in a mathematical framework. However, three months later he began more intensive investigations. Soon thereafter he published his findings, proving that an electric current produces a magnetic field as it flows through a wire. The CGS unit of magnetic induction (oersted) is named in honor of his contributions to the field of electromagnetism.\nHis findings resulted in intensive research throughout the scientific community in electrodynamics. They influenced French physicist André-Marie Ampère's developments of a single mathematical form to represent the magnetic forces between current-carrying conductors. Ørsted's discovery also represented a major step toward a unified concept of energy.\nThis unification, which was observed by Michael Faraday, extended by James Clerk Maxwell, and partially reformulated by Oliver Heaviside and Heinrich Hertz, is one of the key accomplishments of 19th-century mathematical physics. It has had far-reaching consequences, one of which was the understanding of the nature of light. Unlike what was proposed by the electromagnetic theory of that time, light and other electromagnetic waves are at present seen as taking the form of quantized, self-propagating oscillatory electromagnetic field disturbances called photons. Different frequencies of oscillation give rise to the different forms of electromagnetic radiation, from radio waves at the lowest frequencies, to visible light at intermediate frequencies, to gamma rays at the highest frequencies.\nØrsted was not the only person to examine the relationship between electricity and magnetism. In 1802, Gian Domenico Romagnosi, an Italian legal scholar, deflected a magnetic needle using a Voltaic pile. The factual setup of the experiment is not completely clear, nor if current flowed across the needle or not. An account of the discovery was published in 1802 in an Italian newspaper, but it was largely overlooked by the contemporary scientific community, because Romagnosi seemingly did not belong to this community.\n\nAn earlier (1735), and often neglected, connection between electricity and magnetism was reported by a Dr. Cookson. The account stated:A tradesman at Wakefield in Yorkshire, having put up a great number of knives and forks in a large box ...  and having placed the box in the corner of a large room, there happened a sudden storm of thunder, lightning, &c.  ... The owner emptying the box on a counter where some nails lay, the persons who took up the knives, that lay on the nails, observed that the knives took up the nails. On this the whole number was tried, and found to do the same, and that, to such a degree as to take up large nails, packing needles, and other iron things of considerable weight ... E. T. Whittaker suggested in 1910 that this particular event was responsible for lightning to be \"credited with the power of magnetizing steel; and it was doubtless this which led Franklin in 1751 to attempt to magnetize a sewing-needle by means of the discharge of Leyden jars.\"\n\n\n== A fundamental force ==\n\nThe electromagnetic force is the second strongest of the four known fundamental forces and has unlimited range.\nAll other forces, known as non-fundamental forces. (e.g., friction, contact forces) are derived from the four fundamental forces. At high energy, the weak force and electromagnetic force are unified as a single interaction called the electroweak interaction.\nMost of the forces involved in interactions between atoms are explained by electromagnetic forces between electrically charged atomic nuclei and electrons. The electromagnetic force is also involved in all forms of chemical phenomena.\nElectromagnetism explains how materials carry momentum despite being composed of individual particles and empty space. The forces we experience when \"pushing\" or \"pulling\" ordinary material objects result from intermolecular forces between individual molecules in our bodies and in the objects.\nThe effective forces generated by the momentum of electrons' movement is a necessary part of understanding atomic and intermolecular interactions. As electrons move between interacting atoms, they carry momentum with them. As a collection of electrons becomes more confined, their minimum momentum necessarily increases due to the Pauli exclusion principle. The behavior of matter at the molecular scale, including its density, is determined by the balance between the electromagnetic force and the force generated by the exchange of momentum carried by the electrons themselves.\n\n\n== Classical electrodynamics ==\n\nIn 1600, William Gilbert proposed, in his De Magnete, that electricity and magnetism, while both capable of causing attraction and repulsion of objects, were distinct effects. Mariners had noticed that lightning strikes had the ability to disturb a compass needle. The link between lightning and electricity was not confirmed until Benjamin Franklin's proposed experiments in 1752 were conducted on 10 May 1752 by Thomas-François Dalibard of France using a 40-foot-tall (12 m) iron rod instead of a kite and he successfully extracted electrical sparks from a cloud.\nOne of the first to discover and publish a link between human-made electric current and magnetism was Gian Romagnosi, who in 1802 noticed that connecting a wire across a voltaic pile deflected a nearby compass needle. However, the effect did not become widely known until 1820, when Ørsted performed a similar experiment. Ørsted's work influenced Ampère to conduct further experiments, which eventually gave rise to a new area of physics: electrodynamics. By determining a force law for the interaction between elements of electric current, Ampère placed the subject on a solid mathematical foundation.\nA theory of electromagnetism, known as classical electromagnetism, was developed by several physicists during the period between 1820 and 1873, when James Clerk Maxwell's treatise was published, which unified previous developments into a single theory, proposing that light was an electromagnetic wave propagating in the luminiferous ether. In classical electromagnetism, the behavior of the electromagnetic field is described by a set of equations known as Maxwell's equations, and the electromagnetic force is given by the Lorentz force law.\nOne of the peculiarities of classical electromagnetism is that it is difficult to reconcile with classical mechanics, but it is compatible with special relativity. According to Maxwell's equations, the speed of light in vacuum is a universal constant that is dependent only on the electrical permittivity and magnetic permeability of free space. This violates Galilean invariance, a long-standing cornerstone of classical mechanics. One way to reconcile the two theories (electromagnetism and classical mechanics) is to assume the existence of a luminiferous aether through which the light propagates. However, subsequent experimental efforts failed to detect the presence of the aether. After important contributions of Hendrik Lorentz and Henri Poincaré, in 1905, Albert Einstein solved the problem with the introduction of special relativity, which replaced classical kinematics with a new theory of kinematics compatible with classical electromagnetism. (For more information, see History of special relativity.)\nIn addition, relativity theory implies that in moving frames of reference, a magnetic field transforms to a field with a nonzero electric component and conversely, a moving electric field transforms to a nonzero magnetic component, thus firmly showing that the phenomena are two sides of the same coin. Hence the term \"electromagnetism\". (For more information, see Classical electromagnetism and special relativity and Covariant formulation of classical electromagnetism.)\nToday few problems in electromagnetism remain unsolved. These include: the lack of magnetic monopoles, Abraham–Minkowski controversy, the location in space of the electromagnetic field energy, and the mechanism by which some organisms can sense electric and magnetic fields.\n\n\n== Extension to nonlinear phenomena ==\nThe Maxwell equations are linear, in that a change in the sources (the charges and currents) results in a proportional change of the fields. Nonlinear dynamics can occur when electromagnetic fields couple to matter that follows nonlinear dynamical laws. This is studied, for example, in the subject of magnetohydrodynamics, which combines Maxwell theory with the Navier–Stokes equations. Another branch of electromagnetism dealing with nonlinearity is nonlinear optics.\n\n\n== Quantities and units ==\n\nHere is a list of common units related to electromagnetism:\n\nIn the electromagnetic CGS system, electric current is a fundamental quantity defined via Ampère's law and takes the permeability as a dimensionless quantity (relative permeability) whose value in vacuum is unity. As a consequence, the square of the speed of light appears explicitly in some of the equations interrelating quantities in this system.\n\nFormulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on what system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit \"sub-systems\", including Gaussian, \"ESU\", \"EMU\", and Heaviside–Lorentz. Among these choices, Gaussian units are the most common today, and in fact the phrase \"CGS units\" is often used to refer specifically to CGS-Gaussian units.\n\n\n== Applications ==\nThe study of electromagnetism informs electric circuits, magnetic circuits, and semiconductor devices' construction.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n=== Web sources ===\n\n\n=== Textbooks ===\n\n\n=== General coverage ===\n\n\n== External links ==\n\nMagnetic Field Strength Converter\nElectromagnetic Force – from Eric Weisstein's World of Physics",
  },
  {
    title: "Nuclear physics",
    originalContent:
      'Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions, in addition to the study of other forms of nuclear matter.\nNuclear physics should not be confused with atomic physics, which studies the atom as a whole, including its electrons.\nDiscoveries in nuclear physics have led to applications in many fields. This includes nuclear power, nuclear weapons, nuclear medicine and magnetic resonance imaging, industrial and agricultural isotopes, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology. Such applications are studied in the field of nuclear engineering.\nParticle physics evolved out of nuclear physics and the two fields are typically taught in close association. Nuclear astrophysics, the application of nuclear physics to astrophysics, is crucial in explaining the inner workings of stars and the origin of the chemical elements.\n\n\n== History ==\n\nThe history of nuclear physics as a discipline distinct from atomic physics, starts with the discovery of radioactivity by Henri Becquerel in 1896, made while investigating phosphorescence in uranium salts. The discovery of the electron by J. J. Thomson a year later was an indication that the atom had internal structure. At the beginning of the 20th century the accepted model of the atom was J. J. Thomson\'s "plum pudding" model in which the atom was a positively charged ball with smaller negatively charged electrons embedded inside it.\nIn the years that followed, radioactivity was extensively investigated, notably by Marie Curie, a Polish physicist whose maiden name was Sklodowska, Pierre Curie, Ernest Rutherford and others. By the turn of the century, physicists had also discovered three types of radiation emanating from atoms, which they named alpha, beta, and gamma radiation. Experiments by Otto Hahn in 1911 and by James Chadwick in 1914 discovered that the beta decay spectrum was continuous rather than discrete. That is, electrons were ejected from the atom with a continuous range of energies, rather than the discrete amounts of energy that were observed in gamma and alpha decays. This was a problem for nuclear physics at the time, because it seemed to indicate that energy was not conserved in these decays.\nThe 1903 Nobel Prize in Physics was awarded jointly to Becquerel, for his discovery and to Marie and Pierre Curie for their subsequent research into radioactivity. Rutherford was awarded the Nobel Prize in Chemistry in 1908 for his "investigations into the disintegration of the elements and the chemistry of radioactive substances".\nIn 1905, Albert Einstein formulated the idea of mass–energy equivalence. While the work on radioactivity by Becquerel and Marie Curie predates this, an explanation of the source of the energy of radioactivity would have to wait for the discovery that the nucleus itself was composed of smaller constituents, the nucleons.\n\n\n=== Rutherford discovers the nucleus ===\nIn 1906, Ernest Rutherford published "Retardation of the α Particle from Radium in passing through matter." Hans Geiger expanded on this work in a communication to the Royal Society with experiments he and Rutherford had done, passing alpha particles through air, aluminum foil and gold leaf. More work was published in 1909 by Geiger and Ernest Marsden, and further greatly expanded work was published in 1910 by Geiger. In 1911–1912 Rutherford went before the Royal Society to explain the experiments and propound the new theory of the atomic nucleus as we now understand it.\nPublished in 1909, with the eventual classical analysis by Rutherford published May 1911, the key preemptive experiment was performed during 1909, at the University of Manchester. Ernest Rutherford\'s assistant, Professor  Johannes  "Hans" Geiger, and an undergraduate, Marsden, performed an experiment in which Geiger and Marsden under Rutherford\'s supervision fired alpha particles (helium 4 nuclei) at a thin film of gold foil. The plum pudding model had predicted that the alpha particles should come out of the foil with their trajectories being at most slightly bent. But Rutherford instructed his team to look for something that shocked him to observe: a few particles were scattered through large angles, even completely backwards in some cases. He likened it to firing a bullet at tissue paper and having it bounce off. The discovery, with Rutherford\'s analysis of the data in 1911, led to the Rutherford model of the atom, in which the atom had a very small, very dense nucleus containing most of its mass, and consisting of heavy positively charged particles with embedded electrons in order to balance out the charge (since the neutron was unknown). As an example, in this model (which is not the modern one) nitrogen-14 consisted of a nucleus with 14 protons and 7 electrons (21 total particles) and the nucleus was surrounded by 7 more orbiting electrons.\n\n\n=== Eddington and stellar nuclear fusion ===\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars. At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein\'s equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered.\n\n\n=== Studies of nuclear spin ===\nThe Rutherford model worked quite well until studies of nuclear spin were carried out by Franco Rasetti at the California Institute of Technology in 1929. By 1925 it was known that protons and electrons each had a spin of ±+1⁄2. In the Rutherford model of nitrogen-14, 20 of the total 21 nuclear particles should have paired up to cancel each other\'s spin, and the final odd particle should have left the nucleus with a net spin of 1⁄2. Rasetti discovered, however, that nitrogen-14 had a spin of 1.\n\n\n=== James Chadwick discovers the neutron ===\n\nIn 1932 Chadwick realized that radiation that had been observed by Walther Bothe, Herbert Becker, Irène and Frédéric Joliot-Curie was actually due to a neutral particle of about the same mass as the proton, that he called the neutron (following a suggestion from Rutherford about the need for such a particle). In the same year Dmitri Ivanenko suggested that there were no electrons in the nucleus — only protons and neutrons — and that neutrons were spin 1⁄2 particles, which explained the mass not due to protons. The neutron spin immediately solved the problem of the spin of nitrogen-14, as the one unpaired proton and one unpaired neutron in this model each contributed a spin of 1⁄2 in the same direction, giving a final total spin of 1.\nWith the discovery of the neutron, scientists could at last calculate what fraction of binding energy each nucleus had, by comparing the nuclear mass with that of the protons and neutrons which composed it. Differences between nuclear masses were calculated in this way. When nuclear reactions were measured, these were found to agree with Einstein\'s calculation of the equivalence of mass and energy to within 1% as of 1934.\n\n\n=== Proca\'s equations of the massive vector boson field ===\nAlexandru Proca was the first to develop and report the massive vector boson field equations and a theory of the mesonic field of nuclear forces. Proca\'s equations were known to Wolfgang Pauli who mentioned the equations in his Nobel address, and they were also known to Yukawa, Wentzel, Taketani, Sakata, Kemmer, Heitler, and Fröhlich who appreciated the content of Proca\'s equations for developing a theory of the atomic nuclei in Nuclear Physics.\n\n\n=== Yukawa\'s meson postulated to bind nuclei ===\nIn 1935 Hideki Yukawa proposed the first significant theory of the strong force to explain how the nucleus holds together. In the Yukawa interaction a virtual particle, later called a meson, mediated a force between all nucleons, including protons and neutrons. This force explained why nuclei did not disintegrate under the influence of proton repulsion, and it also gave an explanation of why the attractive strong force had a more limited range than the electromagnetic repulsion between protons. Later, the discovery of the pi meson showed it to have the properties of Yukawa\'s particle.\nWith Yukawa\'s papers, the modern model of the atom was complete. The center of the atom contains a tight ball of neutrons and protons, which is held together by the strong nuclear force, unless it is too large. Unstable nuclei may undergo alpha decay, in which they emit an energetic helium nucleus, or beta decay, in which they eject an electron (or positron). After one of these decays the resultant nucleus may be left in an excited state, and in this case it decays to its ground state by emitting high-energy photons (gamma decay).\nThe study of the strong and weak nuclear forces (the latter explained by Enrico Fermi via Fermi\'s interaction in 1934) led physicists to collide nuclei and electrons at ever higher energies. This research became the science of particle physics, the crown jewel of which is the standard model of particle physics, which describes the strong, weak, and electromagnetic forces.\n\n\n== Modern nuclear physics ==\n\nA heavy nucleus can contain hundreds of nucleons. This means that with some approximation it can be treated as a classical system, rather than a quantum-mechanical one. In the resulting liquid-drop model, the nucleus has an energy that arises partly from surface tension and partly from electrical repulsion of the protons. The liquid-drop model is able to reproduce many features of nuclei, including the general trend of binding energy with respect to mass number, as well as the phenomenon of nuclear fission.\nSuperimposed on this classical picture, however, are quantum-mechanical effects, which can be described using the nuclear shell model, developed in large part by Maria Goeppert Mayer and J. Hans D. Jensen. Nuclei with certain "magic" numbers of neutrons and protons are particularly stable, because their shells are filled.\nOther more complicated models for the nucleus have also been proposed, such as the interacting boson model, in which pairs of neutrons and protons interact as bosons.\nAb initio methods try to solve the nuclear many-body problem from the ground up, starting from the nucleons and their interactions.\nMuch of current research in nuclear physics relates to the study of nuclei under extreme conditions such as high spin and excitation energy. Nuclei may also have extreme shapes (similar to that of Rugby balls or even pears) or extreme neutron-to-proton ratios. Experimenters can create such nuclei using artificially induced fusion or nucleon transfer reactions, employing ion beams from an accelerator. Beams with even higher energies can be used to create nuclei at very high temperatures, and there are signs that these experiments have produced a phase transition from normal nuclear matter to a new state, the quark–gluon plasma, in which the quarks mingle with one another, rather than being segregated in triplets as they are in neutrons and protons.\n\n\n=== Nuclear decay ===\n\nEighty elements have at least one stable isotope which is never observed to decay, amounting to a total of about 251 stable nuclides. However, thousands of isotopes have been characterized as unstable. These "radioisotopes" decay over time scales ranging from fractions of a second to trillions of years. Plotted on a chart as a function of atomic and neutron numbers, the binding energy of the nuclides forms what is known as the valley of stability. Stable nuclides lie along the bottom of this energy valley, while increasingly unstable nuclides lie up the valley walls, that is, have weaker binding energy.\nThe most stable nuclei fall within certain ranges or balances of composition of neutrons and protons: too few or too many neutrons (in relation to the number of protons) will cause it to decay. For example, in beta decay, a nitrogen-16 atom (7 protons, 9 neutrons) is converted to an oxygen-16 atom (8 protons, 8 neutrons) within a few seconds of being created. In this decay a neutron in the nitrogen nucleus is converted by the weak interaction into a proton, an electron and an antineutrino. The element is transmuted to another element, with a different number of protons.\nIn alpha decay, which typically occurs in the heaviest nuclei, the radioactive element decays by emitting a helium nucleus (2 protons and 2 neutrons), giving another element, plus helium-4. In many cases this process continues through several steps of this kind, including other types of decays (usually beta decay) until a stable element is formed.\nIn gamma decay, a nucleus decays from an excited state into a lower energy state, by emitting a gamma ray. The element is not changed to another element in the process (no nuclear transmutation is involved).\nOther more exotic decays are possible (see the first main article). For example, in internal conversion decay, the energy from an excited nucleus may eject one of the inner orbital electrons from the atom, in a process which produces high speed electrons but is not beta decay and (unlike beta decay) does not transmute one element to another.\n\n\n=== Nuclear fusion ===\nIn nuclear fusion, two low-mass nuclei come into very close contact with each other so that the strong force fuses them. It requires a large amount of energy for the strong or nuclear forces to overcome the electrical repulsion between the nuclei in order to fuse them; therefore nuclear fusion can only take place at very high temperatures or high pressures. When nuclei fuse, a very large amount of energy is released and the combined nucleus assumes a lower energy level. The binding energy per nucleon increases with mass number up to nickel-62. Stars like the Sun are powered by the fusion of four protons into a helium nucleus, two positrons, and two neutrinos. The uncontrolled fusion of hydrogen into helium is known as thermonuclear runaway. A frontier in current research at various institutions, for example the Joint European Torus (JET) and ITER, is the development of an economically viable method of using energy from a controlled fusion reaction. Nuclear fusion is the origin of the energy (including in the form of light and other electromagnetic radiation) produced by the core of all stars including our own Sun.\n\n\n=== Nuclear fission ===\nNuclear fission is the reverse process to fusion. For nuclei heavier than nickel-62 the binding energy per nucleon decreases with the mass number. It is therefore possible for energy to be released if a heavy nucleus breaks apart into two lighter ones.\nThe process of alpha decay is in essence a special type of spontaneous nuclear fission. It is a highly asymmetrical fission because the four particles which make up the alpha particle are especially tightly bound to each other, making production of this nucleus in fission particularly likely.\nFrom several of the heaviest nuclei whose fission produces free neutrons, and which also easily absorb neutrons to initiate fission, a self-igniting type of neutron-initiated fission can be obtained, in a chain reaction. Chain reactions were known in chemistry before physics, and in fact many familiar processes like fires and chemical explosions are chemical chain reactions. The fission or "nuclear" chain-reaction, using fission-produced neutrons, is the source of energy for nuclear power plants and fission-type nuclear bombs, such as those detonated in Hiroshima and Nagasaki, Japan, at the end of World War II. Heavy nuclei such as uranium and thorium may also undergo spontaneous fission, but they are much more likely to undergo decay by alpha decay.\nFor a neutron-initiated chain reaction to occur, there must be a critical mass of the relevant isotope present in a certain space under certain conditions. The conditions for the smallest critical mass require the conservation of the emitted neutrons and also their slowing or moderation so that there is a greater cross-section or probability of them initiating another fission. In two regions of Oklo, Gabon, Africa, natural nuclear fission reactors were active over 1.5 billion years ago. Measurements of natural neutrino emission have demonstrated that around half of the heat emanating from the Earth\'s core results from radioactive decay. However, it is not known if any of this results from fission chain reactions.\n\n\n=== Production of "heavy" elements ===\n\nAccording to the theory, as the Universe cooled after the Big Bang it eventually became possible for common subatomic particles as we know them (neutrons, protons and electrons) to exist. The most common particles created in the Big Bang which are still easily observable to us today were protons and electrons (in equal numbers). The protons would eventually form hydrogen atoms. Almost all the neutrons created in the Big Bang were absorbed into helium-4 in the first three minutes after the Big Bang, and this helium accounts for most of the helium in the universe today (see Big Bang nucleosynthesis).\nSome relatively small quantities of elements beyond helium (lithium, beryllium, and perhaps some boron) were created in the Big Bang, as the protons and neutrons collided with each other, but all of the "heavier elements" (carbon, element number 6, and elements of greater atomic number) that we see today, were created inside stars during a series of fusion stages, such as the proton–proton chain, the CNO cycle and the triple-alpha process. Progressively heavier elements are created during the evolution of a star.\nEnergy is only released in fusion processes involving smaller atoms than iron because the binding energy per nucleon peaks around iron (56 nucleons). Since the creation of heavier nuclei by fusion requires energy, nature resorts to the process of neutron capture. Neutrons (due to their lack of charge) are readily absorbed by a nucleus. The heavy elements are created by either a slow neutron capture process (the so-called s-process) or the rapid, or r-process. The s process occurs in thermally pulsing stars (called AGB, or asymptotic giant branch stars) and takes hundreds to thousands of years to reach the heaviest elements of lead and bismuth. The r-process is thought to occur in supernova explosions, which provide the necessary conditions of high temperature, high neutron flux and ejected matter. These stellar conditions make the successive neutron captures very fast, involving very neutron-rich species which then beta-decay to heavier elements, especially at the so-called waiting points that correspond to more stable nuclides with closed neutron shells (magic numbers).\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n=== Introductory ===\nSemat, H. and Albright, John R. (1972).  Introduction to Atomic and Nuclear Physics. Springer. ISBN 978-0-412-15670-0.\nLittlefield, T.A. and Thorley, N. (1979) Atomic and Nuclear Physics: An Introduction. Springer US. ISBN 978-0-442-30190-3.\nBelyaev, Alexander; Ross, Douglas (2021). The Basics of Nuclear and Particle Physics. Undergraduate Texts in Physics. Cham: Springer International Publishing. Bibcode:2021bnpp.book.....B. doi:10.1007/978-3-030-80116-8. ISBN 978-3-030-80115-1. Retrieved 2023-02-19.\nPovh, Bogdan; Rith, Klaus; Scholz, Christoph; Zetsche, Frank; Rodejohann, Werner (2015). Particles and Nuclei: An Introduction to the Physical Concepts. Graduate Texts in Physics. Berlin, Heidelberg: Springer Berlin Heidelberg. doi:10.1007/978-3-662-46321-5. ISBN 978-3-662-46320-8. Retrieved 2024-05-27.\n\n\n=== Reference works ===\nHandbook of Nuclear Physics. Isao Tanihata, Hiroshi Toki, Toshitaka Kajino (eds.). Singapore: Springer Nature Singapore. 2020. doi:10.1007/978-981-15-8818-1. ISBN 9789811588181. Retrieved 2023-05-31.{{cite book}}:  CS1 maint: others (link)\n\n\n=== Advanced ===\nCohen, Bernard L, (1971). Concepts of Nuclear Physics, McGraw-Hill, Inc.\nBohr, Aage; Mottelson, Ben R (1998) [1969]. Nuclear Structure: (In 2 Volumes). World Scientific. doi:10.1142/3530. ISBN 978-981-02-3197-2. Archived from the original on 2023-01-20. Retrieved 2023-04-19.\nGreiner, Walter; Maruhn, Joachim A. and Bromley, D.A (1996) Nuclear Models.  Springer ISBN 9783540591801. * Paetz gen. Schieck, Hans (2014). Nuclear Reactions: An Introduction. Lecture Notes in Physics. Vol. 882. Berlin, Heidelberg: Springer Berlin Heidelberg. doi:10.1007/978-3-642-53986-2. ISBN 978-3-642-53985-5. Retrieved 2023-04-04.\n\n\n=== Classics or Historic ===\nFermi, E. (1950). Nuclear Physics.  Univ. Chicago Press\nMott, N. F.; Massey, H. S. W. (1949). The Theory Of Atomic Collisions. The International Series of Monographs on Physics (2. ed.). Oxford: Calrendon Press (OUP).\nBlatt, John M.; Weisskopf, Victor F. (1979) [1952]. Theoretical Nuclear Physics. New York, NY: Springer New York. doi:10.1007/978-1-4612-9959-2. ISBN 978-1-4612-9961-5. Retrieved 2023-02-22.\nBethe, Hans A.; Morrison, Philip (2006) [1956]. Elementary Nuclear Theory. Mineola, NY: Dover Publications. ISBN 978-0-486-45048-3.\n\n\n== External links ==\n\nErnest Rutherford\'s biography at the American Institute of Physics Archived 2016-07-30 at the Wayback Machine\nAmerican Physical Society Division of Nuclear Physics Archived 2017-09-20 at the Wayback Machine\nAmerican Nuclear Society Archived 2008-12-02 at the Wayback Machine\nAnnotated bibliography on nuclear physics from the Alsos Digital Library for Nuclear Issues\nNuclear science wiki Archived 2013-10-21 at the Wayback Machine\nNuclear Data Services – IAEA Archived 2021-03-18 at the Wayback Machine\nNuclear Physics Archived 2017-12-23 at the Wayback Machine, BBC Radio 4 discussion with Jim Al-Khalili, John Gribbin and Catherine Sutton (In Our Time, Jan. 10, 2002)',
  },
  {
    title: "Particle physics",
    originalContent:
      'Particle physics or high-energy physics is the study of fundamental particles and forces that constitute matter and radiation. The field also studies combinations of elementary particles up to the scale of protons and neutrons, while the study of combination of protons and neutrons is called nuclear physics.\nThe fundamental particles in the universe are classified in the Standard Model as fermions (matter particles) and bosons (force-carrying particles). There are three generations of fermions, although ordinary matter is made only from the first fermion generation. The first generation consists of up and down quarks which form protons and neutrons, and electrons and electron neutrinos. The three fundamental interactions known to be mediated by bosons are electromagnetism, the weak interaction, and the strong interaction.\nQuarks cannot exist on their own but form hadrons. Hadrons that contain an odd number of quarks are called baryons and those that contain an even number are called mesons. Two baryons, the proton and the neutron, make up most of the mass of ordinary matter. Mesons are unstable and the longest-lived last for only a few hundredths of a microsecond. They occur after collisions between particles made of quarks, such as fast-moving protons and neutrons in cosmic rays. Mesons are also produced in cyclotrons or other particle accelerators.\nParticles have corresponding antiparticles with the same mass but with opposite electric charges. For example, the antiparticle of the electron is the positron. The electron has a negative electric charge, the positron has a positive charge. These antiparticles can theoretically form a corresponding form of matter called antimatter. Some particles, such as the photon, are their own antiparticle.\nThese elementary particles are excitations of the quantum fields that also govern their interactions. The dominant theory explaining these fundamental particles and fields, along with their dynamics, is called the Standard Model. The reconciliation of gravity to the current particle physics theory is not solved; many theories have addressed this problem, such as loop quantum gravity, string theory and supersymmetry theory.\nPractical particle physics is the study of these particles in radioactive processes and in particle accelerators such as the Large Hadron Collider. Theoretical particle physics is the study of these particles in the context of cosmology and quantum theory. The two are closely interrelated: the Higgs boson was postulated by theoretical particle physicists and its presence confirmed by practical experiments.\n\n\n== History ==\n\nThe idea that all matter is fundamentally composed of elementary particles dates from at least the 6th century BC. In the 19th century, John Dalton, through his work on stoichiometry, concluded that each element of nature was composed of a single, unique type of particle. The word atom, after the Greek word atomos meaning "indivisible", has since then denoted the smallest particle of a chemical element, but physicists later discovered that atoms are not, in fact, the fundamental particles of nature, but are conglomerates of even smaller particles, such as the electron. The early 20th century explorations of nuclear physics and quantum physics led to proofs of nuclear fission in 1939 by Lise Meitner (based on experiments by Otto Hahn), and nuclear fusion by Hans Bethe in that same year; both discoveries also led to the development of nuclear weapons.\nThroughout the 1950s and 1960s, a bewildering variety of particles was found in collisions of particles from beams of increasingly high energy. It was referred to informally as the "particle zoo". Important discoveries such as the CP violation by James Cronin and Val Fitch brought new questions to matter-antimatter imbalance. After the formulation of the Standard Model during the 1970s, physicists clarified the origin of the particle zoo. The large number of particles was explained as combinations of a (relatively) small number of more fundamental particles and framed in the context of quantum field theories. This reclassification marked the beginning of modern particle physics.\n\n\n== Standard Model ==\n\nThe current state of the classification of all elementary particles is explained by the Standard Model, which gained widespread acceptance in the mid-1970s after experimental confirmation of the existence of quarks. It describes the strong, weak, and electromagnetic fundamental interactions, using mediating gauge bosons. The species of gauge bosons are eight gluons, W−, W+ and Z bosons, and the photon. The Standard Model also contains 24 fundamental fermions (12 particles and their associated anti-particles), which are the constituents of all matter. Finally, the Standard Model also predicted the existence of a type of boson known as the Higgs boson. On 4 July 2012, physicists with the Large Hadron Collider at CERN announced they had found a new particle that behaves similarly to what is expected from the Higgs boson.\nThe Standard Model, as currently formulated, has 61 elementary particles. Those elementary particles can combine to form composite particles, accounting for the hundreds of other species of particles that have been discovered since the 1960s. The Standard Model has been found to agree with almost all the experimental tests conducted to date. However, most particle physicists believe that it is an incomplete description of nature and that a more fundamental theory awaits discovery (See Theory of Everything). In recent years, measurements of neutrino mass have provided the first experimental deviations from the Standard Model, since neutrinos do not have mass in the Standard Model.\n\n\n== Subatomic particles ==\n\nModern particle physics research is focused on subatomic particles, including atomic constituents, such as electrons, protons, and neutrons (protons and neutrons are composite particles called baryons, made of quarks), that are produced by radioactive and scattering processes; such particles are photons, neutrinos, and muons, as well as a wide range of exotic particles. All particles and their interactions observed to date can be described almost entirely by the Standard Model.\nDynamics of particles are also governed by quantum mechanics; they exhibit wave–particle duality, displaying particle-like behaviour under certain experimental conditions and wave-like behaviour in others. In more technical terms, they are described by quantum state vectors in a Hilbert space, which is also treated in quantum field theory. Following the convention of particle physicists, the term elementary particles is applied to those particles that are, according to current understanding, presumed to be indivisible and not composed of other particles.\n\n\n=== Quarks and leptons ===\n\nOrdinary matter is made from first-generation quarks (up, down) and leptons (electron, electron neutrino). Collectively, quarks and leptons are called fermions, because they have a quantum spin of half-integers (−1/2, 1/2, 3/2, etc.). This causes the fermions to obey the Pauli exclusion principle, where no two particles may occupy the same quantum state. Quarks have fractional elementary electric charge (−1/3 or 2/3) and leptons have whole-numbered electric charge (0 or 1). Quarks also have color charge, which is labeled arbitrarily with no correlation to actual light color as red, green and blue. Because the interactions between the quarks store energy which can convert to other particles when the quarks are far apart enough, quarks cannot be observed independently. This is called color confinement.\nThere are three known generations of quarks (up and down, strange and charm, top and bottom) and leptons (electron and its neutrino, muon and its neutrino, tau and its neutrino), with strong indirect evidence that a fourth generation of fermions does not exist.\n\n\n=== Bosons ===\n\nBosons are the mediators or carriers of fundamental interactions, such as electromagnetism, the weak interaction, and the strong interaction. Electromagnetism is mediated by the photon, the quanta of light.: 29–30  The weak interaction is mediated by the W and Z bosons. The strong interaction is mediated by the gluon, which can link quarks together to form composite particles. Due to the aforementioned color confinement, gluons are never observed independently. The Higgs boson gives mass to the W and Z bosons via the Higgs mechanism – the gluon and photon are expected to be massless. All bosons have an integer quantum spin (0 and 1) and can have the same quantum state.\n\n\n=== Antiparticles and color charge ===\n\nMost aforementioned particles have corresponding antiparticles, which compose antimatter. Normal particles have positive lepton or baryon number, and antiparticles have these numbers negative. Most properties of corresponding antiparticles and particles are the same, with a few gets reversed; the electron\'s antiparticle, positron, has an opposite charge. To differentiate between antiparticles and particles, a plus or negative sign is added in superscript. For example, the electron and the positron are denoted e− and e+. When a particle and an antiparticle interact with each other, they are annihilated and convert to other particles. Some particles, such as the photon or gluon, have no antiparticles.\nQuarks and gluons additionally have color charges, which influences the strong interaction. Quark\'s color charges are called red, green and blue (though the particle itself have no physical color), and in antiquarks are called antired, antigreen and antiblue. The gluon can have eight color charges, which are the result of quarks\' interactions to form composite particles (gauge symmetry SU(3)).\n\n\n=== Composite ===\n\nThe neutrons and protons in the atomic nuclei are baryons – the neutron is composed of two down quarks and one up quark, and the proton is composed of two up quarks and one down quark. A baryon is composed of three quarks, and a meson is composed of two quarks (one normal, one anti). Baryons and mesons are collectively called hadrons. Quarks inside hadrons are governed by the strong interaction, thus are subjected to quantum chromodynamics (color charges). The bounded quarks must have their color charge to be neutral, or "white" for analogy with mixing the primary colors. More exotic hadrons can have other types, arrangement or number of quarks (tetraquark, pentaquark).\nAn atom is made from protons, neutrons and electrons. By modifying the particles inside a normal atom, exotic atoms can be formed. A simple example would be the hydrogen-4.1, which has one of its electrons replaced with a muon.\n\n\n=== Hypothetical ===\nThe graviton is a hypothetical particle that can mediate the gravitational interaction, but it has not been detected or completely reconciled with current theories. Many other hypothetical particles have been proposed to address the limitations of the Standard Model. Notably, supersymmetric particles aim to solve the hierarchy problem, axions address the strong CP problem, and various other particles are proposed to explain the origins of dark matter and dark energy.\n\n\n== Experimental laboratories ==\n\nThe world\'s major particle physics laboratories are:\n\nBrookhaven National Laboratory (Long Island, New York, United States). Its main facility is the Relativistic Heavy Ion Collider (RHIC), which collides heavy ions such as gold ions and polarized protons. It is the world\'s first heavy ion collider, and the world\'s only polarized proton collider.\nBudker Institute of Nuclear Physics (Novosibirsk, Russia). Its main projects are now the electron-positron colliders VEPP-2000, operated since 2006, and VEPP-4, started experiments in 1994. Earlier facilities include the first electron–electron beam–beam collider VEP-1, which conducted experiments from 1964 to 1968; the electron-positron colliders VEPP-2, operated from 1965 to 1974; and, its successor VEPP-2M, performed experiments from 1974 to 2000.\nCERN (European Organization for Nuclear Research) (Franco-Swiss border, near Geneva, Switzerland). Its main project is now the Large Hadron Collider (LHC), which had its first beam circulation on 10 September 2008, and is now the world\'s most energetic collider of protons. It also became the most energetic collider of heavy ions after it began colliding lead ions. Earlier facilities include the Large Electron–Positron Collider (LEP), which was stopped on 2 November 2000 and then dismantled to give way for LHC; and the Super Proton Synchrotron, which is being reused as a pre-accelerator for the LHC and for fixed-target experiments.\nDESY (Deutsches Elektronen-Synchrotron) (Hamburg, Germany). Its main facility was the Hadron Elektron Ring Anlage (HERA), which collided electrons and positrons with protons. The accelerator complex is now focused on the production of synchrotron radiation with PETRA III, FLASH and the European XFEL.\nFermi National Accelerator Laboratory (Fermilab) (Batavia, Illinois, United States). Its main facility until 2011 was the Tevatron, which collided protons and antiprotons and was the highest-energy particle collider on earth until the Large Hadron Collider surpassed it on 29 November 2009.\nInstitute of High Energy Physics (IHEP) (Beijing, China). IHEP manages a number of China\'s major particle physics facilities, including the Beijing Electron–Positron Collider II(BEPC II), the Beijing Spectrometer (BES), the Beijing Synchrotron Radiation Facility (BSRF), the International Cosmic-Ray Observatory at Yangbajing in Tibet, the Daya Bay Reactor Neutrino Experiment, the China Spallation Neutron Source, the Hard X-ray Modulation Telescope (HXMT), and the Accelerator-driven Sub-critical System (ADS) as well as the Jiangmen Underground Neutrino Observatory (JUNO).\nKEK (Tsukuba, Japan). It is the home of a number of experiments such as the K2K experiment, a neutrino oscillation experiment and Belle II, an experiment measuring the CP violation of B mesons.\nSLAC National Accelerator Laboratory (Menlo Park, California, United States). Its 2-mile-long linear particle accelerator began operating in 1962 and was the basis for numerous electron and positron collision experiments until 2008. Since then the linear accelerator is being used for the Linac Coherent Light Source X-ray laser as well as advanced accelerator design research. SLAC staff continue to participate in developing and building many particle detectors around the world.\n\n\n== Theory ==\n\nTheoretical particle physics attempts to develop the models, theoretical framework, and mathematical tools to understand current experiments and make predictions for future experiments (see also theoretical physics). There are several major interrelated efforts being made in theoretical particle physics today.\nOne important branch attempts to better understand the Standard Model and its tests. Theorists make quantitative predictions of observables at collider and astronomical experiments, which along with experimental measurements is used to extract the parameters of the Standard Model with less uncertainty. This work probes the limits of the Standard Model and therefore expands scientific understanding of nature\'s building blocks. Those efforts are made challenging by the difficulty of calculating high precision quantities in quantum chromodynamics. Some theorists working in this area use the tools of perturbative quantum field theory and effective field theory, referring to themselves as phenomenologists. Others make use of lattice field theory and call themselves lattice theorists.\nAnother major effort is in model building where model builders develop ideas for what physics may lie beyond the Standard Model (at higher energies or smaller distances). This work is often motivated by the hierarchy problem and is constrained by existing experimental data. It may involve work on supersymmetry, alternatives to the Higgs mechanism, extra spatial dimensions (such as the Randall–Sundrum models), Preon theory, combinations of these, or other ideas. Vanishing-dimensions theory is a particle physics theory suggesting that systems with higher energy have a smaller number of dimensions.\nA third major effort in theoretical particle physics is string theory. String theorists attempt to construct a unified description of quantum mechanics and general relativity by building a theory based on small strings, and branes rather than particles. If the theory is successful, it may be considered a "Theory of Everything", or "TOE".\nThere are also other areas of work in theoretical particle physics ranging from particle cosmology to loop quantum gravity.\n\n\n== Practical applications ==\nIn principle, all physics (and practical applications developed therefrom) can be derived from the study of fundamental particles. In practice, even if "particle physics" is taken to mean only "high-energy atom smashers", many technologies have been developed during these pioneering investigations that later find wide uses in society. Particle accelerators are used to produce medical isotopes for research and treatment (for example, isotopes used in PET imaging), or used directly in external beam radiotherapy. The development of superconductors has been pushed forward by their use in particle physics. The World Wide Web and touchscreen technology were initially developed at CERN. Additional applications are found in medicine, national security, industry, computing, science, and workforce development, illustrating a long and growing list of beneficial practical applications with contributions from particle physics.\n\n\n== Future ==\nMajor efforts to look for physics beyond the Standard Model include the Future Circular Collider proposed for CERN and the Particle Physics Project Prioritization Panel (P5) in the US that will update the 2014 P5 study that recommended the Deep Underground Neutrino Experiment, among other experiments.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==',
  },
  {
    title: "String cosmology",
    originalContent:
      "String cosmology is a relatively new field that tries to apply equations of string theory to solve the questions of early cosmology. A related area of study is brane cosmology.\n\n\n== Overview ==\nThis approach can be dated back to a paper by Gabriele Veneziano that shows how an inflationary cosmological model can be obtained from string theory, thus opening the door to a description of pre-Big Bang scenarios.\nThe idea is related to a property of the bosonic string in a curve background, better known as nonlinear sigma model. First calculations from this model showed as the beta function, representing the running of the metric of the model as a function of an energy scale, is proportional to the Ricci tensor giving rise to a Ricci flow. As this model has conformal invariance and this must be kept to have a sensible quantum field theory, the beta function must be zero producing immediately the Einstein field equations. While Einstein equations seem to appear somewhat out of place, nevertheless this result is surely striking showing as a background two-dimensional model could produce higher-dimensional physics. An interesting point here is that such a string theory can be formulated without a requirement of criticality at 26 dimensions for consistency as happens on a flat background. This is a serious hint that the underlying physics of Einstein equations could be described by an effective two-dimensional conformal field theory. Indeed, the fact that we have evidence for an inflationary universe is an important support to string cosmology.\nIn the evolution of the universe, after the inflationary phase, the expansion observed today sets in that is well described by Friedmann equations. A smooth transition is expected between these two different phases. String cosmology appears to have difficulties in explaining this transition. This is known in the literature as the graceful exit problem.\nAn inflationary cosmology implies the presence of a scalar field that drives inflation. In string cosmology, this arises from the so-called dilaton field. This is a scalar term entering into the description of the bosonic string that produces a scalar field term into the effective theory at low energies. The corresponding equations resemble those of a Brans–Dicke theory.\nAnalysis has been worked out from a critical number of dimension (26) down to four. In general, one gets Friedmann equations in an arbitrary number of dimensions. The other way round is to assume that a certain number of dimensions is compactified producing an effective four-dimensional theory to work with. Such a theory is a typical Kaluza–Klein theory with a set of scalar fields arising from compactified dimensions. Such fields are called moduli.\n\n\n== Technical details ==\nThis section presents some of the relevant equations entering into string cosmology. The starting point is the Polyakov action, which can be written as\n\n  \n    \n      \n        \n          S\n          \n            2\n          \n        \n        =\n        \n          \n            1\n            \n              4\n              π\n              \n                α\n                ′\n              \n            \n          \n        \n        ∫\n        \n          d\n          \n            2\n          \n        \n        z\n        \n          \n            γ\n          \n        \n        \n          [\n          \n            \n              γ\n              \n                a\n                b\n              \n            \n            \n              G\n              \n                μ\n                ν\n              \n            \n            (\n            X\n            )\n            \n              ∂\n              \n                a\n              \n            \n            \n              X\n              \n                μ\n              \n            \n            \n              ∂\n              \n                b\n              \n            \n            \n              X\n              \n                ν\n              \n            \n            +\n            \n              α\n              ′\n            \n            \n               \n              \n                (\n                2\n                )\n              \n            \n            R\n            Φ\n            (\n            X\n            )\n          \n          ]\n        \n        ,\n      \n    \n    {\\displaystyle S_{2}={\\frac {1}{4\\pi \\alpha '}}\\int d^{2}z{\\sqrt {\\gamma }}\\left[\\gamma ^{ab}G_{\\mu \\nu }(X)\\partial _{a}X^{\\mu }\\partial _{b}X^{\\nu }+\\alpha '\\ ^{(2)}R\\Phi (X)\\right],}\n  \n\nwhere  \n  \n    \n      \n        \n           \n          \n            (\n            2\n            )\n          \n        \n        R\n      \n    \n    {\\displaystyle \\ ^{(2)}R}\n  \n is the Ricci scalar in two dimensions, \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n the dilaton field, and \n  \n    \n      \n        \n          α\n          ′\n        \n      \n    \n    {\\displaystyle \\alpha '}\n  \n the string constant. The indices \n  \n    \n      \n        a\n        ,\n        b\n      \n    \n    {\\displaystyle a,b}\n  \n range over 1,2, and \n  \n    \n      \n        μ\n        ,\n        ν\n      \n    \n    {\\displaystyle \\mu ,\\nu }\n  \n over \n  \n    \n      \n        1\n        ,\n        …\n        ,\n        D\n      \n    \n    {\\displaystyle 1,\\ldots ,D}\n  \n, where D the dimension of the target space. A further antisymmetric field could be added. This is generally considered when one wants this action generating a potential for inflation. Otherwise, a generic potential is inserted by hand, as well as a cosmological constant.\nThe above string action has a conformal invariance. This is a property of a two dimensional Riemannian manifold. At the quantum level, this property is lost due to anomalies and the theory itself is not consistent, having no unitarity. So it is necessary to require that conformal invariance is kept at any order of perturbation theory. Perturbation theory is the only known approach to manage the quantum field theory. Indeed, the beta functions at two loops are\n\n  \n    \n      \n        \n          β\n          \n            μ\n            ν\n          \n          \n            G\n          \n        \n        =\n        \n          R\n          \n            μ\n            ν\n          \n        \n        +\n        2\n        \n          α\n          ′\n        \n        \n          ∇\n          \n            μ\n          \n        \n        Φ\n        \n          ∇\n          \n            ν\n          \n        \n        Φ\n        +\n        O\n        (\n        \n          α\n          \n            ′\n            \n              2\n            \n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\beta _{\\mu \\nu }^{G}=R_{\\mu \\nu }+2\\alpha '\\nabla _{\\mu }\\Phi \\nabla _{\\nu }\\Phi +O(\\alpha '^{2}),}\n  \n\nand\n\n  \n    \n      \n        \n          β\n          \n            Φ\n          \n        \n        =\n        \n          \n            \n              D\n              −\n              26\n            \n            6\n          \n        \n        −\n        \n          \n            \n              α\n              ′\n            \n            2\n          \n        \n        \n          ∇\n          \n            2\n          \n        \n        Φ\n        +\n        \n          α\n          ′\n        \n        \n          ∇\n          \n            κ\n          \n        \n        Φ\n        \n          ∇\n          \n            κ\n          \n        \n        Φ\n        +\n        O\n        (\n        \n          α\n          \n            ′\n            \n              2\n            \n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle \\beta ^{\\Phi }={\\frac {D-26}{6}}-{\\frac {\\alpha '}{2}}\\nabla ^{2}\\Phi +\\alpha '\\nabla _{\\kappa }\\Phi \\nabla ^{\\kappa }\\Phi +O(\\alpha '^{2}).}\n  \n\nThe assumption that conformal invariance holds implies that\n\n  \n    \n      \n        \n          β\n          \n            μ\n            ν\n          \n          \n            G\n          \n        \n        =\n        \n          β\n          \n            Φ\n          \n        \n        =\n        0\n        ,\n      \n    \n    {\\displaystyle \\beta _{\\mu \\nu }^{G}=\\beta ^{\\Phi }=0,}\n  \n\nproducing the corresponding equations of motion of low-energy physics. These conditions can only be satisfied perturbatively, but this has to hold at any order of perturbation theory. The first term in \n  \n    \n      \n        \n          β\n          \n            Φ\n          \n        \n      \n    \n    {\\displaystyle \\beta ^{\\Phi }}\n  \n is just the anomaly of the bosonic string theory in a flat spacetime. But here there are further terms that can grant compensation of the anomaly also when \n  \n    \n      \n        D\n        ≠\n        26\n      \n    \n    {\\displaystyle D\\neq 26}\n  \n, and from this cosmological models of a pre-big bang, scenario can be constructed. Indeed, this low energy equations can be obtained from the following action:\n\n  \n    \n      \n        S\n        =\n        \n          \n            1\n            \n              2\n              \n                κ\n                \n                  0\n                \n                \n                  2\n                \n              \n            \n          \n        \n        ∫\n        \n          d\n          \n            D\n          \n        \n        x\n        \n          \n            −\n            G\n          \n        \n        \n          e\n          \n            −\n            2\n            Φ\n          \n        \n        \n          [\n          \n            −\n            \n              \n                \n                  2\n                  (\n                  D\n                  −\n                  26\n                  )\n                \n                \n                  3\n                  \n                    α\n                    ′\n                  \n                \n              \n            \n            +\n            R\n            +\n            4\n            \n              ∂\n              \n                μ\n              \n            \n            Φ\n            \n              ∂\n              \n                μ\n              \n            \n            Φ\n            +\n            O\n            (\n            \n              α\n              ′\n            \n            )\n          \n          ]\n        \n        ,\n      \n    \n    {\\displaystyle S={\\frac {1}{2\\kappa _{0}^{2}}}\\int d^{D}x{\\sqrt {-G}}e^{-2\\Phi }\\left[-{\\frac {2(D-26)}{3\\alpha '}}+R+4\\partial _{\\mu }\\Phi \\partial ^{\\mu }\\Phi +O(\\alpha ')\\right],}\n  \n\nwhere \n  \n    \n      \n        \n          κ\n          \n            0\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\kappa _{0}^{2}}\n  \n is a constant that can always be changed by redefining the dilaton field. One can also rewrite this action in a more familiar form by redefining the fields (Einstein frame) as\n\n  \n    \n      \n        \n        \n          g\n          \n            μ\n            ν\n          \n        \n        =\n        \n          e\n          \n            2\n            ω\n          \n        \n        \n          G\n          \n            μ\n            ν\n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle \\,g_{\\mu \\nu }=e^{2\\omega }G_{\\mu \\nu }\\!,}\n  \n\n  \n    \n      \n        ω\n        =\n        \n          \n            \n              2\n              (\n              \n                Φ\n                \n                  0\n                \n              \n              −\n              Φ\n              )\n            \n            \n              D\n              −\n              2\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\omega ={\\frac {2(\\Phi _{0}-\\Phi )}{D-2}},}\n  \n\nand using \n  \n    \n      \n        \n          \n            \n              Φ\n              ~\n            \n          \n        \n        =\n        Φ\n        −\n        \n          Φ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {\\Phi }}=\\Phi -\\Phi _{0}}\n  \n one can write\n\n  \n    \n      \n        S\n        =\n        \n          \n            1\n            \n              2\n              \n                κ\n                \n                  2\n                \n              \n            \n          \n        \n        ∫\n        \n          d\n          \n            D\n          \n        \n        x\n        \n          \n            −\n            g\n          \n        \n        \n          [\n          \n            −\n            \n              \n                \n                  2\n                  (\n                  D\n                  −\n                  26\n                  )\n                \n                \n                  3\n                  \n                    α\n                    ′\n                  \n                \n              \n            \n            \n              e\n              \n                \n                  \n                    4\n                    \n                      \n                        \n                          Φ\n                          ~\n                        \n                      \n                    \n                  \n                  \n                    D\n                    −\n                    2\n                  \n                \n              \n            \n            +\n            \n              \n                \n                  R\n                  ~\n                \n              \n            \n            −\n            \n              \n                4\n                \n                  D\n                  −\n                  2\n                \n              \n            \n            \n              ∂\n              \n                μ\n              \n            \n            \n              \n                \n                  Φ\n                  ~\n                \n              \n            \n            \n              ∂\n              \n                μ\n              \n            \n            \n              \n                \n                  Φ\n                  ~\n                \n              \n            \n            +\n            O\n            (\n            \n              α\n              ′\n            \n            )\n          \n          ]\n        \n        ,\n      \n    \n    {\\displaystyle S={\\frac {1}{2\\kappa ^{2}}}\\int d^{D}x{\\sqrt {-g}}\\left[-{\\frac {2(D-26)}{3\\alpha '}}e^{\\frac {4{\\tilde {\\Phi }}}{D-2}}+{\\tilde {R}}-{\\frac {4}{D-2}}\\partial _{\\mu }{\\tilde {\\Phi }}\\partial ^{\\mu }{\\tilde {\\Phi }}+O(\\alpha ')\\right],}\n  \n\nwhere\n\n  \n    \n      \n        \n          \n            \n              R\n              ~\n            \n          \n        \n        =\n        \n          e\n          \n            −\n            2\n            ω\n          \n        \n        [\n        R\n        −\n        (\n        D\n        −\n        1\n        )\n        \n          ∇\n          \n            2\n          \n        \n        ω\n        −\n        (\n        D\n        −\n        2\n        )\n        (\n        D\n        −\n        1\n        )\n        \n          ∂\n          \n            μ\n          \n        \n        ω\n        \n          ∂\n          \n            μ\n          \n        \n        ω\n        ]\n        .\n      \n    \n    {\\displaystyle {\\tilde {R}}=e^{-2\\omega }[R-(D-1)\\nabla ^{2}\\omega -(D-2)(D-1)\\partial _{\\mu }\\omega \\partial ^{\\mu }\\omega ].}\n  \n\nThis is the formula for the Einstein action describing a scalar field interacting with a gravitational field in D dimensions. Indeed, the following identity holds:\n\n  \n    \n      \n        κ\n        =\n        \n          κ\n          \n            0\n          \n        \n        \n          e\n          \n            2\n            \n              Φ\n              \n                0\n              \n            \n          \n        \n        =\n        (\n        8\n        π\n        \n          G\n          \n            D\n          \n        \n        \n          )\n          \n            \n              1\n              2\n            \n          \n        \n        =\n        \n          \n            \n              8\n              π\n            \n            \n              M\n              \n                p\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\kappa =\\kappa _{0}e^{2\\Phi _{0}}=(8\\pi G_{D})^{\\frac {1}{2}}={\\frac {\\sqrt {8\\pi }}{M_{p}}},}\n  \n\nwhere \n  \n    \n      \n        \n          G\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle G_{D}}\n  \n is the Newton constant in D dimensions and \n  \n    \n      \n        \n          M\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle M_{p}}\n  \n the corresponding Planck mass. When setting \n  \n    \n      \n        D\n        =\n        4\n      \n    \n    {\\displaystyle D=4}\n  \n in this action, the conditions for inflation are not fulfilled unless a potential or antisymmetric term is added to the string action, in which case power-law inflation is possible.\n\n\n== Notes ==\n\n\n== References ==\nPolchinski, Joseph (1998a). String Theory Vol. I: An Introduction to the Bosonic String. Cambridge University Press. ISBN 978-0-521-63303-1.\nPolchinski, Joseph (1998b). String Theory Vol. II: Superstring Theory and Beyond. Cambridge University Press. ISBN 978-0-521-63304-8.\nLidsey, James D.; Wands, David; Copeland, E. J. (2000). \"Superstring Cosmology\". Physics Reports. 337 (4–5): 343–492. arXiv:hep-th/9909061. Bibcode:2000PhR...337..343L. doi:10.1016/S0370-1573(00)00064-8. S2CID 119349072.\nCicoli, Michele; Conlon, Joseph P; Maharana, Anshuman; Parameswaran, Susha; Quevedo, Fernando; Zavala, Ivonne (2023). \"String Cosmology: from the Early Universe to Today\". arXiv:2303.04819. {{cite journal}}: Cite journal requires |journal= (help)\n\n\n== External links ==\nString cosmology on arxiv.org\nMaurizio Gasperini's homepage",
  },
  {
    title: "Computer architecture",
    originalContent:
      'In computer science and computer engineering, computer architecture is a description of the structure of a computer system made from component parts. It can sometimes be a high-level description that ignores details of the implementation. At a more detailed level, the description may include the instruction set architecture design, microarchitecture design, logic design, and implementation.\n\n\n== History ==\nThe first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine. While building the computer Z1 in 1936, Konrad Zuse described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data, i.e., the stored-program concept. Two other early and important examples are:\n\nJohn von Neumann\'s 1945 paper, First Draft of a Report on the EDVAC, which described an organization of logical elements; and\nAlan Turing\'s more detailed Proposed Electronic Calculator for the Automatic Computing Engine, also 1945 and which cited John von Neumann\'s paper.\nThe term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM\'s main research center in 1959. Johnson had the opportunity to write a proprietary research communication about the Stretch, an IBM-developed supercomputer for Los Alamos National Laboratory (at the time known as Los Alamos Scientific Laboratory). To describe the level of detail for discussing the luxuriously embellished computer, he noted that his description of formats, instruction types, hardware parameters, and speed enhancements were at the level of "system architecture", a term that seemed more useful than "machine organization".\nSubsequently, Brooks, a Stretch designer, opened Chapter 2 of a book called Planning a Computer System: Project Stretch by stating, "Computer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints."\nBrooks went on to help develop the IBM System/360 line of computers, in which "architecture" became a noun defining "what the user needs to know". The System/360 line was succeeded by several compatible lines of computers, including the current IBM Z line. Later, computer users came to use the term in many less explicit ways.\nThe earliest computer architectures were designed on paper and then directly built into the final hardware form.\nLater, computer architecture prototypes were physically built in the form of a transistor–transistor logic (TTL) computer—such as the prototypes of the 6800 and the PA-RISC—tested, and tweaked, before committing to the final hardware form.\nAs of the 1990s, new computer architectures are typically "built", tested, and tweaked—inside some other computer architecture in a computer architecture simulator; or inside a FPGA as a soft microprocessor; or both—before committing to the final hardware form.\n\n\n== Subcategories ==\nThe discipline of computer architecture has three main subcategories:\n\nInstruction set architecture (ISA): defines the machine code that a processor reads and acts upon as well as the word size, memory address modes, processor registers, and data type.\nMicroarchitecture: also known as "computer organization", this describes how a particular processor will implement the ISA. The size of a computer\'s CPU cache for instance, is an issue that generally has nothing to do with the ISA.\nSystems design: includes all of the other hardware components within a computing system, such as data processing other than the CPU (e.g., direct memory access), virtualization, and multiprocessing.\nThere are other technologies in computer architecture. The following technologies are used in bigger companies like Intel, and were estimated in 2002 to count for 1% of all of computer architecture:\n\nMacroarchitecture: architectural layers more abstract than microarchitecture\nAssembly instruction set architecture: A smart assembler may convert an abstract assembly language common to a group of machines into slightly different machine language for different implementations.\nProgrammer-visible macroarchitecture: higher-level language tools such as compilers may define a consistent interface or contract to programmers using them, abstracting differences between underlying ISAs and microarchitectures. For example, the C, C++, or Java standards define different programmer-visible macroarchitectures.\nMicrocode: microcode is software that translates instructions to run on a chip. It acts like a wrapper around the hardware, presenting a preferred version of the hardware\'s instruction set interface. This instruction translation facility gives chip designers flexible options: E.g. 1. A new improved version of the chip can use microcode to present the exact same instruction set as the old chip version, so all software targeting that instruction set will run on the new chip without needing changes. E.g. 2. Microcode can present a variety of instruction sets for the same underlying chip, allowing it to run a wider variety of software.\nPin architecture: The hardware functions that a microprocessor should provide to a hardware platform, e.g., the x86 pins A20M, FERR/IGNNE or FLUSH. Also, messages that the processor should emit so that external caches can be invalidated (emptied). Pin architecture functions are more flexible than ISA functions because external hardware can adapt to new encodings, or change from a pin to a message. The term "architecture" fits, because the functions must be provided for compatible systems, even if the detailed method changes.\n\n\n== Roles ==\n\n\n=== Definition ===\nComputer architecture is concerned with balancing the performance, efficiency, cost, and reliability of a computer system. The case of instruction set architecture can be used to illustrate the balance of these competing factors. More complex instruction sets enable programmers to write more space efficient programs, since a single instruction can encode some higher-level abstraction (such as the x86 Loop instruction). However, longer and more complex instructions take longer for the processor to decode and can be more costly to implement effectively. The increased complexity from a large instruction set also creates more room for unreliability when instructions interact in unexpected ways.\nThe implementation involves integrated circuit design, packaging, power, and cooling. Optimization of the design requires familiarity with topics from compilers and operating systems to logic design and packaging.\n\n\n=== Instruction set architecture ===\n\nAn instruction set architecture (ISA) is the interface between the computer\'s software and hardware and also can be viewed as the programmer\'s view of the machine. Computers do not understand high-level programming languages such as Java, C++, or most programming languages used. A processor only understands instructions encoded in some numerical fashion, usually as binary numbers. Software tools, such as compilers, translate those high level languages into instructions that the processor can understand.\nBesides instructions, the ISA defines items in the computer that are available to a program—e.g., data types, registers, addressing modes, and memory.  Instructions locate these available items with register indexes (or names) and memory addressing modes.\nThe ISA of a computer is usually described in a small instruction manual, which describes how the instructions are encoded. Also, it may define short (vaguely) mnemonic names for the instructions. The names can be recognized by a software development tool called an assembler. An assembler is a computer program that translates a human-readable form of the ISA into a computer-readable form. Disassemblers are also widely available, usually in debuggers and software programs to isolate and correct malfunctions in binary computer programs.\nISAs vary in quality and completeness. A good ISA compromises between programmer convenience (how easy the code is to understand), size of the code (how much code is required to do a specific action), cost of the computer to interpret the instructions (more complexity means more hardware needed to decode and execute the instructions), and speed of the computer (with more complex decoding hardware comes longer decode time). Memory organization defines how instructions interact with the memory, and how memory interacts with itself.\nDuring design emulation, emulators can run programs written in a proposed instruction set. Modern emulators can measure size, cost, and speed to determine whether a particular ISA is meeting its goals.\n\n\n=== Computer organization ===\n\nComputer organization helps optimize performance-based products. For example, software engineers need to know the processing power of processors. They may need to optimize software in order to gain the most performance for the lowest price. This can require quite a detailed analysis of the computer\'s organization.  For example, in an SD card, the designers might need to arrange the card so that the most data can be processed in the fastest possible way.\nComputer organization also helps plan the selection of a processor for a particular project. Multimedia projects may need very rapid data access, while virtual machines may need fast interrupts. Sometimes certain tasks need additional components as well. For example, a computer capable of running a virtual machine needs virtual memory hardware so that the memory of different virtual computers can be kept separated. Computer organization and features also affect power consumption and processor cost.\n\n\n=== Implementation ===\nOnce an instruction set and microarchitecture have been designed, a practical machine must be developed. This design process is called the implementation. Implementation is usually not considered architectural design, but rather hardware design engineering. Implementation can be further broken down into several steps:\n\nLogic implementation designs the circuits required at a logic-gate level.\nCircuit implementation does transistor-level designs of basic elements (e.g., gates, multiplexers, latches) as well as of some larger blocks (ALUs, caches etc.) that may be implemented at the logic-gate level, or even at the physical level if the design calls for it.\nPhysical implementation draws physical circuits.  The different circuit components are placed in a chip floor plan or on a board and the wires connecting them are created.\nDesign validation tests the computer as a whole to see if it works in all situations and all timings. Once the design validation process starts, the design at the logic level are tested using logic emulators. However, this is usually too slow to run a realistic test. So, after making corrections based on the first test, prototypes are constructed using Field-Programmable Gate-Arrays (FPGAs). Most hobby projects stop at this stage. The final step is to test prototype integrated circuits, which may require several redesigns.\nFor CPUs, the entire implementation process is organized differently and is often referred to as CPU design.\n\n\n== Design goals ==\nThe exact form of a computer system depends on the constraints and goals. Computer architectures usually trade off standards, power versus performance, cost, memory capacity, latency (latency is the amount of time that it takes for information from one node to travel to the source) and throughput. Sometimes other considerations, such as features, size, weight, reliability, and expandability are also factors.\nThe most common scheme does an in-depth power analysis and figures out how to keep power consumption low while maintaining adequate performance.\n\n\n=== Performance ===\nModern computer performance is often described in instructions per cycle (IPC), which measures the efficiency of the architecture at any clock frequency; a faster IPC rate means the computer is faster. Older computers had IPC counts as low as 0.1 while modern processors easily reach nearly 1. Superscalar processors may reach three to five IPC by executing several instructions per clock cycle.\nCounting machine-language instructions would be misleading because they can do varying amounts of work in different ISAs. The "instruction" in the standard measurements is not a count of the ISA\'s machine-language instructions, but a unit of measurement, usually based on the speed of the VAX computer architecture.\nMany people used to measure a computer\'s speed by the clock rate (usually in MHz or GHz). This refers to the cycles per second of the main clock of the CPU. However, this metric is somewhat misleading, as a machine with a higher clock rate may not necessarily have greater performance. As a result, manufacturers have moved away from clock speed as a measure of performance.\nOther factors influence speed, such as the mix of functional units, bus speeds, available memory, and the type and order of instructions in the programs.\nThere are two main types of speed: latency and throughput. Latency is the time between the start of a process and its completion. Throughput is the amount of work done per unit time. Interrupt latency is the guaranteed maximum response time of the system to an electronic event (like when the disk drive finishes moving some data).\nPerformance is affected by a very wide range of design choices — for example, pipelining a processor usually makes latency worse, but makes throughput better. Computers that control machinery usually need low interrupt latencies. These computers operate in a real-time environment and fail if an operation is not completed in a specified amount of time. For example, computer-controlled anti-lock brakes must begin braking within a predictable and limited time period after the brake pedal is sensed or else failure of the brake will occur.\nBenchmarking takes all these factors into account by measuring the time a computer takes to run through a series of test programs. Although benchmarking shows strengths, it should not be how you choose a computer. Often the measured machines split on different measures. For example, one system might handle scientific applications quickly, while another might render video games more smoothly. Furthermore, designers may target and add special features to their products, through hardware or software, that permit a specific benchmark to execute quickly but do not offer similar advantages to general tasks.\n\n\n=== Power efficiency ===\n\nPower efficiency is another important measurement in modern computers. Higher power efficiency can often be traded for lower speed or higher cost. The typical measurement when referring to power consumption in computer architecture is MIPS/W (millions of instructions per second per watt).\nModern circuits have less power required per transistor as the number of transistors per chip grows. This is because each transistor that is put in a new chip requires its own power supply and requires new pathways to be built to power it. However, the number of transistors per chip is starting to increase at a slower rate. Therefore, power efficiency is starting to become as important, if not more important than fitting more and more transistors into a single chip. Recent processor designs have shown this emphasis as they put more focus on power efficiency rather than cramming as many transistors into a single chip as possible. In the world of embedded computers, power efficiency has long been an important goal next to throughput and latency.\n\n\n=== Shifts in market demand ===\nIncreases in clock frequency have grown more slowly over the past few years, compared to power reduction improvements. This has been driven by the end of Moore\'s Law and demand for longer battery life and reductions in size for mobile technology. This change in focus from higher clock rates to power consumption and miniaturization can be shown by the significant reductions in power consumption, as much as 50%, that were reported by Intel in their release of the Haswell microarchitecture; where they dropped their power consumption benchmark from 30–40 watts down to 10–20 watts. Comparing this to the processing speed increase of 3 GHz to 4 GHz (2002 to 2006), it can be seen that the focus in research and development is shifting away from clock frequency and moving towards consuming less power and taking up less space.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Sources ==\nJohn L. Hennessy and David Patterson (2006). Computer Architecture: A Quantitative Approach (Fourth ed.). Morgan Kaufmann. ISBN 978-0-12-370490-0.\nBarton, Robert S., "Functional Design of Computers", Communications of the ACM 4(9): 405 (1961).\nBarton, Robert S., "A New Approach to the Functional Design of a Digital Computer", Proceedings of the Western Joint Computer Conference, May 1961, pp. 393–396. About the design of the Burroughs B5000 computer.\nBell, C. Gordon; and Newell, Allen (1971). "Computer Structures: Readings and Examples", McGraw-Hill.\nBlaauw, G.A., and Brooks, F.P., Jr., "The Structure of System/360, Part I-Outline of the Logical Structure", IBM Systems Journal, vol. 3, no. 2, pp. 119–135, 1964.\nTanenbaum, Andrew S. (1979). Structured Computer Organization. Englewood Cliffs, New Jersey: Prentice-Hall. ISBN 0-13-148521-0.\n\n\n== External links ==\n\nISCA: Proceedings of the International Symposium on Computer Architecture\nMicro: IEEE/ACM International Symposium on Microarchitecture\nHPCA: International Symposium on High Performance Computer Architecture\nASPLOS: International Conference on Architectural Support for Programming Languages and Operating Systems\nACM Transactions on Architecture and Code Optimization\nIEEE Transactions on Computers\nThe von Neumann Architecture of Computer Systems at the Wayback Machine (archived 2017-10-31)',
  },
  {
    title: "DevOps",
    originalContent:
      'DevOps is a methodology integrating and automating the work of software development (Dev) and information technology operations (Ops). It serves as a means for improving and shortening the systems development life cycle. DevOps is complementary to agile software development; several DevOps aspects came from the agile approach.\nAutomation is an important part of DevOps. Software programmers and architects should use "fitness functions" to keep their software in check.\nAccording to Neal Ford, DevOps, particularly through continuous delivery, employs the "Bring the pain forward" principle, tackling tough tasks early, fostering automation and swift issue detection. \n\n\n== Definition ==\nOther than it being a cross-functional combination (and a portmanteau) of the terms and concepts for "development" and "operations", academics and practitioners have not developed a universal definition for the term "DevOps". Most often, DevOps is characterized by key principles: shared ownership, workflow automation, and rapid feedback.\nFrom an academic perspective, Len Bass, Ingo Weber, and Liming Zhu—three computer science researchers from the CSIRO and the Software Engineering Institute—suggested defining DevOps as "a set of practices intended to reduce the time between committing a change to a system and the change being placed into normal production, while ensuring high quality".\nHowever, the term is used in multiple contexts. At its most successful, DevOps is a combination of specific practices, culture change, and tools.\n\n\n== History ==\nProposals to combine software development methodologies with deployment and operations concepts began to appear in the late 80s and early 90s.\nAround 2007 and 2008, concerns were raised by those within the software development and IT communities that the separation between the two industries, where one wrote and created software entirely separate from those that deploy and support the software was creating a fatal level of dysfunction within the industry.\nIn 2009, the first conference named DevOps Days was held in Ghent, Belgium. The conference was founded by Belgian consultant, project manager and agile practitioner Patrick Debois. The conference has now spread to other countries.\nIn 2012, a report called "State of DevOps" was first published by Alanna Brown at Puppet Labs.\nAs of 2014, the annual State of DevOps report was published by Nicole Forsgren, Gene Kim, Jez Humble and others. They stated that the adoption of DevOps was accelerating. Also in 2014, Lisa Crispin and Janet Gregory wrote the book More Agile Testing, containing a chapter on testing and DevOps.\nIn 2016, the DORA metrics for throughput (deployment frequency, lead time for changes), and stability (mean time to recover, change failure rate) were published in the State of DevOps report. However, the research methodology and metrics were criticized by experts. In response to these criticisms, the 2023 State of DevOps report  published changes that updated the stability metric "mean time to recover" to "failed deployment recovery time" acknowledging the confusion the former metric has caused.\n\n\n== Relevant metrics ==\nDORA metrics are a set of key metrics developed by DevOps Research and Assessment (DORA) which can help to measure software development efficiency and reliability. These metrics include: \n\nDeployment Frequency: Time between code deployments.\nMean Lead Time for Changes: Time between code commit and deployment.\nChange Failure Rate: Percentage of deployments causing production issues.\nMean Time To Recovery: Time to resolve production issues.\nReliability (added in 2021):  Measures operational performance, focusing on availability and adherence to user expectations.\nThese metrics, when applied appropriately and within relevant context, facilitate insights into DevOps performance, enabling teams to optimize deployment speed, reliability and quality, thereby informing data-driven decisions to enhance software development processes. \n\n\n== Relationship to other approaches ==\nMany of the ideas fundamental to DevOps practices are inspired by, or mirror, other well known practices such as Lean and Deming\'s Plan-Do-Check-Act cycle, through to The Toyota Way and the Agile approach of breaking down components and batch sizes. Contrary to the "top-down" prescriptive approach and rigid framework of ITIL in the 1990s, DevOps is "bottom-up" and flexible, having been created by software engineers for their own needs.\n\n\n=== Agile ===\n\nThe motivations for what has become modern DevOps and several standard DevOps practices such as automated build and test, continuous integration, and continuous delivery originated in the Agile world, which dates (informally) to the 1990s, and formally to 2001. Agile development teams using methods such as extreme programming couldn\'t "satisfy the customer through early and continuous delivery of valuable software" unless they took responsibility for operations and infrastructure for their applications, automating much of that work. Because Scrum emerged as the dominant Agile framework in the early 2000s and it omitted the engineering practices that were part of many Agile teams, the movement to automate operations and infrastructure functions splintered from Agile and expanded into what has become modern DevOps. Today, DevOps focuses on the deployment of developed software, whether it is developed using Agile oriented methodologies or other methodologies.\n\n\n=== ArchOps ===\nArchOps presents an extension for DevOps practice, starting from software architecture artifacts, instead of source code, for operation deployment. ArchOps states that architectural models are first-class entities in software development, deployment, and operations.\n\n\n=== Continuous Integration and Delivery (CI/CD) ===\n\nAutomation is a core principle for achieving DevOps success and CI/CD is a critical component. Plus, improved collaboration and communication between and within teams helps achieve faster time to market, with reduced risks.\n\n\n=== Mobile DevOps ===\n\nMobile DevOps is a set of practices that applies the principles of DevOps specifically to the development of mobile applications. Traditional DevOps focuses on streamlining the software development process in general, but mobile development has its own unique challenges that require a tailored approach. Mobile DevOps is not simply as a branch of DevOps specific to mobile app development, instead an extension and reinterpretation of the DevOps philosophy due to very specific requirements of the mobile world.\n\n\n=== Site-reliability engineering ===\n\nIn 2003, Google developed site reliability engineering (SRE), an approach for releasing new features continuously into large-scale high-availability systems while maintaining high-quality end-user experience. While SRE predates the development of DevOps, they are generally viewed as being related to each other. Some of the original authors of the discipline consider SRE as an implementation of DevOps.\n\n\n=== Toyota production system, lean thinking, kaizen ===\n\nToyota production system, also known under the acronym TPS, was the inspiration for lean thinking with its focus on continuous improvement, kaizen, flow and small batches. The andon cord principle to create fast feedback, swarm and solve problems stems from TPS.\n\n\n=== DevSecOps, shifting security left ===\nDevSecOps is an augmentation of DevOps to allow for security practices to be integrated into the DevOps approach. Contrary to a traditional centralized security team model, each delivery team is empowered to factor in the correct security controls into their software delivery. Security practices and testing are performed earlier in the development lifecycle, hence the term "shift left". Security is tested in three main areas: static, software composition, and dynamic.\nChecking software statically via static application security testing (SAST) is white-box testing with special focus on security. Depending on the programming language, different tools are needed to do such static code analysis. The software composition is analyzed, especially libraries, and the version of each component is checked against vulnerability lists published by CERT and other expert groups. When giving software to clients, library licenses and their match to the license of the software distributed are in focus, especially copyleft licenses.\nIn dynamic testing, also called black-box testing, software is tested without knowing its inner functions. In DevSecOps this practice may be referred to as dynamic application security testing (DAST) or penetration testing. The goal is early detection of defects including cross-site scripting and SQL injection vulnerabilities. Threat types are published by the open web application security project, e.g. its TOP10, and by other bodies. \nDevSecOps has also been described as a cultural shift involving a holistic approach to producing secure software by integrating security education, security by design, and security automation.\n\n\n== Cultural change ==\nDevOps initiatives can create cultural changes in companies by transforming the way operations, developers, and testers collaborate during the development and delivery processes. Getting these groups to work cohesively is a critical challenge in enterprise DevOps adoption. DevOps is as much about culture as it is about the toolchain.\n\n\n=== Microservices ===\nAlthough in principle it is possible to practice DevOps with any architectural style, the microservices architectural style is becoming the standard for building continuously deployed systems. Small size service allows the architecture of an individual service to emerge through continuous refactoring.\n\n\n=== DevOps automation ===\nIt also supports consistency, reliability, and efficiency within the organization, and is usually enabled by a shared code repository or version control. As DevOps researcher Ravi Teja Yarlagadda hypothesizes, "Through DevOps, there is an assumption that all functions can be carried out, controlled, and managed in a central place using a simple code."\n\n\n==== Automation with version control ====\nMany organizations use version control to power DevOps automation technologies like virtual machines, containerization (or OS-level virtualization), and CI/CD. The paper "DevOps: development of a toolchain in the banking domain" notes that with teams of developers working on the same project, "All developers need to make changes to the same codebase and sometimes edit even the same files. For efficient working, there has to be a system that helps engineers avoid conflicts and retain the codebase history," with the Git version control system and the GitHub platform referenced as examples.\n\n\n== GitOps ==\nGitOps evolved from DevOps. The specific state of deployment configuration is version-controlled. Because the most popular version-control is Git, GitOps\' approach has been named after Git. Changes to configuration can be managed using code review practices, and can be rolled back using version-controlling. Essentially, all of the changes to a code are tracked, bookmarked, and making any updates to the history can be made easier. As explained by Red Hat, "visibility to change means the ability to trace and reproduce issues quickly, improving overall security."\n\n\n== Best practices for cloud systems ==\nThe following practices can enhance productivity of DevOps pipelines, especially in systems hosted in the cloud: \n\nNumber of Pipelines: Small teams can be more productive by having one repository and one pipeline. In contrast, larger organizations may have separate repositories and pipelines for each team or even separate repositories and pipelines for each service within a team.\nPermissions: In the context of pipeline-related permissions, adhering to the principle of least privilege can be challenging due to the dynamic nature of architecture. Administrators may opt for more permissive permissions while implementing compensating security controls to minimize the blast radius.\n\n\n== See also ==\nDataOps\nDevOps toolchain\nTwelve-Factor App methodology\nInfrastructure as code\nLean software development\nSite reliability engineering\nValue stream\nList of build automation software\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nDavis, Jennifer; Daniels, Ryn (2016-05-30). Effective DevOps : building a culture of collaboration, affinity, and tooling at scale. Sebastopol, CA: O\'Reilly. ISBN 9781491926437. OCLC 951434424.\nKim, Gene; Debois, Patrick; Willis, John; Humble, Jez; Allspaw, John (2015-10-07). The DevOps handbook : how to create world-class agility, reliability, and security in technology organizations (First ed.). Portland, OR. ISBN 9781942788003. OCLC 907166314.{{cite book}}:  CS1 maint: location missing publisher (link)\nForsgren, Nicole; Humble, Jez; Kim, Gene (27 March 2018). Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations (First ed.). IT Revolution Press. ISBN 9781942788331.',
  },
  {
    title: "Continuous integration",
    originalContent:
      'Continuous integration (CI) is the practice of integrating source code changes frequently and ensuring that the integrated codebase is in a workable state.\nTypically, developers merge changes to an integration branch, and an automated system builds and tests the software system. \nOften, the automated process runs on each commit or runs on a schedule such as once a day. \nGrady Booch first proposed the term CI in 1991, although he did not advocate integrating multiple times a day, but later, CI came to include that aspect.\n\n\n== History ==\n\nThe earliest known work (1989) on continuous integration was the Infuse environment developed by G. E. Kaiser, D. E. Perry, and W. M. Schell.\nIn 1994, Grady Booch used the phrase continuous integration in Object-Oriented Analysis and Design with Applications (2nd edition) to explain how, when developing using micro processes, "internal releases represent a sort of continuous integration of the system, and exist to force closure of the micro process".\nIn 1997, Kent Beck and Ron Jeffries invented extreme programming (XP) while on the Chrysler Comprehensive Compensation System project, including continuous integration. Beck published about continuous integration in 1998, emphasising the importance of face-to-face communication over technological support. In 1999, Beck elaborated more in his first full book on Extreme Programming. CruiseControl, one of the first open-source CI tools, was released in 2001.\nIn 2010, Timothy Fitz published an article detailing how IMVU\'s engineering team had built and been using the first practical CD system. While his post was originally met with skepticism, it quickly caught on and found widespread adoption as part of the lean software development methodology, also based on IMVU.\n\n\n== Practices ==\nThe core activities of CI are developers co-locate code changes in a shared, integration area frequently and that the resulting integrated codebase is verified for correctness. The first part generally involves merging changes to a common version control branch. The second part generally involves automated processes including: building, testing and many other processes.\nTypically, a server builds from the integration area frequently; i.e. after each commit or periodically like once a day. The server may perform quality control checks such as running unit tests and collect software quality metrics via processes such as static analysis and performance testing.\n\n\n== Related practices ==\n‹The template How-to is being considered for merging.› \nThis section lists best practices from practitioners for other practices that enhance CI.\n\n\n=== Build automation ===\nBuild automation is a best practice.\n\n\n=== Atomic commits ===\nCI requires the version control system to support atomic commits; i.e., all of a developer\'s changes are handled as a single commit.\n\n\n=== Committing changes ===\nWhen making a code change, a developer creates a branch that is a copy of the current codebase. As other changes are committed to the repository, this copy diverges from the latest version.\nThe longer development continues on a branch without merging to the integration branch, the greater the risk of multiple integration conflicts and failures when the developer branch is eventually merged back. When developers submit code to the repository they must first update their code to reflect the changes in the repository since they took their copy. The more changes the repository contains, the more work developers must do before submitting their own changes.\nEventually, the repository may become so different from the developers\' baselines that they enter what is sometimes referred to as "merge hell", or "integration hell", where the time it takes to integrate exceeds the time it took to make their original changes.\n\n\n=== Testing locally ===\nProponents of CI suggest that developers should \nuse test-driven development and to\nensure that all unit tests pass locally before committing to the integration branch so that one developer\'s work does not break another developer\'s copy. \nIncomplete features can be disabled before committing, using feature toggles.\n\n\n=== Continuous delivery and continuous deployment ===\nContinuous delivery ensures the software checked in on an integration branch is always in a state that can be deployed to users, and continuous deployment automates the deployment process.\nContinuous delivery and continuous deployment are often performed in conjunction with CI and together form a CI/CD pipeline. \n\n\n=== Version control ===\n\nProponents of CI recommend storing all files and information needed for building in version control, (for git a repository); that the system should be buildable from a fresh checkout and not require additional dependencies. \nMartin Fowler recommends that all developers commit to the same integration branch. \n\n\n=== Automate the build ===\n\nBuild automation tools automate building. \nProponents of CI recommend that a single command should have the capability of building the system. \nAutomation often includes automating the integration, which often includes deployment into a production-like environment. In many cases, the build script not only compiles binaries but also generates documentation, website pages, statistics and distribution media (such as Debian DEB, Red Hat RPM or Windows MSI files).\n\n\n=== Commit frequently ===\nDevelopers can reduce the effort of resolving conflicting changes by synchronizing changes with each other frequently; at least daily. Checking in a week\'s worth of work risks conflict both in likelihood of occurrence and complexity to resolve. Relatively small conflicts are significantly easier to resolve than larger ones. Integrating (committing) changes at least once a day is considered good practice, and more often better.\n\n\n=== Daily build ===\nBuilding daily, if not more often, is generally recommended.\n\n\n=== Every commit should be built ===\nThe system should build commits to the current working version to verify that they integrate correctly. A common practice is to use Automated Continuous Integration, although this may be done manually. Automated Continuous Integration employs a continuous integration server or daemon to monitor the revision control system for changes, then automatically run the build process.\n\n\n=== Every bug-fix commit should come with a test case ===\nWhen fixing a bug, it is a good practice to push a test case that reproduces the bug. This avoids the fix to be reverted, and the bug to reappear, which is known as a regression.\n\n\n=== Keep the build fast ===\nThe build needs to complete rapidly so that if there is a problem with integration, it is quickly identified.\n\n\n=== Test in a clone of the production environment ===\n\nHaving a test environment can lead to failures in tested systems when they deploy in the production environment because the production environment may differ from the test environment in a significant way. However, building a replica of a production environment is cost-prohibitive. Instead, the test environment or a separate pre-production environment ("staging") should be built to be a scalable version of the production environment to alleviate costs while maintaining technology stack composition and nuances. Within these test environments, service virtualisation is commonly used to obtain on-demand access to dependencies (e.g., APIs, third-party applications, services, mainframes, etc.) that are beyond the team\'s control, still evolving, or too complex to configure in a virtual test lab.\n\n\n=== Make it easy to get the latest deliverables ===\nMaking builds readily available to stakeholders and testers can reduce the amount of rework necessary when rebuilding a feature that doesn\'t meet requirements. Additionally, early testing reduces the chances that defects survive until deployment. Finding errors earlier can reduce the amount of work necessary to resolve them.\nAll programmers should start the day by updating the project from the repository. That way, they will all stay up to date.\n\n\n=== Everyone can see the results of the latest build ===\nIt should be easy to find out whether the build breaks and, if so, who made the relevant change and what that change was.\n\n\n=== Automate deployment ===\nMost CI systems allow the running of scripts after a build finishes. In most situations, it is possible to write a script to deploy the application to a live test server that everyone can look at. A further advance in this way of thinking is continuous deployment, which calls for the software to be deployed directly into production, often with additional automation to prevent defects or regressions.\n\n\n== Benefits ==\n\nCI benefits include:\n\nFacilitates detecting bugs earlier\nReduces effort to find cause of bugs; if a CI test fails then changes since last good build contain causing change; if build after each change then exactly one change is the cause\nAvoids the chaos of integrating many changes\nWhen a test fails or a bug is found, reverting the codebase to a good state results in fewer lost changes\nFrequent availability of a known-good build for testing, demo, and release\nFrequent code commit encourages modular, less complex code\nQuick feedback on system-wide impact of code changes\nSupports collection of software metrics such as code coverage, code complexity\n\n\n== Risks ==\nRisks of CI include:\n\nBuild system setup requires effort\nWriting and maintaining an automated test suite requires effort\nValue added depends on the quality of tests\nHigh build latency (sitting in queue) limits value\nImplies that incomplete code should not be integrated which is counter to some developer\'s preferred practice\nSafety and mission-critical development assurance (e.g., DO-178C, ISO 26262) require documentation and review which may be difficult to achieve\n\n\n== Best practices for cloud systems ==\nThe following practices can enhance productivity of pipelines, especially in systems hosted in the cloud: \n\nNumber of Pipelines: Small teams can be more productive by having one repository and one pipeline. In contrast, larger organizations may have separate repositories and pipelines for each team or even separate repositories and pipelines for each service within a team.\nPermissions: In the context of pipeline-related permissions, adhering to the principle of least privilege can be challenging due to the dynamic nature of architecture. Administrators may opt for more permissive permissions while implementing compensating security controls to minimize the blast radius.\n\n\n== See also ==\nApplication release automation – Process of packaging and deploymentPages displaying short descriptions of redirect targets\nBuild light indicator – visual device used in agile software development to inform the team on the build progressPages displaying wikidata descriptions as a fallback\nComparison of continuous integration software\nContinuous design – modular design process in which components can be freely substituted to improve the design, modify performance or change another feature at a later timePages displaying wikidata descriptions as a fallback\nContinuous testing – process of executing automated tests as part of the software delivery pipeline to obtain immediate feedback on the business risks associated with a release candidatePages displaying wikidata descriptions as a fallback\nMulti-stage continuous integration – Software development technique\nRapid application development – Concept of software development\n\n\n== References ==\n\n\n== External links ==\n"Continuous Integration" (wiki) (a collegial discussion). C2. {{cite journal}}: Cite journal requires |journal= (help)\nRichardson, Jared. "Continuous Integration: The Cornerstone of a Great Shop" (introduction).\nFlowers, Jay. "A Recipe for Build Maintainability and Reusability". Archived from the original on 25 June 2020. Retrieved 28 May 2006.\nDuvall, Paul (4 December 2007). "Developer works". IBM.\n"Version lifecycle". MediaWiki. June 2024.',
  },
  {
    title: "Edge computing",
    originalContent:
      "Edge computing is a distributed computing model that brings computation and data storage closer to the sources of data. More broadly, it refers to any design that pushes computation physically closer to a user, so as to reduce the latency compared to when an application runs on a centralized data centre.\nThe term began being used in the 1990s to describe content delivery networks—these were used to deliver website and video content from servers located near users. In the early 2000s, these systems expanded their scope to hosting other applications, leading to early edge computing services. These services could do things like find dealers, manage shopping carts, gather real-time data, and place ads.\nThe Internet of Things (IoT), where devices are connected to the internet, is often linked with edge computing. However, it's important to understand that edge computing and IoT are not the same thing.\n\n\n== Definition ==\nEdge computing involves running computer programs that deliver quick responses close to where requests are made. Karim Arabi, during an IEEE DAC 2014 keynote and later at an MIT MTL Seminar in 2015, described edge computing as computing that occurs outside the cloud, at the network's edge, particularly for applications needing immediate data processing. \nEdge computing is often equated with fog computing, particularly in smaller setups. However, in larger deployments, such as smart cities, fog computing serves as a distinct layer between edge computing and cloud computing, with each layer having its own responsibilities.\n\"The State of the Edge\" report explains that edge computing focuses on servers located close to the end-users. Alex Reznik, Chair of the ETSI MEC ISG standards committee, defines 'edge' loosely as anything that's not a traditional data center.\nIn cloud gaming, edge nodes, known as \"gamelets,\" are typically within one or two network hops from the client, ensuring quick response times for real-time games.\nEdge computing might use virtualization technology to simplify deploying and managing various applications on edge servers.\n\n\n== Concept ==\nThe world's data is expected to grow 61 percent to 175 zettabytes by 2025. According to research firm Gartner, around 10 percent of enterprise-generated data is created and processed outside a traditional centralized data center or cloud. By 2025, the firm predicts that this figure will reach 75 percent. The increase in IoT devices at the edge of the network is producing a massive amount of data — storing and using all that data in cloud data centers pushes network bandwidth requirements to the limit. Despite the improvements in network technology, data centers cannot guarantee acceptable transfer rates and response times, which often is a critical requirement for many applications. Furthermore, devices at the edge constantly consume data coming from the cloud, forcing companies to decentralize data storage and service provisioning, leveraging physical proximity to the end user.\nIn a similar way, the aim of edge computing is to move the computation away from data centers towards the edge of the network, exploiting smart objects, mobile phones, or network gateways to perform tasks and provide services on behalf of the cloud. By moving services to the edge, it is possible to provide content caching, service delivery, persistent data storage, and IoT management resulting in better response times and transfer rates. At the same time, distributing the logic to different network nodes introduces new issues and challenges.\n\n\n=== Privacy and security ===\nThe distributed nature of this paradigm introduces a shift in security schemes used in cloud computing. In edge computing, data may travel between different distributed nodes connected through the Internet and thus requires special encryption mechanisms independent of the cloud. Edge nodes may also be resource-constrained devices, limiting the choice in terms of security methods. Moreover, a shift from centralized top-down infrastructure to a decentralized trust model is required.\nOn the other hand, by keeping and processing data at the edge, it is possible to increase privacy by minimizing the transmission of sensitive information to the cloud. Furthermore, the ownership of collected data shifts from service providers to end-users.\n\n\n=== Scalability ===\nScalability in a distributed network must face different issues. First, it must take into account the heterogeneity of the devices, having different performance and energy constraints, the highly dynamic condition, and the reliability of the connections compared to more robust infrastructure of cloud data centers. Moreover, security requirements may introduce further latency in the communication between nodes, which may slow down the scaling process.\nThe state-of-the-art scheduling technique can increase the effective utilization of edge resources and scales the edge server by assigning minimum edge resources to each offloaded task.\n\n\n=== Reliability ===\nManagement of failovers is crucial in order to keep a service alive. If a single node goes down and is unreachable, users should still be able to access a service without interruptions. Moreover, edge computing systems must provide actions to recover from a failure and alert the user about the incident. To this aim, each device must maintain the network topology of the entire distributed system, so that detection of errors and recovery become easily applicable. Other factors that may influence this aspect are the connection technologies in use, which may provide different levels of reliability, and the accuracy of the data produced at the edge that could be unreliable due to particular environment conditions. As an example, an edge computing device, such as a voice assistant, may continue to provide service to local users even during cloud service or internet outages.\n\n\n=== Speed ===\nEdge computing brings analytical computational resources close to the end users and therefore can increase the responsiveness and throughput of applications. A well-designed edge platform would significantly outperform a traditional cloud-based system. Some applications rely on short response times, making edge computing a significantly more feasible option than cloud computing. Examples range from IoT to autonomous driving, anything health or human / public safety relevant, or involving human perception such as facial recognition, which typically takes a human between 370-620 ms to perform. Edge computing is more likely to be able to mimic the same perception speed as humans, which is useful in applications such as augmented reality where the headset should preferably recognize who a person is at the same time as the wearer does.\n\n\n=== Efficiency ===\nDue to the nearness of the analytical resources to the end users, sophisticated analytical tools and Artificial Intelligence tools can run on the edge of the system. This placement at the edge helps to increase operational efficiency and is responsible for many advantages to the system.\nAdditionally, the usage of edge computing as an intermediate stage between client devices and the wider internet results in efficiency savings that can be demonstrated in the following example: A client device requires computationally intensive processing on video files to be performed on external servers. By using servers located on a local edge network to perform those computations, the video files only need to be transmitted in the local network. Avoiding transmission over the internet results in significant bandwidth savings and therefore increases efficiency.  Another example is voice recognition. If the recognition is performed locally, it is possible to send the recognized text to the cloud rather than audio recordings, significantly reducing the amount of required bandwidth.\n\n\n== Applications ==\nEdge application services reduce the volumes of data that must be moved, the consequent traffic, and the distance that data must travel. That provides lower latency and reduces transmission costs. Computation offloading for real-time applications, such as facial recognition algorithms, showed considerable improvements in response times, as demonstrated in early research. Further research showed that using resource-rich machines called cloudlets or micro data centers near mobile users, which offer services typically found in the cloud, provided improvements in execution time when some of the tasks are offloaded to the edge node. On the other hand, offloading every task may result in a slowdown due to transfer times between device and nodes, so depending on the workload, an optimal configuration can be defined.\nIoT-based power grid system enables communication of electricity and data to monitor and control the power grid, which makes energy management more efficient.\nOther notable applications include connected cars, autonomous cars, smart cities, Industry 4.0, home automation and satellite systems. The nascent field of edge artificial intelligence (edge AI) implements the artificial intelligence in an edge computing environment, on the device or close to where data is collected.\n\n\n== See also ==\n\n\n== References ==",
  },
  {
    title: "Mixed reality",
    originalContent:
      'Mixed reality (MR) is a term used to describe the merging of a real-world environment and a computer-generated one. Physical and virtual objects may co-exist in mixed reality environments and interact in real time.\nMixed reality that incorporates haptics has sometimes been referred to as visuo-haptic mixed reality.\nIn a physics context, the term "interreality system" refers to a virtual reality system coupled with its real-world counterpart. A 2007 paper describes an interreality system comprising a real physical pendulum coupled to a pendulum that only exists in virtual reality. This system has two stable states of motion: a "dual reality" state in which the motion of the two pendula are uncorrelated, and a "mixed reality" state in which the pendula exhibit stable phase-locked motion, which is highly correlated. The use of the terms "mixed reality" and "interreality" is clearly defined in the context of physics and may be slightly different in other fields, however, it is generally seen as, "bridging the physical and virtual world".\n\n\n== Applications ==\nMixed reality has been used in applications across fields including design, education, entertainment, military training, healthcare, product content management, and human-in-the-loop operation of robots.\n\n\n=== Education ===\nSimulation-based learning includes VR and AR based training and interactive, experiential learning. There are many potential use cases for mixed reality in both educational settings and professional training settings. In education, AR has been used to simulate historical battles, providing an unparalleled immersive experience for students and potentially enhanced learning experiences. In addition, AR has shown effectiveness in university education for health science and medical students within disciplines that benefit from 3D representations of models, such as physiology and anatomy.\n\n\n=== Entertainment ===\n\nFrom television shows to game consoles, mixed reality has many applications in the field of entertainment.\nThe 2004 British game show Bamzooki called upon child contestants to create virtual "Zooks" and watch them compete in a variety of challenges. The show used mixed reality to bring the Zooks to life. The television show ran for four seasons, ending in 2010.\nThe 2003 game show FightBox also called upon contestants to create competitive characters and used mixed reality to allow them to interact. Unlike Bamzoomi\'s generally non-violent challenges, the goal of FightBox was for new contestants to create the strongest fighter to win the competition.\nIn 2009, researchers presented to the International Symposium on Mixed and Augmented Reality (ISMAR) their social product called "BlogWall", which consisted of a projected screen on a wall. Users could post short text clips or images on the wall and play simple games such as Pong. The BlogWall also featured a poetry mode where it would rearrange the messages it received to form a poem and a polling mode where users could ask others to answer their polls.\nMario Kart Live: Home Circuit is a mixed reality racing game for the Nintendo Switch that was released in October 2020.[16a-New] The game allows players to use their home as a race track Within the first week of release, 73,918 copies were sold in Japan, making it the country\'s best selling game of the week.\nOther research has examined the potential for mixed reality to be applied to theatre, film, and theme parks.\n\n\n=== Military training ===\nThe first fully immersive mixed reality system was the Virtual Fixtures platform, which was developed in 1992 by Louis Rosenberg at the Armstrong Laboratories of the United States Air Force. It enabled human users to control robots in real-world environments that included real physical objects and 3D virtual overlays ("fixtures") that were added enhance human performance of manipulation tasks. Published studies showed that by introducing virtual objects into the real world, significant performance increases could be achieved by human operators.\nCombat reality can be simulated and represented using complex, layered data and visual aides, most of which are head-mounted displays (HMD), which encompass any display technology that can be worn on the user\'s head. Military training solutions are often built on commercial off-the-shelf (COTS) technologies, such as Improbable\'s synthetic environment platform, Virtual Battlespace 3 and VirTra, with the latter two platforms used by the United States Army. As of 2018, VirTra is being used by both civilian and military law enforcement to train personnel in a variety of scenarios, including active shooter, domestic violence, and military traffic stops. Mixed reality technologies have been used by the United States Army Research Laboratory to study how this stress affects decision-making. With mixed reality, researchers may safely study military personnel in scenarios where soldiers would not likely survive.\nIn 2017, the U.S. Army was developing the Synthetic Training Environment (STE), a collection of technologies for training purposes that was expected to include mixed reality. As of 2018, STE was still in development without a projected completion date. Some recorded goals of STE included enhancing realism and increasing simulation training capabilities and STE availability to other systems.\nIt was claimed that mixed-reality environments like STE could reduce training costs, such as reducing the amount of ammunition expended during training. In 2018, it was reported that STE would include representation of any part of the world\'s terrain for training purposes. STE would offer a variety of training opportunities for squad brigade and combat teams, including Stryker, armory, and infantry teams.\n\n\n=== Blended spaces ===\nA blended space is a space in which a physical environment and a virtual environment are deliberately integrated in a close knit way. The aim of blended space design is to provide people with the experience of feeling a sense of presence in the blended space, acting directly on the content of the blended space. Examples of blended spaces include augmented reality devices such as the Microsoft HoloLens and games such as Pokémon Go in addition to many smartphone tourism apps, smart meeting rooms and applications such as bus tracker systems.\nThe idea of blending comes  from the ideas of conceptual integration, or conceptual blending introduced by Gilles Fauconnier and Mark Turner.\nManuel Imaz and David Benyon introduced blending theory to look at concepts in software engineering and human-computer interaction.\nThe simplest implementation of a blended space requires two features. The first required feature is input. The input can range from tactile, to changes in the environment. The next required feature is notifications received from the digital spaces. The correspondences between the physical and digital space have to be abstracted and exploited by the design of the blended space. Seamless integration of both the spaces is rare. Blended spaces need anchoring points or technologies to link the spaces.\nA well designed blended space advertises and conveys the digital content in a subtle and unobtrusive way. Presence can be measured using physiological, behavioral, and subjective measures derived from the space.\n\nThere are two main components to any space. They are:\n\nObjects – The actual distinct objects which make up the medium/space. The objects thus effectively describe the space.\nAgents – Correspondents/users inside the space who interact with it through the objects.\nFor presence in a blended space, there must be a physical space and a digital space. In the context of blended space, the higher the communication between the physical and digital spaces, the richer the experience. This communication happens through the medium of correspondents which relay the state and nature of objects.For the purpose of looking at blended spaces, the nature and characteristics of any space can be represented by these factors:# Ontology – Different types of objects present in the space the total number of objects and the relationships between objects and the space.\n\nTopology – The way objects are placed and positioned.\nVolatility – Frequency with which the objects change.\nAgency – Medium of communication between the objects, and between the objects and users. Agency also encompasses the users inside the space.\nPhysical space – Physical spaces are spaces which afford spatial interaction. This kind of spatial interaction greatly impacts the user\'s cognitive model. Digital space – Digital space (also called the information space) consists of all the information content. This content can be in any form.\n\n\n=== Remote working ===\nMixed reality allows a global workforce of remote teams to work together and tackle an organization\'s business challenges. No matter where they are physically located, an employee can wear a headset and noise-canceling headphones and enter a collaborative, immersive virtual environment. As these applications can accurately translate in real time, language barriers become irrelevant. This process also increases flexibility. While many employers still use inflexible models of fixed working time and location, there is evidence that employees are more productive if they have greater autonomy over where, when, and how they work. Some employees prefer loud work environments, while others need silence. Some work best in the morning; others work best at night. Employees also benefit from autonomy in how they work because of different ways of processing information. The classic model for learning styles differentiates between visual, auditory, and kinesthetic learners.\nMachine maintenance can also be executed with the help of mixed reality. Larger companies with multiple manufacturing locations and a lot of machinery can use mixed reality to educate and instruct their employees. The machines need regular checkups and have to be adjusted every now and then. These adjustments are mostly done by humans, so employees need to be informed about needed adjustments. By using mixed reality, employees from multiple locations can wear headsets and receive live instructions about the changes. Instructors can operate the representation that every employee sees, and can glide through the production area, zooming in to technical details and explaining every change needed. Employees completing a five-minute training session with such a mixed-reality program have been shown to attain the same learning results as reading a 50-page training manual. An extension to this environment is the incorporation of live data from operating machinery into the virtual collaborative space and then associated with three dimensional virtual models of the equipment. This enables training and execution of maintenance, operational and safety work processes, which would otherwise be difficult in a live setting, while making use of expertise, no matter their physical location.\n\n\n=== Functional mockup ===\nMixed reality can be used to build mockups that combine physical and digital elements. With the use of simultaneous localization and mapping (SLAM), mockups can interact with the physical world to gain control of more realistic sensory experiences like object permanence, which would normally be infeasible or extremely difficult to track and analyze without the use of both digital and physical aides.\n\n\n=== Healthcare ===\nSmartglasses can be incorporated into the operating room to aide in surgical procedures; possibly displaying patient data conveniently while overlaying precise visual guides for the surgeon. Mixed reality headsets like the Microsoft HoloLens have been theorized to allow for efficient sharing of information between doctors, in addition to providing a platform for enhanced training. This can, in some situations (i.e. patient infected with contagious disease), improve doctor safety and reduce PPE use. While mixed reality has lots of potential for enhancing healthcare, it does have some drawbacks too. The technology may never fully integrate into scenarios when a patient is present, as there are ethical concerns surrounding the doctor not being able to see the patient. Mixed reality is also useful for healthcare education. For example, according to a 2022 report from the World Economic Forum, 85% of first-year medical students at Case Western Reserve University reported that mixed reality for teaching anatomy was "equivalent" or "better" than the in-person class.\n\n\n=== Product content management ===\nProduct content management before the advent of mixed reality consisted largely of brochures and little customer-product engagement outside of this 2-dimensional realm. With mixed reality technology improvements, new forms of interactive product content management has emerged. Most notably, 3-dimensional digital renderings of normally 2-dimensional products have increased reachability and effectiveness of consumer-product interaction.\n\n\n=== Human-in-the-loop operation of robots ===\nRecent advances in mixed-reality technologies have renewed interest in alternative modes of communication for human-robot interaction. Human operators wearing mixed reality glasses such as HoloLens can interact with (control and monitor) e.g. robots and lifting machines on site in a digital factory setup. This use case typically requires real-time data communication between a mixed reality interface with the machine / process / system,  which could be enabled by incorporating digital twin technology.\n\n\n=== Business firms ===\nMixed reality allows sellers to show the customers how a certain commodity will suit their demands. A seller may demonstrate how a certain product will fit into the homes of the buyer. The buyer with the assistance of the VR can virtually pick the item, spin around and place to their desired points. This improves the buyer\'s confidence of making a purchase and reduces the number of returns.\nArchitectural firms can allow customers to virtually visit their desired homes.\n\n\n== Display technologies and products ==\nWhile mixed reality refers to the intertwining of the virtual world and the physical world at a high level, there are a variety of digital mediums used to accomplish a mixed reality environment. They may range from handheld devices to entire rooms, each having practical uses in different disciplines.\n\n\n=== Cave automatic virtual environment ===\n\nThe cave automatic virtual environment (CAVE) is an environment, typically a small room located in a larger outer room, in which a user is surrounded by projected displays around them, above them, and below them. 3D glasses and surround sound complement the projections to provide the user with a sense of perspective that aims to simulate the physical world. Since being developed, CAVE systems have been adopted by engineers developing and testing prototype products. They allow product designers to test their prototypes before expending resources to produce a physical prototype, while also opening doors for "hands-on" testing on non-tangible objects such as microscopic environments or entire factory floors. After developing the CAVE, the same researchers eventually released the CAVE2, which builds off of the original CAVE\'s shortcomings. The original projections were substituted for 37 megapixel 3D LCD panels, network cables integrate the CAVE2 with the internet, and a more precise camera system allows the environment to shift as the user moves throughout it.\n\n\n=== Head-up display ===\n\nHead-up display (HUD) is a display that projects imagery directly in front of a viewer without heavily obfuscating their environment. A standard HUD is composed of three elements: a projector, which is responsible for overlaying the graphics of the HUD, the combiner, which is the surface the graphics are projected onto, and the computer, which integrates the two other components and computes any real-time calculations or adjustments. Prototype HUDs were first used in military applications to aid fighter pilots in combat, but eventually evolved to aid in all aspects of flight – not just combat. HUDs were then standardized across commercial aviation as well, eventually creeping into the automotive industry. One of the first applications of HUD in automotive transport came with Pioneer\'s Heads-up system, which replaces the driver-side sun visor with a display that projects navigation instructions onto the road in front of the driver. Major manufacturers such as General Motors, Toyota, Audi, and BMW have since included some form of head-up display in certain models.\n\n\n=== Head-mounted display ===\n\nA head-mounted display (HMD), worn over the entire head or worn in front of the eyes, is a device that uses one or two optics to project an image directly in front of the user\'s eyes. Its applications range across medicine, entertainment, aviation, and engineering, providing a layer of visual immersion that traditional displays cannot achieve. Head-mounted displays are most popular with consumers in the entertainment market, with major tech companies developing HMDs to complement their existing products. However, these head-mounted displays are virtual reality displays and do not integrate the physical world. Popular augmented reality HMDs, however, are more favorable in enterprise environments. Microsoft\'s HoloLens is an augmented reality HMD that has applications in medicine, giving doctors more profound real-time insight, as well as engineering, overlaying important information on top of the physical world. Another notable augmented reality HMD has been developed by Magic Leap, a startup developing a similar product with applications in both the private sector and the consumer market.\n\n\n=== Mobile devices ===\n\nMobile devices, including smartphones and tablets, have continued to increase in computing power and portability. Many modern mobile devices come equipped with toolkits for developing augmented reality applications. These applications allow developers to overlay computer graphics over videos of the physical world. The first augmented reality mobile game with widespread success was Pokémon GO, which released in 2016 and accumulated 800 million downloads. While entertainment applications utilizing AR have proven successful, productivity and utility apps have also begun integrating AR features. Google has released updates to their Google Maps application that includes AR navigation directions overlaid onto the streets in front of the user, as well as expanding their translate app to overlay translated text onto physical writing in over 20 foreign languages. Mobile devices are unique display technologies due to the fact that they are commonly equipped at all times.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nSigner, Beat & Curtin, Timothy J. (2017). Tangible Holograms: Towards Mobile Physical Augmentation of Virtual Objects, Technical Report WISE Lab, WISE-2017-01, March 2017.\nFleischmann, Monika; Strauss, Wolfgang (eds.) (2001). Proceedings Archived 3 March 2016 at the Wayback Machine of "CAST01//Living in Mixed Realities" Archived 16 November 2020 at the Wayback Machine Intl. Conf. On Communication of Art, Science and Technology, Fraunhofer IMK 2001, 401. ISSN 1618-1379 (Print), ISSN 1618-1387 (Internet).\nInteractive Multimedia Lab A research lab at the National University of Singapore focuses on Multi-modal Mixed Reality interfaces.\nMixed Reality Geographical Information System (MRGIS)\nCostanza, E., Kunz, A., and Fjeld, M. 2009. Mixed Reality: A Survey Costanza, E., Kunz, A., and Fjeld, M. 2009. Mixed Reality: A Survey. In Human Machine interaction: Research Results of the MMI Program, D. Lalanne and J. Kohlas (Eds.) LNCS 5440, pp. 47–68.\nH. Regenbrecht and C. Ott and M. Wagner and T. Lum and P. Kohler and W. Wilke and E. Mueller, An Augmented Virtuality Approach to 3D Videoconferencing, Proceedings of the 2nd IEEE and ACM International Symposium on Mixed and Augmented Reality, pp 290-291, 2003\nKristian Simsarian and Karl-Petter Akesson, Windows on the World: An example of Augmented Virtuality, Interface Sixth International Conference Montpellier, Man-machine interaction, pp 68-71, 1997\nMixed Reality Project: experimental applications on Mixed Reality (Augmented Reality, Augmented Virtuality) and Virtual Reality.\nMixed Reality Scale – Milgram and Kishino\'s (1994) Virtuality Continuum paraphrase with examples.\nIEICE Transactions on Information Systems, Vol E77-D, No.12 December 1994 - A taxonomy of mixed reality visual displays - Paul Milgram, Fumio Kishino Archived 4 May 2017 at the Wayback Machine\n\n\n== External links ==\n Media related to Mixed reality at Wikimedia Commons',
  },
  {
    title: "Artificial organs",
    originalContent:
      'An artificial organ is a human-made organ device or tissue that is implanted or integrated into a human – interfacing with living tissue – to replace a natural organ, to duplicate or augment a specific function or functions so the patient may return to a normal life as soon as possible. The replaced function does not have to be related to life support, but it often is. For example, replacement bones and joints, such as those found in hip replacements, could also be considered artificial organs.\nImplied by definition, is that the device must not be continuously tethered to a stationary power supply or other stationary resources such as filters or chemical processing units. (Periodic rapid recharging of batteries, refilling of chemicals, and/or cleaning/replacing of filters would exclude a device from being called an artificial organ.) Thus, a dialysis machine, while a very successful and critically important life support device that almost completely replaces the duties of a kidney, is not an artificial organ.\n\n\n== Purpose ==\nConstructing and installing artificial organs, an extremely research-intensive and expensive process initially, may entail many years of ongoing maintenance services not needed by a natural organ:\n\nproviding life support to prevent imminent death while awaiting a transplant (e.g. artificial heart);\ndramatically improving the patient\'s ability for self care (e.g. artificial limb);\nimproving the patient\'s ability to interact socially (e.g. cochlear implant); or\nimproving a patient\'s quality of life through cosmetic restoration after cancer surgery or an accident.\nThe use of any artificial organ by humans is almost always preceded by extensive experiments with animals. Initial testing in humans is frequently limited to those either already facing death or who have exhausted every other treatment possibility.\n\n\n== Examples ==\n\n\n=== Artificial limbs ===\n\nArtificial arms and legs, or prosthetics, are intended to restore a degree of normal function to amputees. Mechanical devices that allow amputees to walk again or continue to use two hands have probably been in use since ancient times, the most notable one being the simple peg leg. Since then, the development of artificial limbs has progressed rapidly. New plastics and other materials, such as carbon fiber have allowed artificial limbs to become stronger and lighter, limiting the amount of extra energy necessary to operate the limb. Additional materials have allowed artificial limbs to look much more realistic. Prostheses can roughly be categorized as upper- and lower-extremity and can take many shapes and sizes.\nNew advances in artificial limbs include additional levels of integration with the human body.  Electrodes can be placed into nervous tissue, and the body can be trained to control the prosthesis.  This technology has been used in both animals and humans. The prosthetic can be controlled by the brain using a direct implant or implant into various muscles.\n\n\n=== Bladder ===\n\nThe two main methods for replacing bladder function involve either redirecting urine flow or replacing the bladder in situ. Standard methods for replacing the bladder involve fashioning a bladder-like pouch from intestinal tissue.  As of 2017 methods to grow bladders using stem cells had been attempted in clinical research but this procedure was not part of medicine.\n\n\n=== Brain ===\n\nNeural prostheses are a series of devices that can substitute a motor, sensory or cognitive modality that might have been damaged as a result of an injury or a disease.\nNeurostimulators, including deep brain stimulators, send electrical impulses to the brain in order to treat neurological and movement disorders, including Parkinson\'s disease, epilepsy, treatment resistant depression, and other conditions such as urinary incontinence. Rather than replacing existing neural networks to restore function, these devices often serve by disrupting the output of existing malfunctioning nerve centers to eliminate symptoms.\nScientists in 2013 created a mini brain that developed key neurological components until the early gestational stages of fetal maturation.\n\n\n=== Corpora cavernosa ===\nTo treat erectile dysfunction, both corpora cavernosa can be irreversibly surgically replaced with manually inflatable penile implants. This is a drastic therapeutic surgery meant only for men who have complete impotence resistant to all other treatment approaches. An implanted pump in the groin or scrotum can be manipulated by hand to fill these artificial cylinders, normally sized to be direct replacements for the natural corpora cavernosa, from an implanted reservoir in order to achieve an erection.\n\n\n=== Ear ===\n\nIn cases when a person is profoundly deaf or severely hard of hearing in both ears, a cochlear implant may be surgically implanted. Cochlear implants bypass most of the peripheral auditory system to provide a sense of sound via a microphone and some electronics that reside outside the skin, generally behind the ear. The external components transmit a signal to an array of electrodes placed in the cochlea, which in turn stimulates the cochlear nerve.\nIn the case of an outer ear trauma, a craniofacial prosthesis may be necessary.\nThomas Cervantes and his colleagues, who are from Massachusetts General Hospital, built an artificial ear from sheep cartilage by a 3D printer. With a lot of calculations and models, they managed to build an ear shaped like a typical human one. Modeled by a plastic surgeon, they had to adjust several times so the artificial ear can have curves and lines just like a human ear.  The researchers said "The technology is now under development for clinical trials, and thus we have scaled up and redesigned the prominent features of the scaffold to match the size of an adult human ear and to preserve the aesthetic appearance after implantation." Their artificial ears have not been announced as successful, but they are  still currently developing the project. Each year, thousands of children were born with a congenital deformity called microtia, where the external ear does not fully develop. This could be a major step forward in medical and surgical microtia treatment.\n\n\n=== Eye ===\n\nThe most successful function-replacing artificial eye so far is actually an external miniature digital camera with a remote unidirectional electronic interface implanted on the retina, optic nerve, or other related locations inside the brain. The present state of the art yields only partial functionality, such as recognizing levels of brightness, swatches of color, and/or basic geometric shapes, proving the concept\'s potential.\nVarious researchers have demonstrated that the retina performs strategic image preprocessing for the brain. The problem of creating a completely functional artificial electronic eye is even more complex. Advances towards tackling the complexity of the artificial connection to the retina, optic nerve, or related brain areas, combined with ongoing advances in computer science, are expected to dramatically improve the performance of this technology.\n\n\n=== Heart ===\n\nCardiovascular-related artificial organs are implanted in cases where the heart, its valves, or another part of the circulatory system is in disorder. The artificial heart is typically used to bridge the time to heart transplantation, or to permanently replace the heart in case heart transplantation is impossible. Artificial pacemakers represent another cardiovascular device that can be implanted to either intermittently augment (defibrillator mode), continuously augment, or completely bypass the natural living cardiac pacemaker as needed. Ventricular assist devices are another alternative, acting as mechanical circulatory devices that partially or completely replace the function of a failing heart, without the removal of the heart itself.\nBesides these, lab-grown hearts and 3D bioprinted hearts are also being researched. Currently, scientists are limited in their ability to grow and print hearts due to difficulties in getting blood vessels and lab-made tissues to function cohesively.\n\n\n=== Liver ===\n\nHepaLife is developing a bioartificial liver device intended for the treatment of liver failure using stem cells. The artificial liver is designed to serve as a supportive device, either allowing the liver to regenerate upon failure, or to bridge the patient\'s liver functions until transplant is available. It is only made possible by the fact that it uses real liver cells (hepatocytes), and even then, it is not a permanent substitute.\n\n\n=== Lungs ===\n\nWith some almost fully functional, artificial lungs promise to be a great success in the near future. An Ann Arbor company MC3 is currently working on this type of medical device.\nExtracorporeal membrane oxygenation (ECMO) can be used to take significant load off of the native lung tissue and heart.  In ECMO, one or more catheters are placed into the patient and a pump is used to flow blood over hollow membrane fibers, which exchange oxygen and carbon dioxide with the blood.  Similar to ECMO, Extracorporeal CO2 Removal (ECCO2R) has a similar set-up, but mainly benefits the patient through carbon dioxide removal, rather than oxygenation, with the goal of allowing the lungs to relax and heal.\n\n\n=== Ovaries ===\n\nThe ground work for the development of the artificial ovary was laid in the early 1990s.\nReproductive age patients who develop cancer often receive chemotherapy or radiation therapy, which damages oocytes and leads to early menopause. An artificial human ovary has been developed at Brown University with self-assembled microtissues created using novel 3-D petri dish technology. In a study funded and conducted by the NIH in 2017, scientists were successful in printing 3-D ovaries and implanting them in sterile mice. In the future, scientists hope to replicate this in larger animals as well as humans. The artificial ovary will be used for the purpose of in vitro maturation of immature oocytes and the development of a system to study the effect of environmental toxins on folliculogenesis.\n\n\n=== Pancreas ===\n\nAn artificial pancreas is used to substitute endocrine functionality of a healthy pancreas for diabetic and other patients who require it. It can be used to improve insulin replacement therapy until glycemic control is practically normal as evident by the avoidance of the complications of hyperglycemia, and it can also ease the burden of therapy for the insulin-dependent. Approaches include using an insulin pump under closed loop control, developing a bio-artificial pancreas consisting of a biocompatible sheet of encapsulated beta cells, or using gene therapy.\n\n\n=== Red blood cells ===\n\nArtificial red blood cells (RBC) have already been in projects for about 60 years, but they started getting interest when the HIV-contaminated-donor blood crisis. Artificial RBCs will be dependent 100% on nanotechnology. A successful artificial RBC should be able to totally replace human RBC, which means it can carry on all the functions that a human RBC does.\nThe first artificial RBC, made by Chang and Poznanski in 1968, was made to transport Oxygen and Carbon Dioxide, also fulfilled antioxidant functions.\nScientists are working on a new kind of artificial RBC, which is one-fiftieth the size of a human RBC. They are made from purified human hemoglobin proteins that have been coated with a synthetic polymer. Thanks to the special materials of the artificial RBC, they can capture oxygen when blood pH is high, and release oxygen when blood pH is low. The polymer coating also keeps the hemoglobin from reacting with nitric oxide in the bloodstream, thus preventing dangerous constriction of the blood vessels. Allan Doctor, MD, stated that the artificial RBC can be used by anyone, with any blood type because the coating is immune silent.\n\n\n=== Testes ===\nMen whom have sustained testicular abnormalities through birth defects or injury have been able to replace the damaged testicle with a testicular prosthesis. Although the prosthetic does not restore biological reproductive function, the device has been shown to improve mental health for these patients.\n\n\n=== Thymus ===\nAn implantable machine that performs the function of a thymus does not exist. However, researchers have been able to grow a thymus from reprogrammed fibroblasts. They expressed hope that the approach could one day replace or supplement neonatal thymus transplantation.\nAs of 2017, researchers at UCLA developed an artificial thymus that, although not yet implantable, is capable of performing all functions of a true thymus.\nThe artificial thymus would play an important role in the immune system, and it would use blood stem cells to produce more T cells, which in turn, help the body fight infections. It would ultimately give the body a better ability to fight cancer cells. As people age, if their thymus stops working well, an artificial thymus could also be a potentially viable option.\nThe idea of using T cells to fight against infections has been around for a time, but until recently, the idea of using a T cell source, an artificial thymus is proposed. "We know that the key to creating a consistent and safe supply of cancer-fighting T cells would be to control the process in a way that deactivates all T cell receptors in the transplanted cells, except for the cancer-fighting receptors," said Dr. Gay Crooks of UCLA. The scientist also found that the T cells produced by the artificial thymus carried a diverse range of T cell receptors and worked similarly to the T cells produced by a normal thymus. Since they can work like human thymus, artificial thymus can supply a consistent amount of T cells to the body for the patients who are in need of treatments.\n\n\n=== Trachea ===\nThe field of artificial tracheas went through a period of high interest and excitement with the work of Paolo Macchiarini at the Karolinska Institute and elsewhere from 2008 to around 2014, with front-page coverage in newspapers and on television.  Concerns were raised about his work in 2014 and by 2016 he had been fired and high level management at Karolinska had been dismissed, including people involved in the Nobel Prize.\nAs of 2017 engineering a trachea – a hollow tube lined with cells – had proved more challenging than originally thought; challenges include the difficult clinical situation of people who present as clinical candidates, who generally have been through multiple procedures already; creating an implant that can become fully developed and integrate with host while withstanding respiratory forces, as well as the rotational and longitudinal movement the trachea undergoes.\n\n\n== Enhancement ==\n\nIt is also possible to construct and install an artificial organ to give its possessor abilities that are not naturally occurring. Research is proceeding in areas of vision, memory, and information processing. Some current research focuses on restoring short-term memory in accident victims and long-term memory in dementia patients.\nOne area of success was achieved when Kevin Warwick carried out a series of experiments extending his nervous system over the internet to control a robotic hand and the first direct electronic communication between the nervous systems of two humans.\nThis might also include the existing practice of implanting subcutaneous chips for identification and location purposes (ex. RFID tags).\n\n\n== Microchips ==\n\nOrgan chips are devices containing hollow microvessels filled with cells simulating tissue and/or organs as a microfluidic system that can provide key chemical and electrical signal information. This is distinct from an alternative use of the term microchip, which refers to small, electronic chips that are commonly used as an identifier and can also contain a transponder.\nThis information can create various applications such as creating "human in vitro models" for both healthy and diseased organs, drug advancements in toxicity screening as well as replacing animal testing.\nUsing 3D cell culture techniques enables scientists to recreate the complex extracellular matrix, ECM, found in in vivo to mimic human response to drugs and human diseases.\nOrgans on chips are used to reduce the failure rate in new drug development; microengineering these allows for a microenvironment to be modeled as an organ.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\nArtificial Organs.  ISSN 1525-1594.\nAmerican Society for Artificial Internal Organs (ASAIO)\n"Elon Musk wants to hook your brain up directly to computers — starting next year" at NBC News',
  },
  {
    title: "Gene editing",
    originalContent:
      "Gene editing may refer to:\n\nGenetic engineering of any organism by genome editing. Gene editing is the emerging molecular biology technique which makes very specific targeted changes by insertion, deletion or substitution of genetic material in an organism's DNA to obtain desired results. Examples of gene editing are CRISPR, zinc finger nuclease, transcription activator-like effector nuclease (TALEN), oligonucleotide directed mutagenesis + meganucleases.\nGenome editing, a type of genetic engineering\nGene therapy, the therapeutic delivery of nucleic acid polymers into a patient's cells as a drug to treat disease\nCRISPR gene editing, a genetic engineering technique.CRISPR are termed as (site directed nucleases) SDN since they target specific part of genome, there are 3 different categories of SDN. SDN1 makes random mutations at target site to repair the damaged host DNA without involving any foreign DNA.  SDN2 uses small non coding homologous repair DNA to achieve specific nucleotide sequence to repair the host DNA by (homology directed repair) HDR which is a natural nucleic acid repair system.  SDN3 uses a large stretch of protein coding donor DNA which is targeted for insertion through HDR at a predefined genomic locus.\nTALEN editing, using transcription activator-like effector nucleases. TALENs are another type of genome editing tool. They work by using engineered proteins that can recognize and bind to specific DNA sequences, which then triggers a cut in the DNA. TALENs are less efficient than CRISPR/Cas9, but they are still a useful tool for genome editing.\nZinc finger editing, using zinc finger nucleases\nNatural genetic engineering (NGE) has been proposed by molecular biologist James A. Shapiro to account for novelty created in the course of biological evolution.\n\n\n== See also ==\nGenetic editing, an approach to scholarly editing of literary texts",
  },
  {
    title: "Immunotherapy",
    originalContent:
      "Immunotherapy or biological therapy is the treatment of disease by activating or suppressing the immune system. Immunotherapies designed to elicit or amplify an immune response are classified as activation immunotherapies, while immunotherapies that reduce or suppress are classified as suppression immunotherapies. Immunotherapy is under preliminary research for its potential to treat various forms of cancer.\nCell-based immunotherapies are effective for some cancers. Immune effector cells such as lymphocytes, macrophages, dendritic cells, natural killer cells, and cytotoxic T lymphocytes work together to defend the body against cancer by targeting abnormal antigens expressed on the surface of tumor cells. Vaccine-induced immunity to COVID-19 relies mostly on an immunomodulatory T-cell response.\nTherapies such as granulocyte colony-stimulating factor (G-CSF), interferons, imiquimod and cellular membrane fractions from bacteria are licensed for medical use. Others including IL-2, IL-7, IL-12, various chemokines, synthetic cytosine phosphate-guanosine (CpG) oligodeoxynucleotides and glucans are involved in clinical and preclinical studies.\n\n\n== Immunomodulators ==\nImmunomodulators are the active agents of immunotherapy. They are a diverse array of recombinant, synthetic, and natural preparations.\n\n\n== Activation immunotherapies ==\n\n\n=== Cancer ===\n\nCancer treatment used to be focused on killing or removing cancer cells and tumours, with chemotherapy or surgery or radiation. In 2018 the Nobel Prize in Physiology or Medicine was awarded to James P. Allison and Tasuku Honjo \"for their discovery of cancer therapy by inhibition of negative immune regulation.\" Cancer immunotherapy attempts to stimulate the immune system to destroy tumours. A variety of strategies are in use or are undergoing research and testing. Randomized controlled studies in different cancers resulting in significant increase in survival and disease free period have been reported  and its efficacy is enhanced by 20–30% when cell-based immunotherapy is combined with conventional treatment methods.\nOne of the oldest forms of cancer immunotherapy is the use of BCG vaccine, which was originally to vaccinate against tuberculosis and later was found to be useful in the treatment of bladder cancer. BCG immunotherapy induces both local and systemic immune responses. The mechanisms by which BCG immunotherapy mediates tumor immunity have been widely studied, but they are still not completely understood.\nThe use of monoclonal antibodies in cancer therapy was first introduced in 1997 with rituximab, an anti-CD20 antibody for treatment of B cell lymphoma. Since then several monoclonal antibodies have been approved for treatment of various haematological malignancies as well as for solid tumours.\nThe extraction of G-CSF lymphocytes from the blood and expanding in vitro against a tumour antigen before reinjecting the cells with appropriate stimulatory cytokines. The cells then destroy the tumour cells that express the antigen. Topical immunotherapy utilizes an immune enhancement cream (imiquimod) which produces interferon, causing the recipient's killer T cells to destroy warts, actinic keratoses, basal cell cancer, vaginal intraepithelial neoplasia, squamous cell cancer, cutaneous lymphoma, and superficial malignant melanoma. Injection immunotherapy (\"intralesional\" or \"intratumoural\") uses mumps, candida, the HPV vaccine or trichophytin antigen injections to treat warts (HPV induced tumours).\nAdoptive cell transfer has been tested on lung and other cancers, with greatest success achieved in melanoma.\n\n\n==== Dendritic cell-based pump-priming or vaccination ====\nDendritic cells (DC) can be stimulated to activate a cytotoxic response towards an antigen. Dendritic cells, a type of antigen-presenting cell, are harvested from the person needing the immunotherapy. These cells are then either pulsed with an antigen or tumour lysate or transfected with a viral vector, causing them to display the antigen. Upon transfusion into the person, these activated cells present the antigen to the effector lymphocytes (CD4+ helper T cells, cytotoxic CD8+ T cells and B cells). This initiates a cytotoxic response against tumour cells expressing the antigen (against which the adaptive response has now been primed). The first FDA-approved cell-based immunotherapy, the cancer vaccine Sipuleucel-T is one example of this approach. The Immune Response Corporation (IRC) developed this immunotherapy and licensed the technology to Dendreon, which obtained FDA clearance.\nThe current approaches for DC-based vaccination are mainly based on antigen loading on in vitro-generated DCs from monocytes or CD34+ cells, activating them with different TLR ligands, cytokine combinations, and injecting them back to the patients. The in vivo targeting approaches comprise administering specific cytokines (e.g., Flt3L, GM-CSF) and targeting the DCs with antibodies to C-type lectin receptors or agonistic antibodies (e.g., anti-CD40) that are conjugated with antigen of interest. Multiple, next-generation anti-CD40 platforms are being actively developed. Future approach may target DC subsets based on their specifically expressed C-type lectin receptors or chemokine receptors. Another potential approach is the generation of genetically engineered DCs from induced pluripotent stem cells and use of neoantigen-loaded DCs for inducing better clinical outcome.\n\n\n==== T-cell adoptive transfer ====\nAdoptive cell transfer in vitro cultivates autologous, extracted T cells for later transfusion.\nAlternatively, Genetically engineered T cells are created by harvesting T cells and then infecting the T cells with a retrovirus that contains a copy of a T cell receptor (TCR) gene that is specialised to recognise tumour antigens. The virus integrates the receptor into the T cells' genome. The cells are expanded non-specifically and/or stimulated. The cells are then reinfused and produce an immune response against the tumour cells. The technique has been tested on refractory stage IV metastatic melanomas and advanced skin cancer. The first FDA-approved CAR-T drug, Kymriah, used this approach. To obtain the clinical and commercial supply of this CAR-T, Novartis purchased the manufacturing plant, the distribution system and hired the production team that produced Sipuleucel-T developed by Dendreon and the Immune Response Corporation.\nWhether T cells are genetically engineered or not, before re-infusion, lympho-depletion of the recipient is required to eliminate regulatory T cells as well as unmodified, endogenous lymphocytes that compete with the transferred cells for homeostatic cytokines. Lymphodepletion may be achieved by myeloablative chemotherapy, to which total body irradiation may be added for greater effect. Transferred cells multiplied in vivo and persisted in peripheral blood in many people, sometimes representing levels of 75% of all CD8+ T cells at 6–12 months after infusion. As of 2012, clinical trials for metastatic melanoma were ongoing at multiple sites. Clinical responses to adoptive transfer of T cells were observed in patients with metastatic melanoma resistant to multiple immunotherapies.\n\n\n==== Checkpoint inhibitors ====\n\nAnti-PD-1/PD-L1 and anti-CTLA-4 antibodies are the two types of checkpoint inhibitors currently available to patients. The approval of anti-cytotoxic T-lymphocyte-associated protein 4 (CTLA-4) and anti-programmed cell death protein 1 (PD-1) antibodies for human use has already resulted in significant improvements in disease outcomes for various cancers.\nAlthough these molecules were originally discovered as molecules playing a role in T cell activation or apoptosis, subsequent preclinical research showed their important role in the maintenance of peripheral immune tolerance.\nImmune checkpoint inhibitors are approved to treat some patients with a variety of cancer types, including melanoma, breast cancer, bladder cancer, cervical cancer, colon cancer, lung cancer head and neck cancer, or Hodgkin lymphoma.\nThese therapies have revolutionized cancer immunotherapy as they showed for the first time in many years of research in metastatic melanoma, which is considered one of the most immunogenic human cancers, an improvement in overall survival, with an increasing group of patients benefiting long-term from these treatments, although caution remains needed for specific subgroups.\nThe next generation of checkpoint inhibitors targets other receptors such as lymphocyte-activation gene 3 (LAG-3), T-cell immunoglobulin and mucin-domain containing-3 (TIM3), and T cell immunoreceptor with Ig and ITIM domains (TIGIT). Antibodies against these receptors have been evaluated in clinical studies, but have not yet been approved for widespread use.\n\n\n== Immune enhancement therapy ==\nAutologous immune enhancement therapy use a person's own peripheral blood-derived natural killer cells, cytotoxic T lymphocytes, epithelial cells and other relevant immune cells are expanded in vitro and then re-infused. The therapy has been tested against hepatitis C, chronic fatigue syndrome and HHV6 infection.\n\n\n== Suppression immunotherapies ==\nImmune suppression dampens an abnormal immune response in autoimmune diseases or reduces a normal immune response to prevent rejection of transplanted organs or cells.\n\n\n=== Immunosuppressive drugs ===\nImmunosuppressive drugs can be used to control the immune system with organ transplantation and with autoimmune disease. Immune responses depend on lymphocyte proliferation. Lymphocyte proliferation is the multiplication of lymphocyte cells used to fight and remember foreign invaders. Cytostatic drugs are a type of immunosuppressive drug that aids in slowing down the growth of rapidly dividing cells. Another example of an immunosuppressive drug is Glucocorticoids which are more specific inhibitors of lymphocyte activation. Glucocorticoids work by emulating actions of natural actions of the body's adrenal glands to help suppress the immune system, which is helpful with autoimmune diseases|, Alternatively, inhibitors of immunophilins more specifically target T lymphocyte activation, the process by which T-lymphocytes stimulate and begin to respond to a specific antigen, There is also Immunosuppressive antibodies which target steps in the immune response to prevent the body from attacking its tissues, which is a problem with autoimmune diseases, There are various other drugs that modulate immune responses and can be used to induce immune regulation. It was observed in a preclinical trial that regulation of the immune system by small immunosuppressive molecules such as vitamin D, dexamethasone, and curcumin could be helpful in preventing or treating chronic inflation. Given that the molecules are administered under a low-dose regimen and subcutaneously. A study provides a promising preclinical demonstration of the effectiveness and ease of preparation of Valrubicin-loaded immunoliposomes (Val-ILs) as a novel nanoparticle technology to target immunosuppressive cells. Val-ILs have the potential to be used as a precise and effective therapy based on targeted vesicle-mediated cell death of immunosuppressive cells. \n\n\n=== Immune tolerance ===\nThe body naturally does not launch an immune system attack on its own tissues. Models generally identify CD4+ T-cells at the centre of the autoimmune response. Loss of T-cell tolerance then unleashes B-cells and other immune effector cells on to the target tissue. The ideal tolerogenic therapy would target the specific T-cell clones co-ordinating the autoimmune attack.\nImmune tolerance therapies seek to reset the immune system so that the body stops mistakenly attacking its own organs or cells in autoimmune disease or accepts foreign tissue in organ transplantation. A  recent therapeutic approach is the infusion of regulatory immune cells into transplant recipients. The transfer of regulatory immune cells has the potential to inhibit the activity of effector.\nCreating immune tolerance reduces or eliminates the need for lifelong immunosuppression and attendant side effects. It has been tested on transplantations, rheumatoid arthritis, type 1 diabetes and other autoimmune disorders.\n\n\n=== Allergen immunotherapy ===\n\nImmunotherapy can also be used to treat allergies. While allergy treatments (such as antihistamines or corticosteroids) treat allergic symptoms, immunotherapy can reduce sensitivity to allergens, lessening its severity. Allergen immunotherapy can also be referred to as allergen desensitization or hypo-sensitization. Immunotherapy may produce long-term benefits. Immunotherapy is partly effective in some people and ineffective in others, but it offers people with allergies a chance to reduce or stop their symptoms.\nSubcutaneous allergen immunotherapy was first introduced in 1911 through the hypothesis that people with hay fever were sensitive to pollen from grass. A process was developed to create an extract by drawing out timothy pollen in distilled water and then boiling it. This was injected into patients in increasing doses to help alleviate symptoms. \nAllergen Immunotherapy is indicated for people who are extremely allergic or who cannot avoid specific allergens and when there is evidence of an IgE-mediated reaction that correlates with allergen symptoms. These IgE-mediated reactions can be identified via a blood IgE test or skin testing. If a specific IgE antibody is negative, there is no evidence that allergen immunotherapy will be effective for that patient.\nHowever, there are risks associated with allergen immunotherapy as it is the administration of an agent the patient is known to be highly allergic to. Patients are at increased risk of fatal anaphylaxis, local reaction at the site of injection, or life-threatening systemic allergic reactions.\nA promising approach to treat food allergies is the use of oral immunotherapy (OIT). OIT consists in a gradual exposure to increasing amounts of allergen can lead to the majority of subjects tolerating doses of food sufficient to prevent reaction on accidental exposure. Dosages increase over time, as the person becomes desensitized. This technique has been tested on infants to prevent peanut allergies.\n\n\n== Helminthic therapies ==\nWhipworm ova (Trichuris suis) and hookworm (Necator americanus) have been tested for immunological diseases and allergies, and have proved beneficial on multiple fronts, yet it is not entirely understood. Scientists have found that the immune response triggered by the burrowing of hookworm larvae to pass through the lungs and blood so the production of mast cells and specific antibodies are now present. They also reduce inflammation or responses ties to autoimmune diseases, but despite this, the hookworm's effects are considered to be negative typically. Helminthic therapy has been investigated as a treatment for relapsing remitting multiple sclerosis, Crohn's, allergies and asthma. While there is much to be learned about this, many researchers think that the change in the immune response is thanks to the parasites shifting to a more anti-inflammatory or regulatory system, which would in turn decrease inflammation and self inflicted immune damage as seen in Crohn's and multiple sclerosis. Specifically, MS patients saw lower relapse rates and calmer symptoms in some cases when experimenting with helminthic therapy. Hypothesized mechanisms include re-polarisation of the Th1 / Th2 response and modulation of dendritic cell function. The helminths downregulate the pro-inflammatory Th1 cytokines, interleukin-12 (IL-12), interferon-gamma (IFN-γ) and tumor necrosis factor-alpha (TNF-α), while promoting the production of regulatory Th2 cytokines such as IL-10, IL-4, IL-5 and IL-13.\nCo-evolution with helminths has shaped some of the genes associated with interleukin expression and immunological disorders, such Crohn's, ulcerative colitis and celiac disease. Helminths' relationship to humans as hosts should be classified as mutualistic or symbiotic. In some ways, the relationship is symbiotic because the worms themselves need the host (humans) for survival, because this body supplies them with nutrients and a home. From another perspective, it could be reasoned that it is mutualistic, being that the above information about benefits in autoimmune disorders continues to remain true and supported. Also, some say that the worms can regulate gut bacteria. Another possibility is one of this being a parasitic relationship, arguing that the possibile rosks of anemia and other disorders outweighs the benefits, yet this is significantly less supported, with the research alluding to the mutualitic and symbiotic approach being much more likely.\n\n\n== See also ==\nBiological response modifier\nSepsivac\nCheckpoint inhibitor\nInterleukin-2 immunotherapy\nImmunostimulant\nMicrotransplantation\nPhotoimmunotherapy in vitro or in vivo\n\n\n== References ==\n\n\n== External links ==\n\nLangreth R (12 February 2009). \"Cancer Miracles\". Forbes.\nInternational Society for Biological Therapy of Cancer\nCancer Research Institute Annual International Cancer Immunotherapy Symposia Series\nThe story behind immunotherapy's innovative cellular voyage",
  },
  {
    title: "Cancer research",
    originalContent:
      'Cancer research is research into cancer to identify causes and develop strategies for prevention, diagnosis, treatment, and cure.\nCancer research ranges from epidemiology, molecular bioscience to the performance of clinical trials to evaluate and compare applications of the various cancer treatments. These applications include surgery, radiation therapy, chemotherapy, hormone therapy, immunotherapy and combined treatment modalities such as chemo-radiotherapy. Starting in the mid-1990s, the emphasis in clinical cancer research shifted towards therapies derived from biotechnology research, such as cancer immunotherapy and gene therapy.\nCancer research is done in academia, research institutes, and corporate environments, and is largely government funded.\n\n\n== History ==\n\nCancer research has been ongoing for centuries. Early research focused on the causes of cancer. Percivall Pott identified the first environmental trigger (chimney soot) for cancer in 1775 and cigarette smoking was identified as a cause of lung cancer in 1950. Early cancer treatment focused on improving surgical techniques for removing tumors. Radiation therapy took hold in the 1900s. Chemotherapeutics were developed and refined throughout the 20th century.\nThe U.S. declared a "War on Cancer" in the 1970s, and increased the funding and support for cancer research.\n\n\n=== Seminal papers ===\nSome of the most highly cited and most influential research reports include:\n\nThe Hallmarks of Cancer, published in 2000, and Hallmarks of Cancer: The Next Generation, published in 2011, by Douglas Hanahan and Robert Weinberg. Together, these articles have been cited in over 30,000 published papers.\n\n\n== Types of research ==\nCancer research encompasses a variety of types and interdisciplinary areas of research. Scientists involved in cancer research may be trained in areas such as chemistry, biochemistry, molecular biology, physiology, medical physics, epidemiology, and biomedical engineering. Research performed on a foundational level is referred to as basic research and is intended to clarify scientific principles and mechanisms. Translational research aims to elucidate mechanisms of cancer development and progression and transform basic scientific findings into concepts that can be applicable to the treatment and prevention of cancer. Clinical research is devoted to the development of pharmaceuticals, surgical procedures, and medical technologies for the eventual treatment of patients.\n\n\n=== Prevention and epidemiology ===\nEpidemiologic analysis indicates that at least 35% of all cancer deaths in the world could now be avoided by primary prevention. According to a newer GBD systematic analysis, in 2019, ~44% of all cancer deaths — or ~4.5 million deaths or ~105 million lost disability-adjusted life years — were due to known clearly preventable risk factors, led by smoking, alcohol use and high BMI.\nHowever, one 2015 study suggested that between ~70% and ~90% of cancers are due to environmental factors and therefore potentially preventable. Furthermore, it is estimated that with further research cancer death rates could be reduced by 70% around the world even without the development of any new therapies. Cancer prevention research receives only 2–9% of global cancer research funding, albeit many of the options for prevention are already well-known without further cancer-specific research but are not reflected in economics and policy. Mutational signatures of various cancers, for example, could reveal further causes of cancer and support causal attribution.\n\n\n=== Detection ===\n\nPrompt detection of cancer is important, since it is usually more difficult to treat in later stages.  Accurate detection of cancer is also important because false positives can cause harm from unnecessary medical procedures.  Some screening protocols are currently not accurate (such as prostate-specific antigen testing).  Others such as a colonoscopy or mammogram are unpleasant and as a result some patients may opt out.  Active research is underway to address all these problems, to develop novel ways of cancer screening and to increase detection rates.\nFor example:\n\nMultimodal learning AI systems are being developed to help detect many cancer types via integrating different types of data.\nScientists work on identifying and measurability of novel biomarkers or sets of such to detect cancer early, such as tumor-associated mycobiomes and bacterial microbiomes\nResearchers investigate whether ants could be used as biosensors to detect cancer via urine\n\n\n=== Treatment ===\n\nEmerging topics of cancer treatment research include:\n\nAnti-cancer vaccines\nOncophage\nSipuleucel-T (Provenge) is a prostate cancer vaccine\nInactivated tumor cells are investigated as potential bifunctional cancer vaccines\nNewer forms of chemotherapy\nGene therapy\nPhotodynamic therapy\nRadiation therapy\nReoviridae (Reolysin drug therapy)\nTargeted therapy\nMedical microbots (including bacterial), nanobots and bacterial \'cyborg cells\'\nVirotherapy\nAntibodies\nPhotoimmunotherapy (for brain cancer)\nNatural killer cells can induce immunological memory. Research is being developed to modify their action against cancer.\nHow treatments can best be combined (in combination therapies)\n\n\n=== Cause and development of cancer ===\n\nResearch into the cause of cancer involves many different disciplines including genetics, diet, environmental factors (i.e. chemical carcinogens).  In regard to investigation of causes and potential targets for therapy, the route used starts with data obtained from clinical observations, enters basic research, and, once convincing and independently confirmed results are obtained, proceeds with clinical research, involving appropriately designed trials on consenting human subjects, with the aim to test safety and efficiency of the therapeutic intervention method.\nAn important part of basic research is characterization of the potential mechanisms of carcinogenesis, in regard to the types of genetic and epigenetic changes that are associated with cancer development. The mouse is often used as a mammalian model for manipulation of the function of genes that play a role in tumor formation, while basic aspects of tumor initiation, such as mutagenesis, are assayed on cultures of bacteria and mammalian cells.\n\n\n==== Genes involved in cancer ====\n\nThe goal of oncogenomics is to identify new oncogenes or tumor suppressor genes that may provide new insights into cancer diagnosis, predicting clinical outcome of cancers, and new targets for cancer therapies. As the Cancer Genome Project stated in a 2004 review article, "a central aim of cancer research has been to identify the mutated genes that are causally implicated in oncogenesis (cancer genes)." The Cancer Genome Atlas project is a related effort investigating the genomic changes associated with cancer, while the COSMIC cancer database documents acquired genetic mutations from hundreds of thousands of human cancer samples.\nThese large scale projects, involving about 350 different types of cancer, have identified ~130,000 mutations in ~3000 genes that have been mutated in the tumors. The majority occurred in 319 genes, of which 286 were tumor suppressor genes and 33 oncogenes.\nSeveral hereditary factors can increase the chance of cancer-causing mutations, including the activation of oncogenes or the inhibition of tumor suppressor genes. The functions of various onco- and tumor suppressor genes can be disrupted at different stages of tumor progression. Mutations in such genes can be used to classify the malignancy of a tumor. \nIn later stages, tumors can develop a resistance to cancer treatment. The identification of oncogenes and tumor suppressor genes is important to understand tumor progression and treatment success. The role of a given gene in cancer progression may vary tremendously, depending on the stage and type of cancer involved.\n\n\n==== Cancer epigenetics ====\n\n\n== Diet and cancer ==\n\nPeriods of intermittent fasting (time-restricted feeding which may not include caloric restriction) is investigated for potential usefulness in cancer prevention and treatment and as of 2021 additional trials are needed to elucidate the risks and benefits. In some cases, "caloric restrictions could hinder both cancer growth and progression, besides enhancing the efficacy of chemotherapy and radiation therapy". Caloric restriction mimetics, including some present in foods like spermidine, are also investigated for these or similar reasons. Such and similar dietary supplements may contribute to prevention or treatment, with candidate substances including apigenin, berberine, jiaogulan, and rhodiola rosea.\n\n\n== Research funding ==\n\nCancer research is funded by government grants, charitable foundations and pharmaceutical and biotechnology companies.\nIn the early 2000s, most funding for cancer research came from taxpayers and charities, rather than from corporations. In the US, less than 30% of all cancer research was funded by commercial researchers such as pharmaceutical companies. Per capita, public spending on cancer research by taxpayers and charities in the US was five times as much in 2002–03 as public spending by taxpayers and charities in the 15 countries that were full members of the European Union. As a percentage of GDP, the non-commercial funding of cancer research in the US was four times the amount dedicated to cancer research in Europe. Half of Europe\'s non-commercial cancer research is funded by charitable organizations.\nThe National Cancer Institute is the major funding institution in the United States. In the 2023 fiscal year, the NCI funded $7.1 billion in cancer research.\n\n\n== Difficulties ==\nDifficulties inherent to cancer research are shared with many types of biomedical research.\nCancer research processes have been criticised. These include, especially in the US, for the financial resources and positions required to conduct research. Other consequences of competition for research resources appear to be a substantial number of research publications whose results cannot be replicated.\n\n\n=== Replicability ===\n\n\n== Public participation ==\n\n\n=== Distributed computing ===\nOne can share computer time for distributed cancer research projects like Help Conquer Cancer. World Community Grid also had a project called Help Defeat Cancer. Other related projects include the Folding@home and Rosetta@home projects, which focus on groundbreaking protein folding and protein structure prediction research. Vodafone has also partnered with the Garvan Institute to create the DreamLab Project, which uses distributed computing via an app on cellphones to perform cancer research.\n\n\n=== Clinical trials ===\n\nMembers of the public can also join clinical trials as healthy control subjects or for methods of cancer detection.\nThere could be software and data-related procedures that increase participation in trials and make them faster and less expensive. One open source platform matches genomically profiled cancer patients to precision medicine drug trials.\n\n\n== Organizations ==\n\nOrganizations exist as associations for scientists participating in cancer research, such as the American Association for Cancer Research and American Society of Clinical Oncology, and as foundations for public awareness or raising funds for cancer research, such as Relay For Life and the American Cancer Society.\n\n\n=== Awareness campaigns ===\nSupporters of different types of cancer have adopted different colored awareness ribbons and promote months of the year as being dedicated to the support of specific types of cancer. The American Cancer Society began promoting October as Breast Cancer Awareness Month in the United States in the 1980s. Pink products are sold to both generate awareness and raise money to be donated for research purposes. This has led to pinkwashing, or the selling of ordinary products turned pink as a promotion for the company.\n\n\n== See also ==\n.cancerresearch\nExposome\n\n\n== References ==\n\n\n== External links ==\nCancer Genome Anatomy Project @ The NIH\nThe Integrative Cancer Biology Program @ National Cancer Institute',
  },
  {
    title: "Zika virus",
    originalContent:
      'Zika virus (ZIKV; pronounced  or ) is a member of the virus family Flaviviridae. It is spread by daytime-active Aedes mosquitoes, such as A. aegypti and A. albopictus. Its name comes from the Ziika Forest of Uganda, where the virus was first isolated in 1947. Zika virus shares a genus with the dengue, yellow fever, Japanese encephalitis, and West Nile viruses. Since the 1950s, it has been known to occur within a narrow equatorial belt from Africa to Asia. From 2007 to 2016, the virus spread eastward, across the Pacific Ocean to the Americas, leading to the 2015–2016 Zika virus epidemic.\nThe infection, known as Zika fever or Zika virus disease, often causes no or only mild symptoms, similar to a very mild form of dengue fever. While there is no specific treatment, paracetamol (acetaminophen) and rest may help with the symptoms. As of April 2019, no vaccines have been approved for clinical use, however a number of vaccines are currently in  clinical trials. Zika can spread from a pregnant woman to her baby. This can result in microcephaly, severe brain malformations, and other birth defects. Zika infections in adults may result rarely in Guillain–Barré syndrome.\nIn January 2016, the United States Centers for Disease Control and Prevention (CDC) issued travel guidance on affected countries, including the use of enhanced precautions, and guidelines for pregnant women including considering postponing travel. Other governments or health agencies also issued similar travel warnings, while Colombia, the Dominican Republic, Puerto Rico, Ecuador, El Salvador, and Jamaica advised women to postpone getting pregnant until more is known about the risks.\n\n\n== Virology ==\nZika virus belongs to the family Flaviviridae and the genus Flavivirus, thus is related to the dengue, yellow fever, Japanese encephalitis, and West Nile viruses. Like other flaviviruses, Zika virus is enveloped and icosahedral and has a nonsegmented, single-stranded, 10 kilobase, positive-sense RNA genome. It is most closely related to the Spondweni virus and is one of the two known viruses in the Spondweni virus clade.\n\nA positive-sense RNA genome can be directly translated into viral proteins. As in other flaviviruses, such as the similarly sized West Nile virus, the RNA genome encodes seven nonstructural proteins and three structural proteins in the form of a single polyprotein (Q32ZE1). One of the structural proteins encapsulates the virus. This protein is the flavivirus envelope glycoprotein, that binds to the endosomal membrane of the host cell to initiate endocytosis. The RNA genome forms a nucleocapsid along with copies of the 12-kDa capsid protein. The nucleocapsid, in turn, is enveloped within a host-derived membrane modified with two viral glycoproteins. Viral genome replication depends on the making of double-stranded RNA from the single-stranded, positive-sense RNA (ssRNA(+)) genome followed by transcription and replication to provide viral mRNAs and new ssRNA(+) genomes.\nA longitudinal study shows that 6 hours after cells are infected with Zika virus, the vacuoles and mitochondria in the cells begin to swell. This swelling becomes so severe, it results in cell death, also known as paraptosis. This form of programmed cell death requires gene expression. IFITM3 is a trans-membrane protein in a cell that is able to protect it from viral infection by blocking virus attachment. Cells are most susceptible to Zika infection when levels of IFITM3 are low. Once the cell has been infected, the virus restructures the endoplasmic reticulum, forming the large vacuoles, resulting in cell death.\nThere are two Zika lineages: the African lineage and the Asian lineage. Phylogenetic studies indicate that the virus spreading in the Americas is 89% identical to African genotypes, but is most closely related to the Asian strain that circulated in French Polynesia during the 2013–2014 outbreak.\nThe Asian strain appears to have first evolved around 1928.\n\n\n== Transmission ==\nThe vertebrate hosts of the virus were primarily monkeys in a so-called enzootic mosquito-monkey-mosquito cycle, with only occasional transmission to humans. Before 2007, Zika "rarely caused recognized \'spillover\' infections in humans, even in highly enzootic areas". Infrequently, however, other arboviruses have become established as a human disease and spread in a mosquito–human–mosquito cycle, like the yellow fever virus and the dengue fever virus (both flaviviruses), and the chikungunya virus (a togavirus). Though the reason for the pandemic is unknown, dengue, a related arbovirus that infects the same species of mosquito vectors, is known in particular to be intensified by urbanization and globalization. Zika is primarily spread by Aedes aegypti mosquitoes, and can also be transmitted through sexual contact or blood transfusions. The basic reproduction number (R0, a measure of transmissibility) of Zika virus has been estimated to be between 1.4 and 6.6 .\nIn 2015, news reports drew attention to the rapid spread of Zika in Latin America and the Caribbean. At that time, the Pan American Health Organization published a list of countries and territories that experienced "local Zika virus transmission" comprising Barbados, Bolivia, Brazil, Colombia, the Dominican Republic, Ecuador, El Salvador, French Guiana, Guadeloupe, Guatemala, Guyana, Haiti, Honduras, Martinique, Mexico, Panama, Paraguay, Puerto Rico, Saint Martin, Suriname, and Venezuela. By August 2016, more than 50 countries had experienced active (local) transmission of Zika virus.\n\n\n=== Mosquito ===\n\nZika is primarily spread by the female Aedes aegypti mosquito, which is active mostly in the daytime. The mosquitos must feed on blood to lay eggs.: 2  The virus has also been isolated from a number of arboreal mosquito species in the genus Aedes, such as A. africanus, A. apicoargenteus, A. furcifer, A. hensilli, A. luteocephalus, and A. vittatus, with an extrinsic incubation period in mosquitoes around 10 days.\nThe true extent of the vectors is still unknown. Zika has been detected in many more species of Aedes, along with Anopheles coustani, Mansonia uniformis, and Culex perfuscus, although this alone does not incriminate them as vectors. To detect the presence of the virus usually requires genetic material to be analysed in a lab using the technique RT-PCR. A much cheaper and faster method involves shining a light at the head and thorax of the mosquito, and detecting chemical compounds characteristic of the virus using near-infrared spectroscopy.\nTransmission by A. albopictus, the tiger mosquito, was reported from a 2007 urban outbreak in Gabon, where it had newly invaded the country and become the primary vector for the concomitant chikungunya and dengue virus outbreaks. New outbreaks can occur if a person carrying the virus travels to another region where A. albopictus is common.\nThe potential societal risk of Zika can be delimited by the distribution of the mosquito species that transmit it. The global distribution of the most cited carrier of Zika, A. aegypti, is expanding due to global trade and travel. A. aegypti distribution is now the most extensive ever recorded – on parts of all continents except Antarctica, including North America and even the European periphery (Madeira, the Netherlands, and the northeastern Black Sea coast). A mosquito population capable of carrying Zika has been found in a Capitol Hill neighborhood of Washington, DC, and genetic evidence suggests they survived at least four consecutive winters in the region. The study authors conclude that mosquitos are adapting for persistence in a northern climate. Zika virus appears to be contagious via mosquitoes for around a week after infection. The virus is thought to be infectious for a longer period of time after infection (at least 2 weeks) when transmitted via semen.\nResearch into its ecological niche suggests that Zika may be influenced to a greater degree by changes in precipitation and temperature than dengue, making it more likely to be confined to tropical areas. However, rising global temperatures would allow for the disease vector to expand its range further north, allowing Zika to follow.\n\n\n=== Sexual ===\nZika can be transmitted from men and women to their sexual partners; most known cases involve transmission from symptomatic men to women. As of April 2016, sexual transmission of Zika has been documented in six countries – Argentina, Australia, France, Italy, New Zealand, and the United States – during the 2015 outbreak. ZIKV can persist in semen for several months, with viral RNA detected up to one year. The virus replicates in the human testis, where it infects several cell types including testicular macrophages, peritubular cells and germ cells, the spermatozoa precursors. Semen parameters can be altered in patients for several weeks post-symptoms onset, and spermatozoa can be infectious. Since October 2016, the CDC has advised men who have traveled to an area with Zika should use condoms or not have sex for at least six months after their return as the virus is still transmissible even if symptoms never develop.\n\n\n=== Pregnancy ===\nZika virus can spread by vertical (or "mother-to-child") transmission, during pregnancy or at delivery. An infection during pregnancy has been linked to changes in neuronal development of the unborn child. Severe progressions of infection have been linked to the development of microcephaly in the unborn child, while mild infections potentially can lead to neurocognitive disorders later in life. Congenital brain abnormalities other than microcephaly have also been reported after a Zika outbreak. Studies in mice have suggested that maternal immunity to dengue virus may enhance fetal infection with Zika, worsen the microcephaly phenotype and/or enhance damage during pregnancy, but it is unknown whether this occurs in humans.\n\n\n=== Blood transfusion ===\nAs of April 2016, two cases of Zika transmission through blood transfusions have been reported globally, both from Brazil, after which the US Food and Drug Administration (FDA) recommended screening blood donors and deferring high-risk donors for 4 weeks. A potential risk had been suspected based on a blood-donor screening study during the French Polynesian Zika outbreak, in which 2.8% (42) of donors from November 2013 and February 2014 tested positive for Zika RNA and were all asymptomatic at the time of blood donation. Eleven of the positive donors reported symptoms of Zika fever after their donation, but only three of 34 samples grew in culture.\n\n\n== Pathogenesis ==\nZika virus replicates in the mosquito\'s midgut epithelial cells and then its salivary gland cells. After 5–10 days, the virus can be found in the mosquito\'s saliva. If the mosquito\'s saliva is inoculated into human skin, the virus can infect epidermal keratinocytes, skin fibroblasts in the skin and the Langerhans cells. The pathogenesis of the virus is hypothesized to continue with a spread to lymph nodes and the bloodstream. Flaviviruses replicate in the cytoplasm, but Zika antigens have been found in infected cell nuclei.\nThe viral protein numbered NS4A can lead to small head size (microcephaly) because it disrupts brain growth by hijacking a pathway which regulates growth of new neurons. In fruit flies, both NS4A and the neighboring NS4B restrict eye growth.\n\n\n== Zika fever ==\n\nZika fever (also known as Zika virus disease) is an illness caused by Zika virus. Around 80% of cases are estimated to be asymptomatic, though the accuracy of this figure is hindered by the wide variance in data quality, and figures from different outbreaks can vary significantly. Symptomatic cases are usually mild and can resemble dengue fever. Symptoms may include fever, red eyes, joint pain, headache, and a maculopapular rash. Symptoms generally last less than seven days. It has not caused any reported deaths during the initial infection. Infection during pregnancy causes microcephaly and other brain malformations in some babies. Infection in adults has been linked to Guillain–Barré syndrome (GBS) and Zika virus has been shown to infect human Schwann cells.\nDiagnosis is by testing the blood, urine, or saliva for the presence of Zika virus RNA when the person is sick. In 2019, an improved diagnostic test, based on research from Washington University in St. Louis, that detects Zika infection in serum was granted market authorization by the FDA.\nPrevention involves decreasing mosquito bites in areas where the disease occurs, and proper use of condoms. Efforts to prevent bites include the use of DEET or picaridin - based insect repellent, covering much of the body with clothing, mosquito nets, and getting rid of standing water where mosquitoes reproduce. There is no vaccine. Health officials recommended that women in areas affected by the 2015–2016 Zika outbreak consider putting off pregnancy and that pregnant women not travel to these areas. While no specific treatment exists, paracetamol (acetaminophen) and rest may help with the symptoms. Admission to a hospital is rarely necessary.\n\n\n=== Treatment ===\nIt is advised, for an affected person with the zika virus, to drink a lot of water to stay hydrated, to lie down, and to treat the fever and agony with liquid solutions; taking drugs like acetaminophen or paracetamol helps to relieve fever and pain. Referring to the US CDC it is not recommended to take anti-inflammatory and non-steroid drugs like aspirin for example. If the patient affected is already taking treatment for another medical condition it is advisable to inform your attending physician before taking any other drug or additional treatment;\n\n\n=== Vaccine development ===\n\nThe World Health Organization has suggested that priority should be to develop inactivated vaccines and other nonlive vaccines, which are safe to use in pregnant women.\nAs of March 2016, 18 companies and institutions were developing vaccines against Zika, but they state a vaccine is unlikely to be widely available for about 10 years.\nIn June 2016, the FDA granted the first approval for a human clinical trial for a Zika vaccine. In March 2017, a DNA vaccine was approved for phase-2 clinical trials. This vaccine consists of a small, circular piece of DNA, known as a plasmid, that expresses the genes for the Zika virus envelope proteins. As the vaccine does not contain the full sequence of the virus, it cannot cause infection. As of April 2017, both subunit and inactivated vaccines have entered clinical trials.\n\n\n== History ==\n\n\n=== Virus isolation in monkeys and mosquitoes, 1947 ===\nThe virus was first isolated in April 1947 from a rhesus macaque monkey placed in a cage in the Ziika Forest of Uganda, near Lake Victoria, by the scientists of the Yellow Fever Research Institute. A second isolation from the mosquito A. africanus followed at the same site in January 1948. When the monkey developed a fever, researchers isolated from its serum a "filterable transmissible agent" which was named Zika in 1948.\n\n\n=== First evidence of human infection, 1952 ===\nZika was first known to infect humans from the results of a serological survey in Uganda, published in 1952. Of 99 human blood samples tested, 6.1% had neutralizing antibodies. As part of a 1954 outbreak investigation of jaundice suspected to be yellow fever, researchers reported isolation of the virus from a patient, but the pathogen was later shown to be the closely related Spondweni virus. Spondweni was also determined to be the cause of a self-inflicted infection in a researcher reported in 1956.\n\n\n=== Spread in equatorial Africa and to Asia, 1951–present ===\nSubsequent serological studies in several African and Asian countries indicated the virus had been widespread within human populations in these regions. The first true case of human infection was identified by Simpson in 1964, who was himself infected while isolating the virus from mosquitoes. From then until 2007, there were only 13 further confirmed human cases of Zika infection from Africa and Southeast Asia. A study published in 2017 showed that the Zika virus, despite only a few cases were reported, has been silently circulated in West Africa for the last two decades when blood samples collected between 1992 and 2016 were tested for the ZIKV IgM antibodies.\nIn 2017, Angola reported two cases of Zika fever. Zika was also occurring in Tanzania as of 2016.\n\n\n=== Micronesia, 2007 ===\n\nIn April 2007, the first outbreak outside of Africa and Asia occurred on the island of Yap in the Federated States of Micronesia, characterized by rash, conjunctivitis, and arthralgia, which was initially thought to be dengue, chikungunya, or Ross River disease. Serum samples from patients in the acute phase of illness contained RNA of Zika. There were 49 confirmed cases, 59 unconfirmed cases, no hospitalizations, and no deaths.\n\n\n=== 2013–2014 ===\n\nAfter October 2013 Oceania\'s first outbreak showed an estimated 11% population infected for French Polynesia that also presented with Guillain–Barre syndrome (GBS). The spread of ZIKV continued to New Caledonia, Easter Island, and the Cook Islands and where 1385 cases were confirmed by January 2014. During the same year, Easter Island acknowledged 51 cases. Australia began seeing cases in 2012. Research showed it was brought by travelers returning from Indonesia and other infected countries. New Zealand also experienced infections rate increases through returning foreign travelers. Oceania countries experiencing Zika today are New Caledonia, Vanuatu, Solomon Islands, Marshall Islands, American Samoa, Samoa, and Tonga.\nBetween 2013 and 2014, further epidemics occurred in French Polynesia, Easter Island, the Cook Islands, and New Caledonia.\n\n\n=== Americas, 2015–present ===\n\nThere was an epidemic in 2015 and 2016 in the Americas. The outbreak began in April 2015 in Brazil, and spread to other countries in South America, Central America, North America, and the Caribbean. In January 2016, the WHO said the virus was likely to spread throughout most of the Americas by the end of the year; and in February 2016, the WHO declared the cluster of microcephaly and Guillain–Barré syndrome cases reported in Brazil – strongly suspected to be associated with the Zika outbreak – a Public Health Emergency of International Concern. It was estimated that 1.5 million people were infected by Zika in Brazil, with over 3,500 cases of microcephaly reported between October 2015 and January 2016.\nA number of countries issued travel warnings, and the outbreak was expected to significantly impact the tourism industry. Several countries took the unusual step of advising their citizens to delay pregnancy until more was known about the virus and its impact on fetal development. With the 2016 Summer Olympics hosted in Rio de Janeiro, health officials worldwide voiced concerns over a potential crisis, both in Brazil and when international athletes and tourists returned home and possibly would spread the virus. Some researchers speculated that only one or two tourists might be infected during the three-week period, or approximately 3.2 infections per 100,000 tourists. In November 2016, the World Health Organization declared that Zika virus was no longer a global emergency while noting that the virus still represents "a highly significant and a long-term problem".\nAs of August 2017 the number of new Zika virus cases in the Americas had fallen dramatically.\n\n\n=== India, Bangladesh ===\nOn May 15, 2017, three cases of Zika virus infection in India were reported in the state of Gujarat. By late 2018, there had been at least 159 cases in Rajasthan and 127 in Madhya Pradesh.\nIn July 2021, the first case of Zika virus infection in the Indian state of Kerala was reported. After the first confirmed case, 19 other people who had previously presented symptoms were tested, and 13 of those had positive results, showing that Zika had been circulating in Kerala since at least May 2021. By August 6th 2021, there had been 65 reported cases in Kerala.\nOn October 22, 2021, an officer in the Indian Air Force in Kanpur tested positive for Zika virus, making it the first reported case in the Indian state of Uttar Pradesh.\nOn 22 March 2016, Reuters reported that Zika was isolated from a 2014 blood sample of an elderly man in Chittagong in Bangladesh as part of a retrospective study.\n\n\n=== East Asia ===\nBetween August and November 2016, 455 cases of Zika virus infection were confirmed in Singapore.\nIn 2023, 722 Zika virus cases were reported in Thailand. From 2019-2022 the Robert Koch-Institut reported 29 imported Zikavirus cases imported into Germany. Of the altogether 16 imported Zika virus cases in 2023, 10 were diagnosed after a trip to Thailand with 62% of all Zika virus cases a significant relative and absolute increase.\n\n\n== See also ==\nWolbachia\nWorld mosquito program\n\n\n== References ==\n\n This article incorporates public domain material from websites or documents of the Centers for Disease Control and Prevention.\n\n\n== External links ==',
  },
  {
    title: "Geothermal energy",
    originalContent:
      "Geothermal energy is thermal energy extracted from the Earth's crust. It combines energy from the formation of the planet and from radioactive decay. Geothermal energy has been exploited as a source of heat and/or electric power for millennia.\nGeothermal heating, using water from hot springs, for example, has been used for bathing since Paleolithic times and for space heating since Roman times. Geothermal power, (generation of electricity from geothermal energy), has been used since the 20th century. Unlike wind and solar energy, geothermal plants produce power at a constant rate, without regard to weather conditions. Geothermal resources are theoretically more than adequate to supply humanity's energy needs. Most extraction occurs in areas near tectonic plate boundaries.\nThe cost of generating geothermal power decreased by 25% during the 1980s and 1990s. Technological advances continued to reduce costs and thereby expand the amount of viable resources. In 2021, the US Department of Energy estimated that power from a plant \"built today\" costs about $0.05/kWh.\nIn 2019, 13,900 megawatts (MW) of geothermal power was available worldwide. An additional 28 gigawatts provided heat for district heating, space heating, spas, industrial processes, desalination, and agricultural applications as of 2010. As of 2019 the industry employed about one hundred thousand people.\nThe adjective geothermal originates from the Greek roots γῆ (gê), meaning Earth, and θερμός (thermós), meaning hot.\n\n\n== History ==\n\nHot springs have been used for bathing since at least Paleolithic times. The oldest known spa is at the site of the Huaqing Chi palace. In the first century CE, Romans conquered Aquae Sulis, now Bath, Somerset, England, and used the hot springs there to supply public baths and underfloor heating. The admission fees for these baths probably represent the first commercial use of geothermal energy. The world's oldest geothermal district heating system, in Chaudes-Aigues, France, has been operating since the 15th century. The earliest industrial exploitation began in 1827 with the use of geyser steam to extract boric acid from volcanic mud in Larderello, Italy.\nIn 1892, the US's first district heating system in Boise, Idaho was powered by geothermal energy. It was copied in Klamath Falls, Oregon, in 1900. The world's first known building to utilize geothermal energy as its primary heat source was the Hot Lake Hotel in Union County, Oregon, beginning in 1907. A geothermal well was used to heat greenhouses in Boise in 1926, and geysers were used to heat greenhouses in Iceland and Tuscany at about the same time. Charles Lieb developed the first downhole heat exchanger in 1930 to heat his house. Geyser steam and water began heating homes in Iceland in 1943.\n\nIn the 20th century, geothermal energy came into use as a generating source. Prince Piero Ginori Conti tested the first geothermal power generator on 4 July 1904, at the Larderello steam field. It successfully lit four light bulbs. In 1911, the world's first commercial geothermal power plant was built there. It was the only industrial producer of geothermal power until New Zealand built a plant in 1958. In 2012, it produced some 594 megawatts.\nIn 1960, Pacific Gas and Electric began operation of the first US geothermal power plant at The Geysers in California. The original turbine lasted for more than 30 years and produced 11 MW net power.\nAn organic fluid based binary cycle power station was first demonstrated in 1967 in the USSR and later introduced to the US in 1981.  This technology allows the use of temperature resources as low as 81 °C. In 2006, a binary cycle plant in Chena Hot Springs, Alaska, came on-line, producing electricity from a record low temperature of 57 °C (135 °F).\n\n\n== Resources ==\n\nThe Earth has an internal heat content of 1031 joules (3·1015 TWh), About 20% of this is residual heat from planetary accretion; the remainder is attributed to past and current radioactive decay of naturally occurring isotopes. For example, a 5275 m deep borehole in United Downs Deep Geothermal Power Project in Cornwall, England, found granite with very high thorium content, whose radioactive decay is believed to power the high temperature of the rock.\nEarth's interior temperature and pressure are high enough to cause some rock to melt and the solid mantle to behave plastically. Parts of the mantle convect upward since it is lighter than the surrounding rock. Temperatures at the core–mantle boundary can reach over 4,000 °C (7,230 °F).\nThe Earth's internal thermal energy flows to the surface by conduction at a rate of 44.2 terawatts (TW), and is replenished by radioactive decay of minerals at a rate of 30 TW. These power rates are more than double humanity's current energy consumption from all primary sources, but most of this energy flux is not recoverable. In addition to the internal heat flows, the top layer of the surface to a depth of 10 m (33 ft) is heated by solar energy during the summer, and cools during the winter.\nOutside of the seasonal variations, the geothermal gradient of temperatures through the crust is 25–30 °C (77–86 °F) per km of depth in most of the world. The conductive heat flux averages 0.1 MW/km2. These values are much higher near tectonic plate boundaries where the crust is thinner. They may be further augmented by combinations of fluid circulation, either through magma conduits, hot springs, hydrothermal circulation.\nThe thermal efficiency and profitability of electricity generation is particularly sensitive to temperature. Applications receive the greatest benefit from a high natural heat flux most easily from a hot spring. The next best option is to drill a well into a hot aquifer. An artificial hot water reservoir may be built by injecting water to hydraulically fracture bedrock. The systems in this last approach are called enhanced geothermal systems.\n2010 estimates of the potential for electricity generation from geothermal energy vary sixfold, from 0.035to2TW depending on the scale of investments. Upper estimates of geothermal resources assume wells as deep as 10 kilometres (6 mi), although 20th century wells rarely reached more than 3 kilometres (2 mi) deep. Wells of this depth are common in the petroleum industry. \n\n\n== Geothermal power ==\n\nGeothermal power is electrical power generated from geothermal energy. Dry steam, flash steam, and binary cycle power stations have been used for this purpose. As of 2010 geothermal electricity was generated in 26 countries.\nAs of 2019, worldwide geothermal power capacity amounted to 15.4 gigawatts (GW), of which 23.86 percent or 3.68 GW were in the United States.\nGeothermal energy supplies a significant share of the electrical power in Iceland, El Salvador, Kenya, the Philippines and New Zealand.\nGeothermal power is considered to be a renewable energy because heat extraction rates are insignificant compared to the Earth's heat content. The greenhouse gas emissions of geothermal electric stations are on average 45 grams of carbon dioxide per kilowatt-hour of electricity, or less than 5 percent of that of coal-fired plants.\n\nGeothermal electric plants were traditionally built on the edges of tectonic plates where high-temperature geothermal resources approach the surface. The development of binary cycle power plants and improvements in drilling and extraction technology enable enhanced geothermal systems over a greater geographical range. Demonstration projects are operational in Landau-Pfalz, Germany, and Soultz-sous-Forêts, France, while an earlier effort in Basel, Switzerland, was shut down after it triggered earthquakes. Other demonstration projects are under construction in Australia, the United Kingdom, and the US. In Myanmar over 39 locations are capable of geothermal power production, some of which are near Yangon.\n\n\n== Geothermal heating ==\n\nGeothermal heating is the use of geothermal energy to heat buildings and water for human use. Humans have done this since the Paleolithic era. Approximately seventy countries made direct use of a total of 270 PJ of geothermal heating in 2004. As of 2007, 28 GW of geothermal heating satisfied 0.07% of global primary energy consumption. Thermal efficiency is high since no energy conversion is needed, but capacity factors tend to be low (around 20%) since the heat is mostly needed in the winter.\nEven cold ground contains heat: below 6 metres (20 ft) the undisturbed ground temperature is consistently at the Mean Annual Air Temperature that may be extracted with a ground source heat pump.\n\n\n== Types ==\n\n\n=== Hydrothermal systems ===\nHydrothermal systems produce geothermal energy by accessing naturally-occurring hydrothermal reservoirs. Hydrothermal systems come in either vapor-dominated or liquid-dominated forms.\n\n\n==== Vapor-dominated plants ====\nLarderello and The Geysers are vapor-dominated. Vapor-dominated sites offer temperatures from 240 to 300 °C that produce superheated steam.\n\n\n==== Liquid-dominated plants ====\nLiquid-dominated reservoirs (LDRs) are more common with temperatures greater than 200 °C (392 °F) and are found near volcanoes in/around the Pacific Ocean and in rift zones and hot spots. Flash plants are the common way to generate electricity from these sources. Steam from the well is sufficient to power the plant. Most wells generate 2–10 MW of electricity. Steam is separated from liquid via cyclone separators and drives electric generators. Condensed liquid returns down the well for reheating/reuse. As of 2013, the largest liquid system was Cerro Prieto in Mexico, which generates 750 MW of electricity from temperatures reaching 350 °C (662 °F).\nLower-temperature LDRs (120–200 °C) require pumping. They are common in extensional terrains, where heating takes place via deep circulation along faults, such as in the Western US and Turkey. Water passes through a heat exchanger in a Rankine cycle binary plant. The water vaporizes an organic working fluid that drives a turbine. These binary plants originated in the Soviet Union in the late 1960s and predominate in new plants. Binary plants have no emissions.\n\n\n=== Engineered geothermal systems ===\nAn engineered geothermal system is a geothermal system that engineers have artificially created or improved. Engineered geothermal systems are used in a variety of geothermal reservoirs that have hot rocks but insufficient natural reservoir quality, for example, insufficient geofluid quantity or insufficient rock permeability or porosity, to operate as natural hydrothermal systems. Types of engineered geothermal systems include enhanced geothermal systems, closed-loop or advanced geothermal systems, and some superhot rock geothermal systems.\n\n\n==== Enhanced geothermal systems ====\n\nEnhanced geothermal systems (EGS) actively inject water into wells to be heated and pumped back out. The water is injected under high pressure to expand existing rock fissures to enable the water to flow freely. The technique was adapted from oil and gas fracking techniques. The geologic formations are deeper and no toxic chemicals are used, reducing the possibility of environmental damage. Instead proppants such as sand or ceramic particles are used to keep the cracks open and producing optimal flow rates. Drillers can employ directional drilling to expand the reservoir size.\nSmall-scale EGS have been installed in the Rhine Graben at Soultz-sous-Forêts in France and at Landau and Insheim in Germany.\n\n\n==== Closed-loop geothermal systems ====\n\nClosed-loop geothermal systems, sometimes colloquially referred to as Advanced Geothermal Systems (AGS), are engineered geothermal systems containing subsurface working fluid that is heated in the hot rock reservoir without direct contact with rock pores and fractures. Instead, the subsurface working fluid stays inside a closed loop of deeply buried pipes that conduct Earth's heat. The advantages of a deep, closed-loop geothermal circuit include: (1) no need for a geofluid, (2) no need for the hot rock to be permeable or porous, and (3) all the introduced working fluid can be recirculated with zero loss. Eavortm, a Canadian-based geothermal startup, piloted their closed-loop system in shallow soft rock formations in Alberta, Canada. Situated within a sedimentary basin, the geothermal gradient proved to be insufficient for electrical power generation. However, the system successfully produced approximately 11,000 MWh of thermal energy during its initial two years of operation.\"\n\n\n== Economics ==\n\nAs with wind and solar energy, geothermal power has minimal operating costs; capital costs dominate. Drilling accounts for over half the costs, and not all wells produce exploitable resources. For example, a typical well pair (one for extraction and one for injection) in Nevada can produce 4.5 megawatts (MW) and costs about $10 million to drill, with a 20% failure rate, making the average cost of a successful well $50 million.\n\nDrilling geothermal wells is more expensive than drilling oil and gas wells of comparable depth for several reasons: \n\nGeothermal reservoirs are usually in igneous or metamorphic rock, which is harder to penetrate than the sedimentary rock of typical hydrocarbon reservoirs.\nThe rock is often fractured, which causes vibrations that damage bits and other drilling tools.\nThe rock is often abrasive, with high quartz content, and sometimes contains highly corrosive fluids.\nThe rock is hot, which limits use of downhole electronics.\nWell casing must be cemented from top to bottom, to resist the casing's tendency to expand and contract with temperature changes. Oil and gas wells are usually cemented only at the bottom.\nWell diameters are considerably larger than typical oil and gas wells.\nAs of 2007 plant construction and well drilling cost about €2–5 million per MW of electrical capacity, while the break-even price was 0.04–0.10 € per kW·h. Enhanced geothermal systems tend to be on the high side of these ranges, with capital costs above $4 million per MW and break-even above $0.054 per kW·h.\nBetween 2013 and 2020, private investments were the main source of funding for renewable energy, comprising approximately 75% of total financing. The mix between private and public funding varies among different renewable energy technologies, influenced by their market appeal and readiness. In 2020, geothermal energy received just 32% of its investment from private sources.\n\n\n=== Socioeconomic benefits ===\nIn January 2024, the Energy Sector Management Assistance Program (ESMAP) report \"Socioeconomic Impacts of Geothermal Energy Development\" was published, highlighting the substantial socioeconomic benefits of geothermal energy development, which notably exceeds those of wind and solar by generating an estimated 34 jobs per megawatt across various sectors. The report details how geothermal projects contribute to skill development through practical on-the-job training and formal education, thereby strengthening the local workforce and expanding employment opportunities. It also underscores the collaborative nature of geothermal development with local communities, which leads to improved infrastructure, skill-building programs, and revenue-sharing models, thereby enhancing access to reliable electricity and heat. These improvements have the potential to boost agricultural productivity and food security. The report further addresses the commitment to advancing gender equality and social inclusion by offering job opportunities, education, and training to underrepresented groups, ensuring fair access to the benefits of geothermal development. Collectively, these efforts are instrumental in driving domestic economic growth, increasing fiscal revenues, and contributing to more stable and diverse national economies, while also offering significant social benefits such as better health, education, and community cohesion.\n\n\n== Development ==\nGeothermal projects have several stages of development. Each phase has associated risks. Many projects are canceled during the stages of reconnaissance and geophysical surveys, which are unsuitable for traditional lending. At later stages can often be equity-financed.\n\n\n=== Precipitate scaling ===\nA common issue encountered in geothermal systems arises when the system is situated in carbonate-rich formations. In such cases, the fluids extracting heat from the subsurface often dissolve fragments of the rock during their ascent towards the surface, where they subsequently cool. As the fluids cool, dissolved cations precipitate out of solution, leading to the formation of calcium scale, a phenomenon known as calcite scaling. This calcite scaling has the potential to decrease flow rates and necessitate system downtime for maintenance purposes.\n\n\n== Sustainability ==\nGeothermal energy is considered to be sustainable because the heat extracted is so small compared to the Earth's heat content, which is approximately 100 billion times 2010 worldwide annual energy consumption. Earth's heat flows are not in equilibrium; the planet is cooling on geologic timescales. Anthropic heat extraction typically does not accelerate the cooling process.\nWells can further be considered renewable because they return the extracted water to the borehole for reheating and re-extraction, albeit at a lower temperature.\nReplacing material use with energy has reduced the human environmental footprint in many applications. Geothermal has the potential to allow further reductions. For example, Iceland has sufficient geothermal energy to eliminate fossil fuels for electricity production and to heat Reykjavik sidewalks and eliminate the need for gritting.\n\nHowever, local effects of heat extraction must be considered. Over the course of decades, individual wells draw down local temperatures and water levels. The three oldest sites, at Larderello, Wairakei, and the Geysers experienced reduced output because of local depletion. Heat and water, in uncertain proportions, were extracted faster than they were replenished. Reducing production and injecting additional water could allow these wells to recover their original capacity. Such strategies have been implemented at some sites. These sites continue to provide significant energy.\nThe Wairakei power station was commissioned in November 1958, and it attained its peak generation of 173 MW in 1965, but already the supply of high-pressure steam was faltering. In 1982 it was down-rated to intermediate pressure and the output to 157 MW. In 2005 two 8 MW isopentane systems were added, boosting output by about 14 MW. Detailed data were lost due to re-organisations.\n\n\n== Environmental effects ==\n\nFluids drawn from underground carry a mixture of gasses, notably carbon dioxide (CO2), hydrogen sulfide (H2S), methane (CH4) and ammonia (NH3). These pollutants contribute to global warming, acid rain and noxious smells if released. Existing geothermal electric plants emit an average of 122 kilograms (269 lb) of CO2 per megawatt-hour (MW·h) of electricity, a small fraction of the emission intensity of fossil fuel plants. A few plants emit more pollutants than gas-fired power, at least in the first few years, such as some geothermal power in Turkey. Plants that experience high levels of acids and volatile chemicals are typically equipped with emission-control systems to reduce the exhaust. New emerging closed looped technologies developed by Eavor have the potential to reduce these emissions to zero.\nWater from geothermal sources may hold in solution trace amounts of toxic elements such as mercury, arsenic, boron, and antimony.  These chemicals precipitate as the water cools, and can damage surroundings if released. The modern practice of returning geothermal fluids into the Earth to stimulate production has the side benefit of reducing this environmental impact.\nConstruction can adversely affect land stability. Subsidence occurred in the Wairakei field. In Staufen im Breisgau, Germany, tectonic uplift occurred instead. A previously isolated anhydrite layer came in contact with water and turned it into gypsum, doubling its volume. Enhanced geothermal systems can trigger earthquakes as part of hydraulic fracturing. A project in Basel, Switzerland was suspended because more than 10,000 seismic events measuring up to 3.4 on the Richter Scale occurred over the first 6 days of water injection.\nGeothermal power production has minimal land and freshwater requirements. Geothermal plants use 3.5 square kilometres (1.4 sq mi) per gigawatt of electrical production (not capacity) versus 32 square kilometres (12 sq mi) and 12 square kilometres (4.6 sq mi) for coal facilities and wind farms respectively. They use 20 litres (5.3 US gal) of freshwater per MW·h versus over 1,000 litres (260 US gal) per MW·h for nuclear, coal, or oil.\n\n\n== Production ==\n\n\n=== Philippines ===\nThe Philippines began geothermal research in 1962 when the Philippine Institute of Volcanology and Seismology inspected the geothermal region in Tiwi, Albay. The first geothermal power plant in the Philippines was built in 1977, located in Tongonan, Leyte. The New Zealand government contracted with the Philippines to build the plant in 1972. The Tongonan Geothermal Field (TGF) added the Upper Mahiao, Matlibog, and South Sambaloran plants, which resulted in a 508 MV capacity.\nThe first geothermal power plant in the Tiwi region opened in 1979, while two other plants followed in 1980 and 1982. The Tiwi geothermal field is located about 450 km from Manila. The three geothermal power plants in the Tiwi region produce 330 MWe, putting the Philippines behind the United States and Mexico in geothermal growth. The Philippines has 7 geothermal fields and continues to exploit geothermal energy by creating the Philippine Energy Plan 2012–2030 that aims to produce 70% of the country's energy by 2030.\n\n\n=== United States ===\nAccording to the Geothermal Energy Association (GEA) installed geothermal capacity in the United States grew by 5%, or 147.05 MW, in 2013. This increase came from seven geothermal projects that began production in 2012. GEA revised its 2011 estimate of installed capacity upward by 128 MW, bringing installed US geothermal capacity to 3,386 MW.\n\n\n=== Hungary ===\nThe municipal government of Szeged is trying to cut down its gas consumption by 50 percent by utilizing geothermal energy for its district heating system. The Szeged geothermal power station has 27 wells, 16 heating plants, and 250 kilometres of distribution pipes.\n\n\n== See also ==\n\n2010 World Geothermal Congress\nDeep water source cooling\nEarth's internal heat budget\nGeothermal activity\nHydrothermal vent\nInternational Geothermal Association\nOcean thermal energy conversion\nRelative cost of electricity generated by different sources\nList of renewable energy topics by country and territory\nThermal battery\n\n\n== References ==\n\n\n== External links ==\n\n\"The Future of Geothermal Energy\" (PDF). International Energy Agency (IEA). December 2024. Archived (PDF) from the original on 14 December 2024.\n\"Hawai'i Groundwater & Geothermal Resources Center\". University of Hawaii at Manoa. 2023-07-16. Retrieved 2023-08-07.\n\"Geothermal Rising :: Using the Earth to Save the Earth\". www.geothermal.org. Retrieved 2023-08-07.\nEnergy Efficiency and Renewable Energy – Geothermal Technologies Office\nInternational Energy Agency Geothermal Energy Homepage\nNREL – Geothermal Research\n2022 discussion of geothermal energy advantages and challenges",
  },
  {
    title: "Bioenergy",
    originalContent:
      'Bioenergy is a type of renewable energy that is derived from plants and animal waste. The biomass that is used as input materials consists of recently living (but now dead) organisms, mainly plants. Thus, fossil fuels are not regarded as biomass under this definition. Types of biomass commonly used for bioenergy include wood, food crops such as corn, energy crops and waste from forests, yards, or farms. \nBioenergy can help with climate change mitigation but in some cases the required biomass production can increase greenhouse gas emissions or lead to local biodiversity loss. The environmental impacts of biomass production can be problematic, depending on how the biomass is produced and harvested.\nThe IEA\'s Net Zero by 2050 scenario calls for traditional bioenergy to be phased out by 2030, with modern bioenergy\'s share increasing from 6.6% in 2020 to 13.1% in 2030 and 18.7% in 2050. Bioenergy has a significant climate change mitigation potential if implemented correctly.: 637  Most of the recommended pathways to limit global warming include substantial contributions from bioenergy in 2050 (average at 200 EJ).: B 7.4 \n\n\n== Definition and terminology ==\n\nThe IPCC Sixth Assessment Report defines bioenergy as "energy derived from any form of biomass or its metabolic by-products".: 1795  It goes on to define biomass in this context as "organic material excluding the material that is fossilised or embedded in geological formations".: 1795  This means that coal or other fossil fuels is not a form of biomass in this context.\nThe term traditional biomass for bioenergy means "the combustion of wood, charcoal, agricultural residues and/or animal dung for cooking or heating in open fires or in inefficient stoves as is common in low-income countries".: 1796 \nSince biomass can also be used as a fuel directly (e.g. wood logs), the terms biomass and biofuel have sometimes been used interchangeably. However, the term biomass usually denotes the biological raw material the fuel is made of. The terms biofuel or biogas are generally reserved for liquid or gaseous fuels respectively.\n\n\n== Input materials ==\n\nWood and wood residues is the largest biomass energy source today. Wood can be used as a fuel directly or processed into pellet fuel or other forms of fuels. Other plants can also be used as fuel, for instance maize, switchgrass, miscanthus and bamboo. The main waste feedstocks are wood waste, agricultural waste, municipal solid waste, and manufacturing waste. Upgrading raw biomass to higher grade fuels can be achieved by different methods, broadly classified as thermal, chemical, or biochemical:\nThermal conversion processes use heat as the dominant mechanism to upgrade biomass into a better and more practical fuel. The basic alternatives are torrefaction, pyrolysis, and gasification, these are separated mainly by the extent to which the chemical reactions involved are allowed to proceed (mainly controlled by the availability of oxygen and conversion temperature).\nMany chemical conversions are based on established coal-based processes, such as the Fischer-Tropsch synthesis. Like coal, biomass can be converted into multiple commodity chemicals.\nBiochemical processes have developed in nature to break down the molecules of which biomass is composed, and many of these can be harnessed. In most cases, microorganisms are used to perform the conversion. The processes are called anaerobic digestion, fermentation, and composting.\n\n\n== Applications ==\n\n\n=== Biomass for heating ===\n\n\n=== Biofuel for transportation ===\nBased on the source of biomass, biofuels are classified broadly into two major categories, depending if food crops are used or not:\nFirst-generation (or "conventional") biofuels are made from food sources grown on arable lands, such as sugarcane and maize. Sugars present in this biomass are fermented to produce bioethanol, an alcohol fuel which serves as an additive to gasoline, or in a fuel cell to produce electricity. Bioethanol is made by fermentation, mostly from carbohydrates produced in sugar or starch crops such as corn, sugarcane, or sweet sorghum. Bioethanol is widely used in the United States and in Brazil. Biodiesel is produced from the oils in for instance rapeseed or sugar beets and is the most common biofuel in Europe.\nSecond-generation biofuels (also called "advanced biofuels") utilize non-food-based biomass sources such as perennial energy crops and agricultural residues/waste. The feedstock used to make the fuels either grow on arable land but are byproducts of the main crop, or they are grown on marginal land. Waste from industry, agriculture, forestry and households can also be used for second-generation biofuels, using e.g. anaerobic digestion to produce biogas, gasification to produce syngas or by direct combustion. Cellulosic biomass, derived from non-food sources, such as trees and grasses, is being developed as a feedstock for ethanol production, and biodiesel can be produced from left-over food products like vegetable oils and animal fats.\n\n\n=== Production of liquid fuels ===\nBiomass to liquid\nBioconversion of biomass to mixed alcohol fuels\n\n\n== Comparison with other renewable energy types ==\n\n\n=== Land requirement ===\nThe surface power production densities of a crop will determine how much land is required for production. The average lifecycle surface power densities for biomass, wind, hydro and solar power production are 0.30 W/m2, 1 W/m2, 3 W/m2 and 5 W/m2, respectively (power in the form of heat for biomass, and electricity for wind, hydro and solar). Lifecycle surface power density includes land used by all supporting infrastructure, manufacturing, mining/harvesting and decommissioning. \nAnother estimate puts the values at 0.08 W/m2 for biomass, 0.14 W/m2 for hydro, 1.84 W/m2 for wind, and 6.63 W/m2 for solar (median values, with none of the renewable sources exceeding 10 W/m2).\n\n\n== Related technologies ==\n\n\n=== Bioenergy with carbon capture and storage (BECCS) ===\nCarbon capture and storage technology can be used to capture emissions from bioenergy power plants. This process is known as bioenergy with carbon capture and storage (BECCS) and can result in net carbon dioxide removal from the atmosphere. However, BECCS can also result in net positive emissions depending on how the biomass material is grown, harvested, and transported. Deployment of BECCS at scales described in some climate change mitigation pathways would require converting large amounts of cropland.\n\n\n== Climate and sustainability aspects ==\n\n\n== Environmental impacts ==\nBioenergy can either mitigate (i.e. reduce) or increase greenhouse gas emissions. There is also agreement that local environmental impacts can be problematic. For example, increased biomass demand can create significant social and environmental pressure in the locations where the biomass is produced. The impact is primarily related to the low surface power density of biomass. The low surface power density has the effect that much larger land areas are needed in order to produce the same amount of energy, compared to for instance fossil fuels.\nLong-distance transport of biomass have been criticised as wasteful and unsustainable, and there have been protests against forest biomass export in Sweden and Canada.\n\n\n== Scale and future trends ==\nIn 2020 bioenergy produced 58 EJ (exajoules) of energy, compared to 172 EJ from crude oil, 157 EJ from coal, 138 EJ from natural gas, 29 EJ from nuclear, 16 EJ from hydro and 15 EJ from wind, solar and geothermal combined. Most of the global bioenergy is produced from forest resources.: 3 : 1 \nGenerally, bioenergy expansion fell by 50% in 2020. China and Europe are the only two regions that reported significant expansion in 2020, adding 2 GW and 1.2 GW of bioenergy capacity, respectively.\nAlmost all available sawmill residue is already being utilized for pellet production, so there is no room for expansion. For the bioenergy sector to significantly expand in the future, more of the harvested pulpwood must go to pellet mills. However, the harvest of pulpwood (tree thinnings) removes the possibility for these trees to grow old and therefore maximize their carbon holding capacity.: 19  Compared to pulpwood, sawmill residues have lower net emissions: "Some types of biomass feedstock can be carbon-neutral, at least over a period of a few years, including in particular sawmill residues. These are wastes from other forest operations that imply no additional harvesting, and if otherwise burnt as waste or left to rot would release carbon to the atmosphere in any case.": 68 \n\n\n== By country ==\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Sources ===',
  },
  {
    title: "Nuclear energy",
    originalContent:
      "Nuclear energy may refer to:\n\nNuclear power, the use of sustained nuclear fission or nuclear fusion to generate heat and electricity\nNuclear binding energy, the energy needed to fuse or split a nucleus of an atom\nNuclear potential energy, the potential energy of the particles inside an atomic nucleus\nNuclear Energy (sculpture), a bronze sculpture by Henry Moore in the University of Chicago",
  },
  {
    title: "Electric power distribution",
    originalContent:
      "Electric power distribution is the final stage in the delivery of electricity. Electricity is carried from the transmission system to individual consumers. Distribution substations connect to the transmission system and lower the transmission voltage to medium voltage ranging between 2 kV and 33 kV with the use of transformers. Primary distribution lines carry this medium voltage power to distribution transformers located near the customer's premises. Distribution transformers again lower the voltage to the utilization voltage used by lighting, industrial equipment and household appliances. Often several customers are supplied from one transformer through secondary distribution lines. Commercial and residential customers are connected to the secondary distribution lines through service drops. Customers demanding a much larger amount of power may be connected directly to the primary distribution level or the subtransmission level.\n\nThe transition from transmission to distribution happens in a power substation, which has the following functions:\n\nCircuit breakers and switches enable the substation to be disconnected from the transmission grid or for distribution lines to be disconnected.\nTransformers step down transmission voltages, 35 kV or more, down to primary distribution voltages. These are medium voltage circuits, usually 600–35000 V.\nFrom the transformer, power goes to the busbar that can split the distribution power off in multiple directions. The bus distributes power to distribution lines, which fan out to customers.\nUrban distribution is mainly underground, sometimes in common utility ducts. Rural distribution is mostly above ground with utility poles, and suburban distribution is a mix.\nCloser to the customer, a distribution transformer steps the primary distribution power down to a low-voltage secondary circuit, usually 120/240 V in the US for residential customers. The power comes to the customer via a service drop and an electricity meter. The final circuit in an urban system may be less than 15 metres (50 ft) but may be over 91 metres (300 ft) for a rural customer.\n\n\n== History ==\n\nElectric power distribution become necessary only in the 1880s, when electricity started being generated at power stations. Until then, electricity was usually generated where it was used. The first power-distribution systems installed in European and US cities were used to supply lighting: arc lighting running on very-high-voltage (around 3,000 V) alternating current (AC) or direct current (DC), and incandescent lighting running on low-voltage (100 V) direct current. Both were supplanting gas lighting systems, with arc lighting taking over large-area and street lighting, and incandescent lighting replacing gas lights for business and residential users.\nThe high voltages used in arc lighting allowed a single generating station to supply a string of lights up to 7 miles (11 km) long. And each doubling of voltage would allow a given cable to transmit the same amount of power four times the distance than at the lower voltage (with the same power loss). By contrast, direct-current indoor incandescent lighting systems, such as Edison's first power station, installed in 1882, had difficulty supplying customers more than a mile away because they used a low voltage (110 V) from generation to end use. The low voltage translated to higher current and required thick copper cables for transmission. In practice, Edison's DC generating plants needed to be within about 1.5 miles (2.4 km) of the farthest customer to avoid even thicker and more expensive conductors.\n\n\n=== Introduction of the transformer ===\nThe problem of transmitting electricity over longer distances became a recognized engineering roadblock to electric power distribution, with many less-than-satisfactory solutions tested by lighting companies. But the mid-1880s saw a breakthrough with the development of functional transformers that allowed AC power to be \"stepped up\" to a much higher voltage for transmission, then dropped down to a lower voltage near the end user. Compared to direct current, AC had much cheaper transmission costs and greater economies of scale — with large AC generating plants capable of supplying whole cities and regions, which led to the use of AC spreading rapidly.\nIn the US the competition between direct current and alternating current took a personal turn in the late 1880s in the form of a \"war of currents\" when Thomas Edison started attacking George Westinghouse and his development of the first US AC transformer systems, highlighting the deaths caused by high-voltage AC systems over the years and claiming any AC system was inherently dangerous. Edison's propaganda campaign was short-lived, with his company switching over to AC in 1892.\nAC became the dominant form of transmission of power with innovations in  Europe and the US in electric motor designs, and the development of engineered universal systems allowing the large number of legacy systems to be connected to large AC grids.\nIn the first half of the 20th century, in many places the electric power industry was vertically integrated, meaning that one company did generation, transmission, distribution, metering and billing. Starting in the 1970s and 1980s, nations began the process of deregulation and privatization, leading to electricity markets. The distribution system would remain regulated, but generation, retail, and sometimes transmission systems were transformed into competitive markets.\n\n\n== Generation and transmission ==\n\nElectric power begins at a generating station, where the potential difference can be as high as 33,000 volts. AC is usually used. Users of large amounts of DC power such as some railway electrification systems, telephone exchanges and industrial processes such as aluminium smelting  use rectifiers to derive DC from the public AC supply, or may have their own generation systems.  High-voltage DC can be advantageous for isolating alternating-current systems or controlling the quantity of electricity transmitted. For example, Hydro-Québec has a direct-current line which goes from the James Bay region to Boston.\nFrom the generating station it goes to the generating station's switchyard where a step-up transformer increases the voltage to a level suitable for transmission, from 44 kV to 765 kV. Once in the transmission system, electricity from each generating station is combined with electricity produced elsewhere. For alternating-current generators, all generating units connected to a common network must be  synchronized, operating at the same frequency within a small tolerance. Alternatively, disparate sources can be combined to serve a common load if some external power converter, such as a rotating machine or a  direct current converter system is interposed. Electricity is consumed as soon as it is produced. It is transmitted at a very high speed, close to the speed of light.\n\n\n== Primary distribution ==\nPrimary distribution voltages range from 4 kV to 35 kV phase-to-phase (2.4 kV to 20 kV phase-to-neutral) Only large consumers are fed directly from distribution voltages; most utility customers are connected to a transformer, which reduces the distribution voltage to the low voltage \"utilization voltage\", \"supply voltage\" or \"mains voltage\" used by lighting and interior wiring systems.\n\n\n=== Network configurations ===\n\nDistribution networks are divided into two types, radial or network. A radial system is arranged like a tree where each customer has one source of supply. A network system has multiple sources of supply operating in parallel. Spot networks are used for concentrated loads. Radial systems are commonly used in rural or suburban areas.\nRadial systems usually include emergency connections where the system can be reconfigured in case of problems, such as a fault or planned maintenance. This can be done by opening and closing switches to isolate a certain section from the grid.\nLong feeders experience voltage drop (power factor distortion) requiring capacitors or voltage regulators to be installed.\nReconfiguration, by exchanging the functional links between the elements of the system, represents one of the most important measures which can improve the operational performance of a distribution system. The problem of optimization through the reconfiguration of a power distribution system, in terms of its definition, is a historical single objective problem with constraints. Since 1975, when Merlin and Back  introduced the idea of distribution system reconfiguration for active power loss reduction, until nowadays, a lot of researchers have proposed diverse methods and algorithms to solve the reconfiguration problem as a single objective problem. Some authors have proposed Pareto optimality based approaches (including active power losses and reliability indices as objectives). For this purpose, different artificial intelligence based methods have been used: microgenetic, branch exchange, particle swarm optimization  and non-dominated sorting genetic algorithm.\n\n\n=== Rural services ===\n\nRural electrification systems tend to use higher distribution voltages because of the longer distances covered by distribution lines (see Rural Electrification Administration). 7.2, 12.47, 25, and 34.5 kV distribution is common in the United States; 11 kV and 33 kV are common in the UK, Australia and New Zealand; 11 kV and 22 kV are common in South Africa; 10, 20 and 35 kV are common in China. Other voltages are occasionally used.\nRural services normally try to minimize the number of poles and wires. It uses higher voltages (than urban distribution), which in turn permits use of galvanized steel wire. The strong steel wire allows for less expensive wide pole spacing. In rural areas a pole-mount transformer may serve only one customer. In New Zealand, Australia, Saskatchewan, Canada, and South Africa, Single-wire earth return systems (SWER) are used to electrify remote rural areas.\nThree phase service provides power for large agricultural facilities, petroleum pumping facilities, water plants, or other customers that have large loads (three-phase equipment).\nIn North America, overhead distribution systems may be three phase, four wire, with a neutral conductor. Rural distribution system may have long runs of one phase conductor and a neutral.\nIn other countries or in extreme rural areas the neutral wire is connected to the ground to use that as a return (single-wire earth return).\n\n\n== Secondary distribution ==\n\nElectricity is delivered at a frequency of either 50 or 60 Hz, depending on the region. It is delivered to domestic customers as single-phase electric power. In some countries as in Europe a three phase supply may be made available for larger properties. Seen with an oscilloscope, the domestic power supply in North America would look like a sine wave, oscillating between −170 volts and 170 volts, giving an effective voltage of 120 volts RMS. Three-phase electric power is more efficient in terms of power delivered per cable used, and is more suited to running large electric motors. Some large European appliances may be powered by three-phase power, such as electric stoves and clothes dryers.\nA ground connection is normally provided for the customer's system as well as for the equipment owned by the utility. The purpose of connecting the customer's system to ground is to limit the voltage that may develop if high voltage conductors fall down onto lower-voltage conductors which are usually mounted lower to the ground, or if a failure occurs within a distribution transformer. Earthing systems can be TT, TN-S, TN-C-S or TN-C.\n\n\n=== Regional variations ===\n\n\n==== 220–240 volt systems ====\nMost of the world uses 50 Hz 220 or 230 V single phase, or  400 V three-phase for residential and light industrial services. In this system, the primary distribution network supplies a few substations per area, and the 230 V / 400 V  power from each substation is directly distributed to end users over a region of normally less than 1 km radius. Three live (hot) wires and the neutral are connected to the building for a three phase service. Single-phase distribution, with one live wire and the neutral is used domestically where total loads are light. In Europe, electricity is normally distributed for industry and domestic use by the three-phase, four wire system. This gives a phase-to-phase voltage of 400 volts wye service and a single-phase voltage of 230 volts between any one phase and neutral.  In the UK a typical urban or suburban low-voltage substation would normally be rated between 150 kVA and 1 MVA and supply a whole neighbourhood of a few hundred houses. Transformers are typically sized on an average load of 1 to 2 kW per household, and the service fuses and cable is sized to allow any one property to draw a peak load of perhaps ten times this.\nFor industrial customers, 3-phase 690 / 400 volt is also available, or may be generated locally. \nLarge industrial customers have their own transformer(s) with an input from 11 kV to 220 kV.\n\n\n==== 100–120 volt systems ====\nMost of the Americas use 60 Hz AC, the 120/240 volt split-phase system domestically and three phase for larger installations. North American transformers usually power homes at 240 volts, similar to Europe's 230 volts. It is the split-phase that allows use of 120 volts in the home.\n\nIn the electricity sector in Japan, the standard voltage is 100 V, with both 50 and 60 Hz AC frequencies being used. Parts of the country use 50 Hz, while other parts use 60 Hz. This is a relic from the 1890s. Some local providers in Tokyo imported 50 Hz German equipment, while the local power providers in Osaka brought in 60 Hz generators from the United States. The grids grew until eventually the entire country was wired. Today the frequency is 50 Hz in Eastern Japan (including Tokyo, Yokohama, Tohoku, and Hokkaido) and 60 Hz in Western Japan (including Nagoya, Osaka, Kyoto, Hiroshima, Shikoku, and Kyushu).\nMost household appliances are made to work on either frequency. The problem of incompatibility came into the public eye when the 2011 Tōhoku earthquake and tsunami knocked out about a third of the east's capacity, and power in the west could not be fully shared with the east since the country does not have a common frequency.\nThere are four high-voltage direct current (HVDC) converter stations that move power across Japan's AC frequency border. Shin Shinano is a back-to-back HVDC facility in Japan which forms one of four frequency changer stations that link Japan's western and eastern power grids. The other three are at Higashi-Shimizu, Minami-Fukumitsu and Sakuma Dam. Together they can move up to 1.2 GW of power east or west.\n\n\n==== 240 volt systems and 120 volt outlets ====\nMost modern North American homes are wired to receive 240 volts from the transformer, and through the use of split-phase electrical power, can have both 120 volt receptacles and 240 volt receptacles. The 120 volts is typically used for lighting and most wall outlets. The 240 volt circuits are typically used for appliances requiring high watt heat output such as ovens and heaters. They may also be used to supply an electric car charger.\n\n\n== Modern distribution systems ==\nTraditionally, the distribution systems would only operate as simple distribution lines where the electricity from the transmission networks would be shared among the customers. Today's distribution systems are heavily integrated with renewable energy generations at the distribution level of the power systems by the means of distributed generation resources, such as solar energy and wind energy. As a result, distribution systems are becoming more independent from the transmission networks day-by-day. Balancing the supply-demand relationship at these modern distribution networks (sometimes referred to as microgrids) is extremely challenging, and it requires the use of various technological and operational means to operate. Such tools include battery storage power station, data analytics, optimization tools, etc.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nIEEE Power Engineering Society\nIEEE Power Engineering Society Distribution Subcommittee\nU.S. Department of Energy Electric Distribution website",
  },
  {
    title: "Energy efficiency",
    originalContent:
      "Energy efficiency may refer to:\n\nEnergy efficiency (physics), the ratio between the useful output and input of an energy conversion process\nElectrical efficiency, useful power output per electrical power consumed\nMechanical efficiency, a ratio of the measured performance to the performance of an ideal machine\nThermal efficiency, the extent to which the energy added by heat is converted to net work output or vice versa\nLuminous efficiency, a measure of how well a light source produces visible light\nFuel efficiency, the efficiency of converting potential energy in a fuel into kinetic energy\nEnergy efficiency in transportation, the fuel economy of various modes of transportation\nEnergy-efficient landscaping, a type of landscaping designed for the purpose of conserving energy\nEfficient energy use, minimizing the amount of energy used for a given, constant energy service\nEnergy conservation, reducing energy consumption by using less of an energy service\n\n\n== See also ==\nEnergy (disambiguation)\nEfficiency (disambiguation)\nEnergy rating (disambiguation)\nAll pages with titles containing Energy efficiency\nAll pages with titles containing Energy efficient",
  },
  {
    title: "Sustainable development",
    originalContent:
      'Sustainable development is an approach to growth and human development that aims to meet the needs of the present without compromising the ability of future generations to meet their own needs. The aim is to have a society where living conditions and resources meet human needs without undermining planetary integrity. Sustainable development aims to balance the needs of the economy, environment, and social well-being. The Brundtland Report in 1987 helped to make the concept of sustainable development better known. \nSustainable development overlaps with the idea of sustainability which is a normative concept. UNESCO formulated a distinction between the two concepts as follows: "Sustainability is often thought of as a long-term goal (i.e. a more sustainable world), while sustainable development refers to the many processes and pathways to achieve it." \nThe Rio Process that began at the 1992 Earth Summit in Rio de Janeiro has placed the concept of sustainable development on the international agenda. Sustainable development is the foundational concept of the Sustainable Development Goals (SDGs). These global goals for the year 2030 were adopted in 2015 by the United Nations General Assembly (UNGA). They address the global challenges, including for example poverty, climate change, biodiversity loss, and peace. \n\nThere are some problems with the concept of sustainable development. Some scholars say it is an oxymoron because according to them, development is inherently unsustainable. Other commentators are disappointed in the lack of progress that has been achieved so far. Scholars have stated that sustainable development is open-ended, much critiqued as ambiguous, incoherent, and therefore easily appropriated. \n\n\n== Definition of sustainable development ==\nIn 1987, the United Nations World Commission on Environment and Development released the report Our Common Future, commonly called the Brundtland Report. The report included a definition of "sustainable development" which is now widely used:\n\nSustainable development is a development that meets the needs of the present without compromising the ability of future generations to meet their own needs. It contains two key concepts within it:\nThe concept of \'needs\', in particular, the essential needs of the world\'s poor, to which overriding priority should be given; and\nThe idea of limitations imposed by the state of technology and social organization on the environment\'s ability to meet present and future needs.Sustainable development thus tries to find a balance between economic development, environmental protection, and social well-being.\nHowever, scholars have pointed out that there are manifold understandings of sustainable development. Also there are incoherencies in the dominant market-based socio-economic-political organisation. Attempts towards universal sustainable development need to account for the extremely varied challenges, circumstances, and choices that shape prospects and prosperity for all, everywhere.\nThe discourse of sustainable development is highly influential in global and national governance frameworks, though its meaning and operationalization are context-dependent and have evolved over time. The evolution of this discourse can for example be seen in the transition from the Millennium Development Goals (MDGs, years 2000 to 2015) to the Sustainable Development Goals (SDGs, years 2015 to 2030).\n\n\n== Development of the concept ==\n\nSustainable development has its roots in ideas regarding sustainable forest management, which were developed in Europe during the 17th and 18th centuries. In response to a growing awareness of the depletion of timber resources in England, John Evelyn argued, in his 1662 essay Sylva, that "sowing and planting of trees had to be regarded as a national duty of every landowner, in order to stop the destructive over- exploitation of natural resources." In 1713, Hans Carl von Carlowitz, a senior mining administrator in the service of Elector Frederick Augustus I of Saxony published Sylvicultura economics, a 400-page work on forestry. Building upon the ideas of Evelyn and French minister Jean-Baptiste Colbert, von Carlowitz developed the concept of managing forests for sustained yield. His work influenced others, including Alexander von Humboldt and Georg Ludwig Hartig, eventually leading to the development of the science of forestry. This, in turn, influenced people like Gifford Pinchot, the first head of the US Forest Service, whose approach to forest management was driven by the idea of wise use of resources, and Aldo Leopold whose land ethic was influential in the development of the environmental movement in the 1960s.\nFollowing the publication of Rachel Carson\'s Silent Spring in 1962, the developing environmental movement drew attention to the relationship between economic growth and environmental degradation. Kenneth E. Boulding, in his influential 1966 essay The Economics of the Coming Spaceship Earth, identified the need for the economic system to fit itself to the ecological system with its limited pools of resources. Another milestone was the 1968 article by Garrett Hardin that popularized the term "tragedy of the commons".\nThe direct linking of sustainability and development in a contemporary sense can be traced to the early 1970s. "Strategy of Progress", a 1972 book (in German) by Ernst Basler, explained how the long-acknowledged sustainability concept of preserving forests for future wood production can be directly transferred to the broader importance of preserving environmental resources to sustain the world for future generations. That same year, the interrelationship of environment and development was formally demonstrated in a systems dynamic simulation model reported in the classic report on Limits to Growth. This was commissioned by the Club of Rome and written by a group of scientists led by Dennis and Donella Meadows of the Massachusetts Institute of Technology. Describing the desirable "state of global equilibrium", the authors wrote: "We are searching for a model output that represents a world system that is sustainable without sudden and uncontrolled collapse and capable of satisfying the basic material requirements of all of its people." The year 1972 also saw the publication of the influential book, A Blueprint for Survival.\nIn 1975, an MIT research group prepared ten days of hearings on "Growth and Its Implication for the Future" for the US Congress, the first hearings ever held on sustainable development.\nIn 1980, the International Union for Conservation of Nature published a world conservation strategy that included one of the first references to sustainable development as a global priority and introduced the term "sustainable development".: 4  Two years later, the United Nations World Charter for Nature raised five principles of conservation by which human conduct affecting nature is to be guided and judged.\nSince the Brundtland Report, the concept of sustainable development has developed beyond the initial intergenerational framework to focus more on the goal of "socially inclusive and environmentally sustainable economic growth".: 5  In 1992, the UN Conference on Environment and Development published the Earth Charter, which outlines the building of a just, sustainable, and peaceful global society in the 21st century. The action plan Agenda 21 for sustainable development identified information, integration, and participation as key building blocks to help countries achieve development that recognizes these interdependent pillars. Furthermore, Agenda 21 emphasizes that broad public participation in decision-making is a fundamental prerequisite for achieving sustainable development.\nThe Rio Protocol was a huge leap forward: for the first time, the world agreed on a sustainability agenda. In fact, a global consensus was facilitated by neglecting concrete goals and operational details. \n\n\n== Global governance framework ==\nThe most comprehensive global governance framework for sustainable development is the 2030 Agenda for Sustainable Development with its 17 Sustainable Development Goals (SDGs). This agenda was a follow-up to the Millennium Declaration from the year 2000 with its eight Millennium Development Goals (MDGs), the first comprehensive global governance framework for the achievement of sustainable development. The SDGs have concrete targets (unlike the results from the Rio Process) but no methods for sanctions.: 137  They contain goals, targets and indicators for example in the areas of poverty reduction, environmental protection, human prosperity and peace.\nSustainability means different things to different people, and the concept of sustainable development has led to a diversity of discourses that legitimize competing sociopolitical projects. Global environmental governance scholars have identified a comprehensive set of discourses within the public space that mostly convey four sustainability frames: mainstream sustainability, progressive sustainability, a limits discourse, and radical sustainability.\nFirst, mainstream sustainability is a conservative approach on both economic and political terms. Second, progressive sustainability is an economically conservative, yet politically reformist approach. Under this framing, sustainable development is still centered on economic growth, which is deemed compatible with environmental sustainability. However, human well-being and development can only be achieved through a redistribution of power to even out inequalities between developed and developing countries. Third, a limits discourse is an economically reformist, yet politically conservative approach to sustainability. Fourth, radical sustainability is a transformative approach seeking to break with existing global economic and political structures.\n\n\n== Related concepts ==\n\n\n=== Sustainability ===\n\n\n== Dimensions ==\n\nSustainable development, like sustainability, is regarded to have three dimensions: the environment, economy and society. The idea is that a good balance between the three dimensions should be achieved. Instead of calling them dimensions, other terms commonly used are pillars, domains, aspects, spheres.\n\n\n== Pathways ==\n\nSix interdependent capacities are deemed to be necessary for the successful pursuit of sustainable development. These are the capacities to measure progress towards sustainable development; promote equity within and between generations; adapt to shocks and surprises; transform the system onto more sustainable development pathways; link knowledge with action for sustainability; and to devise governance arrangements that allow people to work together.\nDuring the MDG era (year 2000 to 2015), the key objective of sustainable development was poverty reduction to be reached through economic growth and participation in the global trade system. The SDGs take a much more comprehensive approach to sustainable development than the MDGs did. They offer a more people-centred development agenda. Out of the 17 SDGs, for example, 11 goals contain targets related to equity, equality or inclusion, and SDG 10 is solely devoted to addressing inequality within and among countries.\n\n\n=== Improving on environmental sustainability ===\n An unsustainable situation occurs when natural capital (the total of nature\'s resources) is used up faster than it can be replenished.: 58  Sustainability requires that human activity only uses nature\'s resources at a rate at which they can be replenished naturally. The concept of sustainable development is intertwined with the concept of carrying capacity. Theoretically, the long-term result of environmental degradation is the inability to sustain human life.\nImportant operational principles of sustainable development were published by Herman Daly in 1990: renewable resources should provide a sustainable yield (the rate of harvest should not exceed the rate of regeneration); for non-renewable resources there should be equivalent development of renewable substitutes; waste generation should not exceed the assimilative capacity of the environment.\nIn 2019, a summary for policymakers of the largest, most comprehensive study to date of biodiversity and ecosystem services was published by the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services. It recommended that human civilization will need a transformative change, including sustainable agriculture, reductions in consumption and waste, fishing quotas and collaborative water management.\n\nEnvironmental problems associated with industrial agriculture and agribusiness are now being addressed through approaches such as sustainable agriculture, organic farming and more sustainable business practices. At the local level there are various movements working towards sustainable food systems which may include less meat consumption, local food production, slow food, sustainable gardening, and organic gardening. The environmental effects of different dietary patterns depend on many factors, including the proportion of animal and plant foods consumed and the method of food production.\nAs global population and affluence have increased, so has the use of various materials increased in volume, diversity, and distance transported. By 2050, humanity could consume an estimated 140 billion tons of minerals, ores, fossil fuels and biomass per year (three times its current amount) unless the economic growth rate is decoupled from the rate of natural resource consumption.\nSustainable use of materials has targeted the idea of dematerialization, converting the linear path of materials (extraction, use, disposal in landfill) to a circular material flow that reuses materials as much as possible, much like the cycling and reuse of waste in nature. This way of thinking is expressed in the concept of circular economy, which employs reuse, sharing, repair, refurbishment, remanufacturing and recycling to create a closed-loop system, minimizing the use of resource inputs and the creation of waste, pollution and carbon emissions. The European Commission has adopted an ambitious Circular Economy Action Plan in 2020, which aims at making sustainable products the norm in the EU.\n\n\n=== Improving on economic and social aspects ===\n\nIt has been suggested that because of the rural poverty and overexploitation, environmental resources should be treated as important economic assets, called natural capital. Economic development has traditionally required a growth in the gross domestic product. This model of unlimited personal and GDP growth may be over. Sustainable development may involve improvements in the quality of life for many but may necessitate a decrease in resource consumption. "Growth" generally ignores the direct effect that the environment may have on social welfare, whereas "development" takes it into account.\nAs early as the 1970s, the concept of sustainability was used to describe an economy "in equilibrium with basic ecological support systems". Scientists in many fields have highlighted The Limits to Growth, and economists have presented alternatives, for example a \'steady-state economy\', to address concerns over the impacts of expanding human development on the planet. In 1987, the economist Edward Barbier published the study The Concept of Sustainable Economic Development, where he recognized that goals of environmental conservation and economic development are not conflicting and can be reinforcing each other.\nA World Bank study from 1999 concluded that based on the theory of genuine savings (defined as "traditional net savings less the value of resource depletion and environmental degradation plus the value of investment in human capital"), policymakers have many possible interventions to increase sustainability, in macroeconomics or purely environmental. Several studies have noted that efficient policies for renewable energy and pollution are compatible with increasing human welfare, eventually reaching a golden-rule steady state.\nA meta review in 2002 looked at environmental and economic valuations and found a "lack of concrete understanding of what "sustainability policies" might entail in practice". A study concluded in 2007 that knowledge, manufactured and human capital (health and education) has not compensated for the degradation of natural capital in many parts of the world. It has been suggested that intergenerational equity can be incorporated into a sustainable development and decision making, as has become common in economic valuations of climate economics.\nThe World Business Council for Sustainable Development published a Vision 2050 document in 2021 to show "How business can lead the transformations the world needs". The vision states that "we envision a world in which 9+billion people can live well, within planetary boundaries, by 2050." This report was highlighted by The Guardian as "the largest concerted corporate sustainability action plan to date – include reversing the damage done to ecosystems, addressing rising greenhouse gas emissions and ensuring societies move to sustainable agriculture."\n\n\n== Barriers ==\n\n\n== Assessments and reactions ==\n\nThe concept of sustainable development has been and still is, subject to criticism, including the question of what is to be sustained in sustainable development. It has been argued that there is no such thing as sustainable use of a non-renewable resource, since any positive rate of exploitation will eventually lead to the exhaustion of earth\'s finite stock;: 13  this perspective renders the Industrial Revolution as a whole unsustainable.: 20f : 61–67 : 22f \nThe sustainable development debate is based on the assumption that societies need to manage three types of capital (economic, social, and natural), which may be non-substitutable and whose consumption might be irreversible. Natural capital can not necessarily be substituted by economic capital. While it is possible that we can find ways to replace some natural resources, it is much less likely that they will ever be able to replace ecosystem services, such as the protection provided by the ozone layer, or the climate stabilizing function of the Amazonian forest.\nThe concept of sustainable development has been criticized from different angles. While some see it as paradoxical (or an oxymoron) and regard development as inherently unsustainable, others are disappointed in the lack of progress that has been achieved so far. Part of the problem is that "development" itself is not consistently defined.: 16 \nThe vagueness of the Brundtland definition of sustainable development has been criticized as follows:: 17  The definition has "opened up the possibility of downplaying sustainability. Hence, governments spread the message that we can have it all at the same time, i.e. economic growth, prospering societies and a healthy environment. No new ethic is required. This so-called weak version of sustainability is popular among governments, and businesses, but profoundly wrong and not even weak, as there is no alternative to preserving the earth\'s ecological integrity.": 2 \nScholars have stated that sustainable development is open-ended, much critiqued as ambiguous, incoherent, and therefore easily appropriated.\n\n\n== Society and culture ==\n\n\n=== Sustainable development goals ===\nSustainable development is the foundational concept of the Sustainable Development Goals (SDGs). Policies to achieve the SDGs are meant to cohere around this concept.\n\n\n=== Education for sustainable development ===\nEducation for sustainable development (ESD) is a term officially used by the United Nations. It is defined as education practices that encourage changes in knowledge, skills, values, and attitudes to enable a more sustainable and just society for humanity. ESD aims to empower and equip current and future generations to meet their needs using a balanced and integrated approach to sustainable development\'s economic, social, and environmental dimensions.\nAgenda 21 was the first international document that identified education as an essential tool for achieving sustainable development and highlighted areas of action for education. ESD is a component of measurement in an indicator for Sustainable Development Goal 12 (SDG) for "responsible consumption and production". SDG 12 has 11 targets, and target 12.8 is "By 2030, ensure that people everywhere have the relevant information and awareness for sustainable development and lifestyles in harmony with nature." 20 years after the Agenda 21 document was declared, the \'Future we want\' document was proclaimed in the Rio+20 UN Conference on Sustainable Development, stating that "We resolve to promote education for sustainable development and to integrate sustainable development more actively into education beyond the Decade of Education for Sustainable Development."\nOne version of education for Sustainable Development recognizes modern-day environmental challenges. It seeks to define new ways to adjust to a changing biosphere, as well as engage individuals to address societal issues that come with them  In the International Encyclopedia of Education, this approach to education is seen as an attempt to "shift consciousness toward an ethics of life-giving relationships that respects the interconnectedness of man to his natural world" to equip future members of society with environmental awareness and a sense of responsibility to sustainability.\nFor UNESCO, education for sustainable development involves:\n\nintegrating key sustainable development issues into teaching and learning. This may include, for example, instruction about climate change, disaster risk reduction, biodiversity, and poverty reduction and sustainable consumption. It also requires participatory teaching and learning methods that motivate and empower learners to change their behaviours and take action for sustainable development. ESD consequently promotes competencies like critical thinking, imagining future scenarios and making decisions in a collaborative way.\nThe Thessaloniki Declaration, presented at the "International Conference on Environment and Society: Education and Public Awareness for Sustainability" by UNESCO and the Government of Greece (December 1997), highlights the importance of sustainability not only with regards to the natural environment, but also with "poverty, health, food security, democracy, human rights, and peace".\n\n\n== See also ==\n\nList of sustainability topics\nOutline of sustainability – Overview of and topical guide to sustainability\nPolicy coherence for development – Approach in development assistance\nSustainability measurement – Quantitbasis for the informed management of sustainability\nSustainable remediation\nUnited Nations Decade of Education for Sustainable Development\n\n\n== References ==\n\n\n== External links ==\n\nSustainable Development Knowledge Platform of the UN\nSustainable Development Solutions Network',
  },
  {
    title: "Greenhouse gases",
    originalContent:
      "Greenhouse gases (GHGs) are the gases in the atmosphere that raise the surface temperature of planets such as the Earth. What distinguishes them from other gases is that they absorb the wavelengths of radiation that a planet emits, resulting in the greenhouse effect. The Earth is warmed by sunlight, causing its surface to radiate heat, which is then mostly absorbed by greenhouse gases. Without greenhouse gases in the atmosphere, the average temperature of Earth's surface would be about −18 °C (0 °F), rather than the present average of 15 °C (59 °F).\nThe five most abundant greenhouse gases in Earth's atmosphere, listed in decreasing order of average global mole fraction, are: water vapor, carbon dioxide, methane, nitrous oxide, ozone. Other greenhouse gases of concern include chlorofluorocarbons (CFCs and HCFCs), hydrofluorocarbons (HFCs), perfluorocarbons, SF6, and NF3. Water vapor causes about half of the greenhouse effect, acting in response to other gases as a climate change feedback.\nHuman activities since the beginning of the Industrial Revolution (around 1750) have increased carbon dioxide by over 50%, and methane levels by 150%. Carbon dioxide emissions are causing about three-quarters of global warming, while methane emissions cause most of the rest. The vast majority of carbon dioxide emissions by humans come from the burning of fossil fuels, with remaining contributions from agriculture and industry.: 687  Methane emissions originate from agriculture, fossil fuel production, waste, and other sources. The carbon cycle takes thousands of years to fully absorb CO2 from the atmosphere, while methane lasts in the atmosphere for an average of only 12 years.\nNatural flows of carbon happen between the atmosphere, terrestrial ecosystems, the ocean, and sediments. These flows have been fairly balanced over the past 1 million years, although greenhouse gas levels have varied widely in the more distant past. Carbon dioxide levels are now higher than they have been for 3 million years. If current emission rates continue then global warming will surpass 2.0 °C (3.6 °F) sometime between 2040 and 2070. This is a level which the Intergovernmental Panel on Climate Change (IPCC) says is \"dangerous\".\n\n\n== Properties and mechanisms ==\n\nGreenhouse gases are infrared active, meaning that they absorb and emit infrared radiation in the same long wavelength range as what is emitted by the Earth's surface, clouds and atmosphere.: 2233 \n99% of the Earth's dry atmosphere (excluding water vapor) is made up of nitrogen (N2) (78%) and oxygen (O2) (21%). Because their molecules contain two atoms of the same element, they have no asymmetry in the distribution of their electrical charges, and so are almost totally unaffected by infrared thermal radiation, with only an extremely minor effect from collision-induced absorption. A further 0.9% of the atmosphere is made up by argon (Ar), which is monatomic, and so completely transparent to thermal radiation. On the other hand, carbon dioxide (0.04%), methane, nitrous oxide and even less abundant trace gases account for less than 0.1% of Earth's atmosphere, but because their molecules contain atoms of different elements, there is an asymmetry in electric charge distribution which allows molecular vibrations to interact with electromagnetic radiation. This makes them infrared active, and so their presence causes greenhouse effect.\n\n\n=== Radiative forcing ===\n\nEarth absorbs some of the radiant energy received from the sun, reflects some of it as light and reflects or radiates the rest back to space as heat. A planet's surface temperature depends on this balance between incoming and outgoing energy. When Earth's energy balance is shifted, its surface becomes warmer or cooler, leading to a variety of changes in global climate. Radiative forcing is a metric calculated in watts per square meter, which characterizes the impact of an external change in a factor that influences climate. It is calculated as the difference in top-of-atmosphere (TOA) energy balance immediately caused by such an external change. A positive forcing, such as from increased concentrations of greenhouse gases, means more energy arriving than leaving at the top-of-atmosphere, which causes additional warming, while negative forcing, like from sulfates forming in the atmosphere from sulfur dioxide, leads to cooling.: 2245  \nWithin the lower atmosphere, greenhouse gases exchange thermal radiation with the surface and limit radiative heat flow away from it, which reduces the overall rate of upward radiative heat transfer.: 139  The increased concentration of greenhouse gases is also cooling the upper atmosphere, as it is much thinner than the lower layers, and any heat re-emitted from greenhouse gases is more likely to travel further to space than to interact with the fewer gas molecules in the upper layers. The upper atmosphere is also shrinking as the result.\n\n\n== Contributions of specific gases to the greenhouse effect ==\nAnthropogenic changes to the natural greenhouse effect are sometimes referred to as the enhanced greenhouse effect.: 2223  \nThis table shows the most important contributions to the overall greenhouse effect, without which the average temperature of Earth's surface would be about −18 °C (0 °F), instead of around 15 °C (59 °F). This table also specifies tropospheric ozone, because this gas has a cooling effect in the stratosphere, but a warming influence comparable to nitrous oxide and CFCs in the troposphere.\n\n\n=== Special role of water vapor ===\n\nWater vapor is the most important greenhouse gas overall, being responsible for 41–67% of the greenhouse effect, but its global concentrations are not directly affected by human activity. While local water vapor concentrations can be affected by developments such as irrigation, it has little impact on the global scale due to its short residence time of about nine days. Indirectly, an increase in global temperatures cause will also increase water vapor concentrations and thus their warming effect, in a process known as water vapor feedback. It occurs because Clausius–Clapeyron relation establishes that more water vapor will be present per unit volume at elevated temperatures. Thus, local atmospheric concentration of water vapor varies from less than 0.01% in extremely cold regions and up to 3% by mass in saturated air at about 32 °C.\n\n\n=== Global warming potential (GWP) and CO2 equivalents ===\n\n\n== List of all greenhouse gases ==\n\nThe contribution of each gas to the enhanced greenhouse effect is determined by the characteristics of that gas, its abundance, and any indirect effects it may cause. For example, the direct radiative effect of a mass of methane is about 84 times stronger than the same mass of carbon dioxide over a 20-year time frame.  Since the 1980s, greenhouse gas forcing contributions (relative to year 1750) are also estimated with high accuracy using IPCC-recommended expressions derived from radiative transfer models.\nThe concentration of a greenhouse gas is typically measured in parts per million (ppm) or parts per billion (ppb) by volume. A CO2 concentration of 420 ppm means that 420 out of every million air molecules is a CO2 molecule. The first 30 ppm increase in CO2 concentrations took place in about 200 years, from the start of the Industrial Revolution to 1958; however the next 90 ppm increase took place within 56 years, from 1958 to 2014. Similarly, the average annual increase in the 1960s was only 37% of what it was in 2000 through 2007.\nMany observations are available online in a variety of Atmospheric Chemistry Observational Databases. The table below shows the most influential long-lived, well-mixed greenhouse gases, along with their tropospheric concentrations and direct radiative forcings, as identified by the Intergovernmental Panel on Climate Change (IPCC).   Abundances of these trace gases are regularly measured by atmospheric scientists from samples collected throughout the world. It excludes water vapor because changes in its concentrations are calculated as a climate change feedback indirectly caused by changes in other greenhouse gases, as well as ozone, whose concentrations are only modified indirectly by various refrigerants that cause ozone depletion. Some short-lived gases (e.g. carbon monoxide, NOx) and aerosols (e.g. mineral dust or black carbon) are also excluded because of limited role and strong variation, along with minor refrigerants and other halogenated gases, which have been mass-produced in smaller quantities than those in the table.: 731–738  and Annex III of the 2021 IPCC WG1 Report: 4–9 \n\na Mole fractions: μmol/mol = ppm = parts per million (106); nmol/mol = ppb = parts per billion (109); pmol/mol = ppt = parts per trillion (1012).\nA  The IPCC states that \"no single atmospheric lifetime can be given\" for CO2.: 731   This is mostly due to the rapid growth and cumulative magnitude of the disturbances to Earth's carbon cycle by the geologic extraction and burning of fossil carbon. As of year 2014, fossil CO2 emitted as a theoretical 10 to 100 GtC pulse on top of the existing atmospheric concentration was expected to be 50% removed by land vegetation and ocean sinks in less than about a century, as based on the projections of coupled models referenced in the AR5 assessment. A substantial fraction (20–35%) was also projected to remain in the atmosphere for centuries to millennia, where fractional persistence increases with pulse size.\nB  Values are relative to year 1750.  AR6 reports the effective radiative forcing which includes effects of rapid adjustments in the atmosphere and at the surface.\n\n\n== Factors affecting concentrations ==\nAtmospheric concentrations are determined by the balance between sources (emissions of the gas from human activities and natural systems) and sinks (the removal of the gas from the atmosphere by conversion to a different chemical compound or absorption by bodies of water).: 512 \n\n\n=== Airborne fraction ===\n\nThe proportion of an emission remaining in the atmosphere after a specified time is the \"airborne fraction\" (AF). The annual airborne fraction is the ratio of the atmospheric increase in a given year to that year's total emissions. The annual airborne fraction for CO2 had been stable at 0.45 for the past six decades even as the emissions have been increasing. This means that the other 0.55 of emitted CO2 is absorbed by the land and atmosphere carbon sinks within the first year of an emission. In the high-emission scenarios, the effectiveness of carbon sinks will be lower, increasing the atmospheric fraction of CO2 even though the raw amount of emissions absorbed will be higher than in the present.: 746 \n\n\n=== Atmospheric lifetime ===\n\nMajor greenhouse gases are well mixed and take many years to leave the atmosphere. \nThe atmospheric lifetime of a greenhouse gas refers to the time required to restore equilibrium following a sudden increase or decrease in its concentration in the atmosphere. Individual atoms or molecules may be lost or deposited to sinks such as the soil, the oceans and other waters, or vegetation and other biological systems, reducing the excess to background concentrations. The average time taken to achieve this is the mean lifetime. This can be represented through the following formula, where the  lifetime \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n of an atmospheric species X in a one-box model is the average time that a molecule of X remains in the box.\n\n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n can also be defined as the ratio of the mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n (in kg) of X in the box to its removal rate, which is the sum of the flow of X out of the box\n(\n  \n    \n      \n        \n          F\n          \n            out\n          \n        \n      \n    \n    {\\displaystyle F_{\\text{out}}}\n  \n),\nchemical loss of X\n(\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n),\nand deposition of X\n(\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n)\n(all in kg/s):\n\n  \n    \n      \n        τ\n        =\n        \n          \n            m\n            \n              \n                F\n                \n                  out\n                \n              \n              +\n              L\n              +\n              D\n            \n          \n        \n      \n    \n    {\\displaystyle \\tau ={\\frac {m}{F_{\\text{out}}+L+D}}}\n  \n.\nIf input of this gas into the box ceased, then after time \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n, its concentration would decrease by about 63%.\nChanges to any of these variables can alter the atmospheric lifetime of a greenhouse gas. For instance, methane's atmospheric lifetime is estimated to have been lower in the 19th century than now, but to have been higher in the second half of the 20th century than after 2000. Carbon dioxide has an even more variable lifetime, which cannot be specified down to a single number.: 2237  Scientists instead say that while the first 10% of carbon dioxide's airborne fraction (not counting the ~50%  absorbed by land and ocean sinks within the emission's first year) is removed \"quickly\", the vast majority of the airborne fraction – 80% – lasts for \"centuries to millennia\". The remaining 10% stays for tens of thousands of years. In some models, this longest-lasting fraction is as large as 30%.\n\n\n=== During geologic time scales ===\n\n\n== Monitoring ==\n\nGreenhouse gas monitoring involves the direct measurement of atmospheric concentrations and direct and indirect measurement of greenhouse gas emissions. Indirect methods calculate emissions of greenhouse gases based on related metrics such as fossil fuel extraction.\nThere are several different methods of measuring carbon dioxide concentrations in the atmosphere, including infrared analyzing and manometry. Methane and nitrous oxide are measured by other instruments, such as the range-resolved infrared differential absorption lidar (DIAL). Greenhouse gases are measured from space such as by the Orbiting Carbon Observatory and through networks of ground stations such as the Integrated Carbon Observation System.\nThe Annual Greenhouse Gas Index (AGGI) is defined by atmospheric scientists at NOAA as the ratio of total direct radiative forcing due to long-lived and well-mixed greenhouse gases for any year for which adequate global measurements exist, to that present in year 1990. These radiative forcing levels are relative to those present in year 1750 (i.e. prior to the start of the industrial era). 1990 is chosen because it is the baseline year for the Kyoto Protocol, and is the publication year of the first IPCC Scientific Assessment of Climate Change. As such, NOAA states that the AGGI \"measures the commitment that (global) society has already made to living in a changing climate. It is based on the highest quality atmospheric observations from sites around the world. Its uncertainty is very low.\"\n\n\n=== Data networks ===\n\n\n== Types of sources ==\n\n\n=== Natural sources ===\n\nThe natural flows of carbon between the atmosphere, ocean, terrestrial ecosystems, and sediments are fairly balanced; so carbon levels would be roughly stable without human influence. Carbon dioxide is removed from the atmosphere primarily through photosynthesis and enters the terrestrial and oceanic biospheres. Carbon dioxide also dissolves directly from the atmosphere into bodies of water (ocean, lakes, etc.), as well as dissolving in precipitation as raindrops fall through the atmosphere. When dissolved in water, carbon dioxide reacts with water molecules and forms carbonic acid, which contributes to ocean acidity. It can then be absorbed by rocks through weathering. It also can acidify other surfaces it touches or be washed into the ocean.\n\n\n=== Human-made sources ===\n\nThe vast majority of carbon dioxide emissions by humans come from the burning of fossil fuels. Additional contributions come from cement manufacturing, fertilizer production, and changes in land use like deforestation.: 687  Methane emissions originate from agriculture, fossil fuel production, waste, and other sources. \nIf current emission rates continue then temperature rises will surpass 2.0 °C (3.6 °F) sometime between 2040 and 2070, which is the level the United Nations' Intergovernmental Panel on Climate Change (IPCC) says is \"dangerous\".\n\nMost greenhouse gases have both natural and human-caused sources. An exception are purely human-produced synthetic halocarbons which have no natural sources. During the pre-industrial Holocene, concentrations of existing gases were roughly constant, because the large natural sources and sinks roughly balanced. In the industrial era, human activities have added greenhouse gases to the atmosphere, mainly through the burning of fossil fuels and clearing of forests.: 115 \n\n\n== Reducing human-caused greenhouse gases ==\n\n\n=== Needed emissions cuts ===\n\n\n=== Removal from the atmosphere through negative emissions ===\n\nSeveral technologies remove greenhouse gas emissions from the atmosphere. Most widely analyzed are those that remove carbon dioxide from the atmosphere, either to geologic formations such as bio-energy with carbon capture and storage and carbon dioxide air capture, or to the soil as in the case with biochar. Many long-term climate scenario models require large-scale human-made negative emissions to avoid serious climate change. \nNegative emissions approaches are also being studied for atmospheric methane, called atmospheric methane removal.\n\n\n== History of discovery ==\n\nIn the late 19th century, scientists experimentally discovered that N2 and O2 do not absorb infrared radiation (called, at that time, \"dark radiation\"), while water (both as true vapor and condensed in the form of microscopic droplets suspended in clouds) and CO2 and other poly-atomic gaseous molecules do absorb infrared radiation. In the early 20th century, researchers realized that greenhouse gases in the atmosphere made Earth's overall temperature higher than it would be without them. The term greenhouse was first applied to this phenomenon by Nils Gustaf Ekholm in 1901.\nDuring the late 20th century, a scientific consensus evolved that increasing concentrations of greenhouse gases in the atmosphere cause a substantial rise in global temperatures and changes to other parts of the climate system, with consequences for the environment and for human health.\n\n\n== Other planets ==\n\nGreenhouse gases exist in many atmospheres, creating greenhouse effects on Mars, Titan, and particularly in the thick atmosphere of Venus. While Venus has been described as the ultimate end state of runaway greenhouse effect, such a process would have virtually no chance of occurring from any increases in greenhouse gas concentrations caused by humans, as the Sun's brightness is too low and it would likely need to increase by some tens of percents, which will take a few billion years.\n\n\n== See also ==\n\nCarbon accounting – Processes used to measure emissions of carbon dioxide equivalents\nCarbon budget – Limit on carbon dioxide emission for a given climate impact\nCarbon sequestration – Storing carbon in a carbon pool\nClimate change feedback – Feedback related to climate changePages displaying short descriptions of redirect targets\nGreenhouse gas monitoring – Measurement of greenhouse gas emissions and levels\nGreenhouse gas inventory – Inventory for emissions of greenhouse gases\n\n\n== References ==\n\n\n== External links ==\n\n Media related to Greenhouse gases at Wikimedia Commons\nCarbon Dioxide Information Analysis Center (CDIAC), U.S. Department of Energy, retrieved 26 July 2020\nAnnual Greenhouse Gas Index (AGGI) from NOAA\nAtmospheric spectra of GHGs and other trace gases. Archived 25 March 2013 at the Wayback Machine.",
  },
  {
    title: "Arctic",
    originalContent:
      "The Arctic ( or ) (from Greek ἄρκτος, 'bear') is a polar region located at the northernmost part of Earth. The Arctic region, from the IERS Reference Meridian travelling east, consists of parts of northern Norway (Nordland, Troms, Finnmark, Svalbard and Jan Mayen), northernmost Sweden (Västerbotten, Norrbotten and Lappland), northern Finland (North Ostrobothnia, Kainuu and Lappi), Russia (Murmansk, Siberia, Nenets Okrug, Novaya Zemlya), the United States (Alaska), Canada (Yukon, Northwest Territories, Nunavut), Danish Realm (Greenland), and northern Iceland (Grímsey and Kolbeinsey), along with the Arctic Ocean and adjacent seas. Land within the Arctic region has seasonally varying snow and ice cover, with predominantly treeless permafrost under the tundra. Arctic seas contain seasonal sea ice in many places.\nThe Arctic region is a unique area among Earth's ecosystems. The cultures in the region and the Arctic indigenous peoples have adapted to its cold and extreme conditions. Life in the Arctic includes zooplankton and phytoplankton, fish and marine mammals, birds, land animals, plants and human societies. Arctic land is bordered by the subarctic.\n\n\n== Definition and etymology ==\nThe word Arctic comes from the Greek word ἀρκτικός (arktikos), \"near the Bear, northern\" and from the word ἄρκτος (arktos), meaning bear. The name refers either to the constellation known as Ursa Major, the \"Great Bear\", which is prominent in the northern portion of the celestial sphere, or to the constellation Ursa Minor, the \"Little Bear\", which contains the celestial north pole (currently very near Polaris, the current north Pole Star, or North Star).\nThere are a number of definitions of what area is contained within the Arctic. The area can be defined as north of the Arctic Circle (about 66° 34'N), the approximate southern limit of the midnight sun and the polar night. Another definition of the Arctic, which is popular with ecologists, is the region in the Northern Hemisphere where the average temperature for the warmest month (July) is below 10 °C (50 °F); the northernmost tree line roughly follows the isotherm at the boundary of this region.\n\n\n== Climate ==\n\nThe climate of the Arctic region is characterized by cold winters and cool summers. Its precipitation mostly comes in the form of snow and is low, with most of the area receiving less than 50 cm (20 in). High winds often stir up snow, creating the illusion of continuous snowfall. Average winter temperatures can go as low as −40 °C (−40 °F), and the coldest recorded temperature is approximately −68 °C (−90 °F). Coastal Arctic climates are moderated by oceanic influences, having generally warmer temperatures and heavier snowfalls than the colder and drier interior areas. The Arctic is affected by current global warming, leading to climate change in the Arctic, including Arctic sea ice decline, diminished ice in the Greenland ice sheet, and Arctic methane emissions as the permafrost thaws. The melting of Greenland's ice sheet is linked to polar amplification.\nDue to the poleward migration of the planet's isotherms (about 56 km (35 mi) per decade during the past 30 years as a consequence of global warming), the Arctic region (as defined by tree line and temperature) is currently shrinking. Perhaps the most alarming result of this is Arctic sea ice shrinkage. There is a large variance in predictions of Arctic sea ice loss, with models showing near-complete to complete loss in September from 2035 to some time around 2067.\n\n\n== Flora and fauna ==\nArctic life is characterized by adaptation to short growing seasons with long periods of sunlight, and cold, dark, snow-covered winter conditions.\n\n\n=== Plants ===\n\nArctic vegetation is composed of plants such as dwarf shrubs, graminoids, herbs, lichens, and mosses, which all grow relatively close to the ground, forming tundra. An example of a dwarf shrub is the bearberry. As one moves northward, the amount of warmth available for plant growth decreases considerably. In the northernmost areas, plants are at their metabolic limits, and small differences in the total amount of summer warmth make large differences in the amount of energy available for maintenance, growth and reproduction. Colder summer temperatures cause the size, abundance, productivity and variety of plants to decrease. Trees cannot grow in the Arctic, but in its warmest parts, shrubs are common and can reach 2 m (6 ft 7 in) in height; sedges, mosses and lichens can form thick layers. In the coldest parts of the Arctic, much of the ground is bare; non-vascular plants such as lichens and mosses predominate, along with a few scattered grasses and forbs (like the Arctic poppy).\n\n\n=== Animals ===\n\nHerbivores on the tundra include the Arctic hare, lemming, muskox, and reindeer (caribou). They are preyed on by the snowy owl, Arctic fox, grizzly bear, and Arctic wolf. The polar bear is also a predator, though it prefers to hunt for marine life from the ice. There are also many birds and marine species endemic to the colder regions. Other terrestrial animals include wolverines, moose, Dall sheep, ermines, and Arctic ground squirrels. Marine mammals include seals, walruses, and several species of cetacean—baleen whales and also narwhals, orcas, and belugas. An excellent and famous example of a ring species exists and has been described around the Arctic Circle in the form of the Larus gulls.\n\n\n== Natural resources ==\n\nThere are copious natural resources in the Arctic (oil, gas, minerals, fresh water, fish and, if the subarctic is included, forest) to which modern technology and the economic opening up of Russia have given significant new opportunities. The interest of the tourism industry is also on the increase.\nThe Arctic contains some of the last and most extensive continuous wilderness areas in the world, and its significance in preserving biodiversity and genotypes is considerable. The increasing presence of humans fragments vital habitats. The Arctic is particularly susceptible to the abrasion of groundcover and to the disturbance of the rare breeding grounds of the animals that are characteristic to the region. The Arctic also holds 1/5 of the Earth's water supply.\n\n\n== Paleontology ==\nDuring the Cretaceous time period, the Arctic still had seasonal snows, though only a light dusting and not enough to permanently hinder plant growth. Animals such as the Chasmosaurus, Hypacrosaurus, Troodon, and Edmontosaurus may have all migrated north to take advantage of the summer growing season, and migrated south to warmer climes when winter came. A similar situation may also have been found amongst dinosaurs that lived in Antarctic regions, such as the Muttaburrasaurus of Australia.\nHowever, others claim that dinosaurs lived year-round at very high latitudes, such as near the Colville River, which is now at about 70° N but at the time (70 million years ago) was 10° further north.\n\n\n== Indigenous population ==\n\nThe earliest inhabitants of North America's central and eastern Arctic are referred to as the Arctic small tool tradition (AST) and existed c. 2500 BCE. AST consisted of several Paleo-Eskimo cultures, including the Independence cultures and Pre-Dorset culture. The Dorset culture (Inuktitut: Tuniit or Tunit) refers to the next inhabitants of central and eastern Arctic. The Dorset culture evolved because of technological and economic changes during the period of 1050–550 BCE. With the exception of the Quebec / Labrador peninsula, the Dorset culture vanished around 1500 CE. Supported by genetic testing, evidence shows that descendants of the Dorset culture, known as the Sadlermiut, survived in Aivilik, Southampton and Coats Islands, until the beginning of the 20th century.\nThe Dorset / Thule culture transition dates around the ninth–10th centuries CE. Scientists theorize that there may have been cross-contact of the two cultures with sharing of technology, such as fashioning harpoon heads, or the Thule may have found Dorset remnants and adapted their ways with the predecessor culture. The evidence suggested that Inuit descend from the Birnirk of Siberia, who through the Thule culture expanded into northern Canada and Greenland, where they genetically and culturally completely replaced the Indigenous Dorset people some time after 1300 CE. The question of why the Dorset disappeared so completely has led some to suggest that Thule invaders wiped out the Dorset people in \"an example of prehistoric genocide.\"\nBy 1300 CE, the Inuit, present-day Arctic inhabitants and descendants of Thule culture, had settled in west Greenland, and moved into east Greenland over the following century (Inughuit, Kalaallit and Tunumiit are modern Greenlandic Inuit groups descended from Thule). Over time, the Inuit have migrated throughout the Arctic regions of Eastern Russia, the United States, Canada, and Greenland.\nOther Circumpolar North indigenous peoples include the Chukchi, Evenks, Iñupiat, Khanty, Koryaks, Nenets, Sámi, Yukaghir, Gwichʼin, and Yupik.\n\n\n== International cooperation and politics ==\n\nThe eight Arctic nations (Canada, Kingdom of Denmark [Greenland & The Faroe Islands], Finland, Iceland, Norway, Sweden, Russia, and US) are all members of the Arctic Council, as are organizations representing six indigenous populations (The Aleut International Association, Arctic Athabaskan Council, Gwich'in Council International, Inuit Circumpolar Council, Russian Association of Indigenous Peoples of the North, and Saami Council). The council operates on consensus basis, mostly dealing with environmental treaties and not addressing boundary or resource disputes.\nThough Arctic policy priorities differ, every Arctic nation is concerned about sovereignty/defense, resource development, shipping routes, and environmental protection. Much work remains on regulatory agreements regarding shipping, tourism, and resource development in Arctic waters. Arctic shipping is subject to some regulatory control through the International Code for Ships Operating in Polar Waters, adopted by the International Maritime Organization on 1 January 2017 and applies to all ships in Arctic waters over 500 tonnes.\nResearch in the Arctic has long been a collaborative international effort, evidenced by the International Polar Year. The International Arctic Science Committee, hundreds of scientists and specialists of the Arctic Council, and the Barents Euro-Arctic Council are more examples of collaborative international Arctic research.\n\n\n=== Territorial claims ===\n\nWhile there are several ongoing territorial claims in the Arctic, no country owns the geographic North Pole or the region of the Arctic Ocean surrounding it. The surrounding six Arctic states that border the Arctic Ocean—Canada, Kingdom of Denmark (with Greenland), Iceland, Norway, Russia, and the United States—are limited to a 200 nautical miles (370 km; 230 mi) exclusive economic zone (EEZ) off their coasts. Two Arctic states (Finland and Sweden) do not have direct access to the Arctic Ocean.\nUpon ratification of the United Nations Convention on the Law of the Sea, a country has ten years to make claims to an extended continental shelf beyond its 200 nautical mile zone. Due to this, Norway (which ratified the convention in 1996), Russia (ratified in 1997), Canada (ratified in 2003) and the Kingdom of Denmark (ratified in 2004) launched projects to establish claims that certain sectors of the Arctic seabed should belong to their territories.\nOn 2 August 2007, two Russian bathyscaphes, MIR-1 and MIR-2, for the first time in history descended to the Arctic seabed beneath the North Pole and placed there a Russian flag made of rust-proof titanium alloy. The flag-placing, during Arktika 2007, generated commentary on and concern for a race for control of the Arctic's vast hydrocarbon resources.\n\nForeign ministers and other officials representing Canada, the Kingdom of Denmark, Norway, Russia, and the United States met in Ilulissat, Greenland on 28 May 2008 at the Arctic Ocean Conference and announced the Ilulissat Declaration, blocking any \"new comprehensive international legal regime to govern the Arctic Ocean,\" and pledging \"the orderly settlement of any possible overlapping claims.\"\nAs of 2012, the Kingdom of Denmark is claiming the continental shelf based on the Lomonosov Ridge between Greenland and over the North Pole to the northern limit of the exclusive economic zone of Russia.\nThe Russian Federation is also claiming a large swath of seabed along the Lomonosov Ridge but, unlike Denmark, confined its claim to its side of the Arctic region. In August 2015, Russia made a supplementary submission for the expansion of the external borders of its continental shelf in the Arctic Ocean, asserting that the eastern part of the Lomonosov Ridge and the Mendeleyev Ridge are an extension of the Eurasian continent. In August 2016, the UN Commission on the Limits of the Continental Shelf began to consider Russia's submission.\nCanada claims the Northwest Passage as part of its internal waters belonging to Canada, while the United States and most maritime nations regards it as an international strait, which means that foreign vessels have right of transit passage.\n\n\n=== Exploration ===\n\nSince 1937, the larger portion of the Asian-side Arctic region has been extensively explored by Soviet and Russian crewed drifting ice stations. Between 1937 and 1991, 88 international polar crews established and occupied scientific settlements on the drift ice and were carried thousands of kilometres by the ice flow.\n\n\n=== Pollution ===\n\nThe Arctic is comparatively clean, although there are certain ecologically difficult localized pollution problems that present a serious threat to people's health living around these pollution sources. Due to the prevailing worldwide sea and air currents, the Arctic area is the fallout region for long-range transport pollutants, and in some places the concentrations exceed the levels of densely populated urban areas. An example of this is the phenomenon of Arctic haze, which is commonly blamed on long-range pollutants. Another example is with the bioaccumulation of PCB's (polychlorinated biphenyls) in Arctic wildlife and people.\n\n\n=== Preservation ===\n\nThere have been many proposals to preserve the Arctic over the years. Most recently a group of stars at the United Nations Conference on Sustainable Development, on 21 June 2012, proposed protecting the Arctic, similar to the Antarctic Treaty System. The initial focus of the campaign will be a UN resolution creating a global sanctuary around the pole, and a ban on oil drilling and unsustainable fishing in the Arctic.\nThe Arctic has climate change rates that are amongst the highest in the world. Due to the major impacts to the region from climate change the near climate future of the region will be extremely different under all scenarios of warming.\n\n\n== Climate change ==\n\nThe effects of climate change in the Arctic include rising temperatures, loss of sea ice, and melting of the Greenland ice sheet. Potential methane release from the region, especially through the thawing of permafrost and methane clathrates, is also a concern. Because of the amplified response of the Arctic to global warming, it is often seen as a leading indicator of global warming. The melting of Greenland's ice sheet is linked to polar amplification.\nThe Arctic region is especially vulnerable to the effects of any climate change, as has become apparent with the reduction of sea ice in recent years. Climate models predict much greater climate change in the Arctic than the global average, resulting in significant international attention to the region. In particular, there are concerns that Arctic shrinkage, a consequence of melting glaciers and other ice in Greenland, could soon contribute to a substantial rise in sea levels worldwide.\nThe current Arctic warming is leading to ancient carbon being released from thawing permafrost, leading to methane and carbon dioxide production by micro-organisms. Release of methane and carbon dioxide stored in permafrost could cause abrupt and severe global warming, as they are potent greenhouse gases.\n\nClimate change is also predicted to have a large impact on tundra vegetation, causing an increase of shrubs, and having a negative impact on bryophytes and lichens.\nApart from concerns regarding the detrimental effects of warming in the Arctic, some potential opportunities have gained attention. The melting of the ice is making the Northwest Passage, shipping routes through the northernmost latitudes, more navigable, raising the possibility that the Arctic region will become a prime trade route. One harbinger of the opening navigability of the Arctic took place in the summer of 2016 when the Crystal Serenity successfully navigated the Northwest Passage, a first for a large cruise ship.\nIn addition, it is believed that the Arctic seabed may contain substantial oil fields which may become accessible if the ice covering them melts. These factors have led to recent international debates as to which nations can claim sovereignty or ownership over the waters of the Arctic.\n\n\n== Arctic waters ==\n\n\n== Arctic lands ==\n\n\n== See also ==\n\nArctic ecology\nArctic Search and Rescue Agreement\nList of countries by northernmost point\nArctic sanctuary\nPoverty in the Arctic\nArctic Winter Games\nWinter City\nGlobal North\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Bibliography ==\nGibbon, Guy E.; Kenneth M. Ames (1998). Archaeology of prehistoric native America: an encyclopedia. Vol. 1537 of Garland reference library of the humanities. Taylor & Francis. ISBN 978-0-8153-0725-9.\n\n\n== Further reading ==\nBrian W. Coad, James D. Reist. (2017). Marine Fishes of Arctic Canada. University of Toronto Press. ISBN 978-1-4426-4710-7\n\"Global Security, Climate Change, and the Arctic\" Archived 29 December 2017 at the Wayback Machine – 24-page special journal issue (Fall 2009), Swords and Ploughshares, Program in Arms Control, Disarmament, and International Security (ACDIS), University of Illinois\nGLOBIO Human Impact maps Report on human impacts on the Arctic\nKrupnik, Igor, Michael A. Lang, and Scott E. Miller, eds. Smithsonian at the Poles: Contributions to International Polar Year Science. Washington, D.C.: Smithsonian Institution Scholarly Press, 2009.\nKonyshev, Valery & Sergunin, Alexander: The Arctic at the Crossroads of Geopolitical Interests Russian Politics and Law, 2012, Vol.50, No.2, pp. 34–54\nKäpylä, Juha & Mikkola, Harri: The Global Arctic: The Growing Arctic Interests of Russia, China, the United States and the European Union Archived 15 September 2013 at the Wayback Machine FIIA Briefing Paper 133, August 2013, The Finnish Institute of International Affairs.\nKonyshev, Valery & Sergunin, Alexander. The Arctic at the crossroads of geopolitical interests // Russian Politics and Law, 2012. Vol. 50, No. 2. p. 34–54\nKonyshev, Valery & Sergunin, Alexander: Is Russia a revisionist military power in the Arctic? Defense & Security Analysis, September 2014.\nKonyshev, Valery & Sergunin, Alexander. Russia in search of its Arctic strategy: between hard and soft power? Polar Journal, April 2014.\nMcCannon, John. A History of the Arctic: Nature, Exploration and Exploitation. Reaktion Books and University of Chicago Press, 2012. ISBN 9781780230184\nO'Rourke, Ronald (14 October 2016). Changes in the Arctic: Background and Issues for Congress (PDF). Washington, DC: Congressional Research Service. Archived (PDF) from the original on 9 October 2022. Retrieved 20 October 2016.\nSperry, Armstrong (1957). All About the Arctic and Antarctic. Random House. LCCN 57007518.\n\n\n== External links ==\n\nArctic Report Card\nBlossoming Arctic\nInternational Arctic Research Center",
  },
  {
    title: "Population growth",
    originalContent:
      'Population growth is the increase in the number of people in a population or dispersed group. Actual global human population growth amounts to around 83 million annually, or 1.1% per year. The global population has grown from 1 billion in 1800 to 8.1 billion in 2024. The UN projected population to keep growing, and estimates have put the total population at 8.6 billion by mid-2030, 9.8 billion by mid-2050 and 11.2 billion by 2100. However, some academics outside the UN have increasingly developed human population models that account for additional downward pressures on population growth; in such a scenario population would peak before 2100. Others have challenged many recent population projections as having underestimated population growth.\nThe world human population has been growing since the end of the Black Death, around the year 1350. A mix of technological advancement that improved agricultural productivity and sanitation and medical advancement that reduced mortality increased population growth. In some geographies, this has slowed through the process called the demographic transition, where many nations with high standards of living have seen a significant slowing of population growth. This is in direct contrast with less developed contexts, where population growth is still happening. Globally, the rate of population growth has declined from a peak of 2.2% per year in 1963. \nPopulation growth alongside increased consumption is a driver of environmental concerns, such as biodiversity loss and climate change, due to overexploitation of natural resources for human development. International policy focused on mitigating the impact of human population growth is concentrated in the Sustainable Development Goals which seeks to improve the standard of living globally while reducing the impact of society on the environment while advancing human well-being. \n\n\n== History ==\n\nWorld population has been rising continuously since the end of the Black Death, around the year 1350. Population began growing rapidly in the Western world during the industrial revolution. The most significant increase in the world\'s population has been since the 1950s, mainly due to medical advancements and increases in agricultural productivity.\n\n\n=== Haber process ===\n\nDue to its dramatic impact on the human ability to grow food, the Haber process, named after one of its inventors, the German chemist Fritz Haber, served as the "detonator of the population explosion", enabling the global population to increase from 1.6 billion in 1900 to 7.7 billion by November 2019.\n\n\n=== Thomas McKeown hypotheses ===\nSome of the reasons for the "Modern Rise of Population" were particularly investigated by the British health scientist Thomas McKeown (1912–1988). In his publications, McKeown challenged four theories about the population growth:\n\nMcKeown stated that the growth in Western population, particularly surging in the 19th century, was not so much caused by an increase in fertility, but largely by a decline of mortality particularly of childhood mortality followed by infant mortality,\nThe decline of mortality could largely be attributed to rising standards of living, whereby McKeown put most emphasis on improved nutritional status,\nMcKeown questioned the effectiveness of public health measures, including sanitary reforms, vaccination and quarantine,\nThe “McKeown thesis" states that curative medicine measures played little role in mortality decline, not only prior to the mid-20th century but also until well into the 20th century.\nAlthough the McKeown thesis has been heavily disputed, recent studies have confirmed the value of his ideas. His work is pivotal for present day thinking about population growth, birth control, public health and medical care. McKeown had a major influence on many population researchers, such as health economists and Nobel prize winners Robert W. Fogel (1993) and Angus Deaton (2015). The latter considered McKeown as "the founder of social medicine".\n\n\n== Growth rate models ==\nThe "population growth rate" is the rate at which the number of individuals in a population increases in a given time period, expressed as a fraction of the initial population. Specifically, population growth rate refers to the change in population over a unit time period, often expressed as a percentage of the number of individuals in the population at the beginning of that period. This can be written as the formula, valid for a sufficiently small time interval:\n\n  \n    \n      \n        P\n        o\n        p\n        u\n        l\n        a\n        t\n        i\n        o\n        n\n         \n        g\n        r\n        o\n        w\n        t\n        h\n         \n        r\n        a\n        t\n        e\n        =\n        \n          \n            \n              P\n              (\n              \n                t\n                \n                  2\n                \n              \n              )\n              −\n              P\n              (\n              \n                t\n                \n                  1\n                \n              \n              )\n            \n            \n              P\n              (\n              \n                t\n                \n                  1\n                \n              \n              )\n              (\n              \n                t\n                \n                  2\n                \n              \n              −\n              \n                t\n                \n                  1\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle Population\\ growth\\ rate={\\frac {P(t_{2})-P(t_{1})}{P(t_{1})(t_{2}-t_{1})}}}\n  \n\nA positive growth rate indicates that the population is increasing, while a negative growth rate indicates that the population is decreasing. A growth ratio of zero indicates that there were the same number of individuals at the beginning and end of the period—a growth rate may be zero even when there are significant changes in the birth rates, death rates, immigration rates, and age distribution between the two times.\nA related measure is the net reproduction rate. In the absence of migration, a net reproduction rate of more than 1 indicates that the population of females is increasing, while a net reproduction rate less than one (sub-replacement fertility) indicates that the population of females is decreasing.\nMost populations do not grow exponentially, rather they follow a logistic model. Once the population has reached its carrying capacity, it will stabilize and the exponential curve will level off towards the carrying capacity, which is usually when a population has depleted most its natural resources. In the world human population, growth may be said to have been following a linear trend throughout the last few decades.\n\n\n=== Logistic equation ===\nThe growth of a population can often be modelled by the logistic equation\n\n  \n    \n      \n        \n          \n            \n              d\n              P\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        r\n        P\n        \n          (\n          \n            1\n            −\n            \n              \n                P\n                K\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {dP}{dt}}=rP\\left(1-{\\frac {P}{K}}\\right),}\n  \n\nwhere\n\n  \n    \n      \n        P\n        (\n        t\n        )\n      \n    \n    {\\displaystyle P(t)}\n  \n = the population after time t;\n\n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n = time a population grows;\n\n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n = the relative growth rate coefficient;\n\n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n = the carrying capacity of the population; defined by ecologists as the maximum population size that a particular environment can sustain.\nAs it is a separable differential equation, the population may be solved explicitly, producing a logistic function:\n\n  \n    \n      \n        P\n        (\n        t\n        )\n        =\n        \n          \n            K\n            \n              1\n              +\n              A\n              \n                e\n                \n                  −\n                  r\n                  t\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle P(t)={\\frac {K}{1+Ae^{-rt}}}}\n  \n,\nwhere \n  \n    \n      \n        A\n        =\n        \n          \n            \n              K\n              −\n              \n                P\n                \n                  0\n                \n              \n            \n            \n              P\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle A={\\frac {K-P_{0}}{P_{0}}}}\n  \n and \n  \n    \n      \n        \n          P\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle P_{0}}\n  \n is the initial population at time 0.\n\n\n== Global population growth rate ==\n\nThe world population growth rate peaked in 1963 at 2.2% per year and subsequently declined. In 2017, the estimated annual growth rate was 1.1%. The CIA World Factbook gives the world annual birthrate, mortality rate, and growth rate as 1.86%, 0.78%, and 1.08% respectively. The last 100 years have seen a massive fourfold increase in the population, due to medical advances, lower mortality rates, and an increase in agricultural productivity made possible by the Green Revolution.\nThe annual increase in the number of living humans peaked at 88.0 million in 1989, then slowly declined to 73.9 million in 2003, after which it rose again to 75.2 million in 2006. In 2017, the human population increased by 83 million. Generally, developed nations have seen a decline in their growth rates in recent decades, though annual growth rates remain above 2% in some countries of the Middle East and Sub-Saharan Africa, and also in South Asia, Southeast Asia, and Latin America.\nIn some countries the population is declining, especially in Eastern Europe, mainly due to low fertility rates, high death rates and emigration. In Southern Africa, growth is slowing due to the high number of AIDS-related deaths. Some Western Europe countries might also experience population decline. Japan\'s population began decreasing in 2005.\nThe United Nations Population Division projects world population to reach 11.2 billion by the end of the 21st century. \nThe Institute for Health Metrics and Evaluation projects that the global population will peak in 2064 at 9.73 billion and decline to 8.89 billion in 2100.\n  A 2014 study in Science concludes that the global population will reach 11 billion by 2100, with a 70% chance of continued growth into the 22nd century.   The German Foundation for World Population reported in December 2019 that the global human population grows by 2.6 people every second, and could reach 8 billion by 2023.\n\n\n== Growth by country ==\n\nAccording to United Nations population statistics, the world population grew by 30%, or 1.6 billion humans, between 1990 and 2010. In number of people the increase was highest in India (350 million) and China (196 million). Population growth rate was among highest in the United Arab Emirates (315%) and Qatar (271%).\n\nMany of the world\'s countries, including many in Sub-Saharan Africa, the Middle East, South Asia and South East Asia, have seen a sharp rise in population since the end of the Cold War. The fear is that high population numbers are putting further strain on natural resources, food supplies, fuel supplies, employment, housing, etc. in some of the less fortunate countries. For example, the population of Chad has ultimately grown from 6,279,921 in 1993 to 10,329,208 in 2009, further straining its resources. Vietnam, Mexico, Nigeria, Egypt, Ethiopia, and the DRC are witnessing a similar growth in population.\nThe following table gives some example countries or territories:\n\nNotes\n* Eritrea left Ethiopia in 1991.\n† Split into the nations of Sudan and South Sudan during 2011.\n‡ Japan and the Ryukyu Islands merged in 1972.\n# India and Sikkim merged in 1975.\n\n\n== Future population ==\n\n\n== See also ==\nList of countries by population growth rate\nDemographic history\nDemographic transition\nDensity dependence\nEcological overshoot\nEpidemiological transition\nHuman population planning\nIrruptive growth\nOvershoot (population)\nPopulation decline\nPopulation density\nWorld population\nEstimates of historical world population\nZero population growth\n\n\n== References ==\n\n\n== External links ==\n\n"World Population Prospects". Website of the United Nations Population Division. Archived from the original on 11 July 2017.\n"Food Production and Population Growth". Daniel Quinn, Alan D. Thornhill, PhD. Ecofuture. Population and Sustainability Media, Non-fiction.\n"Probabilistic Population Projections, 2nd Revision". Website of the United Nations Population Division. Archived from the original on 4 October 2013.\n"Population Growth and the Food Supply". Population Institute of Canada.\n"World population growth and trends 1950-2050". US Census. Archived from the original on 7 July 2010.\n"Feeding the Ten Billion-Plants and Population Growth". PGR Newsletter FAO-Bioversity L.T. Evans. 2000. Cambridge University Press. ISBN 0-521-64685-5. Published in Issue No. 125, page 39 to 40 - (5802) characters',
  },
  {
    title: "Food production",
    originalContent:
      'The food industry is a complex, global network of diverse businesses that supplies most of the food consumed by the world\'s population. The food industry today has become highly diversified, with manufacturing ranging from small, traditional, family-run activities that are highly labour-intensive, to large, capital-intensive and highly mechanized industrial processes. Many food industries depend almost entirely on local agriculture, animal farms, produce, and/or fishing.\nIt is challenging to find an inclusive way to cover all aspects of food production and sale. The UK Food Standards Agency describes it as "the whole food industry – from farming and food production, packaging and distribution, to retail and catering". The Economic Research Service of the USDA uses the term food system to describe the same thing, stating: "The U.S. food system is a complex network of farmers and the industries that link to them. Those links include makers of farm equipment and chemicals as well as firms that provide services to agribusinesses, such as providers of transportation and financial services. The system also includes the food marketing industries that link farms to consumers, and which include food and fiber processors, wholesalers, retailers, and foodservice establishments." The food industry includes:\n\nAgriculture: raising crops, livestock, and seafood. Agricultural economics.\nManufacturing: agrichemicals, agricultural construction, farm machinery and supplies, seed, etc.\nFood processing: preparation of fresh products for market, and manufacture of prepared food products\nMarketing: promotion of generic products (e.g., milk board), new products, advertising, marketing campaigns, packaging, public relations, etc.\nWholesale and food distribution: logistics, transportation, warehousing\nFoodservice (which includes catering)\nGrocery, farmers\' markets, public markets and other retailing\nRegulation: local, regional, national, and international rules and regulations for food production and sale, including food quality, food security, food safety, marketing/advertising, and industry lobbying activities\nEducation: academic, consultancy, vocational\nResearch and development: food science, food microbiology, food technology, food chemistry, and food engineering\nFinancial services: credit, insurance\nAreas of research such as food grading, food preservation, food rheology, food storage directly deal with the quality and maintenance of quality overlapping many of the above processes.\nOnly subsistence farmers, those who survive on what they grow, and hunter-gatherers can be considered outside the scope of the modern food industry.\nThe dominant companies in the food industry have sometimes been referred to as Big Food, a term coined by the writer Neil Hamilton.\n\n\n== Food production ==\n\nMost food produced for the food industry comes from commodity crops using conventional agricultural practices. Agriculture is the process of producing food, feeding products, fiber and other desired products by the cultivation of certain plants and the raising of domesticated animals (livestock).  On average, 83% of the food consumed by humans is produced using terrestrial agriculture.In addition to terrestrial agriculture, aquaculture and fishing play vital roles in global food production. Aquaculture involves the cultivation of aquatic organisms such as fish, shrimp, and mollusks in controlled environments like ponds, tanks, or cages. It contributes significantly to the world\'s seafood supply and provides an important source of protein for human consumption. Fishing, on the other hand, relies on harvesting wild aquatic species from oceans, rivers, and lakes, further diversifying the sources of food for human populations and supporting livelihoods in coastal communities worldwide. Together, terrestrial agriculture, aquaculture, and fishing collectively ensure a diverse and ample supply of food to meet the dietary needs of people across the globe.\nScientists, inventors, and others devoted to improving farming methods and implements are also said to be engaged in agriculture. One in three people worldwide are employed in agriculture, yet it only contributes 3% to global GDP. In 2017, on average, agriculture contributes 4% of national GDPs. Global agricultural production is responsible for between 14 and 28% of global greenhouse gas emissions, making it one of the largest contributors to global warming, in large part due to conventional agricultural practices, including nitrogen fertilizers and poor land management.\nAgronomy is the science and technology of producing and using plants for food, fuel, fibre, and land reclamation. Agronomy encompasses work in the areas of plant genetics, plant physiology, meteorology, and soil science. Agronomy is the application of a combination of sciences. Agronomists today are involved with many issues including producing food, creating healthier food, managing the environmental impact of agriculture, and extracting energy from plants.\n\n\n== Food processing ==\n\nFood processing includes the methods and techniques used to transform raw ingredients into food for human consumption. Food processing takes clean, harvested or slaughtered and butchered components and uses them to produce marketable food products. There are several different ways in which food can be produced.\nOne-off production: This method is used when customers make an order for something to be made to their own specifications, for example, a wedding cake. The making of one-off products could take days depending on how intricate the design is.\nBatch production: This method is used when the size of the market for a product is not clear, and where there is a range within a product line.  A certain number of the same goods will be produced to make up a batch or run, for example a bakery may bake a limited number of cupcakes.  This method involves estimating consumer demand.\nMass production: This method is used when there is a mass market for a large number of identical products, for example chocolate bars, ready meals and canned food. The product passes from one stage of production to another along a production line.\nJust-in-time (JIT) (production): This method of production is mainly used in restaurants. All components of the product are available in-house and the customer chooses what they want in the product. It is then prepared in a kitchen, or in front of the buyer as in sandwich delicatessens, pizzerias, and  sushi bars.\n\n\n== Industry influence ==\nThe food industry has a large influence on consumerism.  Organizations, such as The American Academy of Family Physicians (AAFP), have been criticized for accepting monetary donations from companies within the food industry, such as Coca-Cola. These donations have been criticized for creating a conflict of interest and favoring an interest such as financial gains.\n\n\n== Criticism ==\n\n\n=== Media ===\nThere are a number of books, film, TV and web-related exposés and critiques of the food industry, including:\n\nEat This, Not That (nonfiction series published in Men\'s Health magazine)\nFast Food Nation (2001 nonfiction book)\nChew On This (2005 book adaptation of Fast Food Nation for younger readers)\nFast Food Nation (2006 documentary film)\nFood, Inc. (2008 documentary film)\nPanic Nation (2006 nonfiction book)\nSuper Size Me (2004 documentary film)\nForks over Knives (2011 documentary film)\nThe Jungle (1906 novel by Upton Sinclair that exposed health violations and unsanitary practices in the American meat packing industry during the early 20th century, based on his investigation for a socialist newspaper)\n\n\n=== Corporate Influence ===\nThe Bretton Woods Institutions - The World Bank and International Monetary Fund - play a large role in how the food industry functions today. These global funds were born after World War II, to help rebuild Europe and prevent another Great Depression. Overall, their main purpose was to stabilize economies. The IMF provided short term loans while the World Bank was focused on larger projects that would bring electricity back to cities, roads, and other "essential" needs. The World Banks mission and purpose, however, transformed as its President Robert McNamara issued a system of loans known as Structural Adjustment. In accepting loans from the World Bank, countries - especially the Global South - became economically, politically, and socially tied to the West. Many countries struggled to pay back their loans, beginning the process of global debt, privatization, and the downfall of local economies. As a result of Western intervention, many small scale farmers have been displaced, as US corporations have bought out land in other countries and continued to monopolize on food. Today, several multinational corporations have pushed agricultural technologies on developing countries including improved seeds, chemical fertilizers, and pesticides, crop production.\n\n\n== Policy ==\n\nIn 2020 scientists reported that reducing emissions from the global food system is essential to achieving the Paris Agreement\'s climate goals. In 2020, an evidence review for the European Union\'s Scientific Advice Mechanism found that, without significant change, emissions would increase by 30–40% by 2050 due to population growth and changing consumption patterns, and concluded that "the combined environmental cost of food production is estimated to amount to some $12 trillion per year, increasing to $16 trillion by 2050". The IPCC\'s and the EU\'s reports concluded that adapting the food system to reduce greenhouse gas emissions impacts and food security concerns, while shifting towards a sustainable diet, is feasible.\n\n\n=== Regulation ===\n\nSince World War II, agriculture in the United States and the entire national food system in its entirety has been characterized by models that focus on monetary profitability at the expense of social and environmental integrity. Regulations exist to protect consumers and somewhat balance this economic orientation with public interests for food quality, food security, food safety, animal well-being, environmental protection and health.\n\n\n=== Proactive guidance ===\nIn 2020, researchers published projections and models of potential impacts of policy-dependent mechanisms of modulation, or lack thereof, of how, where, and what food is produced. They analyzed policy-effects for specific regions or nations such as reduction of meat production and consumption, reductions in food waste and loss, increases in crop yields and international land-use planning. Their conclusions include that raising agricultural yields is highly beneficial for biodiversity-conservation in sub-Saharan Africa while measures leading to shifts of diets are highly beneficial in North America and that global coordination and rapid action are necessary.\n\n\n== Wholesale and distribution ==\n\nA vast global cargo network connects the numerous parts of the industry.  These include suppliers, manufacturers, warehousers, retailers and the end consumers.) Wholesale markets for fresh food products have tended to decline in importance in urbanizing countries, including Latin America and some Asian countries as a result of the growth of supermarkets, which procure directly from farmers or through preferred suppliers, rather than going through markets.\nThe constant and uninterrupted flow of product from distribution centers to store locations is a critical link in food industry operations. Distribution centers run more efficiently, throughput can be increased, costs can be lowered, and manpower better utilized if the proper steps are taken when setting up a material handling system in a warehouse.\n\n\n== Retail ==\nWith worldwide urbanization, food buying is increasingly removed from food production.  During the 20th century, the supermarket became the defining retail element of the food industry.  There, tens of thousands of products are gathered in one location, in continuous, year-round supply.\nFood preparation is another area where the change in recent decades has been dramatic. Today, two food industry sectors are in apparent competition for the retail food dollar.  The grocery industry sells fresh and largely raw products for consumers to use as ingredients in home cooking.  The food service industry, by contrast, offers prepared food, either as finished products or as partially prepared components for final "assembly".  Restaurants, cafes, bakeries and mobile food trucks provide opportunities for consumers to purchase food.\nIn the 21st century online grocery stores emerged and digital technologies for community-supported agriculture have enabled farmers to directly sell produce. Some online grocery stores have voluntarily set social goals or values beyond meeting consumer demand and the accumulation of profit.\n\n\n== Food industry technologies ==\n\nModern food production is defined by sophisticated technologies. These include many areas. Agricultural machinery, originally led by the tractor, has practically eliminated human labor in many areas of production. Biotechnology is driving much change, in areas as diverse as agrochemicals, plant breeding and food processing. Many other types of technology are also involved, to the point where it is hard to find an area that does not have a direct impact on the food industry. As in other fields, computer technology is also a central force. Other than that, there few more modern technologies that can help to improve the industry as well which are, robotics and automation, blockchain, nanotech, 3D printing, artificial intelligence, smart farming and others. These new technologies can improve the industry in the following ways:\n\nRobotics and automation: Robotics and automation are being used to automate processes such as packaging, sorting, and quality control, which reduces labor costs and increases efficiency. These technologies also reduce the likelihood of contamination by reducing human contact with food.\nBlockchain: Blockchain technology is being used to improve food safety by providing transparency in the supply chain. This technology allows for real-time tracking of food products, from farm to table, which helps to identify any potential safety hazards and enables quick response to any issues.\nNanotechnology: Nanotechnology is being used to develop new packaging materials that can extend the shelf life of food and reduce food waste. These materials can also be designed to be biodegradable, reducing the environmental impact of packaging.\n3D printing: 3D printing is being used to create custom food products and to make food production more efficient. With 3D printing, it is possible to create complex shapes and designs that would be difficult to achieve with traditional manufacturing techniques.\nArtificial intelligence: (AI) is being used to analyze large amounts of data in the food industry, which can help to identify trends and patterns. This technology can be used to optimize processes and to improve the quality and safety of food products.\nSmart farming: Smart farming involves the use of sensors and data analytics to optimize crop yields and reduce waste. This technology can help farmers to make more informed decisions about when to plant, water, and harvest crops, which can improve the efficiency and sustainability of agriculture.\n\n\n== Marketing ==\n\nAs consumers grow increasingly removed from food production, the role of product creation, advertising, and publicity become the primary vehicles for information about food. With processed food as the dominant category, marketers have almost infinite possibilities in product creation. Of the food advertised to children on television, 73% is fast or convenience foods.\nOne of the main challenges in food industry marketing is the high level of competition in the market. Companies must differentiate themselves from their competitors by offering unique products or using innovative marketing techniques. For example, many food companies are now using social media platforms to promote their products and engage with customers.\nAnother important aspect of food industry marketing is understanding consumer behavior and preferences. This includes factors such as age, gender, income, and cultural background. Companies must also be aware of changing consumer trends and adapt their marketing strategies accordingly.\n\n\n== Labor and education ==\n\nUntil the last 100 years, agriculture was labor-intensive. Farming was a common occupation and millions of people were involved in food production. Farmers, largely trained from generation to generation, carried on the family business. That situation has changed dramatically today. In America in 1870, 70–80% of the US population was employed in agriculture. As of 2021, less than 2% of the population is directly employed in agriculture, and about 83% of the population lives in cities.\n\n\n== See also ==\n\nAgroindustry\nAgricultural expansion\nDietary supplement\nFactory farming\nFood fortification, also called Nutrification\nGeography of food\nLocal food\nUltra-processed food\n\n\n== References ==\n\n\n=== Works cited ===\nIPCC (2019). Shukla, P. R.; Skea, J.; Calvo Buendia, E.; Masson-Delmotte, V.; et al. (eds.). IPCC Special Report on Climate Change, Desertification, Land Degradation, Sustainable Land Management, Food Security, and Greenhouse gas fluxes in Terrestrial Ecosystems (PDF). In press.\n\n\n== Further reading ==\nNelson, Scott Reynolds. Oceans of Grain: How American Wheat Remade the World (2022) excerpt\nNestle, M. (2013). Food Politics: How the Food Industry Influences Nutrition and Health. California Studies in Food and Culture. University of California Press. ISBN 978-0-520-95506-6. 534 pages.\nVasconcellos, J.A. (2003). Quality Assurance for the Food Industry: A Practical Approach. CRC Press. ISBN 978-0-203-49810-1. 448 pages.\nKress-Rogers, E.; Brimelow, C.J.B. (2001). Instrumentation and Sensors for the Food Industry. Woodhead Publishing Series in Food Science, Technology and Nutrition. Woodhead. ISBN 978-1-85573-560-6. 836 pages.\nTraill, B.; Pitts, E. (1998). Competitiveness in the Food Industry. Springer. ISBN 978-0-7514-0431-9. 301 pages.\nFood Fight: The Inside Story of the Food Industry\n\n\n== External links ==\n\n"The Food Industry Center". University of Minnesota.',
  },
  {
    title: "Cultural heritage",
    originalContent:
      'Cultural heritage is the heritage of tangible and intangible heritage assets of a group or society that is inherited from past generations. Not all heritages of past generations are "heritage"; rather, heritage is a product of selection by society.\nCultural heritage includes tangible culture (such as buildings, monuments, landscapes, archive materials, books, works of art, and artifacts), intangible culture (such as folklore, traditions, language, and knowledge), and natural heritage (including culturally significant landscapes, and biodiversity). The term is often used in connection with issues relating to the protection of Indigenous intellectual property.\nThe deliberate action of keeping cultural heritage from the present for the future is known as preservation (American English) or conservation (British English), which cultural and historical ethnic museums and cultural centers promote, though these terms may have more specific or technical meanings in the same contexts in the other dialect. Preserved heritage has become an anchor of the global tourism industry, a major contributor of economic value to local communities.\nLegal protection of cultural property comprises a number of international agreements and national laws.\nUnited Nations, UNESCO and Blue Shield International deal with the protection of cultural heritage. This also applies to the integration of United Nations peacekeeping.\n\n\n== Types of heritage ==\n\n\n=== Cultural property ===\n\nCultural property includes the physical, or "tangible" cultural heritage, such as artworks. These are generally split into two groups of movable and immovable heritage. Immovable heritage includes buildings (which themselves may include installed art such as organs, stained glass windows, and frescos), large industrial installations, residential projects, or other historic places and monuments. Moveable heritage includes books, documents, moveable artworks, machines, clothing, and other artifacts, that are considered worthy of preservation for the future. These include objects significant to the archaeology, architecture, science, or technology of a specified culture.\nAspects and disciplines of the preservation and conservation of tangible culture include:\n\nMuseology\nArchival science\nConservation (cultural heritage)\nArt conservation\nArchaeological conservation\nArchitectural conservation\nFilm preservation\nPhonograph record preservation\nDigital preservation\n\n\n=== Intangible culture ===\n\n"Intangible cultural heritage" consists of non-physical aspects of a particular culture, more often maintained by social customs during a specific period in history. The concept includes the ways and means of behavior in a society and the often formal rules for operating in a particular cultural climate. These include social values and traditions, customs and practices, aesthetic and spiritual beliefs, artistic expression, language and other aspects of human activity. The significance of physical artifacts can be interpreted as an act against the backdrop of socioeconomic, political, ethnic, religious, and philosophical values of a particular group of people. Naturally, intangible cultural heritage is more difficult to preserve than physical objects.\nAspects of the preservation and conservation of cultural intangibles include:\n\nfolklore\noral history\nlanguage preservation\n\n\n=== Natural heritage ===\n\n"Natural heritage" is also an important part of a society\'s heritage, encompassing the countryside and natural environment, including flora and fauna, scientifically known as biodiversity, as well as geological elements (including mineralogical, geomorphological, paleontological, etc.), scientifically known as geodiversity. These kinds of heritage sites often serve as an important component in a country\'s tourist industry, attracting many visitors from abroad as well as locally. Heritage can also include cultural landscapes (natural features that may have cultural attributes).\nAspects of the preservation and conservation of natural heritage include:\n\nRare breeds conservation\nHeirloom plants\n\n\n=== Digital heritage ===\n\nDigital heritage is made up of computer-based materials such as texts, databases, images, sounds and software being retained for future generations. Digital heritage includes physical objects such as documents which have been digitized for retention and artifacts which are "born digital", i.e. originally created digitally and having no physical form.\n\n\n== Protection of cultural heritage ==\n\n\n=== History ===\nThere have been examples of respect for the cultural assets of enemies since ancient times. The roots of today\'s legal situation for the precise protection of cultural heritage also lie in some of the regulations of Austria\'s ruler Maria Theresa (1717 - 1780) and the demands of the Congress of Vienna (1814/15) not to remove works of art from their place of origin in the war. The 1863 Lieber code, a military legal code governing the wartime conduct of the Union Army also set rules for the protection of cultural heritage. The process continued at the end of the 19th century when, in 1874 (in Brussels), at least a draft international agreement on the laws and customs of war was agreed. 25 years later, in 1899, an international peace conference was held in the Netherlands on the initiative of Tsar Nicholas II of Russia, with the aim of revising the declaration (which was never ratified) and adopting a convention. The Hague Conventions of 1899 and 1907 also significantly advanced international law and laid down the principle of the immunity of cultural property. Three decades later, in 1935, the preamble to the Treaty on the Protection of Artistic and Scientific Institutions (Roerich Pact) was formulated. On the initiative of UNESCO, the Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict was signed in 1954.\nProtection of cultural heritage or protection of cultural goods refers to all measures aimed at protecting cultural property against damage, destruction, theft, embezzlement, or other loss. The term "monument protection" is also used for immovable cultural property. Protection of cultural heritage relates in particular to the prevention of robbery digs at archaeological sites, the looting or destruction of cultural sites and the theft of works of art from churches and museums all over the world and basically measures regarding the conservation and general access to our common cultural heritage. Legal protection of cultural heritage comprises a number of international agreements and national laws.\nThere is a close partnership between the UN, United Nations peacekeeping, UNESCO, the International Committee of the Red Cross and Blue Shield International.\n\nThe protection of cultural heritage should also preserve the particularly sensitive cultural memory, the growing cultural diversity, and the economic basis of a state, a municipality, or a region. Whereby there is also a connection between cultural user disruption or cultural heritage and the cause of flight. But only through fundamental cooperation, including the military units and the planning staff, with the locals can the protection of world heritage sites, archaeological finds, exhibits, and archaeological sites from destruction, looting, and robbery be implemented sustainably. The founding president of Blue Shield International Karl von Habsburg summed it up with the words: "Without the local community and without the local participants, that would be completely impossible".\n\n\n== The ethics and rationale of cultural preservation ==\nObjects are a part of the study of human history because they provide a concrete basis for ideas, and can validate them. Their preservation demonstrates a recognition of the necessity of the past and of the things that tell its story. In The Past is a Foreign Country, David Lowenthal observes that preserved objects also validate memories. While digital acquisition techniques can provide a technological solution that is able to acquire the shape and the appearance of artifacts with unprecedented precision in human history, the actuality of the object, as opposed to a reproduction, draws people in and gives them a literal way of touching the past. This poses a danger as places and things are damaged by the hands of tourists, the light required to display them, and other risks of making an object known and available. The reality of this risk reinforces the fact that all artifacts are in a constant state of chemical transformation so that what is considered to be preserved is actually changing – it is never as it once was. Similarly changing is the value each generation may place on the past and on the artifacts that link it to the past.\n\nThe equality or inseparability of cultural preservation and the protection of human life has been argued by several agencies and writers, for example, former French president François Hollande stated in 2016Our responsibility is to save lives and also to save the stones -- there is no choice to be made, because today both are destroyed.\n\nClassical civilizations, especially Indian, have attributed supreme importance to the preservation of tradition. Its central idea was that social institutions, scientific knowledge, and technological applications need to use a "heritage" as a "resource". Using contemporary language, we could say that ancient Indians considered, as social resources, both economic assets (like natural resources and their exploitation structure) and factors promoting social integration (like institutions for the preservation of knowledge and for the maintenance of civil order). Ethics considered that what had been inherited should not be consumed, but should be handed over, possibly enriched, to successive generations. This was a moral imperative for all, except in the final life stage of sannyasa.\nWhat one generation considers "cultural heritage" may be rejected by the next generation, only to be revived by a subsequent generation.\n\n\n== World heritage movement ==\n\nSignificant was the Convention Concerning the Protection of World Cultural and Natural Heritage that was adopted by the General Conference of UNESCO in 1972. As of 2011, there are 936 World Heritage Sites: 725 cultural, 183 natural, and 28 mixed properties, in 153 countries. Each of these sites is considered important to the international community.\nThe underwater cultural heritage is protected by the UNESCO Convention on the Protection of the Underwater Cultural Heritage. This convention is a legal instrument helping state parties to improve the protection of their underwater cultural heritage.\nIn addition, UNESCO has begun designating masterpieces of the Oral and Intangible Heritage of Humanity. The Committee on Economic, Social and Cultural Rights sitting as part of the United Nations Economic and Social Council with article 15 of its Covenant had sought to instill the principles under which cultural heritage is protected as part of a basic human right.\nKey international documents and bodies include:\n\nAthens Charter, 1931\nRoerich Pact, 1935\nHague Convention for the Protection of Cultural Property in the Event of Armed Conflict, 1954, (with a definition of cultural heritage item adopted by some national law)\nVenice Charter, 1964\nBarcelona Charter, 2002 (regarding maritime vessel preservation)\nICOMOS\nThe Blue Shield, a network of committees of dedicated individuals across the world that is "committed to the protection of the world\'s cultural property, and is concerned with the protection of cultural and natural heritage, tangible and intangible, in the event of armed conflict, natural- or human-made disaster."\nInternational Institute for Conservation\nThe U.S. Government Accountability Office issued a report describing some of the United States\' cultural property protection efforts.\n\n\n== National and regional heritage movements ==\n\nMuch of heritage preservation work is done at the national, regional, or local levels of society. Various national and regional regimes include:\n\nAustralia:\nBurra Charter\nHeritage Overlay in Victoria, Australia\nBosnia\nKONS\nBrazil:\nNational Institute of Historic and Artistic Heritage\nCanada\nHeritage conservation in Canada\nChile\nNational Monuments Council (Chile)\nChina\nState Administration of Cultural Heritage\nEgypt\nSupreme Council of Antiquities\nEstonia\nMinistry of Culture (Estonia)\nNational Heritage Board (Estonia)\nGhana\nGhana\'s material cultural heritage\nHonduras\nSecretary of State for Culture, Arts and Sports\nHong Kong\nHeritage conservation in Hong Kong\nIndia\nMinistry of Culture (India)\nNational Archives of India\nArchaeological Survey of India\nAnthropological Survey of India\nCulture of India\nIndian National Trust for Art and Cultural Heritage\nNational Museum Institute of the History of Art, Conservation and Museology\nList of World Heritage Sites in India\nIndian Heritage Cities Network, Mysore\nHeritage structures in Hyderabad\nIran\nCultural Heritage, Handcrafts and Tourism Organization\nJapan\nCultural Properties of Japan\nKenya\nNational Museums of Kenya\nInternational Inventories Programme\nNorth Macedonia\nInstitute for Protection of Cultural Monuments\nMalaysia\nThe National Heritage Act\nNamibia\nNational Heritage Council of Namibia\nNational Monuments Council\nNew Zealand\nNew Zealand Historic Places Trust\nPakistan\nLahore Museum of Art and Cultural History\nLok Virsa Heritage Museum\nNational Museum of Pakistan\nPakistan Monument and Heritage Museum\nPhilippines\nNational Commission for Culture and the Arts\nNational Historical Commission of the Philippines\nPoland\nNational Ossoliński Institute\nSerbia\n Immovable Cultural Heritage of Exceptional Importance\n Immovable Cultural Heritage of Great Importance\nSouth Africa\nSouth African Heritage Resources Agency\nProvincial heritage resources authorities\nAmafa aKwaZulu-Natali\nHeritage Western Cape\nNorthern Cape Heritage Resources Authority\nNational Monuments Council\nHistorical Monuments Commission\nUnited Kingdom\nConservation in the United Kingdom\nEnglish Heritage\nEnglish Heritage Archive\nNational Trust\nCadw\nNorthern Ireland Environment Agency\nHistoric Environment Scotland\nNational Trust for Scotland\nUnited States of America\nNational Park Service\nNational Register of Historic Places\nNational Historic Site (United States)\nList of national memorials of the United States\nNational Military Park\nZambia\nNational Heritage Conservation Commission\nNational Museums Board\n\nZimbabwe\nNational Monuments of Zimbabwe\n\n\n== Issues in cultural heritage ==\n\nBroad philosophical, technical, and political issues and dimensions of cultural heritage include:\n\nCultural heritage repatriation\nCultural heritage management\nCultural property law\nHeritage tourism\nVirtual heritage\nSustainable preservation\nClimate change and World Heritage\n\n\n== Management of cultural heritage ==\nIssues in cultural heritage management include:\n\nExhibition of cultural heritage objects\nRadiography of cultural objects\nStorage of cultural heritage objects\nCollections maintenance\nDisaster preparedness\n\n\n== Cultural heritage digital preservation ==\nAncient archaeological artefacts and archaeological sites are naturally prone to damage due to their age and environmental conditions. Also, there have been tragic occurrences of unexpected human-made disasters, such as in the cases of a fire that took place in the 200 years old National Museum of Brazil and the UNESCO World Heritage Site of the Notre Dame Cathedral in Paris.\nTherefore, there is a growing need to digitize cultural heritage in order to preserve them in the face of potential calamities such as climate change, natural disaster, poor policy or inadequate infrastructure. For example, the Library of Congress has started to digitize its collections in a special program called the National Digital Library Program. The Smithsonian has also been actively digitizing its collection with the release of the "Smithsonian X 3D Explorer," allowing anyone to engage with the digitized versions of the museum\'s millions of artifacts, of which only two percent are on display.\n3D scanning devices have become a practical reality in the field of heritage preservation. 3D scanners can produce a high-precision digital reference model that not only digitizes condition but also provides a 3D virtual model for replication. The high cost and relative complexity of 3D scanning technologies have made it quite impractical for many heritage institutions in the past, but this is changing, as technology advances and its relative costs are decreasing to reach a level where even mobile based scanning applications can be used to create a virtual museum.\nThere is still a low level of digital archiving of archaeological data obtained via excavation, even in the UK where the lead digital archive for archaeology, the Archaeology Data Service, was established in the 1990s. Across the globe, countries are at different stages of dealing with digital archaeological archives, all dealing with differences in statutory requirements, legal ownership of archives and infrastructure.\n\n\n== See also ==\nAntiquarian\nArchitectural Heritage\nCollecting\nHeritage film\nInternational Council on Monuments and Sites\nValues (heritage)\n\n\n=== Digital methods in preservation ===\nDigiCULT\nERPANET\nIntellectual property issues in cultural heritage (IPinCH)\nMICHAEL (webportal)\n\n\n== References ==\n\n\n== Further reading ==\nMichael Falser. Cultural Heritage as Civilizing Mission. From Decay to Recovery. Heidelberg, New York: Springer (2015), ISBN 978-3-319-13638-7.\nMichael Falser, Monica Juneja (eds.). \'Archaeologizing\' Heritage? Transcultural Entanglements between Local Social Practices and Global Virtual Realities. Heidelberg, New York: Springer (2013), ISBN 978-3-642-35870-8.\nFiankan-Bokonga, Catherine (17 October 2017). "A historic resolution to protect cultural heritage". UNESCO. Retrieved 3 August 2021.\nAnn Marie Sullivan, Cultural Heritage & New Media: A Future for the Past, 15 J. MARSHALL REV. INTELL. PROP. L. 604 (2016) https://repository.jmls.edu/cgi/viewcontent.cgi?article=1392&context=ripl\nBarbara T. Hoffman, Art and cultural heritage: law, policy, and practice, Cambridge University Press, 2006\nLeila A. Amineddoleh, "Protecting Cultural Heritage by Strictly Scrutinizing Museum Acquisitions," Fordham Intellectual Property, Media & Entertainment Law Journal, Vol. 24, No. 3. Available at: https://ssrn.com/abstract=2467100\nPaolo Davide Farah, Riccardo Tremolada, Desirability of Commodification of Intangible Cultural Heritage: The Unsatisfying Role of IPRs, in TRANSNATIONAL DISPUTE MANAGEMENT, Special Issues "The New Frontiers of Cultural Law: Intangible Heritage Disputes", Volume 11, Issue 2, March 2014, ISSN 1875-4120 Available at: https://ssrn.com/abstract=2472339\nPaolo Davide Farah, Riccardo Tremolada, Intellectual Property Rights, Human Rights and Intangible Cultural Heritage, Journal of Intellectual Property Law, Issue 2, Part I, June 2014, ISSN 0035-614X, Giuffrè, pp. 21–47. Available at: https://ssrn.com/abstract=2472388\nNora Lafi, Building and Destroying Authenticity in Aleppo: Heritage between Conservation, Transformation, Destruction, and Re-Invention in Christoph Bernhardt, Martin Sabrow, Achim Saupe. Gebaute Geschichte. Historische Authentizität im Stadtraum, Wallstein, pp.206-228, 2017\nDallen J. Timothy and Gyan P. Nyaupane, Cultural heritage and tourism in the developing world : a regional perspective, Taylor & Francis, 2009\nPeter Probst, "Osogbo and the Art of Heritage: Monuments, Deities, and Money", Indiana University Press, 2011\nConstantine Sandis (ed.), Cultural Heritage Ethics: Between Theory and Practice, Open Book Publishers, 2014\nZuckermann, Ghil\'ad et al., ENGAGING - A Guide to Interacting Respectfully and Reciprocally with Aboriginal and Torres Strait Islander People, and their Arts Practices and Intellectual Property, Australian Government: Indigenous Culture Support, 2015\nWalters, Diana; Laven, Daniel; Davis, Peter (2017). Heritage & Peacebuilding. Suffolk, UK: Boydell Press. ISBN 9781783272167. Archived from the original on 31 March 2017. Retrieved 31 March 2017.\nKocój E., Między mainstremem a undergroundem. Dziedzictwo regionalne w kulturze europejskiej – odkrywanie znaczeń, [w:] Dziedzictwo kulturowe w regionach europejskich. Odkrywanie, ochrona i (re)interpretacja, Seria wydawnicza:, Studia nad dziedzictwem i pamięcią kulturową", tom I, Kraków 2019, red. Ewa Kocój, Tomasz Kosiek, Joanna Szulborska-Łukaszewicz, pp. 10–35.\nDziedzictwo kulturowe w regionach europejskich. Odkrywanie, ochrona i (re)interpretacja, Seria wydawnicza:, Studia nad dziedzictwem i pamięcią kulturową", tom I, red. Ewa Kocój, Tomasz Kosiek, Joanna Szulborska-Łukaszewicz, Kraków 2019, p. 300.\nHudson-Ward, A., Widholm, J. R., & Scott, W. (Eds.). (2023). Cultural Heritage and the Campus Community: Academic Libraries and Museums in Collaboration. ACRL.\n\n\n== External links ==\n\nCultural heritage policy - history and resources Getty Museum - list of major international cultural heritage documents, charters, and treaties\nUNESCO World Heritage Centre – Official website of the United Nations organization for cultural heritage\nInternational Council on Monuments and Sites\nInternational Council of Museums\nInternational Centre for the Study of the Preservation and Restoration of Cultural Property\nCultural routes and landscapes, a common heritage of Europe (English and French language)\nEPOCH – European Research Network on Excellence in Processing Open Cultural Heritage\nPeace Palace Library - Research Guide Archived 12 September 2016 at the Wayback Machine\nNational Council for Preservation Education\nDédalo Open source management system for Cultural heritage\n Cultural heritage travel guide from Wikivoyage\nCentral European University (CEU)\nUNESCO UIS_cultural heritage\nHeritage for Peace',
  },
  {
    title: "World Heritage Sites",
    originalContent:
      'World Heritage Sites are landmarks and areas with legal protection under an international treaty administered by UNESCO for having cultural, historical, or scientific significance. The sites are judged to contain "cultural and natural heritage around the world considered to be of outstanding value to humanity".\nTo be selected, a World Heritage Site is nominated by its host country and determined by the UNESCO\'s World Heritage Committee to be a unique landmark which is geographically and historically identifiable, having a special cultural or physical significance, and to be under a sufficient system of legal protection. For example, World Heritage Sites might be ancient ruins or historical structures, buildings, cities, deserts, forests, islands, lakes, monuments, mountains or wilderness areas. \nA World Heritage Site may signify a remarkable accomplishment of humankind and serve as evidence of our intellectual history on the planet, or it might be a place of great natural beauty. As of July 2024, a total of 1,223 World Heritage Sites (952 cultural, 231 natural and 40 mixed cultural and natural properties) exist across 168 countries. With 60 selected areas, Italy is the country with the most sites, followed by China with 59, and Germany with 54.\nThe sites are intended for practical conservation for posterity, which otherwise would be subject to risk from human or animal trespassing, unmonitored, uncontrolled or unrestricted access, or threat from local administrative negligence. Sites are demarcated by UNESCO as protected zones. The World Heritage Sites list is maintained by the international World Heritage Program administered by the UNESCO World Heritage Committee, composed of 21 "states parties" that are elected by the United Nations General Assembly, and advised by reviews of international panels of experts in natural or cultural history, and education. \nThe Program catalogues, names, and conserves sites of outstanding cultural or natural importance to the common culture and heritage of humankind. The programme began with the Convention Concerning the Protection of the World Cultural and Natural Heritage, which was adopted by the General Conference of UNESCO on 16 November 1972. Since then, 196 states have ratified the convention, making it one of the most widely recognised international agreements and the world\'s most popular cultural programme.\n\n\n== History ==\n\n\n=== Origin ===\n\nIn 1954, the government of Egypt decided to build the new Aswan High Dam, whose resulting future reservoir would eventually inundate a large stretch of the Nile valley containing cultural treasures of ancient Egypt and ancient Nubia. In 1959, the governments of Egypt and Sudan requested the United Nations Educational, Scientific and Cultural Organization (UNESCO) to assist them to protect and rescue the endangered monuments and sites. \n\nIn 1960, the Director-General of UNESCO launched the International Campaign to Save the Monuments of Nubia. This resulted in the excavation and recording of hundreds of sites, the recovery of thousands of objects, as well as the salvage and relocation to higher ground of several important temples. The most famous of these are the temple complexes of Abu Simbel and Philae. The campaign ended in 1980 and was considered a success. To thank countries which especially contributed to the campaign\'s success, Egypt donated four temples; the Temple of Dendur was moved to the Metropolitan Museum of Art in New York City, the Temple of Debod to the Parque del Oeste in Madrid, the Temple of Taffeh to the Rijksmuseum van Oudheden in Leiden, and the Temple of Ellesyia to Museo Egizio in Turin.The project cost US$80 million (equivalent to $295.83 million in 2023), about $40 million of which was collected from 50 countries. The project\'s success led to other safeguarding campaigns, such as saving Venice and its lagoon in Italy, the ruins of Mohenjo-daro in Pakistan, and the Borobodur Temple Compounds in Indonesia. Together with the International Council on Monuments and Sites, UNESCO then initiated a draft convention to protect cultural heritage.\n\n\n=== Convention and background ===\n\nThe convention (the signed document of international agreement) guiding the work of the World Heritage Committee was developed over a seven-year period (1965–1972).\nThe United States initiated the idea of safeguarding places of high cultural or natural importance. A White House conference in 1965 called for a "World Heritage Trust" to preserve "the world\'s superb natural and scenic areas and historic sites for the present and the future of the entire world citizenry". The International Union for Conservation of Nature developed similar proposals in 1968, which were presented in 1972 at the United Nations Conference on the Human Environment in Stockholm. Under the World Heritage Committee, signatory countries are required to produce and submit periodic data reporting providing the committee with an overview of each participating nation\'s implementation of the World Heritage Convention and a "snapshot" of current conditions at World Heritage properties.\nBased on the draft convention that UNESCO had initiated, a single text was eventually agreed upon by all parties, and the Convention Concerning the Protection of the World Cultural and Natural Heritage was adopted by the General Conference of UNESCO on 16 November 1972. The convention came into force on 17 December 1975. As of November 2024, it has been ratified by 196 states: 192 UN member states, two UN observer states (the Holy See and the State of Palestine), and two states in free association with New Zealand (the Cook Islands and Niue). Only one UN member state, Liechtenstein, has not ratified the convention.\n\n\n== Objectives ==\nBy assigning places as World Heritage Sites, UNESCO wants to help preserve them for future generations. Its motivation is that "heritage is our legacy from the past, what we live with today" and that both cultural and natural heritage are "irreplaceable sources of life and inspiration". UNESCO\'s mission with respect to World Heritage consists of eight sub targets. These include encouraging the commitment of countries and local population to World Heritage conservation in various ways, providing emergency assistance for sites in danger, offering technical assistance and professional training, and supporting States Parties\' public awareness-building activities.\nBeing listed as a World Heritage Site can positively affect the site, its environment, and interactions between them. A listed site gains international recognition and legal protection, and can obtain funds from, among others, the World Heritage Fund to facilitate its conservation under certain conditions. UNESCO reckons the restorations of the following four sites among its success stories: Angkor in Cambodia, the Old City of Dubrovnik in Croatia, the Wieliczka Salt Mine near Kraków in Poland, and the Ngorongoro Conservation Area in Tanzania. Additionally, the local population around a site may benefit from significantly increased tourism revenue. When there are significant interactions between people and the natural environment, these can be recognised as "cultural landscapes".\n\n\n== Nomination process ==\nA country must first identify its significant cultural and natural sites in a document known as the Tentative List. Next, it can place sites selected from that list into a Nomination File, which is evaluated by the International Council on Monuments and Sites and the World Conservation Union. A country may not nominate sites that have not been first included on its Tentative List. The two international bodies make recommendations to the World Heritage Committee for new designations. The Committee meets once a year to determine which nominated properties to add to the World Heritage List; sometimes it defers its decision or requests more information from the country that nominated the site. There are ten selection criteria – a site must meet at least one to be included on the list.\n\n\n== Selection criteria ==\nUntil 2004, there were six sets of criteria for cultural heritage and four for natural heritage. In 2005, UNESCO modified these and now has one set of ten criteria. Nominated sites must be of "outstanding universal value" and must meet at least one of the ten criteria. \n\n\n=== Cultural ===\n\n"To represent a masterpiece of human creative genius"\n"To exhibit an important interchange of human values, over a span of time or within a cultural area of the world, on developments in architecture or technology, monumental arts, town-planning or landscape design"\n"To bear a unique or at least exceptional testimony to a cultural tradition or to a civilization which is living, or which has disappeared"\n"To be an outstanding example of a type of building, architectural or technological ensemble or landscape which illustrates (a) significant stage(s) in human history"\n"To be an outstanding example of a traditional human settlement, land-use, or sea-use which is representative of a culture (or cultures), or human interaction with the environment especially when it has become vulnerable under the impact of irreversible change"\n"To be directly or tangibly associated with events or living traditions, with ideas, or with beliefs, with artistic and literary works of outstanding universal significance"\n\n\n=== Natural ===\n\n"To contain superlative natural phenomena or areas of exceptional natural beauty and aesthetic importance"\n"To be outstanding examples representing major stages of earth\'s history, including the record of life, significant on-going geological processes in the development of landforms, or significant geomorphic or physiographic features"\n"To be outstanding examples representing significant on-going ecological and biological processes in the evolution and development of terrestrial, fresh water, coastal and marine ecosystems and communities of plants and animals"\n"To contain the most important and significant natural habitats for in-situ conservation of biological diversity, including those containing threatened species of outstanding universal value from the point of view of science or conservation"\n\n\n== Extensions and other modifications ==\nA country may request to extend or reduce the boundaries, modify the official name, or change the selection criteria of one of its already listed sites. Any proposal for a significant boundary change or to modify the site\'s selection criteria must be submitted as if it were a new nomination, including first placing it on the Tentative List and then onto the Nomination File. \nA request for a minor boundary change, one that does not have a significant impact on the extent of the property or affect its "outstanding universal value", is also evaluated by the advisory bodies before being sent to the committee. Such proposals can be rejected by either the advisory bodies or the Committee if they judge it to be a significant change instead of a minor one. Proposals to change a site\'s official name are sent directly to the committee.\n\n\n== Endangerment ==\n\nA site may be added to the List of World Heritage in Danger if conditions threaten the characteristics for which the landmark or area was inscribed on the World Heritage List. Such problems may involve armed conflict and war, natural disasters, pollution, poaching, or uncontrolled urbanisation or human development. This danger list is intended to increase international awareness of the threats and to encourage counteractive measures. Threats to a site can be either proven imminent threats or potential dangers that could have adverse effects on a site.\nThe state of conservation for each site on the danger list is reviewed yearly; after this, the Committee may request additional measures, delete the property from the list if the threats have ceased or consider deletion from both the List of World Heritage in Danger and the World Heritage List. Only three sites have ever been delisted: the Arabian Oryx Sanctuary in Oman, the Dresden Elbe Valley in Germany, and the Liverpool Maritime Mercantile City in the United Kingdom.\nThe Arabian Oryx Sanctuary was directly delisted in 2007, instead of first being put on the danger list, after the Omani government decided to reduce the protected area\'s size by 90%. The Dresden Elbe Valley was first placed on the danger list in 2006 when the World Heritage Committee decided that plans to construct the Waldschlösschen Bridge would significantly alter the valley\'s landscape. In response, the Dresden City Council attempted to stop the bridge\'s construction. However, after several court decisions allowed the building of the bridge to proceed, the valley was removed from the World Heritage List in 2009. Liverpool\'s World Heritage status was revoked in July 2021, following developments (Liverpool Waters and Bramley-Moore Dock Stadium) on the northern docks of the World Heritage site leading to the "irreversible loss of attributes" on the site.\nThe first global assessment to quantitatively measure threats to Natural World Heritage Sites found that 63% of sites have been damaged by increasing human pressures including encroaching roads, agriculture infrastructure and settlements over the last two decades. These activities endanger Natural World Heritage Sites and could compromise their unique values. Of the Natural World Heritage Sites that contain forest, 91% experienced some loss since 2000. Many of them are more threatened than previously thought and require immediate conservation action.\nThe destruction of cultural assets and identity-establishing sites is one of the primary goals of modern asymmetrical warfare. Terrorists, rebels, and mercenary armies deliberately smash archaeological sites, sacred and secular monuments and loot libraries, archives and museums. The UN, United Nations peacekeeping and UNESCO in cooperation with Blue Shield International are active in preventing such acts. "No strike lists" are also created to protect cultural assets from air strikes.\nThe founding president of Blue Shield International Karl von Habsburg summed it up with the words: "Without the local community and without the local participants, that would be completely impossible".\n\n\n== Criticism ==\nThe UNESCO-administered project has attracted criticism. This was caused by perceived under-representation of heritage sites outside Europe, disputed decisions on site selection and adverse impact of mass tourism on sites unable to manage rapid growth in visitor numbers. A large lobbying industry has grown around the awards, because World Heritage listing can significantly increase tourism returns. Site listing bids are often lengthy and costly, putting poorer countries at a disadvantage. Eritrea\'s efforts to promote Asmara are one example.\nIn 2016, the Australian government was reported to have successfully lobbied for the World Heritage Site Great Barrier Reef conservation efforts to be removed from a UNESCO report titled "World Heritage and Tourism in a Changing Climate". The Australian government\'s actions, involving considerable expense for lobbying and visits for diplomats, were in response to their concern about the negative impact that an "at risk" label could have on tourism revenue at a previously designated UNESCO World Heritage Site. \nIn 2021, international scientists recommended UNESCO to put the Great Barrier Reef on the endangered list, as global climate change had caused a further negative state of the corals and water quality. Again, the Australian government campaigned against this, and in July 2021, the World Heritage Committee, made up of diplomatic representatives of 21 countries, ignored UNESCO\'s assessment, based on studies of scientists, "that the reef was clearly in danger from climate change and so should be placed on the list." According to environmental protection groups, this "decision was a victory for cynical lobbying and [...] Australia, as custodians of the world\'s biggest coral reef, was now on probation."\nSeveral listed locations, such as Casco Viejo in Panama and Hội An in Vietnam, have struggled to strike a balance between the economic benefits of catering to greatly increased visitor numbers after the recognition and preserving the original culture and local communities.\nAnother criticism is that there is a homogeneity to these sites, which contain similar styles, visitor centres, etc., meaning that a lot of the individuality of these sites has been removed to become more attractive to tourists.\nAnthropologist Jasper Chalcraft said that World Heritage recognition often ignores contemporary local usage of certain sites. This leads to conflicts on the local level which can result in the site being damaged. Rock art under world heritage protection at the Tadrart Acacus in Libya have occasionally been intentionally destroyed. Chalcraft links this destruction to Libyan national authorities prioritizing World Heritage status over local sensibilities by limiting access to the sites without consulting with the local population.\nUNESCO has also been criticized for alleged geographic bias, racism, and colourism in world heritage inscription. A major chunk of all world heritage inscriptions are located in regions whose populations generally have lighter skin, including Europe, East Asia, and North America.\n\n\n== Statistics ==\n\nThe World Heritage Committee has divided the world into five geographic regions: Africa, Arab states, Asia and the Pacific, Europe and North America, and Latin America and the Caribbean. Russia and the Caucasus states are classified as European, while Mexico and the Caribbean are classified as belonging to the Latin America and the Caribbean region. The UNESCO geographic regions also give greater emphasis on administrative, rather than geographic associations. Hence, Gough Island, located in the South Atlantic, is part of the Europe and North America region because the British government nominated the site.\nThe table below includes a breakdown of the sites according to these regions and their classification as of July 2024:\n\n\n=== Countries with 15 or more sites ===\n\nThis overview lists the 23 countries with 15 or more World Heritage Sites:\n\n\n== See also ==\n\nGoUNESCO – initiative to promote awareness and provide tools for laypersons to engage with heritage\nIndex of conservation articles\nLists of World Heritage Sites\nFormer UNESCO World Heritage Sites\nMemory of the World Programme\nUNESCO Intangible Cultural Heritage Lists\nRamsar Convention – international agreement on wetlands recognition\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Footnotes ===\n\n\n=== Bibliography ===\n\n\n== External links ==\n\nUNESCO World Heritage portal – Official website (in English and French)\nThe World Heritage List – Official searchable list of all Inscribed Properties\nKML file of the World Heritage List – Official KML version of the list for Google Earth and NASA Worldwind\nUNESCO Information System on the State of Conservation of World Heritage properties – Searchable online tool with over 3,400 reports on World Heritage Sites\nOfficial overview of the World Heritage Forest Program\nConvention Concerning the Protection of the World Cultural and Natural Heritage – Official 1972 Convention Text in seven languages\nThe 1972 Convention at Law-Ref.org – Fully indexed and crosslinked with other documents\nProtected Planet – View all Natural World Heritage Sites in the World Database on Protected Areas\nWorld Heritage Site – Smithsonian Ocean Portal\nUNESCO chair in ICT to develop and promote sustainable tourism in World Heritage Sites\nUNESCO World Heritage Sites showcased in Google Arts & Culture',
  },
  {
    title: "Roman mythology",
    originalContent:
      'Roman mythology is the body of myths of ancient Rome as represented in the literature and visual arts of the Romans, and is a form of Roman folklore. "Roman mythology"  may also refer to the modern study of these representations, and to the subject matter as represented in the literature and art of other cultures in any period. Roman mythology draws from the mythology of the Italic peoples and shares mythemes with Proto-Indo-European mythology.\nThe Romans usually treated their traditional narratives as historical, even when these have miraculous or supernatural elements. The stories are often concerned with politics and morality, and how an individual\'s personal integrity relates to his or her responsibility to the community or Roman state. Heroism is an important theme. When the stories illuminate Roman religious practices, they are more concerned with ritual, augury, and institutions than with theology or cosmogony.\nRoman mythology also draws on Greek mythology, primarily during the Hellenistic period of Greek influence and through the Roman conquest of Greece, via the artistic imitation of Greek literary models by Roman authors. The Romans identified their own gods with those of the ancient Greeks and reinterpreted myths about Greek deities under the names of their Roman counterparts. The influence of Greek mythology likely began as early as Rome\'s protohistory.\nClassical mythology is the amalgamated tradition of Greek and Roman mythologies, as disseminated especially by Latin literature in Europe throughout the Middle Ages, into the Renaissance, and up to present-day uses of myths in fiction and movies. The interpretations of Greek myths by the Romans often had a greater influence on narrative and pictorial representations of myths than Greek sources. In particular, the versions of Greek myths in Ovid\'s Metamorphoses, written during the reign of Augustus, came to be regarded as canonical.\n\n\n== Nature of Roman myth ==\n\nBecause ritual played the central role in Roman religion that myth did for the Greeks, it is sometimes doubted that the Romans had much of a native mythology. This perception is a product of Romanticism and the classical scholarship of the 19th century, which valued Greek civilization as more "authentically creative." From the Renaissance to the 18th century, however, Roman myths were an inspiration particularly for European painting. The Roman tradition is rich in historical myths, or legends, concerning the foundation and rise of the city. These narratives focus on human actors, with only occasional intervention from deities but a pervasive sense of divinely ordered destiny. In Rome\'s earliest period, history and myth have a mutual and complementary relationship. As T. P. Wiseman notes:\n\nThe Roman stories still matter, as they mattered to Dante in 1300 and Shakespeare in 1600 and the founding fathers of the United States in 1776. What does it take to be a free citizen? Can a superpower still be a republic? How does well-meaning authority turn into murderous tyranny?\n\nMajor sources for Roman myth include the Aeneid of Virgil and the first few books of Livy\'s history as well as Dionysius\'s Roman Antiquities. Other important sources are the Fasti of Ovid, a six-book poem structured by the Roman religious calendar, and the fourth book of elegies by Propertius. Scenes from Roman myth also appear in Roman wall painting, coins, and sculpture, particularly reliefs.\n\n\n=== Founding myths ===\n\nThe Aeneid and Livy\'s early history are the best extant sources for Rome\'s founding myths. Material from Greek heroic legend was grafted onto this native stock at an early date.  The Trojan prince Aeneas was cast as husband of Lavinia, daughter of King Latinus, patronymical ancestor of the Latini, and therefore through a convoluted revisionist genealogy as forebear of Romulus and Remus.  By extension, the Trojans were adopted as the mythical ancestors of the Roman people.\n\n\n=== Other myths ===\n\nThe characteristic myths of Rome are often political or moral, that is, they deal with the development of Roman government in accordance with divine law, as expressed by Roman religion, and with demonstrations of the individual\'s adherence to moral expectations (mos maiorum) or failures to do so.\n\nRape of the Sabine women, explaining the importance of the Sabines in the formation of Roman culture, and the growth of Rome through conflict and alliance.\nNuma Pompilius, the Sabine second king of Rome who consorted with the nymph Egeria and established many of Rome\'s legal and religious institutions.\nServius Tullius, the sixth king of Rome, whose mysterious origins were freely mythologized and who was said to have been the lover of the goddess Fortuna.\nThe Tarpeian Rock, and why it was used for the execution of traitors.\nLucretia, whose self-sacrifice prompted the overthrow of the early Roman monarchy and led to the establishment of the Republic.\nCloelia, a Roman woman taken hostage by Lars Porsena, who escaped but after negotiations returned voluntarily to save others and preserve the peace treaty.\nHoratius at the bridge, on the importance of individual valor.\nMucius Scaevola, who thrust his right hand into the fire to prove his loyalty to Rome.\nCaeculus and the founding of Praeneste.\nManlius and the geese, about divine intervention at the Gallic siege of Rome.\nStories pertaining to the Nonae Caprotinae and Poplifugia festivals.\nCoriolanus, a story of politics and morality.\nThe Etruscan city of Corythus as the "cradle" of Trojan and Italian civilization.\nThe arrival of the Great Mother (Cybele) in Rome.\n\n\n== Religion and myth ==\n\nNarratives of divine activity played a more important role in the system of Greek religious belief than among the Romans, for whom ritual and cultus were primary. Although Roman religion was not based on scriptures and their exegesis, priestly literature was one of the earliest written forms of Latin prose. The books (libri) and commentaries (commentarii) of the College of Pontiffs and of the augurs contained religious procedures, prayers, and rulings and opinions on points of religious law. Although at least some of this archived material was available for consultation by the Roman senate, it was often occultum genus litterarum, an arcane form of literature to which by definition only priests had access. Prophecies pertaining to world history and to Rome\'s destiny turn up fortuitously at critical junctures in history, discovered suddenly in the nebulous Sibylline books, which Tarquin the Proud (according to legend) purchased in the late 6th century BC from the Cumaean Sibyl. Some aspects of archaic Roman religion survived in the lost theological works of the 1st-century BC scholar Varro, known through other classical and Christian authors.\n\nAlthough traditional Roman religion was conservative in ritual rather than dogmatic in doctrine, the meaning of the rituals they perpetuated could be adapted, expanded, and reinterpreted by accretions of myths, etiologies, commentary, and the influences of other cultures in response to social change. The earliest pantheon included Janus, Vesta, and the so-called Archaic Triad of Jupiter, Mars, and Quirinus, whose three patrician flamens were of the highest order. According to tradition, Numa Pompilius, the Sabine second king of Rome, founded Roman religion; Numa was believed to have had as his consort and adviser a Roman goddess or nymph of fountains and of prophecy, Egeria. The Etruscan-influenced Capitoline Triad of Jupiter, Juno and Minerva later became central to official religion, replacing the Archaic Triad – an unusual example within Indo-European religion of a supreme triad formed of two female deities and only one male. The cult of Diana became established on the Aventine Hill, but the most famous Roman manifestation of this goddess may be Diana Nemorensis, owing to the attention paid to her cult by J.G. Frazer in the mythographic classic The Golden Bough. What modern scholars call the Aventine Triad – Ceres, Liber, and Libera – developed in association with the rise of plebeians to positions of wealth and influence.\n\nThe gods represented distinctly the practical needs of daily life, and the Romans scrupulously accorded them the appropriate rites and offerings. Early Roman divinities included a host of "specialist gods" whose names were invoked in the carrying out of various specific activities. Fragments of old ritual accompanying such acts as plowing or sowing reveal that at every stage of the operation a separate deity was invoked, the name of each deity being regularly derived from the verb for the operation. Tutelary deities were particularly important in ancient Rome.\nThus, Janus and Vesta guarded the door and hearth, the Lares protected the field and house, Pales the pasture, Saturn the sowing, Ceres the growth of the grain, Pomona the fruit, and Consus and Ops the harvest. Jupiter, the ruler of the gods, was honored for the aid his rains might give to the farms and vineyards. In his more encompassing character he was considered, through his weapon of lightning, the director of human activity. Owing to his widespread domain, the Romans regarded him as their protector in their military activities beyond the borders of their own community. Prominent in early times were the gods Mars and Quirinus, who were often identified with each other. Mars was a god of both war and agriculture; he was honored in March and October. Quirinus was the patron of the armed community in time of peace.\nThe 19th-century scholar Georg Wissowa thought that the Romans distinguished two classes of gods, the di indigetes and the di novensides or novensiles:  the indigetes were the original gods of the Roman state, their names and nature indicated by the titles of the earliest priests and by the fixed festivals of the calendar, with 30 such gods honored by special festivals; the novensides were later divinities whose cults were introduced to the city in the historical period, usually at a known date and in response to a specific crisis or felt need. Arnaldo Momigliano and others, however, have argued that this distinction cannot be maintained. During the war with Hannibal, any distinction between "indigenous" and "immigrant" gods begins to fade, and the Romans embraced diverse gods from various cultures as a sign of strength and universal divine favor.\n\n\n=== Foreign gods ===\n\nThe absorption of neighboring local gods took place as the Roman state conquered neighboring territories. The Romans commonly granted the local gods of a conquered territory the same honors as the earlier gods of the Roman state religion. In addition to Castor and Pollux, the conquered settlements in Italy seem to have contributed to the Roman pantheon Diana, Minerva, Hercules, Venus, and deities of lesser rank, some of whom were Italic divinities, others originally derived from the Greek culture of Magna Graecia. In 203 BC, Rome imported the cult object embodying Cybele from Pessinus in Phrygia and welcomed its arrival with due ceremony. Both Lucretius and Catullus, poets contemporary in the mid-1st century BC, offer disapproving glimpses of Cybele\'s wildly ecstatic cult.\nIn some instances, deities of an enemy power were formally invited through the ritual of evocatio to take up their abode in new sanctuaries at Rome.\nCommunities of foreigners (peregrini) and former slaves (libertini) continued their own religious practices within the city. In this way Mithras came to Rome and his popularity within the Roman army spread his cult as far afield as Roman Britain. The important Roman deities were eventually identified with the more anthropomorphic Greek gods and goddesses, and assumed many of their attributes and myths.\n\n\n== Astronomy ==\n\nMany astronomical objects are named after Roman deities, like the planets Mercury, Venus, Mars, Jupiter, Saturn, and Neptune.\nIn Roman and Greek mythology, Jupiter places his son born by a mortal woman, the infant Hercules, on Juno\'s breast while she is asleep so the baby will drink her divine milk and thus become immortal, an act which would endow the baby with godlike qualities. When Juno woke and realized that she was breastfeeding an unknown infant, she pushed him away, some of her milk spills, and the spurting milk became the Milky Way. In another version of the myth, the abandoned Hercules is given by Minerva to Juno for feeding, but Hercules\' forcefulness causes Minerva to rip him from her breast in pain. The milk that squirts out forms the Milky Way.\n\n\n== See also ==\n\nList of Ovid\'s Metamorphoses characters\nList of Roman deities\nRoman Polytheistic Reconstructionism\nPillar of Yzeures-sur-Creuse\n\n\n== References ==\n\n\n== Sources ==\nBeard, Mary. 1993. "Looking (Harder) for Roman Myth: Dumézil, Declamation, and the Problems of Definition." In Mythos in Mythenloser Gesellschaft: Das Paradigma Roms. Edited by Fritz Graf, 44–64. Stuttgart, Germany: Teubner.\nBraund, David, and Christopher Gill, eds. 2003. Myth, History, and Culture in Republican Rome: Studies in Honour of T. P. Wiseman. Exeter, UK: Univ. of Exeter Press.\nCameron, Alan. 2004. Greek Mythography in the Roman World. Oxford: Oxford Univ. Press.\nDumézil, Georges. 1996. Archaic Roman Religion. Rev. ed. Translated by Philip Krapp. Baltimore: Johns Hopkins Univ. Press.\nFox, Matthew. 2011. "The Myth of Rome" In A Companion to Greek Mythology. Blackwell Companions to the Ancient World. Literature and Culture.Edited by Ken Dowden and Niall Livingstone.   Chichester; Malden, MA:  Wiley-Blackwell.\nGardner, Jane F. 1993. Roman Myths: The Legendary Past. Austin: Univ. of Texas Press.\nGrandazzi, Alexandre. 1997. The Foundation of Rome: Myth and History. Translated by Jane Marie Todd. Ithaca, NY: Cornell Univ. Press.\nHall, Edith 2013. "Pantomime: Visualising Myth in the Roman Empire." In Performance in Greek and Roman Theatre. Edited by George Harrison and George William Mallory, 451–743. Leiden; Boston: Brill.\nMiller, Paul Allen. 2013. "Mythology and the Abject in Imperial Satire." In Classical Myth and Psychoanalysis: Ancient and Modern Stories of the Self. Edited by Vanda Zajko and Ellen O\'Gorman, 213–230. Oxford; New York: Oxford University Press.\nNewby, Zahra. 2012. "The Aesthetics of Violence: Myth and Danger in Roman Domestic Landscapes." Classical Antiquity 31.2: 349–389.\nWiseman, T. P. 2004. The Myths of Rome. Exeter: Univ. of Exeter Press.\nWoodard, Roger D. 2013. Myth, Ritual, and the Warrior in Roman and Indo-European Antiquity.  Cambridge; New York:  Cambridge University Press.\n\n\n== External links ==\n\nLexicon Iconographicum Mythologiae Classicae (LIMC) (1981–1999, Artemis-Verlag, 9 volumes), Supplementum (2009, Artemis_Verlag).\nLIMC-France (LIMC): Databases Dedicated to Graeco-Roman Mythology and its Iconography.',
  },
  {
    title: "Norse mythology",
    originalContent:
      "Norse, Nordic, or Scandinavian mythology, is the body of myths belonging to the North Germanic peoples, stemming from Old Norse religion and continuing after the Christianization of Scandinavia as the Nordic folklore of the modern period. The northernmost extension of Germanic mythology and stemming from Proto-Germanic folklore, Norse mythology consists of tales of various deities, beings, and heroes derived from numerous sources from both before and after the pagan period, including medieval manuscripts, archaeological representations, and folk tradition. The source texts mention numerous gods such as the thunder-god Thor, the raven-flanked god Odin, the goddess Freyja, and numerous other deities.\n\nMost of the surviving mythology centers on the plights of the gods and their interaction with several other beings, such as humanity and the jötnar, beings who may be friends, lovers, foes, or family members of the gods. The cosmos in Norse mythology consists of Nine Worlds that flank a central sacred tree, Yggdrasil. Units of time and elements of the cosmology are personified as deities or beings. Various forms of a creation myth are recounted, where the world is created from the flesh of the primordial being Ymir, and the first two humans are Ask and Embla. These worlds are foretold to be reborn after the events of Ragnarök when an immense battle occurs between the gods and their enemies, and the world is enveloped in flames, only to be reborn anew. There the surviving gods will meet, and the land will be fertile and green, and two humans will repopulate the world.\nNorse mythology has been the subject of scholarly discourse since the 17th century when key texts attracted the attention of the intellectual circles of Europe. By way of comparative mythology and historical linguistics, scholars have identified elements of Germanic mythology reaching as far back as Proto-Indo-European mythology. During the modern period, the Romanticist Viking revival re-awoke an interest in the subject matter, and references to Norse mythology may now be found throughout modern popular culture. The myths have further been revived in a religious context among adherents of Germanic Neopaganism.\n\n\n== Terminology ==\nThe historical religion of the Norse people is commonly referred to as Norse mythology. Other terms are Scandinavian mythology, North Germanic mythology or Nordic mythology.\n\n\n== Sources ==\n\nNorse mythology is primarily attested in dialects of Old Norse, a North Germanic language spoken by the Scandinavian people during the European Middle Ages and the ancestor of modern Scandinavian languages. The majority of these Old Norse texts were created in Iceland, where the oral tradition stemming from the pre-Christian inhabitants of the island was collected and recorded in manuscripts. This occurred primarily in the 13th century. These texts include the Prose Edda, composed in the 13th century by the Icelandic scholar, lawspeaker, and historian Snorri Sturluson, and the Poetic Edda, a collection of poems from earlier traditional material anonymously compiled in the 13th century.\nThe Prose Edda was composed as a prose manual for producing skaldic poetry—traditional Old Norse poetry composed by skalds. Originally composed and transmitted orally, skaldic poetry utilizes alliterative verse, kennings, and several metrical forms. The Prose Edda presents numerous examples of works by various skalds from before and after the Christianization process and also frequently refers back to the poems found in the Poetic Edda. The Poetic Edda consists almost entirely of poems, with some prose narrative added, and this poetry—Eddic poetry—utilizes fewer kennings. In comparison to skaldic poetry, Eddic poetry is relatively unadorned.\n\nThe Prose Edda features layers of euhemerization, a process in which deities and supernatural beings are presented as having been either actual, magic-wielding human beings who have been deified in time or beings demonized by way of Christian mythology. Texts such as Heimskringla, composed in the 13th century by Snorri and Gesta Danorum, composed in Latin by Saxo Grammaticus in Denmark in the 12th century, are the results of heavy amounts of euhemerization.\nNumerous additional texts, such as the sagas, provide further information. The saga corpus consists of thousands of tales recorded in Old Norse ranging from Icelandic family histories (Sagas of Icelanders) to Migration period tales mentioning historic figures such as Attila the Hun (legendary sagas). Objects and monuments such as the Rök runestone and the Kvinneby amulet feature runic inscriptions—texts written in the runic alphabet, the indigenous alphabet of the Germanic peoples—that mention figures and events from Norse mythology.\nObjects from the archaeological record may also be interpreted as depictions of subjects from Norse mythology, such as amulets of the god Thor's hammer Mjölnir found among pagan burials and small silver female figures interpreted as valkyries or dísir, beings associated with war, fate or ancestor cults. By way of historical linguistics and comparative mythology, comparisons to other attested branches of Germanic mythology (such as the Old High German Merseburg Incantations) may also lend insight. Wider comparisons to the mythology of other Indo-European peoples by scholars has resulted in the potential reconstruction of far earlier myths.\nOnly a tiny amount of poems and tales survive of the many mythical tales and poems that are presumed to have existed during the Middle Ages, Viking Age, Migration Period, and before. Later sources reaching into the modern period, such as a medieval charm recorded as used by the Norwegian woman Ragnhild Tregagås—convicted of witchcraft in Norway in the 14th century—and spells found in the 17th century Icelandic Galdrabók grimoire also sometimes make references to Norse mythology. Other traces, such as place names bearing the names of gods may provide further information about deities, such as a potential association between deities based on the placement of locations bearing their names, their local popularity, and associations with geological features.\n\n\n== Mythology ==\n\n\n=== Gods and other beings ===\n\nCentral to accounts of Norse mythology are the plights of the gods and their interaction with various other beings, such as with the jötnar, who may be friends, lovers, foes, or family members of the gods. Numerous gods are mentioned in the source texts. As evidenced by records of personal names and place names, the most popular god among the Scandinavians during the Viking Age was Thor the thunder god, who is portrayed as unrelentingly pursuing his foes, his mountain-crushing, thunderous hammer Mjölnir in hand. In the mythology, Thor lays waste to numerous jötnar who are foes to the gods or humanity, and is wed to the beautiful, golden-haired goddess Sif.\nThe god Odin is also frequently mentioned in surviving texts. One-eyed, wolf- and raven-flanked, with a spear in hand, Odin pursues knowledge throughout the nine realms. In an act of self-sacrifice, Odin is described as having hanged himself upside-down for nine days and nights on the cosmological tree Yggdrasil to gain knowledge of the runic alphabet, which he passed on to humanity, and is associated closely with death, wisdom, and poetry. Odin is portrayed as the ruler of Asgard, and leader of the Aesir. Odin's wife is the powerful goddess Frigg who can see the future but tells no one, and together they have a beloved son, Baldr. After a series of dreams had by Baldr of his impending death, his death is engineered by Loki, and Baldr thereafter resides in Hel, a realm ruled over by an entity of the same name.\nOdin must share half of his share of the dead with a powerful goddess, Freyja. She is beautiful, sensual, wears a feathered cloak, and practices seiðr. She rides to battle to choose among the slain and brings her chosen to her afterlife field Fólkvangr. Freyja weeps for her missing husband Óðr and seeks after him in faraway lands. Freyja's brother, the god Freyr, is also frequently mentioned in surviving texts, and in his association with the weather, royalty, human sexuality, and agriculture brings peace and pleasure to humanity. Deeply lovesick after catching sight of the beautiful jötunn Gerðr, Freyr seeks and wins her love, yet at the price of his future doom. Their father is the powerful god Njörðr. Njörðr is strongly associated with ships and seafaring, and so also wealth and prosperity. Freyja and Freyr's mother is Njörðr's unnamed sister (her name is unprovided in the source material). However, there is more information about his pairing with the skiing and hunting goddess Skaði. Their relationship is ill-fated, as Skaði cannot stand to be away from her beloved mountains, nor Njörðr from the seashore. Together, Freyja, Freyr, and Njörðr form a portion of gods known as the Vanir. While the Aesir and the Vanir retain distinct identification, they came together as the result of the Aesir–Vanir War.\nWhile they receive less mention, numerous other gods and goddesses appear in the source material. (For a list of these deities, see List of Germanic deities.) Some of the gods heard less of include the apple-bearing goddess Iðunn and her husband, the skaldic god Bragi; the gold-toothed god Heimdallr, born of nine mothers; the ancient god Týr, who lost his right hand while binding the great wolf Fenrir; and the goddess Gefjon, who formed modern-day Zealand, Denmark.\nVarious beings outside of the gods are mentioned. Elves and dwarfs are commonly mentioned and appear to be connected, but their attributes are vague and the relation between the two is ambiguous. Elves are described as radiant and beautiful, whereas dwarfs often act as earthen smiths. A group of beings variously described as jötnar, thursar, and trolls (in English these are all often glossed as \"giants\") frequently appear. These beings may either aid, deter, or take their place among the gods. The Norns, dísir, and aforementioned valkyries also receive frequent mention. While their functions and roles may overlap and differ, all are collective female beings associated with fate.\n\n\n=== Cosmology ===\n\nIn Norse cosmology, all beings live in Nine Worlds that center around the cosmological tree Yggdrasil. The gods inhabit the heavenly realm of Asgard whereas humanity inhabits Midgard, a region in the center of the cosmos. Outside of the gods, humanity, and the jötnar, these Nine Worlds are inhabited by beings, such as elves and dwarfs. Travel between the worlds is frequently recounted in the myths, where the gods and other beings may interact directly with humanity. Numerous creatures live on Yggdrasil, such as the insulting messenger squirrel Ratatoskr and the perching hawk Veðrfölnir. The tree itself has three major roots, and at the base of one of these roots live the Norns, female entities associated with fate. Elements of the cosmos are personified, such as the Sun (Sól, a goddess), the Moon (Máni, a god), and Earth (Jörð, a goddess), as well as units of time, such as day (Dagr, a god) and night (Nótt, a jötunn).\nThe afterlife is a complex matter in Norse mythology. The dead may go to the murky realm of Hel—a realm ruled over by a female being of the same name, may be ferried away by valkyries to Odin's martial hall Valhalla, or may be chosen by the goddess Freyja to dwell in her field Fólkvangr. The goddess Rán may claim those that die at sea, and the goddess Gefjon is said to be attended by virgins upon their death. Texts also make reference to reincarnation. Time itself is presented between cyclic and linear, and some scholars have argued that cyclic time was the original format for the mythology. Various forms of a cosmological creation story are provided in Icelandic sources, and references to a future destruction and rebirth of the world—Ragnarok—are frequently mentioned in some texts.\n\n\n=== Humanity ===\nAccording to the Prose Edda and the Poetic Edda poem, Völuspá, the first human couple consisted of Ask and Embla; driftwood found by a trio of gods and imbued with life in the form of three gifts. After the cataclysm of Ragnarok, this process is mirrored in the survival of two humans from a wood; Líf and Lífþrasir. From these two humankind is foretold to repopulate the new and green earth.\n\n\n== See also ==\n\nAlliterative verse\nFamily tree of the Norse gods\nList of Germanic deities\nList of valkyrie names in Norse mythology\nGreek mythology\nRoman mythology\nThe horse in Nordic mythology\n\n\n== References ==\n\n\n=== General sources ===\n\n\n== Further reading ==\n\n\n=== General secondary works ===\nAbram, Christopher (2011). Myths of the Pagan North: the Gods of the Norsemen. London: Continuum. ISBN 978-1-84725-247-0.\nAðalsteinsson, Jón Hnefill (1998). A Piece of Horse Liver: Myth, Ritual and Folklore in Old Icelandic Sources (translated by Terry Gunnell & Joan Turville-Petre). Reykjavík: Félagsvísindastofnun. ISBN 9979-54-264-0.\nAndrén, Anders. Jennbert, Kristina. Raudvere, Catharina. (editors) (2006). Old Norse Religion in Long-Term Perspectives: Origins, Changes, and Interactions. Lund: Nordic Academic Press. ISBN 91-89116-81-X.\nBranston, Brian (1980). Gods of the North. London: Thames and Hudson. (Revised from an earlier hardback edition of 1955). ISBN 0-500-27177-1.\nChristiansen, Eric (2002). The Norsemen in the Viking Age. Malden, Mass.: Blackwell. ISBN 1-4051-4964-7.\nClunies Ross, Margaret (1994). Prolonged Echoes: Old Norse Myths in Medieval Northern Society, vol. 1: The Myths. Odense: Odense Univ. Press. ISBN 87-7838-008-1.\nDavidson, H. R. Ellis (1964). Gods and Myths of Northern Europe. Baltimore: Penguin. New edition 1990 by Penguin Books. ISBN 0-14-013627-4. (Several runestones)\nDavidson, H. R. Ellis (1969). Scandinavian Mythology. London & New York: Hamlyn. ISBN 0-87226-041-0. Reissued 1996 as Viking and Norse Mythology. New York: Barnes and Noble.\nDavidson, H. R. Ellis (1988). Myths and Symbols in Pagan Europe. Syracuse, NY: Syracuse Univ. Press. ISBN 0-8156-2438-7.\nDavidson, H. R. Ellis (1993). The Lost Beliefs of Northern Europe. London & New York: Routledge. ISBN 0-415-04937-7.\nde Vries, Jan. Altgermanische Religionsgeschichte, 2 vols., 2nd. ed., Grundriss der germanischen Philologie, 12–13. Berlin: W. de Gruyter.\nDuBois, Thomas A. (1999). Nordic Religions in the Viking Age. Philadelphia: Univ. Pennsylvania Press. ISBN 0-8122-1714-4.\nDumézil, Georges (1973). Gods of the Ancient Northmen. Ed. & trans. Einar Haugen. Berkeley: University of California Press. ISBN 0-520-03507-0.\nGrimm, Jacob (1888). Teutonic Mythology, 4 vols. Trans. S. Stallybras. London. Reprinted 2003 by Kessinger. ISBN 0-7661-7742-4, ISBN 0-7661-7743-2, ISBN 0-7661-7744-0, ISBN 0-7661-7745-9. Reprinted 2004 Dover Publications. ISBN 0-486-43615-2 (4 vols.), ISBN 0-486-43546-6, ISBN 0-486-43547-4, ISBN 0-486-43548-2, ISBN 0-486-43549-0.\nLindow, John (1988). Scandinavian Mythology: An Annotated Bibliography, Garland Folklore Bibliographies, 13. New York: Garland. ISBN 0-8240-9173-6.\nLindow, John (2001). Norse Mythology: A Guide to the Gods, Heroes, Rituals, and Beliefs. Oxford: Oxford University Press. ISBN 0-19-515382-0. (A dictionary of Norse mythology.)\nMirachandra (2006). Treasure of Norse Mythology Volume I ISBN 978-3-922800-99-6.\nMotz, Lotte (1996). The King, the Champion and the Sorcerer: A Study in Germanic Myth. Wien: Fassbaender. ISBN 3-900538-57-3.\nO'Donoghue, Heather (2007). From Asgard to Valhalla: the remarkable history of the Norse myths. London: I. B. Tauris. ISBN 1-84511-357-8.\nOrchard, Andy (1997). Cassell's Dictionary of Norse Myth and Legend. London: Cassell. ISBN 0-304-36385-5.\nPage, R. I. (1990). Norse Myths (The Legendary Past). London: British Museum; and Austin: University of Texas Press. ISBN 0-292-75546-5.\nPrice, Neil S (2002). The Viking Way: Religion and War in Late Iron Age Scandinavia. Uppsala: Dissertation, Dept. Archaeology & Ancient History. ISBN 91-506-1626-9.\nSimek, Rudolf (1993). Dictionary of Northern Mythology. Trans. Angela Hall. Cambridge: D. S. Brewer. ISBN 0-85991-369-4. New edition 2000, ISBN 0-85991-513-1.\nSimrock, Karl Joseph (1853–1855) Handbuch der deutschen Mythologie.\nSvanberg, Fredrik (2003). Decolonizing the Viking Age. Stockholm: Almqvist & Wiksell. ISBN 9122020063 (v. 1); ISBN 9122020071 (v. 2).\nTurville-Petre, E O Gabriel (1964). Myth and Religion of the North: The Religion of Ancient Scandinavia. London: Weidenfeld & Nicolson. Reprinted 1975, Westport, CN: Greenwood Press. ISBN 0-8371-7420-1.\n\n\n=== Romanticism ===\nAnderson, Rasmus (1875). Norse Mythology, or, The Religion of Our Forefathers. Chicago: S.C. Griggs.\nGuerber, H. A. (1909). Myths of the Norsemen: From the Eddas and Sagas. London: George G. Harrap. Reprinted 1992, Mineola, NY: Dover. ISBN 0-486-27348-2.\nKeary, A & E (1909), The Heroes of Asgard. New York: Macmillan Company. Reprinted 1982 by Smithmark Pub. ISBN 0-8317-4475-8. Reprinted 1979 by Pan Macmillan ISBN 0-333-07802-0.\nMable, Hamilton Wright (1901). Norse Stories Retold from the Eddas. Mead and Company. Reprinted 1999, New York: Hippocrene Books. ISBN 0-7818-0770-0.\nMackenzie, Donald A (1912). Teutonic Myth and Legend. New York: W H Wise & Co. 1934. Reprinted 2003 by University Press of the Pacific. ISBN 1-4102-0740-4.\nRydberg, Viktor (1889). Teutonic Mythology, trans. Rasmus B. Anderson. London: Swan Sonnenschein & Co. Reprinted 2001, Elibron Classics. ISBN 1-4021-9391-2. Reprinted 2004, Kessinger Publishing Company. ISBN 0-7661-8891-4.\n\n\n=== Modern retellings ===\nBradish, Sarah Powers (1900). Old Norse stories. New York: American Book Company / Internet Archive.\nColum, Padraic (1920). The Children of Odin: The Book of Northern Myths, illustrated by Willy Pogány. New York: Macmillan. Reprinted 2004 by Aladdin, ISBN 0-689-86885-5.\nCrossley-Holland, Kevin (1981). The Norse Myths. New York: Pantheon Books. ISBN 0-394-74846-8. Also released as The Penguin Book of Norse Myths: Gods of the Vikings. Harmondsworth: Penguin. ISBN 0-14-025869-8.\nd'Aulaire, Ingri and Edgar (1967). \"d'Aulaire's Book of Norse Myths\". New York, New York Review of Books.\nMunch, Peter Andreas (1927). Norse Mythology: Legends of Gods and Heroes, Scandinavian Classics. Trans. Sigurd Bernhard Hustvedt (1963). New York: American–Scandinavian Foundation. ISBN 0-404-04538-3.\nGaiman, Neil (2017). Norse Mythology. W.W. Norton & Company. ISBN 0-393-60909-X.\nSyran, Nora Louise (2000). Einar's Ragnarok\n\n\n== External links ==\n Media related to Norse mythology at Wikimedia Commons",
  },
  {
    title: "Hindu mythology",
    originalContent:
      'Hindu mythology is the body of myths attributed to, and espoused by, the adherents of the Hindu religion, found in Hindu texts such as the Vedas, the itihasa (the epics of the Mahabharata and Ramayana,) the Puranas, and mythological stories specific to a particular ethnolinguistic group like the Tamil Periya Puranam and Divya Prabandham, and the Mangal Kavya of Bengal. Hindu myths are also found in widely translated popular texts such as the fables of the Panchatantra and the Hitopadesha, as well as in Southeast Asian texts.\n\n\n== Meaning of "myth" ==\nMyth is a genre of folklore or theology consisting primarily of narratives that play a fundamental role in a society, such as foundational tales or origin myths. For folklorists, historians, philosophers or theologians this is very different from the use of "myth" simply indicating that something is not true. Instead, the truth value of a myth is not a defining criterion. \nHindu myths can be found in the Vedas, the itihasa (Ramayana and Mahabharata), and the major Puranas. Other sources include the Bengali literature, such as Mangal-Kāvya, and the Tamil literature, such as Divya Prabandham, Tirumurai and the Five Great Epics.  These narratives play a crucial role in the Hindu tradition and are considered real and significant within their cultural and spiritual context, offering profound insights into the beliefs and values of Hinduism.\n\n\n== Origins and development ==\n\n\n=== Indus Valley Civilisation ===\n\nAccording to Joseph Campbell, the Indus Valley (2600–1900 BCE) may have left traces in the beliefs and traditions of Hinduism. Artefacts have revealed motifs that are also employed and revered by Hindus today, such as primary male deities worshipped by a ruling elite, mother goddesses, nature spirits, snake worship, as well as the reverence of other theriomorphic (animal-shaped) beings. These themes would be maintained by the Dravidian folk religion even after the decline of its parent civilisation around 1800 BCE.\n\n\n=== Vedic Period ===\n\nA major factor in the development of Hinduism was the Vedic religion. The Indo-Aryan migration brought their distinct beliefs to the Indian subcontinent, where the Vedas were composed around 1500 BCE. The Indo-Aryans Vedic pantheon of deities included the chief god Indra, the sun deity Surya, Ushas, as well as Agni.\n\n\n=== Brahmanical Period ===\n\nThis period saw the composition of commentaries referred to as the Brahmanas.\n\n\n=== Upanishad Period ===\n\nAccording to Williams, from 900 to 600 BCE, the protests of the populace against sacrifices made towards the Vedic gods and rebellions against the Brahmin class led to the embrace of reform by the latter and the composition of the fourth Veda and the Vedanta texts. About half of the Upanishads were mystical and unitive, speaking of experiencing the divine as the one (ekam), while the other half promoted devotion to one or more deities. New gods and goddesses were celebrated, and devotional practices began to be introduced.\n\n\n=== Sramanic movements ===\nElements such as those emerging from Buddhism and Jainism made their "heteroprax" contributions to later Hindu mythology, such as temples, indoor shrines, and rituals modeled after service to a divine king. Renunciate traditions contributed elements that questioned sacrifices and the killing of animals, and promoted asceticism and vegetarianism. All of these themes would be incorporated by the Brahmin classes into the later Hindu synthesis, which developed in response to the sramanic movements between ca. 500–300 BCE and 500 CE, and also found their way into Hindu mythology.\n\n\n=== Epic Period ===\n\nThe era from 400 BCE to 400 CE was the period of the compilation of India’s great epics, the Mahabharata and Ramayana. These were central manifestations of the newly developing Hindu synthesis, contributing to a  specific Hindu mythology, emphasising divine action on earth in Vishnu\'s incarnations and other divine manifestations. The lore of the devas and the asuras expanded. Epic mythology foreshadowed the rich polytheism of the next two periods. The Mahabharata contained two appendices that were extremely important sources for later mythological development, the Bhagavad Gîta and the Harivamsa.\n\n\n=== Puranic Period ===\n\nAccording to Williams, the mythology of the Puranas can be broken into three periods (300–500; 500–1000; 1000–1800), or the whole period may simply be referred to as the Hindu Middle Ages. This age saw the composition of the major Puranic texts of the faith, along with the rise of sectarianism, with followers amassing around the cults of Vishnu, Shiva, or Devi. The three denominations within this period help locate in time historical developments within the sectarian communities, the rise and decline of Tantrism and its influence on mainstream mythology, the tendencies in Puranic mythologising of subordinating Vedic gods and past heroes to ever-increasing moral weaknesses, going on to be identified as a period of exuberant polytheism. However, this was also accompanied with the belief in monotheism, the idea that all paths lead to the Ultimate Reality, Brahman.\n\n\n=== Tantric Period ===\n\nAccording to Williams, during the Tantric period from 900 to 1600 CE, the mythology of Tantra and Shaktism revived and enriched blood sacrifice and the pursuit of pleasure as central themes. Tantra’s stories differed radically in meaning from those of epic mythology, which favored devotion, asceticism, and duty. There was either a revival or emphasis that was placed on the shakti or the cosmic energy of goddesses, a concept that had emerged during the Indus Valley Civilisation.\n\n\n=== Modern Period ===\nIn the contemporary era, the mythologies of the dominant traditions of Vaishnavism, Shaivism, and Shaktism prevail. Several myths were found or invented to make tribals or former "outcastes" Hindus and bring them within the cultural whole of a reconstructed Hindu mythological community.\n\n\n== Mythical themes and types ==\n\nAcademic studies of mythology often define mythology as deeply valued stories that explain a society\'s existence and world order: those narratives of a society\'s creation, the society\'s origins and foundations, their god(s), their original heroes, mankind\'s connection to the "divine", and their narratives of eschatology (what happens in the "after-life").  This is a very general outline of some of the basic sacred stories with those themes. In its broadest academic sense, the word myth simply means a traditional story. However, many scholars restrict the term "myth" to sacred stories. Folklorists often go further, defining myths as "tales believed as true, usually sacred, set in the distant past or other worlds or parts of the world, and with extra-human, inhuman, or heroic characters".\nIn classical Greek, muthos, from which the English word myth derives, meant "story, narrative." Hindu mythology does not often have a consistent, monolithic structure. The same myth typically appears in various versions, and can be represented differently across different regional and socio-religious traditions. Many of these legends evolve across these texts, where the character names change or the story is embellished with greater details. According to Suthren Hirst, these myths have been given a complex range of interpretations. While according to Doniger O\'Flaherty, the central message and moral values remain the same.  They have been modified by various philosophical schools over time, and are taken to have deeper, often symbolic, meaning.\n\n\n== Cosmology ==\n\n\n=== Deities ===\n\nPantheism\n\nBrahman The Ultimate Reality\nVaishnavism (Vishnu-centric)\n\nVishnu The God of Preservation\nLakshmi The Goddess of Prosperity\nDashavatara Ten incarnations of Vishnu, chiefly Krishna and Rama\nShaivism (Shiva-centric)\n\nShiva The God of Destruction\nParvati The Goddess of Power\nGanesha The God of Auspiciousness\nKartikeya The God of Victory and War\nShaktism (Goddess-centric)\n\nMahadevi Supreme Goddess\nSaraswati Goddess of Wisdom\nLakshmi Goddess of Prosperity\nParvati Goddess of Power\nDurga Goddess of War\nKali Goddess of time and destruction\n\nHenotheism and Polytheism\n\nBrahma The God of Creation\nVishnu The God of Preservation\nShiva The God of Destruction\nIndra The King of the Devas and Svarga\nSaraswati The Goddess of Wisdom\nLakshmi The Goddess of Prosperity\nParvati The Goddess of Power\nGanesha The God of Auspiciousness\nKrishna The God of love and protection\nRadha The goddess of love, chief consort of Krishna\nRukmini The first queen consort and principal wife of Krishna\nSatyabhama The third queen consort of Krishna\nYamuna one of the main sacred river goddesses in Hinduism and the fourth queen consort of Krishna\nBhudevi Goddess of the Earth\nKartikeya (Murugan) God of Victory and War\nRama The seventh incarnation of  Vishnu\nKali A terrible aspect of Parvati\nDurga A principal aspect of Mahadevi\nAshvins Twin gods of medicine\nAgni God of Fire\nRudra God of the storm\nShakti Personification of power\nVayu God of the wind\nSurya God of the Sun\nVaruna God of the oceans\nLakshmana Younger Brother of Rama\nHanuman Highest devotee of Rama\nSita Consort of Rama and incarnation of Lakshmi\nSati An incarnation of the goddess Shakti\nKubera God of Wealth\nParshurama The sixth incarnation of Vishnu\nYama God of Death and the Underworld\nChandra God of the Moon\nBalarama incarnation of Shesha and in some traditions an avatar of Vishnu\nPrajapati Creator deity\nKalki Prophesied final incarnation of Vishnu\nDashavatara (Ten Incarnations of Vishnu)\nNarada Divine sage, messenger of gods\nSundaravalli Daughter of Vishnu, consort of Murugan\nDevasena Daughter of Vishnu, consort of Murugan\nKamadeva The God of love and desire\nRati The Goddess of love and desire\nShani Divine Personification of the planet saturn\nDravidian folk religion\' (Indigenous Dravidian faith)\n\nMariamman Mother goddess\nAyyanar Guardian deity\nAyyappan God of Dharma\n\n\n=== Connections to other belief systems ===\n\nHinduism shares mythemes with Buddhism, Jainism, and Sikhism.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== Citations ==\n\n\n== General sources ==\n\n\n== Further reading ==\nBhairav, J Furcifer; Rakesh Khanna (2020). Ghosts, Monsters, and Demons of India. Blaft Publications Private Limited. ISBN 9789380636467. OCLC 1259298225.\nBrockington, J. L. (1998). The Sanskrit Epics. BRILL Academic. ISBN 90-04-10260-4.\nBuitenen, J. A. B. van; Dimmitt, Cornelia (1978). Classical Hindu Mythology: A Reader in the Sanskrit Puranas. Philadelphia: Temple University Press. ISBN 0-87722-122-7.\nCampbell, Joseph (2003). Myths of Light: Eastern Metaphors of the Eternal. Novato, Calif.: New World Library. ISBN 1-57731-403-4.\nDalal, Roshen (2010). Hinduism: An Alphabetical Guide. Penguin Books India. ISBN 978-0-14-341421-6.\nDallapiccola, Anna L. (2002). Dictionary of Hindu Lore and Legend. ISBN 0-500-51088-1.\nDimitrova, Stefania (2017). The Day of Brahma: The Myths of India—Epics of Human Destiny. Alpha-Omega. ISBN 978-954-9694-27-7.\nDowson, John (1888). A Classical Dictionary of Hindu Mythology and Religion, Geography, History, and Literature. London: Trubner & Co.\nKrishna, Nanditha (2009). The Book of Vishnu. Penguin Books India. ISBN 978-0-14-306762-7.\nKrishna, Nanditha (2010). Sacred Animals of India. Penguin Books India. ISBN 978-0-14-306619-4.\nMacdonell, Arthur Anthony (1995). Vedic Mythology. Delhi: Motilal Banarsidass. ISBN 81-208-1113-5.\nPattanaik, Devdutt (2003). Indian Mythology: Tales, Symbols, and Rituals from the Heart of the Subcontinent. Inner Traditions/Bear & Company. ISBN 0-89281-870-0.\nRao, T. A. Gopinatha (1914). Elements of Hindu Iconography. Vol. 1: Part I. Madras: Law Printing House.\nWalker, Benjamin (1968). Hindu World: An Encyclopedic Survey of Hinduism. London: Allen & Unwin.\nWilkins, W. J. (1882). Hindu Mythology, Vedic and Purānic. Thacker, Spink & Co.\nGoldberg, Philip. American Veda. Harmony Books, 2010\n\n\n== External links ==\n\nClay Sanskrit Library publishes classical Indian literature, including the Mahabharata and Ramayana, with facing-page text and translation. Also offers searchable corpus and downloadable materials.\nSanskrit Documents Collection: Documents in ITX format of Upanishads, Stotras etc.\nHindu Mythology Stories from Ancient India',
  },
  {
    title: "Enlightenment",
    originalContent:
      'Enlightenment or enlighten may refer to:\n\n\n== Age of Enlightenment ==\nAge of Enlightenment, period in Western intellectual history from the late 17th to late 18th century, centered in France but also encompassing (alphabetically by country or culture):\nEngland: Midlands Enlightenment, period in 18th-century England\nGreece: Modern Greek Enlightenment, an 18th-century national revival and educational movement in Greece\nItaly: Italian Enlightenment, period in 18th-century Italy\nJewish: Haskalah, Jewish Enlightenment, movement among European Jews in the late 18th century\nPoland: Enlightenment in Poland, ideas of the Age of Enlightenment in Poland\nRussia: Russian Enlightenment, 18th-century period of active government encouragement of proliferation of arts and sciences in Russia\nScotland: Scottish Enlightenment, period in 18th-century Scotland\nSpain: Enlightenment in Spain, came to Spain with a new dynasty, the Bourbons, subsequent reform and \'enlightened despotism\'\nUSA: American Enlightenment, intellectual culture of the British North American colonies and the early United States\nArab Enlightenment or Nahda, late 19th to early 20th century.\n\n\n== Religion ==\nEnlightenment in Buddhism, translation of the term bodhi ("awakening"}\nMoksha\nMoksha (Jainism)\nKevala jnana, awakened knowledge in Jainism\nDivine illumination\n\n\n== Computing ==\nEnlightenment (window manager), an X Window System window manager\nEnlighten (radiosity engine), code to do real-time calculation of indirect lighting ("radiosity") in video\nEnlightenment Foundation Libraries, a set of graphics libraries\n\n\n== Events ==\nEnlighten Canberra, an annual arts and cultural festival in Canberra, Australia\n"Enlightenment", the main artistic performance in the 2012 Summer Paralympics opening ceremony\n\n\n== Film and television ==\nEnlightenment (Doctor Who), a 1983 Doctor Who serial\n\n\n== Music ==\nEnlightenment (Van Morrison album), 1990\nEnlightenment (McCoy Tyner album), 1973\n"Enlightenment" (Van Morrison song), 1990\nEnlightenment (soundtrack album), the soundtrack of the 2012 Summer Paralympics opening ceremony\n\n\n== Other uses ==\nIonian Enlightenment, the origin of ancient Greek advances in philosophy and science\nDark Enlightenment, an anti-democratic and reactionary movement that broadly rejects egalitarianism and Whig historiography\nEnlightenment Intensive, a group retreat designed to enable a spiritual enlightenment\nEnlightenment Movement (Afghanistan), a Hazara grassroots civil disobedience group created in Afghanistan in 2016\nProject Enlightenment, an educational program\n\n\n== See also ==\nCounter-Enlightenment, a term used by some 20th century commentators to describe contemporary reasoned opposition to the Age of Enlightenment\nAll pages with titles beginning with Enlighten\nEnlightened (disambiguation)\nIllumination (disambiguation)',
  },
  {
    title: "Teleportation",
    originalContent:
      'Teleportation is the hypothetical transfer of matter or energy from one point to another without traversing the physical space between them. It is a common subject in science fiction and fantasy literature. Teleportation is often paired with time travel, being that the traveling between the two points takes an unknown period of time, sometimes being immediate. An apport is a similar phenomenon featured in parapsychology and spiritualism.\nThere is no known physical mechanism that would allow for teleportation. Some scientific papers and media articles describe "quantum teleportation", a scheme for quantum information transfer, which does not allow for faster-than-light communication.\n\n\n== Etymology ==\nThe use of the term teleport to describe the hypothetical movement of material objects between one place and another without physically traversing the distance between them has been documented as early as 1878.\nAmerican writer Charles Fort is credited with having coined the word teleportation in 1931 to describe the strange disappearances and appearances of anomalies, which he suggested may be connected. As in the earlier usage, he joined the Greek prefix tele- (meaning "remote") to the root of the Latin verb portare (meaning "to carry"). Fort\'s first formal use of the word occurred in the second chapter of his 1931 book Lo!:\n\nMostly in this book I shall specialize upon indications that there exists a transportory force that I shall call Teleportation. I shall be accused of having assembled lies, yarns, hoaxes, and superstitions. To some degree I think so, myself. To some degree, I do not. I offer the data.\n\n\n== Cultural references ==\n\n\n=== Fiction ===\n\nTeleportation is a common subject in science fiction literature, film, video games, and television. The use of matter transmitters in science fiction originated at least as early as the 19th century. An early example of scientific teleportation (as opposed to magical or spiritual teleportation) is found in the 1897 novel To Venus in Five Seconds by Fred T. Jane. Jane\'s protagonist is transported from a strange-machinery-containing gazebo on Earth to planet Venus – hence the title.\nThe earliest recorded story of a "matter transmitter" was Edward Page Mitchell\'s "The Man Without a Body" in 1877.\nThe Catholic Saint Padre Pio has documented miracles of Bilocation including a vision received by Pope John Paul II. This phenomenon has also been reported throughout church history as in the New Testament with Jesus Christ where he was taken to a mountaintop and tempted by Satan.\n\n\n=== Live performance ===\nTeleportation illusions have featured in live performances throughout history, often under the fiction of miracles, psychic phenomenon, or magic.  The cups and balls trick has been performed since 3 BC and can involve balls vanishing, reappearing, teleporting and transposing (objects in two locations interchanging places). A common trick of close-up magic is the apparent teleportation of a small object, such as a marked playing card, which can involve sleight-of-hand, misdirection, and pickpocketing. Magic shows were popular entertainments at fairs in the 18th century and moved into permanent theatres in the mid-19th century. Theatres provided greater control of the environment and viewing angles for more elaborate illusions, and teleportation tricks grew in scale and ambition.  To increase audience excitement, the teleportation illusion could be conducted under the theme of a predicament escape. Magic shows achieved widespread success during the Golden Age of Magic in the late 19th and early 20th centuries.\n\n\n== Quantum teleportation ==\n\nQuantum teleportation is distinct from regular teleportation, as it does not transfer matter from one place to another, but rather transmits the quantum information necessary to prepare a (microscopic) target system in the same quantum state as the source system. The scheme was named quantum "teleportation", because certain properties of the source system are recreated in the target system without any apparent quantum information carrier propagating between the two.\nIn 1993, Bennett et al proposed that a quantum state of a particle could be transferred to another distant particle, without moving the two particles at all. This is called quantum state teleportation. There are many following theoretical and experimental papers published.\nIn 2008, M. Hotta proposed that it may be possible to teleport energy by exploiting quantum energy fluctuations of an entangled vacuum state of a quantum field. In 2023, zero temperature quantum energy teleportation was observed and recorded by Kazuki Ikeda for the first-time across microscopic distances using IBM superconducting computers that are used for quantum computing.\nIn 2014, researcher Ronald Hanson and colleagues from the Technical University Delft in the Netherlands, demonstrated the teleportation of information between two entangled quantumbits three metres apart.\nA generalization of quantum mechanics suggests particles could be teleport from one place to another. This is called particle teleportation. With this concept, superconductivity can be viewed as the teleportation of some electrons in the superconductor and superfluidity as the teleportation of some of the atoms in the cellular tube. Further analysis shows that the teleportation time increases with the square root of mass and longer teleportation times require sustained quantum coherence. While particle teleportation may be feasible for an electron, a proton may not be feasible.\n\n\n== Philosophy ==\nPhilosopher Derek Parfit used teleportation in his teletransportation paradox.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\nDavid Darling (2005). Teleportation: The Impossible Leap. John Wiley & Sons. ISBN 978-0-471-71545-0.\nLawrence M. Krauss (1995), The Physics of Star Trek, Basic Books, ISBN 978-0465002047\nEric W. Davis (2004), Teleportation Physics Study, Air Force Research Laboratory AFRL-PR-ED-TR-2003-0034\nBernd Thaller (2005). Advanced Visual Quantum Mechanics. Springer. 4.3.3 Classical teleportation is impossible pp. 170–171. ISBN 978-0-387-27127-9.\nWill Human Teleportation Ever Be Possible?\nHuman teleportation is far more impractical than we thought Archived 16 October 2015 at the Wayback Machine\nY. Wei (2016), How to teleport a particle rather than a state Phys Rev E 93. 066103',
  },
  {
    title: "Parallel universe",
    originalContent:
      'Parallel universe may refer to:\n\n\n== Science ==\nMany-worlds interpretation of quantum mechanics, which implies the existence of parallel universes\nMultiverse, the sum of all universes, e.g. everything that exists\n\n\n== Philosophy ==\nPossible world, a construct in metaphysics to bring rigor to talk of logical possibility\nModal realism, an account of possible worlds according to which they are all just as real as the actual world\nExtended modal realism, the view that all worlds, possible as well as impossible, are as real as the actual world\n\n\n== Arts and media ==\nParallel universes in fiction, a hypothetical self-contained plane of existence, co-existing with one\'s own\nAlternate history, a genre of fiction in which historical events differ from reality\nAlternative universe (fan fiction), fiction by fan authors that departs from the fictional universe of the source work\n\n\n=== Literature, film, and television ===\n"Parallel Universe" (Red Dwarf), a 1988 TV episode\nParallel Universes (film), a 2001 British TV documentary\nMirror Universe (Star Trek), a fictional parallel universe in the Star Trek franchise\n\n\n=== Music ===\nParallel Universe (4hero album) or the title song, 1994\nParallel Universe (Garnet Crow album), 2010\nParallel Universe, an album by  Plain White T\'s, 2018\n"Parallel Universe" (song), by the Red Hot Chili Peppers, 1999\nParallel Dimensions (album), by Perseo Miranda, 2008\n\n\n== See also ==\nMetaverse, a collective virtual shared space\nAlternate reality (disambiguation)\nMultiverse (disambiguation)\nOtherworld\nParallel World (disambiguation)',
  },
  {
    title: "Singularity",
    originalContent:
      'Singularity or singular point may refer to:\n\n\n== Science, technology, and mathematics ==\n\n\n=== Mathematics ===\nMathematical singularity, a point at which a given mathematical object is not defined or not "well-behaved", for example infinite or not differentiable\n\n\n==== Geometry ====\nSingular point of a curve, where the curve is not given by a smooth embedding of a parameter\nSingular point of an algebraic variety, a point where an algebraic variety is not locally flat\nRational singularity\n\n\n=== Natural sciences ===\nSingularity (system theory), in dynamical and social systems, a context in which a small change can cause a large effect\nGravitational singularity, in general relativity, a point in which gravity is so intense that spacetime itself becomes ill-defined\nInitial singularity, a hypothesized singularity of infinite density before quantum fluctuations caused the Big Bang and subsequent inflation that created the Universe\nPenrose–Hawking singularity theorems, in general relativity theory, theorems about how gravitation produces singularities such as in black holes\nPrandtl–Glauert singularity, the point at which a sudden drop in air pressure occurs\nSingularity (climate), a weather phenomenon associated with a specific calendar date\nVan Hove singularity in the density of states of a material\n\n\n=== Technology ===\nSingularity (operating system), an experimental operating system developed by Microsoft Research\nMechanical singularity, a position or configuration of a mechanism or a machine where the subsequent behavior cannot be predicted\nSingularity (software), a container technology that does not require root permissions to run\n\n\n=== Futurology ===\nTechnological singularity, a hypothetical point in time when technological growth becomes uncontrollable and irreversible\n\n\n== Arts and entertainment ==\n\n\n=== Film and television ===\n\n"Singularity", a first-season episode of Disney\'s So Weird\n"Singularity" (Star Trek: Enterprise), a second-season episode of Star Trek: Enterprise\n"The Singularity" (Agents of S.H.I.E.L.D.), a third-season episode of Agents of S.H.I.E.L.D.\n"Singularity", a first-season episode of Stargate SG-1\nThe Singularity (film), a 2012 documentary about the technological singularity\nSingularity (2017 film), American science fiction film starring John Cusack\nSingularity,  the working title for The Lovers, starring Josh Hartnett and Bipasha Basu\nGodzilla Singular Point, a 2021 Japanese anime surrounding Godzilla\n\n\n=== Literature ===\nSingularity (Sleator novel), a 1985 science-fiction novel by William Sleator\nSingularity (DeSmedt novel), a 2004 novel by Bill DeSmedt\nSingularity (audio drama), a 2005 Doctor Who audio drama\nSingularity 7, a graphic novel by Ben Templesmith\nThe Singularity Is Near, a 2005 book by Ray Kurzweil on the technological singularity\nThe Singularity Is Nearer, a 2024 sequel to Ray Kurzweil\'s 2005 book\nSingularity (comics), the Marvel Comics character\nThe Singularity, Anne Milano Appel\'s 2024 English translation of Dino Buzzati\'s 1960 novel Il grande ritratto (Larger than Life)\n\n\n=== Music ===\n\n\n==== Albums ====\nSingularity (Joe Morris album), 2001\nSingularity (Peter Hammill album), 2006\nSingularity (Mae album), 2007\nSingularity (Robby Krieger album), 2010\nSingularity (Northlane album), 2013\nSingularity (Jon Hopkins album), 2018\nSingularity, a 2014 EP by Lemaitre\nThe Singularity (Phase I – Neohumanity), 2014 album by Swedish metal band Scar Symmetry\nSingularity (Lead album), 2020 album\n\n\n==== Songs ====\n"Singularity" (song), performed by V of the South Korean group BTS from their 2018 album Love Yourself: Tear\n"Singularity", by Believer on the 1993 album Dimensions\n"Singularity", by Caligula\'s Horse on the 2011 album Moments from Ephemeral City\n"Singularity", by Textures on the 2011 album Dualism\n"Singularity", by Born of Osiris on the 2011 album The Discovery\n"Singularity", by Steve Aoki & Angger Dimas featuring My Name Is Kay from It\'s the End of the World as We Know It, 2013\n"Singularity", by Tesseract on the 2013 album Altered State\n"Singularity", by New Order on the 2015 album Music Complete\n"Singularity", by Devin Townsend on the 2019 album Empath\n\n\n==== People ====\nSingularity, stage name of Grey\'s Kyle Trewartha\n\n\n=== Video games ===\nSingularity (video game), a 2010 video game developed by Raven Software\nEndgame: Singularity, a 2005 video game\n\n\n== Organizations ==\nSingularity University, a California Benefit Corporation part think-tank, part business incubator, based on Ray Kurzweil\'s theory of technological singularity\nMachine Intelligence Research Institute (MIRI), formerly "The Singularity Institute for Artificial Intelligence" (SIAI)\nSingularity Summit, its annual conference\n\n\n== See also ==\nSingular (disambiguation)',
  },
  {
    title: "Epistemic justification",
    originalContent:
      'Justification (also called epistemic justification) is a property of beliefs that fulfill certain norms about what a person should believe. Epistemologists often identify justification as a component of knowledge distinguishing it from mere true opinion. They study the reasons why someone holds a belief. Epistemologists are concerned with various features of belief, which include the ideas of warrant (a proper justification for holding a belief), knowledge, rationality, and probability, among others.\nDebates surrounding epistemic justification often involve the structure of justification, including whether there are foundational justified beliefs or whether mere coherence is sufficient for a system of beliefs to qualify as justified. Another major subject of debate is the sources of justification, which might include perceptual experience (the evidence of the senses), reason, and authoritative testimony, among others.\n\n\n== Justification and knowledge ==\n"Justification" involves the reasons why someone holds a belief that one should hold based on one\'s current evidence. Justification is a property of beliefs insofar as they are held blamelessly. In other words, a justified belief is a belief that a person is entitled to hold.\nMany philosophers from Plato onward have treated "justified true belief" (JTB) as constituting knowledge. It is particularly associated with a theory discussed in his dialogues Meno and Theaetetus. While in fact Plato seems to disavow justified true belief as constituting knowledge at the end of Theaetetus, the claim that Plato unquestioningly accepted this view of knowledge stuck until the proposal of the Gettier problem.\nThe subject of justification has played a major role in the value of knowledge as "justified true belief". Some contemporary epistemologists, such as Jonathan Kvanvig assert that justification isn\'t necessary in getting to the truth and avoiding errors. Kvanvig attempts to show that knowledge is no more valuable than true belief, and in the process dismissed the necessity of justification due to justification not being connected to the truth.\n\n\n== Conceptions of justification ==\nWilliam P. Alston identifies two conceptions of justification.: 15–16  One conception is "deontological" justification, which holds that justification evaluates the obligation and responsibility of a person having only true beliefs. This conception implies, for instance, that a person who has made his best effort but is incapable of concluding the correct belief from his evidence is still justified. The deontological conception of justification corresponds to epistemic internalism. Another conception is "truth-conducive" justification, which holds that justification is based on having sufficient evidence or reasons that entails that the belief is at least likely to be true. The truth-conductive conception of justification corresponds to epistemic externalism.\n\n\n== Theories of justification ==\nThere are several different views as to what entails justification, mostly focusing on the question "How sure do we need to be that our beliefs correspond to the actual world?" Different theories of justification require different conditions before a belief can be considered justified. Theories of justification generally include other aspects of epistemology, such as defining knowledge.\nNotable theories of justification include:\n\nFoundationalism – Basic beliefs justify other, non-basic beliefs.\nEpistemic coherentism – Beliefs are justified if they cohere with other beliefs a person holds, each belief is justified if it coheres with the overall system of beliefs.\nInfinitism – Beliefs are justified by infinite chains of reasons.\nFoundherentism – Both fallible foundations and coherence are components of justification—proposed by Susan Haack.\nInternalism and externalism – The believer must be able to justify a belief through internal knowledge (internalism), or outside sources of knowledge (externalism).\nReformed epistemology – Beliefs are warranted by proper cognitive function—proposed by Alvin Plantinga.\nEvidentialism – Beliefs depend solely on the evidence for them.\nReliabilism – A belief is justified if it is the result of a reliable process.\nInfallibilism – Knowledge is incompatible with the possibility of being wrong.\nFallibilism – Claims can be accepted even though they cannot be conclusively proven or justified.\nNon-justificationism – Knowledge is produced by attacking claims and refuting them instead of justifying them.\nSkepticism – Knowledge is impossible or undecidable.\n\n\n== Criticism of theories of justification ==\nRobert Fogelin claims to detect a suspicious resemblance between the theories of justification and Agrippa\'s five modes leading to the suspension of belief. He concludes that the modern proponents have made no significant progress in responding to the ancient modes of Pyrrhonian skepticism.\nWilliam P. Alston criticizes the very idea of a theory of justification. He claims: "There isn\'t any unique, epistemically crucial property of beliefs picked out by \'justified\'. Epistemologists who suppose the contrary have been chasing a will-o\'-the-wisp. What has really been happening is this. Different epistemologists have been emphasizing, concentrating on, "pushing" different epistemic desiderata, different features of belief that are positively valuable from the standpoint of the aims of cognition.": 22 \n\n\n== See also ==\nDream argument\nRegress argument (epistemology)\nMünchhausen trilemma\n\n\n== References ==\n\n\n== External links ==\nStanford Encyclopedia of Philosophy entry on Foundationalist Theories of Epistemic Justification\nStanford Encyclopedia of Philosophy entry on Epistemology, 2. What is Justification?\nStanford Encyclopedia of Philosophy entry on Internalist vs. Externalist Conceptions of Epistemic Justification\nStanford Encyclopedia of Philosophy entry on Coherentist Theories of Epistemic Justification\n\n\n=== Internet Encyclopedia of Philosophy ===\nInternet Encyclopedia of Philosophy entry on Epistemic Justification\nInternet Encyclopedia of Philosophy entry on Epistemic Entitlement\nInternet Encyclopedia of Philosophy entry on Internalism and Externalism in Epistemology\nInternet Encyclopedia of Philosophy entry on Epistemic Consequentialism\nInternet Encyclopedia of Philosophy entry on Coherentism in Epistemology\nInternet Encyclopedia of Philosophy entry on Contextualism in Epistemology\nInternet Encyclopedia of Philosophy entry on Knowledge-First Theories of Justification',
  },
  {
    title: "Deontology",
    originalContent:
      'In moral philosophy, deontological ethics or deontology (from Greek: δέον, \'obligation, duty\' + λόγος, \'study\') is the normative ethical theory that the morality of an action should be based on whether that action itself is right or wrong under a series of rules and principles, rather than based on the consequences of the action. It is sometimes described as duty-, obligation-, or rule-based ethics. Deontological ethics is commonly contrasted to consequentialism, utilitarianism, virtue ethics, and pragmatic ethics. In this terminology, action is more important than the consequences.\nThe term deontological was first used to describe the current, specialised definition by C. D. Broad in his 1930 book, Five Types of Ethical Theory. Older usage of the term goes back to Jeremy Bentham, who coined it prior to 1816 as a synonym of dicastic or censorial ethics (i.e., ethics based on judgement). The more general sense of the word is retained in French, especially in the term code de déontologie (ethical code), in the context of professional ethics.\nDepending on the system of deontological ethics under consideration, a moral obligation may arise from an external or internal source, such as a set of rules inherent to the universe (ethical naturalism), religious law, or a set of personal or cultural values (any of which may be in conflict with personal desires).\n\n\n== Deontological philosophies ==\nThere are numerous formulations of deontological ethics.\n\n\n=== Kantianism ===\n\nImmanuel Kant\'s theory of ethics is considered deontological for several different reasons. First, Kant argues that in order to act in the morally right way, people must act from duty (Pflicht). Second, Kant argued that it was not the consequences of actions that make them right or wrong, but the motives of the person who carries out the action.\nKant\'s first argument begins with the premise that the highest good must be both good in itself and good without qualification. Something is "good in itself" when it is intrinsically good; and is "good without qualification" when the addition of that thing never makes a situation ethically worse. Kant then argues that those things that are usually thought to be good, such as intelligence, perseverance, and pleasure, fail to be either intrinsically good or good without qualification. Pleasure, for example, appears not to be good without qualification, because when people take pleasure in watching someone suffer, this seems to make the situation ethically worse. He concludes that there is only one thing that is truly good:\n\nNothing in the world—indeed nothing even beyond the world—can possibly be conceived which could be called good without qualification except a good will.Kant then argues that the consequences of an act of willing cannot be used to determine that the person has a good will; good consequences could arise by accident from an action that was motivated by a desire to cause harm to an innocent person, and bad consequences could arise from an action that was well-motivated. Instead, he claims, a person has a good will when they "act out of respect for the moral law." People "act out of respect for the moral law" when they act in some way because they have a duty to do so. Thus, the only thing that is truly good in itself is a good will, and a good will is only good when the willer chooses to do something because it is that person\'s duty; i.e., out of respect for the law. He defines respect as "the concept of a worth which thwarts my self-love."\nKant\'s three significant formulations of the categorical imperative (a way of evaluating motivations for action) are:\n\nAct only according to that maxim by which you can also will that it would become a universal law;\nAct in such a way that you always treat humanity, whether in your own person or in the person of any other, never simply as a means, but always at the same time as an end;\nEvery rational being must so act as if he were through his maxim always a legislating member in a universal kingdom of ends.\nKant argued that the only absolutely good thing is a good will, and so the single determining factor of whether an action is morally right is the will, or motive of the person doing it. If they are acting on a bad maxim—e.g., \'I will lie\'—then their action is wrong, even if some good consequences come of it.\n\nIn his essay, "On a Supposed Right to Lie Because of Philanthropic Concerns", arguing against the position of Benjamin Constant, Des réactions politiques, Kant states that:Hence a lie defined merely as an intentionally untruthful declaration to another man does not require the additional condition that it must do harm to another, as jurists require in their definition (mendacium est falsiloquium in praeiudicium alterius). For a lie always harms another; if not some human being, then it nevertheless does harm to humanity in general, inasmuch as it vitiates the very source of right [Rechtsquelle].… All practical principles of right must contain rigorous truth.… This is because such exceptions would destroy the universality on account of which alone they bear the name of principles.\n\n\n=== Divine command theory ===\n\nAlthough not all deontologists are religious, some believe in the divine command theory, which is actually a cluster of related theories that essentially state that an action is right if God has decreed that it is right. According to English philosopher Ralph Cudworth, William of Ockham, René Descartes, and 18th-century Calvinists all accepted various versions of this moral theory, as they all held that moral obligations arise from God\'s commands.\nThe divine command theory is a form of deontology because, according to it, the rightness of any action depends upon that action being performed because it is a duty, not because of any good consequences arising from that action. If God commands people not to work on Sabbath, then people act rightly if they do not work on Sabbath because God has commanded that they do not do so. If they do not work on Sabbath because they are lazy, then their action is not, truly speaking, "right" even though the actual physical action performed is the same. If God commands not to covet a neighbour\'s goods, this theory holds that it would be immoral to do so, even if coveting provides the beneficial outcome of a drive to succeed or do well.\nOne thing that clearly distinguishes Kantian deontologism from divine command deontology is that Kantianism maintains that man, as a rational being, makes the moral law universal, whereas divine command maintains that God makes the moral law universal.\n\n\n=== Ross\'s deontological pluralism ===\nW. D. Ross objects to Kant\'s monistic deontology, which bases ethics in only one foundational principle, the categorical imperative. He contends that there is a plurality (7, although this number is seen to vary to interpretation) of prima facie duties determining what is right.: xii \nThese duties are identified by W. D. Ross:\n\nthe duty of fidelity (to keep promises and to tell the truth)\nthe duty of reparation (to make amends for wrongful acts)\nthe duty of gratitude (to return kindnesses received)\nthe duty of non-injury (not to hurt others)\nthe duty of beneficence (to promote the maximum of aggregate good)\nthe duty of self-improvement (to improve one\'s own condition)\nthe duty of justice (to distribute benefits and burdens equably).: 21–5 \nOne problem the deontological pluralist has to face is that cases can arise where the demands of one duty violate another duty, so-called moral dilemmas. For example, there are cases where it is necessary to break a promise in order to relieve someone\'s distress.: 28  Ross makes use of the distinction between prima facie duties and absolute duty to solve this problem.: 28  The duties listed above are prima facie duties (moral actions that are required unless a greater obligation trumps them); they are general principles whose validity is self-evident to morally mature persons.They are factors that do not take all considerations into account. Absolute duty, on the other hand, is particular to one specific situation, taking everything into account, and has to be judged on a case-by-case basis. It is absolute duty that determines which acts are right or wrong.\n\n\n=== Contemporary deontology ===\nContemporary deontologists (i.e., scholars born in the first half of the 20th century) include Józef Maria Bocheński, Thomas Nagel, T. M. Scanlon, and Roger Scruton.\nBocheński (1965) makes a distinction between deontic and epistemic authority:\n\nA typical example of epistemic authority in Bocheński\'s usage would be "the relation of a teacher to her students." A teacher has epistemic authority when making declarative sentences that the student presumes is reliable knowledge and appropriate but feels no obligation to accept or obey.\nAn example of deontic authority would be "the relation between an employer and her employee." An employer has deontic authority in the act of issuing an order that the employee is obliged to accept and obey regardless of its reliability or appropriateness.\nScruton (2017), in his book On Human Nature, is critical of consequentialism and similar ethical theories, such as hedonism and utilitarianism, instead proposing a deontological ethical approach. He implies that proportional duty and obligation are essential components of the ways in which we decide to act, and he defends natural law against opposing theories. He also expresses admiration for virtue ethics, and believes that the two ethical theories are not, as is frequently portrayed, mutually exclusive.\n\n\n=== Deontology and consequentialism ===\n\n\n==== Principle of permissible harm ====\nFrances Kamm\'s "Principle of Permissible Harm" (1996) is an effort to derive a deontological constraint that coheres with our considered case judgments while also relying heavily on Kant\'s categorical imperative. The principle states that one may harm in order to save more if and only if the harm is an effect or an aspect of the greater good itself. This principle is meant to address what Kamm feels are most people\'s considered case judgments, many of which involve deontological intuitions. For instance, Kamm argues that we believe it would be impermissible to kill one person to harvest his organs in order to save the lives of five others. Yet, we think it is morally permissible to divert a runaway trolley that would otherwise kill five innocent, immobile people, onto a sidetrack where only one innocent and immobile person will be killed. Kamm believes the Principle of Permissible Harm explains the moral difference between these and other cases, and more importantly expresses a constraint telling us exactly when we may not act to bring about good ends—such as in the organ harvesting case.\nIn 2007, Kamm published Intricate Ethics, a book that presents a new theory, the "Doctrine of Productive Purity", that incorporates aspects of her "Principle of Permissible Harm". Like the "Principle", the "Doctrine of Productive Purity" is an attempt to provide a deontological prescription for determining the circumstances in which people are permitted to act in a way that harms others.\n\n\n==== Reconciling deontology with consequentialism ====\nVarious attempts have been made to reconcile deontology with consequentialism. Threshold deontology holds that rules ought to govern up to a point despite adverse consequences; but when the consequences become so dire that they cross a stipulated threshold, consequentialism takes over. Theories put forth by Thomas Nagel and Michael S. Moore attempt to reconcile deontology with consequentialism by assigning each a jurisdiction. Iain King\'s 2008 book How to Make Good Decisions and Be Right All the Time uses quasi-realism and a modified form of utilitarianism to develop deontological principles that are compatible with ethics based on virtues and consequences. King develops a hierarchy of principles to link his meta-ethics, which is more inclined towards consequentialism, with the deontological conclusions he presents in his book.\n\n\n=== Secular deontology ===\nIntuition-based deontology is a concept within secular ethics. A classical example of literature on secular ethics is the Kural text, authored by the ancient Tamil Indian philosopher Valluvar. It can be argued that some concepts from deontological ethics date back to this text. Concerning ethical intuitionism, 20th century philosopher C.D. Broad coined the term "deontological ethics" to refer to the normative doctrines associated with intuitionism, leaving the phrase "ethical intuitionism" free to refer to the epistemological doctrines.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Bibliography ===\nBeauchamp, Tom L. 1991. Philosophical Ethics: An Introduction to Moral Philosophy (2nd ed.) New York: McGraw Hill.\nBroad, C. D. 1930. Five Types of Ethical Theory. New York: Harcourt, Brace and Co.\nFlew, Antony. 1979. "Consequentialism." In A Dictionary of Philosophy (2nd ed.). New York: St. Martin\'s.\nKamm, Frances M. 1996. Morality, Mortality Vol. II: Rights, Duties, and Status. New York: Oxford University Press.\n—— 2007. Intricate Ethics: Rights, Responsibilities, and Permissible Harm. Oxford: Oxford University Press. ISBN 978-0-19-518969-8, 978-0-19-534590-2.\nKant, Immanuel (1964). Groundwork of the Metaphysic of Morals. Harper and Row Publishers, Inc. ISBN 978-0-06-131159-8.\n«Législation, éthique et déontologie», Bruxelles: Editions de Boeck Université, 2011, Karine BREHAUX, ISBN 978-2-84371-558-7\nOlson, Robert G. 1967. "Deontological Ethics." In The Encyclopedia of Philosophy, edited by P. Edwards. London: Collier Macmillan.\nRoss, W. D. 1930. The Right and the Good. Oxford: Clarendon Press.\nSalzman, Todd A. 1995. Deontology and Teleology: An Investigation of the Normative Debate in Roman Catholic Moral Theology. University Press.\nWaller, Bruce N. 2005. Consider Ethics: Theory, Readings, and Contemporary Issues. New York: Pearson Longman.\nWierenga, Edward. 1983. "A Defensible Divine Command Theory." Noûs 17(3):387–407.\n\n\n== External links ==\n\nKantian Ethics – Summary A concise summary of the key details of Kant\'s deontology\nFreedom and the Boundary of Morals, Lecture 22 from Stephen Palmquist\'s book, The Tree of Philosophy (fourth edition, 2000).\nDeontology and Ethical Ends\nStanford Encyclopedia of Philosophy entry on Special Obligations\nLog in to ePortfolios@FedUni – ePortfolios@FedUni Deontology framework ethics',
  },
  {
    title: "Cognitivism",
    originalContent:
      "Cognitivism may refer to:\n\nCognitivism (ethics), the philosophical view that ethical sentences express propositions and are capable of being true or false\nCognitivism (psychology), a psychological approach that argues that mental function can be understood as the internal manipulation of symbols\nCognitivism (aesthetics), a view that cognitive psychology can help understand art and the response to it\nAnecdotal cognitivism, a psychological methodology for interpreting animal behavior in terms of mental states\n\n\n== See also ==\nCognition, the study of the human mind\nCognitive anthropology\nCognitive science\nComputationalism\nPhilosophy of mind\nSituated cognition\nSocio-cognitive\nSymbol grounding",
  },
  {
    title: "Constructivism",
    originalContent:
      "Constructivism may refer to:\n\n\n== Art and architecture ==\nConstructivism (art), an early 20th-century artistic movement that extols art as a practice for social purposes\nConstructivist architecture, an architectural movement in the Soviet Union in the 1920s and 1930s\nBritish Constructivists, a group of British artists who were active between 1951 to 1955.\n\n\n== Education ==\nConstructivism (philosophy of education), a theory about the nature of learning that focuses on how humans make meaning from their experiences\nConstructivism in science education\nConstructivist teaching methods, based on constructivist learning theory\n\n\n== Mathematics ==\nConstructivism (philosophy of mathematics), a logic for founding mathematics that accepts only objects that can be effectively constructed\nConstructivist set theory\nConstructivist type theory\n\n\n== Philosophy ==\nConstructivism (philosophy of mathematics), a philosophical view that asserts the necessity of constructing a mathematical object to prove that it exists\nConstructivism (philosophy of education), a theory that suggests that learners do not passively acquire knowledge through direct instruction; instead, they construct their understanding through experiences and social interaction, integrating new information with their existing knowledge\nConstructivism (philosophy of science), a philosophical view maintaining that science consists of mental constructs created as the result of measuring the natural world\nMoral constructivism or ethical constructivism, the view that moral facts are constructed rather than discovered\n\n\n== Political and social sciences ==\nConstructivism (international relations), a theory that stresses the socially constructed character of international relations\nConstructivism (ethnic politics), a theory that ethnic identities are not unchanging entities and that political developments can shape which identities get activated\nConstructivist institutionalism\nSocial constructivism, the view that human development is socially situated and knowledge is constructed through interaction with others\n\n\n== Psychology ==\nConstructivism (psychological school), a psychological approach that assumes that human knowledge is active and constructive\n\n\n== See also ==\nConstructionism (disambiguation)\nConstructive theology\nConstructive empiricism\nDeconstructivism, a movement of postmodern architecture from the 1980s\nNeuroconstructivism\nTransactionalism",
  },
  {
    title: "Dualism",
    originalContent:
      'Dualism most commonly refers to:\n\nMind–body dualism, a philosophical view which holds that mental phenomena are, at least in certain respects, not physical phenomena, or that the mind and the body are distinct and separable from one another\nProperty dualism, a view in the philosophy of mind and metaphysics which holds that, although the world is composed of just one kind of substance—the physical kind—there exist two distinct kinds of properties: physical properties and mental properties\nCosmological dualism, the theological or spiritual view that there are only two fundamental concepts, such as "good" and "evil", and that these two concepts are in every way opposed to one another\nDualism may also refer to:\n\nDualism (cybernetics), systems or problems in which an intelligent adversary attempts to exploit the weaknesses of the investigator\nDualism (Indian philosophy), the belief held by certain schools of Indian philosophy that reality is fundamentally composed of two parts\nDualism (politics), the separation of powers between the cabinet and parliament\nDualism in medieval politics, opposition to hierocracy (medieval)\nEpistemological dualism, the epistemological question of whether the world we see around us is the real world itself or merely an internal perceptual copy of that world generated by neural processes in our brain\nEthical dualism, the attribution of good solely to one group of people and evil to another\nMonism and dualism in international law, a principle in contending that international and domestic law are distinct systems of law, and that international law only applies to the extent that it does not conflict with domestic law\nSoul dualism, the belief that a person has two (or more) kinds of souls\n\n\n== Media ==\nDualism (album), a 2011 album by Dutch metal band Textures\nDualist (album), a 2011 album by Taken by Cars\nDualism, a novel by Bill DeSmedt\n\n\n== See also ==',
  },
  {
    title: "Materialism",
    originalContent:
      'Materialism is a form of philosophical monism which holds that matter is the fundamental substance in nature, and that all things, including mental states and consciousness, are results of material interactions of material things. According to philosophical materialism, mind and consciousness are caused by physical processes, such as the neurochemistry of the human brain and nervous system, without which they cannot exist. Materialism directly contrasts with monistic idealism, according to which consciousness is the fundamental substance of nature.\nMaterialism is closely related to physicalism—the view that all that exists is ultimately physical. Philosophical physicalism has evolved from materialism with the theories of the physical sciences to incorporate forms of physicality in addition to ordinary matter (e.g. spacetime, physical energies and forces, and exotic matter). Thus, some prefer the term physicalism to materialism, while others use the terms as if they were synonymous.\nDiscoveries of neural correlates between consciousness and the brain are taken as empirical support for materialism, but some philosophers of mind find that association fallacious or consider it compatible with non-materialist ideas. Alternative philosophies opposed or alternative to materialism or physicalism include idealism, pluralism, dualism, panpsychism, and other forms of monism. Epicureanism is a philosophy of materialism from classical antiquity that was a major forerunner of modern science. Though ostensibly a deist, Epicurus affirmed the literal existence of the Greek gods in either some type of celestial "heaven" cognate from which they ruled the universe (if not on a literal Mount Olympus), and his philosophy promulgated atomism, while Platonism taught roughly the opposite, despite Plato\'s teaching of Zeus as God.\n\n\n== Overview ==\n\nMaterialism belongs to the class of monist ontology, and is thus different from ontological theories based on dualism or pluralism. For singular explanations of the phenomenal reality, materialism is in contrast to idealism, neutral monism, and spiritualism. It can also contrast with phenomenalism, vitalism, and dual-aspect monism. Its materiality can, in some ways, be linked to the concept of determinism, as espoused by Enlightenment thinkers.\nDespite the large number of philosophical schools and their nuances, all philosophies are said to fall into one of two primary categories, defined in contrast to each other: idealism and materialism.[a] The basic proposition of these two categories pertains to the nature of reality: the primary difference between them is how they answer two fundamental questions—what reality consists of, and how it originated. To idealists, spirit or mind or the objects of mind (ideas) are primary, and matter secondary. To materialists, matter is primary, and mind or spirit or ideas are secondary—the product of matter acting upon matter.\nThe materialist view is perhaps best understood in its opposition to the doctrines of immaterial substance applied to the mind historically by René Descartes; by itself, materialism says nothing about how material substance should be characterized. In practice, it is frequently assimilated to one variety of physicalism or another.\nModern philosophical materialists extend the definition of other scientifically observable entities such as energy, forces, and the spacetime continuum; some philosophers, such as Mary Midgley, suggest that the concept of "matter" is elusive and poorly defined.\nDuring the 19th century, Karl Marx and Friedrich Engels extended the concept of materialism to elaborate a materialist conception of history centered on the roughly empirical world of human activity (practice, including labor) and the institutions created, reproduced or destroyed by that activity. They also developed dialectical materialism, by taking Hegelian dialectics, stripping them of their idealist aspects, and fusing them with materialism (see Modern philosophy).\n\n\n=== Non-reductive materialism ===\nMaterialism is often associated with reductionism, according to which the objects or phenomena individuated at one level of description, if they are genuine, must be explicable in terms of the objects or phenomena at some other level of description—typically, at a more reduced level.\nNon-reductive materialism explicitly rejects this notion, taking the material constitution of all particulars to be consistent with the existence of real objects, properties or phenomena not explicable in the terms canonically used for the basic material constituents. Jerry Fodor held this view, according to which empirical laws and explanations in "special sciences" like psychology or geology are invisible from the perspective of basic physics.\n\n\n== History ==\n\n\n=== Early history ===\n\n\n==== Before Common Era ====\n\nMaterialism developed, possibly independently, in several geographically separated regions of Eurasia during what Karl Jaspers termed the Axial Age (c. 800–200 BC).\nIn ancient Indian philosophy, materialism developed around 600 BC with the works of Ajita Kesakambali, Payasi, Kanada and the proponents of the Cārvāka school of philosophy. Kanada became one of the early proponents of atomism. The Nyaya–Vaisesika school (c. 600–100 BC) developed one of the earliest forms of atomism (although their proofs of God and their positing that consciousness was not material precludes labelling them as materialists). Buddhist atomism and the Jaina school continued the atomic tradition.\nAncient Greek atomists like Leucippus, Democritus and Epicurus prefigure later materialists. The Latin poem De Rerum Natura by Lucretius (99 – c. 55 BC) reflects the mechanistic philosophy of Democritus and Epicurus. According to this view, all that exists is matter and void, and all phenomena result from different motions and conglomerations of base material particles called atoms (literally "indivisibles"). De Rerum Natura provides mechanistic explanations for phenomena such as erosion, evaporation, wind, and sound. Famous principles like "nothing can touch body but body" first appeared in Lucretius\'s work. Democritus and Epicurus did not espouse a monist ontology, instead espousing the ontological separation of matter and space (i.e. that space is "another kind" of being).\n\n\n==== Early Common Era ====\nWang Chong (27 – c. 100 AD) was a Chinese thinker of the early Common Era said to be a materialist. Later Indian materialist Jayaraashi Bhatta (6th century) in his work Tattvopaplavasimha (The Upsetting of All Principles) refuted the Nyāya Sūtra epistemology. The materialistic Cārvāka philosophy appears to have died out some time after 1400; when Madhavacharya compiled Sarva-darśana-samgraha (A Digest of All Philosophies) in the 14th century, he had no Cārvāka (or Lokāyata) text to quote from or refer to.\nIn early 12th-century al-Andalus, Arabian philosopher Ibn Tufail (a.k.a. Abubacer) discussed materialism in his philosophical novel, Hayy ibn Yaqdhan (Philosophus Autodidactus), while vaguely foreshadowing historical materialism.\n\n\n==== Modern philosophy ====\n\nIn France, Pierre Gassendi (1592–1665) represented the materialist tradition in opposition to the attempts of René Descartes (1596–1650) to provide the natural sciences with dualist foundations. There followed the materialist and atheist abbé Jean Meslier (1664–1729), along with the French materialists: Julien Offray de La Mettrie (1709–1751), Denis Diderot (1713–1784), Étienne Bonnot de Condillac (1714–1780), Claude Adrien Helvétius (1715–1771), German-French Baron d\'Holbach (1723–1789), and other French Enlightenment thinkers.\nIn England, materialism was developed in the philosophies of Francis Bacon (1561–1626), Thomas Hobbes (1588–1679), and John Locke (1632–1704). Scottish Enlightenment philosopher David Hume (1711–1776) became one of the most important materialist philosophers in the 18th century.  John "Walking" Stewart (1747–1822) believed matter has a moral dimension, which had a major impact on the philosophical poetry of William Wordsworth (1770–1850).\nIn late modern philosophy, German atheist anthropologist Ludwig Feuerbach signaled a new turn in materialism in his 1841 book The Essence of Christianity, which presented a humanist account of religion as the outward projection of man\'s inward nature. Feuerbach introduced anthropological materialism, a version of materialism that views materialist anthropology as the universal science.\nFeuerbach\'s variety of materialism heavily influenced Karl Marx, who in the late 19th century elaborated the concept of historical materialism—the basis for what Marx and Friedrich Engels outlined as scientific socialism:\n\nThe materialist conception of history starts from the proposition that the production of the means to support human life and, next to production, the exchange of things produced, is the basis of all social structure; that in every society that has appeared in history, the manner in which wealth is distributed and society divided into classes or orders is dependent upon what is produced, how it is produced, and how the products are exchanged. From this point of view, the final causes of all social changes and political revolutions are to be sought, not in men\'s brains, not in men\'s better insights into eternal truth and justice, but in changes in the modes of production and exchange. They are to be sought, not in the philosophy, but in the economics of each particular epoch.\nThrough his Dialectics of Nature (1883), Engels later developed a "materialist dialectic" philosophy of nature, a worldview that Georgi Plekhanov, the father of Russian Marxism, called dialectical materialism. In early 20th-century Russian philosophy, Vladimir Lenin further developed dialectical materialism in his 1909 book Materialism and Empirio-criticism, which connects his opponents\' political conceptions to their anti-materialist philosophies.\nA more naturalist-oriented materialist school of thought that developed in the mid-19th century was German materialism, which included Ludwig Büchner (1824–1899), the Dutch-born Jacob Moleschott (1822–1893), and Carl Vogt (1817–1895), even though they had different views on core issues such as the evolution and the origins of life.\n\n\n=== Contemporary history ===\n\n\n==== Analytic philosophy ====\n\nContemporary analytic philosophers (e.g. Daniel Dennett, Willard Van Orman Quine, Donald Davidson, and Jerry Fodor) operate within a broadly physicalist or scientific materialist framework, producing rival accounts of how best to accommodate the mind, including functionalism, anomalous monism, and identity theory.\nScientific materialism is often synonymous with, and has typically been described as, a reductive materialism. In the early 21st century, Paul and Patricia Churchland advocated a radically contrasting position (at least in regard to certain hypotheses): eliminative materialism. Eliminative materialism holds that some mental phenomena simply do not exist at all, and that talk of such phenomena reflects a spurious "folk psychology" and introspection illusion. A materialist of this variety might believe that a concept like "belief" has no basis in fact (e.g. the way folk science speaks of demon-caused illnesses).\nWith reductive materialism at one end of a continuum (our theories will reduce to facts) and eliminative materialism at the other (certain theories will need to be eliminated in light of new facts), revisionary materialism is somewhere in the middle.\n\n\n==== Continental philosophy ====\n\nContemporary continental philosopher Gilles Deleuze has attempted to rework and strengthen classical materialist ideas. Contemporary theorists such as Manuel DeLanda, working with this reinvigorated materialism, have come to be classified as new materialists. New materialism has become its own subfield, with courses on it at major universities, as well as numerous conferences, edited collections and monographs devoted to it.\nJane Bennett\'s 2010 book Vibrant Matter has been particularly instrumental in bringing theories of monist ontology and vitalism back into a critical theoretical fold dominated by poststructuralist theories of language and discourse. Scholars such as Mel Y. Chen and Zakiyyah Iman Jackson have critiqued this body of new materialist literature for neglecting to consider the materiality of race and gender in particular.\nMétis scholar Zoe Todd, as well as Mohawk (Bear Clan, Six Nations) and Anishinaabe scholar Vanessa Watts, query the colonial orientation of the race for a "new" materialism. Watts in particular describes the tendency to regard matter as a subject of feminist or philosophical care as a tendency too invested in the reanimation of a Eurocentric tradition of inquiry at the expense of an Indigenous ethic of responsibility. Other scholars, such as Helene Vosters, echo their concerns and have questioned whether there is anything particularly "new" about "new materialism", as Indigenous and other animist ontologies have attested to what might be called the "vibrancy of matter" for centuries. Others, such as Thomas Nail, have critiqued "vitalist" versions of new materialism for depoliticizing "flat ontology" and being ahistorical.\nQuentin Meillassoux proposed speculative materialism, a post-Kantian return to David Hume also based on materialist ideas.\n\n\n== Defining "matter" ==\nThe nature and definition of matter—like other key concepts in science and philosophy—have occasioned much debate:\n\nIs there a single kind of matter (hyle) that everything is made of, or are there multiple kinds?\nIs matter a continuous substance capable of expressing multiple forms (hylomorphism) or a number of discrete, unchanging constituents (atomism)?\nDoes matter have intrinsic properties (substance theory) or lack them (prima materia)?\nOne challenge to the conventional concept of matter as tangible "stuff" came with the rise of field physics in the 19th century. Relativity shows that matter and energy (including the spatially distributed energy of fields) are interchangeable. This enables the ontological view that energy is prima materia and matter is one of its forms. In contrast, the Standard Model of particle physics uses quantum field theory to describe all interactions. On this view it could be said that fields are prima materia and the energy is a property of the field.\nAccording to the dominant cosmological model, the Lambda-CDM model, less than 5% of the universe\'s energy density is made up of the "matter" the Standard Model describes, and most of the universe is composed of dark matter and dark energy, with little agreement among scientists about what these are made of.\nWith the advent of quantum physics, some scientists believed the concept of matter had merely changed, while others believed the conventional position could no longer be maintained. Werner Heisenberg said: "The ontology of materialism rested upon the illusion that the kind of existence, the direct \'actuality\' of the world around us, can be extrapolated into the atomic range. This extrapolation, however, is impossible...atoms are not things."\nThe concept of matter has changed in response to new scientific discoveries. Thus materialism has no definite content independent of the particular theory of matter on which it is based. According to Noam Chomsky, any property can be considered material, if one defines matter such that it has that property.\nThe philosophical materialist Gustavo Bueno uses a more precise term than matter, the stroma.\nIn Materialism and Empirio-Criticism, Lenin argues that the truth of dialectical materialism is unrelated to any particular understanding of matter. To him, such changes actually confirm the dialectical form of materialism.\n\n\n== Physicalism ==\n\nGeorge Stack distinguishes between materialism and physicalism: In the twentieth century, physicalism has emerged out of positivism. Physicalism restricts meaningful statements to physical bodies or processes that are verifiable or in principle verifiable. It is an empirical hypothesis that is subject to revision and, hence, lacks the dogmatic stance of classical materialism. Herbert Feigl defended physicalism in the United States and consistently held that mental states are brain states and that mental terms have the same referent as physical terms. The twentieth century has witnessed many materialist theories of the mental, and much debate surrounding them.\nBut not all conceptions of physicalism are tied to verificationist theories of meaning or direct realist accounts of perception. Rather, physicalists believe that no "element of reality" is missing from the mathematical formalism of our best description of the world. "Materialist" physicalists also believe that the formalism describes fields of insentience. In other words, the intrinsic nature of the physical is non-experiential.\n\n\n== Religious and spiritual views ==\n\n\n=== Christianity ===\n\n\n=== Hinduism and Transcendental Club ===\n\nMost Hindus and transcendentalists regard all matter as an illusion, or maya, blinding humans from the truth. Transcendental experiences like the perception of Brahman are considered to destroy the illusion.\n\n\n== Criticism and alternatives ==\n\n\n=== From contemporary physicists ===\nRudolf Peierls, a physicist who played a major role in the Manhattan Project, rejected materialism: "The premise that you can describe in terms of physics the whole function of a human being ... including knowledge and consciousness, is untenable. There is still something missing."\nErwin Schrödinger said, "Consciousness cannot be accounted for in physical terms. For consciousness is absolutely fundamental. It cannot be accounted for in terms of anything else."\nWerner Heisenberg wrote: "The ontology of materialism rested upon the illusion that the kind of existence, the direct \'actuality\' of the world around us, can be extrapolated into the atomic range. This extrapolation, however, is impossible ... Atoms are not things."\n\n\n==== Quantum mechanics ====\nSome 20th-century physicists (e.g., Eugene Wigner and Henry Stapp), and some modern physicists and science writers (e.g., Stephen Barr, Paul Davies, and John Gribbin) have argued that materialism is flawed due to certain recent findings in physics, such as quantum mechanics and chaos theory. According to Gribbin and Davies (1991):\n\nThen came our Quantum theory, which totally transformed our image of matter. The old assumption that the microscopic world of atoms was simply a scaled-down version of the everyday world had to be abandoned. Newton\'s deterministic machine was replaced by a shadowy and paradoxical conjunction of waves and particles, governed by the laws of chance, rather than the rigid rules of causality. An extension of the quantum theory goes beyond even this; it paints a picture in which solid matter dissolves away, to be replaced by weird excitations and vibrations of invisible field energy.\nQuantum physics undermines materialism because it reveals that matter has far less "substance" than we might believe. But another development goes even further by demolishing Newton\'s image of matter as inert lumps. This development is the theory of chaos, which has recently gained widespread attention.\n\n\n==== Digital physics ====\nThe objections of Davies and Gribbin are shared by proponents of digital physics, who view information rather than matter as fundamental. The physicist and proponent of digital physics John Archibald Wheeler wrote, "all matter and all things physical are information-theoretic in origin and this is a participatory universe." Some founders of quantum theory, such as Max Planck, shared their objections. He wrote:\n\nAs a man who has devoted his whole life to the most clear headed science, to the study of matter, I can tell you as a result of my research about atoms this much: There is no matter as such. All matter originates and exists only by virtue of a force which brings the particle of an atom to vibration and holds this most minute solar system of the atom together. We must assume behind this force the existence of a conscious and intelligent Mind. This Mind is the matrix of all matter.\nJames Jeans concurred with Planck, saying, "The Universe begins to look more like a great thought than like a great machine. Mind no longer appears to be an accidental intruder into the realm of matter."\n\n\n=== Philosophical objections ===\nIn the Critique of Pure Reason, Immanuel Kant argued against materialism in defending his transcendental idealism (as well as offering arguments against subjective idealism and mind–body dualism). But Kant argues that change and time require an enduring substrate.\nPostmodern/poststructuralist thinkers also express skepticism about any all-encompassing metaphysical scheme. Philosopher Mary Midgley argues that materialism is a self-refuting idea, at least in its eliminative materialist form.\n\n\n==== Varieties of idealism ====\nArguments for idealism, such as those of Hegel and Berkeley, often take the form of an argument against materialism; indeed, Berkeley\'s idealism was called immaterialism. Now, matter can be argued to be redundant, as in bundle theory, and mind-independent properties can, in turn, be reduced to subjective percepts. Berkeley gives an example of the latter by pointing out that it is impossible to gather direct evidence of matter, as there is no direct experience of matter; all that is experienced is perception, whether internal or external. As such, matter\'s existence can only be inferred from the apparent (perceived) stability of perceptions; it finds absolutely no evidence in direct experience.\nIf matter and energy are seen as necessary to explain the physical world, but incapable of explaining mind, dualism results. Emergence, holism and process philosophy seek to ameliorate the perceived shortcomings of traditional (especially mechanistic) materialism without abandoning materialism entirely.\n\n\n=== Materialism as methodology ===\nSome critics object to materialism as part of an overly skeptical, narrow or reductivist approach to theorizing, rather than to the ontological claim that matter is the only substance. Particle physicist and Anglican theologian John Polkinghorne objects to what he calls promissory materialism—claims that materialistic science will eventually succeed in explaining phenomena it has not so far been able to explain. Polkinghorne prefers "dual-aspect monism" to materialism.\nSome scientific materialists have been criticized for failing to provide clear definitions of matter, leaving the term materialism without any definite meaning. Noam Chomsky states that since the concept of matter may be affected by new scientific discoveries, as has happened in the past, scientific materialists are being dogmatic in assuming the opposite.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\n"Materialism" . Encyclopædia Britannica. Vol. 17 (11th ed.). 1911.\nStanford Encyclopedia:\nPhysicalism\nEliminative Materialism\nPhilosophical Materialism (by Richard C. Vitzthum) from infidels.org\nDictionary of the Philosophy of Mind on Materialism from the University of Waterloo\nA new theory of ideomaterialism being a synthesis of idealism and materialism',
  },
  {
    title: "Intelligence",
    originalContent:
      'Intelligence has been defined in many ways: the capacity for abstraction, logic, understanding, self-awareness, learning, emotional knowledge, reasoning, planning, creativity, critical thinking, and problem-solving. It can be described as the ability to perceive or infer information; and to retain it as knowledge to be applied to adaptive behaviors within an environment or context.\nThe term rose to prominence during the early 1900s. Most psychologists believe that intelligence can be divided into various domains or competencies.\nIntelligence has been long-studied in humans, and across numerous disciplines. It has also been observed in the cognition of non-human animals. Some researchers have suggested that plants exhibit forms of intelligence, though this remains controversial.\nIntelligence in computers or other machines is called artificial intelligence.\n\n\n== Etymology ==\n\nThe word intelligence derives from the Latin nouns intelligentia or intellēctus, which in turn stem from the verb intelligere, to comprehend or perceive. In the Middle Ages, the word intellectus became the scholarly technical term for understanding and a translation for the Greek philosophical term nous. This term, however, was strongly linked to the metaphysical and cosmological theories of teleological scholasticism, including theories of the immortality of the soul, and the concept of the active intellect (also known as the active intelligence). This approach to the study of nature was strongly rejected by  early modern philosophers such as Francis Bacon, Thomas Hobbes, John Locke, and David Hume, all of whom preferred "understanding" (in place of "intellectus" or "intelligence") in their English philosophical works. Hobbes for example, in his Latin De Corpore, used "intellectus intelligit", translated in the English version as "the understanding understandeth", as a typical example of a logical absurdity. "Intelligence" has therefore become less common in English language philosophy, but it has later been taken up (with the scholastic theories that it now implies) in more contemporary psychology.\n\n\n== Definitions ==\nThere is controversy over how to define intelligence. Scholars describe its constituent abilities in various ways, and differ in the degree to which they conceive of intelligence as quantifiable.\nA consensus report called Intelligence: Knowns and Unknowns, published in 1995 by the Board of Scientific Affairs of the American Psychological Association, states:\n\nIndividuals differ from one another in their ability to understand complex ideas, to adapt effectively to the environment, to learn from experience, to engage in various forms of reasoning, to overcome obstacles by taking thought. Although these individual differences can be substantial, they are never entirely consistent: a given person\'s intellectual performance will vary on different occasions, in different domains, as judged by different criteria. Concepts of "intelligence" are attempts to clarify and organize this complex set of phenomena. Although considerable clarity has been achieved in some areas, no such conceptualization has yet answered all the important questions, and none commands universal assent. Indeed, when two dozen prominent theorists were recently asked to define intelligence, they gave two dozen, somewhat different, definitions.\nPsychologists and learning researchers also have suggested definitions of intelligence such as the following:\n\n\n== Human ==\n\nHuman intelligence is the intellectual power of humans, which is marked by complex cognitive feats and high levels of motivation and self-awareness. Intelligence enables humans to remember descriptions of things and use those descriptions in future behaviors. It gives humans the cognitive abilities to learn, form concepts, understand, and reason, including the capacities to recognize patterns, innovate, plan, solve problems, and employ language to communicate. These cognitive abilities can be organized into frameworks like fluid vs. crystallized and the Unified Cattell-Horn-Carroll model, which contains abilities like fluid reasoning, perceptual speed, verbal abilities, and others.\nIntelligence is different from learning. Learning refers to the act of retaining facts and information or abilities and being able to recall them for future use. Intelligence, on the other hand, is the cognitive ability of someone to perform these and other processes. \n\n\n=== Intelligence quotient (IQ) ===\n\nThere have been various attempts to quantify intelligence via psychometric testing. Prominent among these are the various Intelligence Quotient (IQ) tests, which were first developed in the early 20th century to screen children for intellectual disability. Over time, IQ tests became more pervasive, being used to screen immigrants, military recruits, and job applicants. As the tests became more popular, belief that IQ tests measure a fundamental and unchanging attribute that all humans possess became widespread. \nAn influential theory that promoted the idea that IQ measures a fundamental quality possessed by every person is the theory of General Intelligence, or g factor. The g factor is a construct that summarizes the correlations observed between an individual\'s scores on a range of cognitive tests. \nToday, most psychologists agree that IQ measures at least some aspects of human intelligence, particularly the ability to thrive in an academic context. However, many psychologists question the validity of IQ tests as a measure of intelligence as a whole.\nThere is debate about the heritability of IQ, that is, what proportion of differences in IQ test performance between individuals are explained by genetic or environmental factors. The scientific consensus is that genetics does not explain average differences in IQ test performance between racial groups.\n\n\n=== Emotional ===\n\nEmotional intelligence is thought to be the ability to convey emotion to others in an understandable way as well as to read the emotions of others accurately. Some theories imply that a heightened emotional intelligence could also lead to faster generating and processing of emotions in addition to the accuracy. In addition, higher emotional intelligence is thought to help us manage emotions, which is beneficial for our problem-solving skills. Emotional intelligence is important to our mental health and has ties to social intelligence.\n\n\n=== Social ===\n\nSocial intelligence is the ability to understand the social cues and motivations of others and oneself in social situations. It is thought to be distinct to other types of intelligence, but has relations to emotional intelligence. Social intelligence has coincided with other studies that focus on how we make judgements of others, the accuracy with which we do so, and why people would be viewed as having positive or negative social character. There is debate as to whether or not these studies and social intelligence come from the same theories or if there is a distinction between them, and they are generally thought to be of two different schools of thought.\n\n\n=== Moral ===\n\nMoral intelligence is the capacity to understand right from wrong and to behave based on the value that is believed to be right. It is considered a distinct form of intelligence, independent to both emotional and cognitive intelligence.\n\n\n=== Book smart and street smart ===\nConcepts of "book smarts" and "street smart" are contrasting views based on the premise that some people have knowledge gained through academic study, but may lack the experience to sensibly apply that knowledge, while others have knowledge gained through practical experience, but may lack accurate information usually gained through study by which to effectively apply that knowledge. Artificial intelligence researcher Hector Levesque has noted that:\n\nGiven the importance of learning through text in our own personal lives and in our culture, it is perhaps surprising how utterly dismissive we tend to be of it. It is sometimes derided as being merely "book knowledge", and having it is being "book smart". In contrast, knowledge acquired through direct experience and apprenticeship is called "street knowledge", and having it is being "street smart".\n\n\n== Nonhuman animal ==\n\nAlthough humans have been the primary focus of intelligence researchers, scientists have also attempted to investigate animal intelligence, or more broadly, animal cognition. These researchers are interested in studying both mental ability in a particular species, and comparing abilities between species. They study various measures of problem solving, as well as numerical and verbal reasoning abilities. Some challenges include defining intelligence so it has the same meaning across species, and operationalizing a measure that accurately compares mental ability across species and contexts.\nWolfgang Köhler\'s research on the intelligence of apes is an example of research in this area, as is Stanley Coren\'s book, The Intelligence of Dogs. Non-human animals particularly noted and studied for their intelligence include chimpanzees, bonobos (notably the language-using Kanzi) and other great apes, dolphins, elephants and to some extent parrots, rats and ravens.\nCephalopod intelligence provides an important comparative study. Cephalopods appear to exhibit characteristics of significant intelligence, yet their nervous systems differ radically from those of backboned animals. Vertebrates such as mammals, birds, reptiles and fish have shown a fairly high degree of intellect that varies according to each species. The same is true with arthropods.\n\n\n=== g factor in non-humans ===\n\nEvidence of a general factor of intelligence has been observed in non-human animals. First described in humans, the g factor has since been identified in a number of non-human species.\nCognitive ability and intelligence cannot be measured using the same, largely verbally dependent, scales developed for humans. Instead, intelligence is measured using a variety of interactive and observational tools focusing on innovation, habit reversal, social learning, and responses to novelty. Studies have shown that g is responsible for 47% of the individual variance in cognitive ability measures in primates and between 55% and 60% of the variance in mice (Locurto, Locurto). These values are similar to the accepted variance in IQ explained by g in humans (40–50%).\n\n\n== Plant ==\n\nIt has been argued that plants should also be classified as intelligent based on their ability to sense and model external and internal environments and adjust their morphology, physiology and phenotype accordingly to ensure self-preservation and reproduction.\nA counter argument is that intelligence is commonly understood to involve the creation and use of persistent memories as opposed to computation that does not involve learning. If this is accepted as definitive of intelligence, then it includes the artificial intelligence of robots capable of "machine learning", but excludes those purely autonomic sense-reaction responses that can be observed in many plants. Plants are not limited to automated sensory-motor responses, however, they are capable of discriminating positive and negative experiences and of "learning" (registering memories) from their past experiences. They are also capable of communication, accurately computing their circumstances, using sophisticated cost–benefit analysis and taking tightly controlled actions to mitigate and control the diverse environmental stressors.\n\n\n== Artificial ==\n\nScholars studying artificial intelligence have proposed definitions of intelligence that include the intelligence demonstrated by machines. Some of these definitions are meant to be general enough to encompass human and other animal intelligence as well. An intelligent agent can be defined as a system that perceives its environment and takes actions which maximize its chances of success. Kaplan and Haenlein define artificial intelligence as "a system\'s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation". Progress in artificial intelligence can be demonstrated in benchmarks ranging from games to practical tasks such as protein folding. Existing AI lags humans in terms of general intelligence, which is sometimes defined as the "capacity to learn how to carry out a huge range of tasks".\nMathematician Olle Häggström defines intelligence in terms of "optimization power", an agent\'s capacity for efficient cross-domain optimization of the world according to the agent\'s preferences, or more simply the ability to "steer the future into regions of possibility ranked high in a preference ordering". In this optimization framework, Deep Blue has the power to "steer a chessboard\'s future into a subspace of possibility which it labels as \'winning\', despite attempts by Garry Kasparov to steer the future elsewhere." Hutter and Legg, after surveying the literature, define intelligence as "an agent\'s ability to achieve goals in a wide range of environments". While cognitive ability is sometimes measured as a one-dimensional parameter, it could also be represented as a "hypersurface in a multidimensional space" to compare systems that are good at different intellectual tasks. Some skeptics believe that there is no meaningful way to define intelligence, aside from "just pointing to ourselves".\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nGleick, James, "The Fate of Free Will" (review of Kevin J. Mitchell, Free Agents: How Evolution Gave Us Free Will, Princeton University Press, 2023, 333 pp.), The New York Review of Books, vol. LXXI, no. 1 (18 January 2024), pp. 27–28, 30. "Agency is what distinguishes us from machines. For biological creatures, reason and purpose come from acting in the world and experiencing the consequences. Artificial intelligences – disembodied, strangers to blood, sweat, and tears – have no occasion for that." (p. 30.)\nHughes-Castleberry, Kenna, "A Murder Mystery Puzzle: The literary puzzle Cain\'s Jawbone, which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms", Scientific American, vol. 329, no. 4 (November 2023), pp. 81–82. "This murder mystery competition has revealed that although NLP (natural-language processing) models are capable of incredible feats, their abilities are very much limited by the amount of context they receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyze ancient languages. In some cases, there are few historical records on long-gone civilizations to serve as training data for such a purpose." (p. 82.)\nImmerwahr, Daniel, "Your Lying Eyes: People now use A.I. to generate fake videos indistinguishable from real ones. How much does it matter?", The New Yorker, 20 November 2023, pp. 54–59. "If by \'deepfakes\' we mean realistic videos produced using artificial intelligence that actually deceive people, then they barely exist. The fakes aren\'t deep, and the deeps aren\'t fake. [...] A.I.-generated videos are not, in general, operating in our media as counterfeited evidence. Their role better resembles that of cartoons, especially smutty ones." (p. 59.)\nPress, Eyal, "In Front of Their Faces: Does facial-recognition technology lead police to ignore contradictory evidence?", The New Yorker, 20 November 2023, pp. 20–26.\nRoivainen, Eka, "AI\'s IQ: ChatGPT aced a [standard intelligence] test but showed that intelligence cannot be measured by IQ alone", Scientific American, vol. 329, no. 1 (July/August 2023), p. 7. "Despite its high IQ, ChatGPT fails at tasks that require real humanlike reasoning or an understanding of the physical and social world.... ChatGPT seemed unable to reason logically and tried to rely on its vast database of... facts derived from online texts."\nCukier, Kenneth, "Ready for Robots? How to Think about the Future of AI", Foreign Affairs, vol. 98, no. 4 (July/August 2019), pp. 192–98. George Dyson, historian of computing, writes (in what might be called "Dyson\'s Law") that "Any system simple enough to be understandable will not be complicated enough to behave intelligently, while any system complicated enough to behave intelligently will be too complicated to understand." (p. 197.) Computer scientist Alex Pentland writes: "Current AI machine-learning algorithms are, at their core, dead simple stupid. They work, but they work by brute force." (p. 198.)\nDomingos, Pedro, "Our Digital Doubles: AI will serve our species, not control it", Scientific American, vol. 319, no. 3 (September 2018), pp. 88–93. "AIs are like autistic savants and will remain so for the foreseeable future.... AIs lack common sense and can easily make errors that a human never would... They are also liable to take our instructions too literally, giving us precisely what we asked for instead of what we actually wanted." (p. 93.)\nMarcus, Gary, "Am I Human?: Researchers need new ways to distinguish artificial intelligence from the natural kind", Scientific American, vol. 316, no. 3 (March 2017), pp. 61–63. Marcus points out a so far insuperable stumbling block to artificial intelligence: an incapacity for reliable disambiguation. "[V]irtually every sentence [that people generate] is ambiguous, often in multiple ways. Our brain is so good at comprehending language that we do not usually notice." A prominent example is the "pronoun disambiguation problem" ("PDP"): a machine has no way of determining to whom or what a pronoun in a sentence—such as "he", "she" or "it"—refers.\nSternberg, Robert J.; Kaufman, Scott Barry, eds. (2011). The Cambridge Handbook of Intelligence. Cambridge: Cambridge University Press. doi:10.1017/9781108770422. ISBN 978-0521739115. S2CID 241027150.\nMackintosh, N. J. (2011). IQ and Human Intelligence (second ed.). Oxford: Oxford University Press. ISBN 978-0-19-958559-5.\nFlynn, James R. (2009). What Is Intelligence: Beyond the Flynn Effect (expanded paperback ed.). Cambridge: Cambridge University Press. ISBN 978-0-521-74147-7.\nLay summary in: C Shalizi (27 April 2009). "What Is Intelligence? Beyond the Flynn Effect". University of Michigan (Review). Archived from the original on 14 June 2010.\nStanovich, Keith (2009). What Intelligence Tests Miss: The Psychology of Rational Thought. New Haven (CT): Yale University Press. ISBN 978-0-300-12385-2.\nLay summary in: Jamie Hale. "What Intelligence Tests Miss". Psych Central (Review). Archived from the original on 24 December 2013.\nBlakeslee, Sandra; Hawkins, Jeff (2004). On intelligence. New York: Times Books. ISBN 978-0-8050-7456-7. OCLC 55510125.\nBock, Gregory; Goode, Jamie; Webb, Kate, eds. (2000). The Nature of Intelligence. Novartis Foundation Symposium 233. Vol. 233. Chichester: Wiley. doi:10.1002/0470870850. ISBN 978-0471494348.\nLay summary in: William D. Casebeer (30 November 2001). "The Nature of Intelligence". Mental Help (Review). Archived from the original on 26 May 2013.\nWolman, Benjamin B., ed. (1985). Handbook of Intelligence. consulting editors: Douglas K. Detterman, Alan S. Kaufman, Joseph D. Matarazzo. New York: Wiley. ISBN 978-0-471-89738-5.\nTerman, Lewis Madison; Merrill, Maude A. (1937). Measuring intelligence: A guide to the administration of the new revised Stanford-Binet tests of intelligence. Riverside textbooks in education. Boston (MA): Houghton Mifflin. OCLC 964301.\nBinet, Alfred; Simon, Th. (1916). The development of intelligence in children: The Binet-Simon Scale. Publications of the Training School at Vineland New Jersey Department of Research No. 11. E. S. Kite (Trans.). Baltimore: Williams & Wilkins. p. 1. Retrieved 18 July 2010.\n\n\n== External links ==\n\nIntelligence on In Our Time at the BBC\nHistory of Influences in the Development of Intelligence Theory and Testing. Archived 11 November 2007 at the Wayback Machine. Developed by Jonathan Plucker at Indiana University.\nThe Limits of Intelligence: The laws of physics may well prevent the human brain from evolving into an ever more powerful thinking machine. By Douglas Fox in Scientific American, 14 June 2011.\nA Collection of Definitions of Intelligence',
  },
  {
    title: "Learning theory",
    originalContent:
      "Learning theory may refer to:\n\n\n== Education ==\nLearning theory (education), the process of how humans learn\nConnectivism\nEducational philosophies, an academic field that examines the definitions, goals and meaning of education, or of specific educational philosophies.\nBehaviorism (philosophy of education)\nCognitivism (philosophy of education)\nConstructivism (philosophy of education)\nHumanism (philosophy of education)\nE-learning (theory), a cognitive science of effective multimedia e-learning\nInstructional theory\nSocial cognitive theory\nSocial learning theory\n\n\n== Computer science ==\nAlgorithmic learning theory, a branch of computational learning theory. Sometimes also referred to as algorithmic inductive inference.\nComputational learning theory, a mathematical theory to analyze machine learning algorithms.\nOnline machine learning, the process of teaching a machine.\nStatistical learning theory",
  },
  {
    title: "Depression",
    originalContent:
      "Depression may refer to:\n\n\n== Mental health ==\nDepression (mood), a state of low mood and aversion to activity\nMood disorders characterized by depression are commonly referred to as simply depression, including:\nMajor depressive disorder, also known as clinical depression\nBipolar disorder, also known as manic depression\nDysthymia, also known as persistent depressive disorder\n\n\n== Economics ==\nEconomic depression, a sustained, long-term downturn in economic activity in one or more economies\nGreat Depression, a severe economic depression during the 1930s, commonly referred to as simply the Depression\nLong Depression, an economic depression during 1873–96, known at the time as the Great Depression\n\n\n== Biology ==\nDepression (kinesiology), an anatomical term of motion, refers to downward movement, the opposite of elevation\nDepression (physiology), a reduction in a biological variable or the function of an organ\nCentral nervous system depression, physiological depression of the central nervous system that can result in loss of consciousness\n\n\n== Earth science ==\nDepression (geology), a landform sunken or depressed below the surrounding area\nDepression (weather), an area of low atmospheric pressure characterized by rain and unstable weather",
  },
  {
    title: "Counseling",
    originalContent:
      "Counseling is the professional guidance of the individual by utilizing psychological methods especially in collecting case history data, using various techniques of the personal interview, and testing interests and aptitudes.\nThis is a list of counseling topics.\n\n\n== Therapeutic modalities ==\n\n\n== Common areas ==\n\n\n== See also ==\nList of psychotherapies\nOutline of communication\nOutline of psychology\nOutline of sociology\nSubfields of sociology\nOutline of self\nPsychopharmacology\n\n\n== References ==",
  },
  {
    title: "Family therapy",
    originalContent:
      'Family therapy (also referred to as family counseling, family systems therapy, marriage and family therapy, couple and family therapy) is a branch of psychotherapy focused on families and couples in intimate relationships to nurture change and development. It tends to view change in terms of the systems of interaction between family members.\nThe different schools of family therapy have in common a belief that, regardless of the origin of the problem, and regardless of whether the clients consider it an "individual" or "family" issue, involving families in solutions often benefits clients. This involvement of families is commonly accomplished by their direct participation in the therapy session. The skills of the family therapist thus include the ability to influence conversations in a way that catalyses the strengths, wisdom, and support of the wider system.\nIn the field\'s early years, many clinicians defined the family in a narrow, traditional manner usually including parents and children. As the field has evolved, the concept of the family is more commonly defined in terms of strongly supportive, long-term roles and relationships between people who may or may not be related by blood or marriage.\nThe conceptual frameworks developed by family therapists, especially those of\nfamily systems theorists, have been applied to a wide range of human behavior, including organisational dynamics and the study of greatness.\n\n\n== History and theoretical frameworks ==\nFormal interventions with families to help individuals and families experiencing various kinds of problems have been a part of many cultures, probably throughout history. These interventions have sometimes involved formal procedures or rituals, and often included the extended family as well as non-kin members of the community (see for example Ho\'oponopono). Following the emergence of specialization in various societies, these interventions were often conducted by particular members of a community – for example, a chief, priest, physician, and so on – usually as an ancillary function.\nFamily therapy as a distinct professional practice within Western cultures can be argued to have had its origins in the social work movements of the 19th century in the United Kingdom and the United States. As a branch of psychotherapy, its roots can be traced somewhat later to the early 20th century with the emergence of the child guidance movement and marriage counseling. The formal development of family therapy dates from the 1940s and early 1950s with the founding in 1942 of the American Association of Marriage Counselors (the precursor of the AAMFT), and through the work of various independent clinicians and groups – in the United Kingdom (John Bowlby at the Tavistock Clinic), the United States (Donald deAvila Jackson, John Elderkin Bell, Nathan Ackerman, Christian Midelfort, Theodore Lidz, Lyman Wynne, Murray Bowen, Carl Whitaker, Virginia Satir, Ivan Boszormenyi-Nagy), and in Hungary, D.L.P. Liebermann – who began seeing family members together for observation or therapy sessions. There was initially a strong influence from psychoanalysis (most of the early founders of the field had psychoanalytic backgrounds) and social psychiatry, and later from learning theory and behavior therapy – and significantly, these clinicians began to articulate various theories about the nature and functioning of the family as an entity that was more than a mere aggregation of individuals.\nThe movement received an important boost starting in the early 1950s through the work of anthropologist Gregory Bateson and colleagues – Jay Haley, Donald D. Jackson, John Weakland, William Fry, and later, Virginia Satir, Ivan Boszormenyi-Nagy, Paul Watzlawick and others – at Palo Alto in the United States, who introduced ideas from cybernetics and general systems theory into social psychology and psychotherapy, focusing in particular on the role of communication (see Bateson Project). This approach eschewed the traditional focus on individual psychology and historical factors – that involve so-called linear causation and content – and emphasized instead feedback and homeostatic mechanisms and "rules" in here-and-now interactions – so-called circular causation and process – that were thought to maintain or exacerbate problems, whatever the original cause(s). (See also systems psychology and systemic therapy.)  This group was also influenced significantly by the work of US psychiatrist, hypnotherapist, and brief therapist Milton H. Erickson – especially his innovative use of strategies for change, such as paradoxical directives The members of the Bateson Project (like the founders of a number of other schools of family therapy, including Carl Whitaker, Murray Bowen, and Ivan Boszormenyi-Nagy) had a particular interest in the possible psychosocial causes and treatment of schizophrenia, especially in terms of the putative "meaning" and "function" of signs and symptoms within the family system. The research of psychiatrists and psychoanalysts Lyman Wynne and Theodore Lidz on communication deviance and roles (e.g., pseudo-mutuality, pseudo-hostility, schism and skew) in families of people with schizophrenia also became influential with systems-communications-oriented theorists and therapists. A related theme, applying to dysfunction and psychopathology more generally, was that of the "identified patient" or "presenting problem" as a manifestation of or surrogate for the family\'s, or even society\'s, problems. (See also double bind; family nexus.)\nBy the mid-1960s, a number of distinct schools of family therapy had emerged. From those groups that were most strongly influenced by cybernetics and systems theory, there came MRI Brief Therapy, and slightly later, strategic therapy, Salvador Minuchin\'s structural family therapy and the Milan systems model. Partly in reaction to some aspects of these systemic models, came the experiential approaches of Virginia Satir and Carl Whitaker, which downplayed theoretical constructs, and emphasized subjective experience and unexpressed feelings (including the subconscious), authentic communication, spontaneity, creativity, total therapist engagement, and often included the extended family. Concurrently and somewhat independently, there emerged the various intergenerational therapies of Murray Bowen, Ivan Boszormenyi-Nagy, James Framo, and Norman Paul, which present different theories about the intergenerational transmission of health and dysfunction, but which all deal usually with at least three generations of a family (in person or conceptually), either directly in therapy sessions, or via "homework", "journeys home", etc. Psychodynamic family therapy – which, more than any other school of family therapy, deals directly with individual psychology and the unconscious in the context of current relationships – continued to develop through a number of groups that were influenced by the ideas and methods of Nathan Ackerman, and also by the British School of Object Relations and John Bowlby\'s work on attachment. Multiple-family group therapy, a precursor of psychoeducational family intervention, emerged, in part, as a pragmatic alternative form of intervention – especially as an adjunct to the treatment of serious mental disorders with a significant biological basis, such as schizophrenia – and represented something of a conceptual challenge to some of the systemic (and thus potentially "family-blaming") paradigms of pathogenesis that were implicit in many of the dominant models of family therapy. The late 1960s and early 1970s saw the development of network therapy (which bears some resemblance to traditional practices such as Ho\'oponopono) by Ross Speck and Carolyn Attneave, and the emergence of behavioral marital therapy (renamed behavioral couples therapy in the 1990s) and behavioral family therapy as models in their own right.\nBy the late 1970s, the weight of clinical experience – especially in relation to the treatment of serious mental disorders – had led to some revision of a number of the original models and a moderation of some of the earlier stridency and theoretical purism. There were the beginnings of a general softening of the strict demarcations between schools, with moves toward rapprochement, integration, and eclecticism – although there was, nevertheless, some hardening of positions within some schools. These trends were reflected in and influenced by lively debates within the field and critiques from various sources, including feminism and post-modernism, that reflected in part the cultural and political tenor of the times, and which foreshadowed the emergence (in the 1980s and 1990s) of the various post-systems constructivist and social constructionist approaches. While there was still debate within the field about whether, or to what degree, the systemic-constructivist and medical-biological paradigms were necessarily antithetical to each other (see also Anti-psychiatry; Biopsychosocial model), there was a growing willingness and tendency on the part of family therapists to work in multi-modal clinical partnerships with other members of the helping and medical professions.\nFrom the mid-1980s to the present, the field has been marked by a diversity of approaches that partly reflect the original schools, but which also draw on other theories and methods from individual psychotherapy and elsewhere – these approaches and sources include: brief therapy, structural therapy, constructivist approaches (e.g., Milan systems, post-Milan/collaborative/conversational, reflective), Bring forthism approach (e.g. Dr. Karl Tomm\'s IPscope model and Interventive interviewing), solution-focused therapy, narrative therapy, a range of cognitive and behavioral approaches, psychodynamic and object relations approaches, attachment and emotionally focused therapy, intergenerational approaches, network therapy, and multisystemic therapy (MST). Multicultural, intercultural, and integrative approaches are being developed, with Vincenzo Di Nicola weaving a synthesis of family therapy and transcultural psychiatry in his model of cultural family therapy, A Stranger in the Family: Culture, Families, and Therapy. Many practitioners claim to be eclectic, using techniques from several areas, depending upon their own inclinations and/or the needs of the client(s), and there is a growing movement toward a single "generic" family therapy that seeks to incorporate the best of the accumulated knowledge in the field and which can be adapted to many different contexts; however, there are still a significant number of therapists who adhere more or less strictly to a particular, or limited number of, approach(es).\nThe Liberation Based Healing framework for family therapy offers a complete paradigm shift for working with families while addressing the intersections of race, class, gender identity, sexual orientation and other socio-political identity markers. This theoretical approach and praxis is informed by critical pedagogy, feminism, critical race theory, and decolonizing theory.  This framework necessitates an understanding of the ways colonization, cis-heteronormativity, patriarchy, white supremacy and other systems of domination impact individuals, families and communities and centers the need to disrupt the status quo in how power operates. Traditional Western models of family therapy have historically ignored these dimensions and when white, male privilege has been critiqued, largely by feminist theory practitioners, it has often been to the benefit of middle-class, white women\'s experiences.  While an understanding of intersectionality is of particular significance in working with families with violence, a liberatory framework examines how power, privilege and oppression operate within and across all relationships. Liberatory practices are based on the principles of critical consciousness, Accountability and Empowerment. These principles guide not only the content of the therapeutic work with clients but also the supervisory and training process of therapists. Dr. Rhea Almeida developed the cultural context model as a way to operationalize these concepts into practice through the integration of culture circles, sponsors, and a socio-educational process within the therapeutic work.\nIdeas and methods from family therapy have been influential in psychotherapy generally: a survey of over 2,500 US therapists in 2006 revealed that of the 10 most influential therapists of the previous quarter-century, three were prominent family therapists and that the marital and family systems model was the second most utilized model after cognitive behavioral therapy.\n\n\n== Techniques ==\nFamily therapy uses a range of counseling and other techniques including:\n\nStructural therapy – identifies and re-orders the organisation of the family system\nStrategic therapy – looks at patterns of interactions between family members\nSystemic/Milan therapy – focuses on belief systems\nNarrative therapy – restorying of dominant problem-saturated narrative, emphasis on context, separation of the problem from the person\nTransgenerational therapy – transgenerational transmission of unhelpful patterns of belief and behaviour\nIPscope model and Interventive Interviewing\nCommunication theory\nPsychoeducation\nPsychotherapy\nRelationship counseling\nRelationship education\nSystemic coaching\nSystems theory\nReality therapy\nthe genogram\nThe number of sessions depends on the situation, but the average is 5–20 sessions. A family therapist usually meets several members of the family at the same time. This has the advantage of making differences between the ways family members perceive mutual relations as well as interaction patterns in the session apparent both for the therapist and the family. These patterns frequently mirror habitual interaction patterns at home, even though the therapist is now incorporated into the family system. Therapy interventions usually focus on relationship patterns rather than on analyzing impulses of the unconscious mind or early childhood trauma of individuals as a Freudian therapist would do – although some schools of family therapy, for example psychodynamic and intergenerational, do consider such individual and historical factors (thus embracing both linear and circular causation) and they may use instruments such as the genogram to help to elucidate the patterns of relationship across generations.\nThe distinctive feature of family therapy is its perspective and analytical framework rather than the number of people present at a therapy session.  Specifically, family therapists are relational therapists:  They are generally more interested in what goes on between individuals rather than within one or more individuals, although some family therapists – in particular those who identify as psychodynamic, object relations, intergenerational, or experiential family therapists (EFTs) – tend to be as interested in individuals as in the systems those individuals and their relationships constitute.  Depending on the conflicts at issue and the progress of therapy to date, a therapist may focus on analyzing specific previous instances of conflict, as by reviewing a past incident and suggesting alternative ways family members might have responded to one another during it, or instead proceed directly to addressing the sources of conflict at a more abstract level, as by pointing out patterns of interaction that the family might have not noticed.\nFamily therapists tend to be more interested in the maintenance and/or solving of problems rather than in trying to identify a single cause.  Some families may perceive cause-effect analyses as attempts to allocate blame to one or more individuals, with the effect that for many families a focus on causation is of little or no clinical utility. It is important to note that a circular way of problem evaluation is used as opposed to a linear route. Using this method, families can be helped by finding patterns of behaviour, what the causes are, and what can be done to better their situation.\n\n\n== Evidence base ==\nFamily therapy has an evolving evidence base. A summary of current evidence is available via the UK\'s Association of Family Therapy. Evaluation and outcome studies can also be found on the Family Therapy and Systemic Research Centre website. The website also includes quantitative and qualitative research studies of many aspects of family therapy.\nAccording to a 2004 French government study conducted by French Institute of Health and Medical Research, family and couples therapy was the second most effective therapy after Cognitive behavioral therapy. The study used meta-analysis of over a hundred secondary studies to find some level of effectiveness that was either "proven" or "presumed" to exist. Of the treatments studied, family therapy was presumed or proven effective at treating schizophrenia, bipolar disorder, anorexia and alcohol dependency.\n\n\n== Concerns and criticism ==\nIn a 1999 address to the Coalition of Marriage, Family and Couples Education conference in Washington, D.C., University of Minnesota Professor William Doherty said:\n\nI take no joy in being a whistle blower, but it\'s time. I am a committed marriage and family therapist, having practiced this form of therapy since 1977. I train marriage and family therapists. I believe that marriage therapy can be very helpful in the hands of therapists who are committed to the profession and the practice. But there are a lot of problems out there with the practice of therapy – a lot of problems.\nDoherty suggested questions prospective clients should ask a therapist before beginning treatment:\n\n"Can you describe your background and training in marital therapy?"\n"What is your attitude toward salvaging a troubled marriage versus helping couples break up?"\n"What is your approach when one partner is seriously considering ending the marriage and the other wants to save it?"\n"What percentage of your practice is marital therapy?"\n"Of the couples you treat, what percentage would you say work out enough of their problems to stay married with a reasonable amount of satisfaction with the relationship." "What percentage break up while they are seeing you?" "What percentage do not improve?" "What do you think makes the differences in these results?"\n\n\n== Licensing and degrees ==\nFamily therapy practitioners come from a range of professional backgrounds, and some are specifically qualified or licensed/registered in family therapy (licensing is not required in some jurisdictions and requirements vary from place to place). In the United Kingdom, family therapists will have a prior relevant professional training in one of the helping professions usually psychologists, psychotherapists, or counselors who have done further training in family therapy, either a diploma or an M.Sc. In the United States there is a specific degree and license as a marriage and family therapist; however, psychologists, nurses, psychotherapists, social workers, or counselors, and other licensed mental health professionals may practice family therapy. In the UK, family therapists who have completed a four-year qualifying programme of study (MSc) are eligible to register with the professional body the Association of Family Therapy (AFT), and with the UK Council for Psychotherapy (UKCP).\nA master\'s degree is required to work as a Marriage and Family Therapist (MFT) in some American states. Most commonly, MFTs will first earn a M.S. or M.A. degree in marriage and family therapy, counseling, psychology, family studies, or social work. After graduation, prospective MFTs work as interns under the supervision of a licensed professional and are referred to as an MFTi.\nPrior to 1999 in California, counselors who specialized in this area were called Marriage, Family and Child Counselors. Today, they are known as Marriage and Family Therapists (MFT), and work variously in private practice, in clinical settings such as hospitals, institutions, or counseling organizations.\nMarriage and family therapists in the United States and Canada often seek degrees from accredited Masters or Doctoral programs recognized by the Commission on Accreditation for Marriage and Family Therapy Education (COAMFTE), a division of the American Association of Marriage and Family Therapy.\nRequirements vary, but in most states about 3000 hours of supervised work as an intern are needed to sit for a licensing exam. MFTs must be licensed by the state to practice. Only after completing their education and internship and passing the state licensing exam can a person call themselves a Marital and Family Therapist and work unsupervised.\nLicense restrictions can vary considerably from state to state.  Contact information about licensing boards in the United States are provided by the Association of Marital and Family Regulatory Boards.\nThere have been concerns raised within the profession about the fact that specialist training in couples therapy – as distinct from family therapy in general – is not required to gain a license as an MFT or membership of the main professional body, the AAMFT.\n\n\n=== Values and ethics ===\nSince issues of interpersonal conflict, power, control, values, and ethics are often more pronounced in relationship therapy than in individual therapy, there has been debate within the profession about the different values that are implicit in the various theoretical models of therapy and the role of the therapist\'s own values in the therapeutic process, and how prospective clients should best go about finding a therapist whose values and objectives are most consistent with their own. An early paper on ethics in family therapy written by Vincenzo Di Nicola in consultation with a bioethicist asked basic questions about whether strategic interventions "mean what they say" and if it is ethical to invent opinions offered to families about the treatment process, such as statements saying that half of the treatment team believes one thing and half believes another. Specific issues that have emerged have included an increasing questioning of the longstanding notion of therapeutic neutrality, a concern with questions of justice and self-determination, connectedness and independence, functioning versus authenticity, and questions about the degree of the therapist\'s pro-marriage/family versus pro-individual commitment.\nThe American Association for Marriage and Family Therapy requires members to adhere to a code of ethics, including a commitment to "continue therapeutic relationships only so long as it is reasonably clear that clients are benefiting from the relationship."\n\n\n== Founders and key influences ==\nSome key developers of family therapy are:\n\n\n== Summary of theories and techniques ==\n(references:)\n\n\n== Journals ==\nAustralian and New Zealand Journal of Family Therapy\nContemporary Family Therapy\nFamily Process\nFamily Relations\nFamily Relations, Interdisciplinary Journal of Applied Family Studies ISSN 0197-6664\nJournal of Family Therapy\nMarriage Fitness\nMurmurations: Journal of Transformative Systemic Practice\nSexual and Relationship Therapy\nJournal of Marital & Family Therapy\nFamilies, Systems and Health\n\n\n== See also ==\n\n\n== Footnotes ==\n\n\n== Further reading ==\n\nDeborah Weinstein, The Pathological Family: Postwar America and the Rise of Family Therapy. Ithaca, NY: Cornell University Press, 2013.\nSatir, V., Banmen, J., Gerber, J., & Gomori, M. (1991). The Satir Model: Family Therapy and Beyond. Palo Alto, CA: Science and Behavior Books.\nThe Systemic Thinking and Practice Series. Routledge\nGehring, T. M., Debry, M. & Smith, P. K. (Eds.). (2016). The Family System Test FAST. Theory and application. Hove: Brunner-Routledge.',
  },
  {
    title: "Child therapy",
    originalContent:
      'Child psychotherapy, or mental health interventions for children refers to the psychological treatment of various mental disorders diagnosed in children and adolescents. The therapeutic techniques developed for younger age ranges specialize in prioritizing the relationship between the child and the therapist. The goal of maintaining positive therapist-client relationships is typically achieved using therapeutic conversations and can take place with the client alone, or through engagement with family members.\nThe term, "psychotherapy" includes the implementation of educational and psychoanalytic support for the client and is effective in problem-solving, emotional regulation, and encouraging pro-social behaviors as children develop positive changes to their current mindsets. Terms describing child-focused treatments may vary from one part of the world to another, with particular differences in the use of such terms, as "therapy", "child psychotherapy" or "child analysis"."\n\n\n== Evolution of child psychotherapy ==\nChild Psychotherapy has developed varied approaches over the last century. Two distinct historic pathways can be identified for present-day provision in Western Europe and in the United States: one through the Child Guidance Movement, the other stemming from adult psychiatry or psychological medicine, which evolved a separate child psychiatry specialism.\n\n\n=== The separation of child and adult psychology ===\nThe attempt to create a unified method of child mental health care led to the increase of child guidance clinics in England throughout the mid-twentieth century. The spread of clinics across Europe coincided with the absence of hospital care as the lack of distinction between child and adult psychiatry prevented further analysis of child diagnosis and treatment. The first Chair of Child Psychiatry officially coined the term, Child and Adolescent Psychiatry in 1973, but it was not until the DSM-III where a full list of distinct child psychiatric disorders were mentioned. \n\n\n== Psychoanalytic child psychotherapy ==\nPsychoanalytic psychotherapy with infants, children and adolescents is mainly delivered by people qualified specifically in psychoanalytic child psychotherapy, or by trainees under supervision from a specialist in child-focused treatment. Recent evidence, covering 34 research papers (nine of which were randomized controlled trials) showed psychoanalytic psychotherapy to be particularly effective for children with the following conditions:\n\ndepression\nanxiety and behavior disorders\npersonality disorders\nlearning difficulties\neating disorders\ndevelopmental issues\nFurthermore, follow-up research shows that in psychoanalytic psychotherapy, therapeutic improvements continue well beyond the termination of the therapy itself. This has been termed a, "sleeper effect."\nIn the UK, psychoanalytic psychotherapy is recommended by NICE as an evidence-based treatment for trauma from sexual abuse and severe depression in adolescents following the IMPACT study.\n\n\n== Evidence-based child and adolescent psychiatry ==\nThere are various therapeutic assessments to address mental health concerns among children and adolescents. Some approaches are backed by strong scientific evidence, while some are not. Some research suggests that it is the quality of the relationship with the therapist, rather than the particular form of therapeutic intervention, that is the strongest factor in helping change develop.\n\n\n=== Parent–infant psychotherapy ===\nIf the normal course of secure attachment between parent and infant is disrupted, parent–infant psychotherapy is a catch-all term to describe psychotherapies that either aim to restore this bond or to work with vulnerable parents to overcome disruption and prevent further occurrence. Examples of this kind of therapy include, "Watch, Wait, Wonder," and psychoanalytic parent-infant psychotherapy. Many of these techniques require a three-way relationship between the parent, child, and therapist. During therapy sessions, the parent may express his or her thoughts and feelings which are based on a combination of factors including:\n\nThe parent\'s experiences as a child\nThe parent\'s expectations and hopes for the child\'s future\nThe relationships the parent has with other people\nThe therapist\'s role is as an observer and an interpreter of the interaction between the infant and the parent. He might share some of his thoughts about the behavior of the child with the parent and by doing so offering the parent an alternative way of experiencing the child. This technique helps the parent to resolve issues with his or her own infancy-experiences in order to restore secure attachment with the infant. And it helps lower the risk for psychopathological developments of the child in the future.\n\n\n== Group art therapy ==\nGroup art therapy gives the child a safe environment to access their emotions through a creative medium in the presence of a therapist. This nonverbal therapeutic practice alleviates the stress that a child may feel when trying to find the words to express themselves; thus it helps rebuild social skills and gain trust in others. Studies have also found that this practice can alleviate self-harm engagement. This method of psychotherapy has been found particularly helpful for children who exhibit any of the following:\n\nAutism\nAsperger\'s\nAnxiety and behavior disorders\nGroup art therapy has eight subcategories of specific mechanisms of change. Among them are:\n\nAs a form of expression to reveal what\'s inside\nAs a way of becoming aware of oneself\na way to form a narrative of life\nintegrative activation of the brain through experience\na form of exploration and/or reflection\nthe specifics of the art materials/techniques offered in art therapy\nas a form to practice and/or learn skills\nart therapy as an easily accessible, positive and safe intervention by the use of art materials\nBy bundling together these specific groups, the general groups are as follows:\n\nart therapy as a form of group process\nthe therapeutic alliance in art therapy\nWithin this approach, three types of behaviors can be exhibited by the therapist; non-directive, directive, and eclectic. Non-directive refers to a following behavior in which the therapist takes on an attitude of observing self-exploration of emotions rather than facilitation or interpretation. Directive attitudes however follow a facilitative pattern by asking specific questions to guide the clients artwork. With these two processes in mind, eclectic combines them to create a facilitative and lenient approach simultaneously and often utilizes emotion check-in\'s at the start of sessions, and emotion check-outs at the end of sessions.\nThis approach adopts various psychological elements such as psycho-educational, mindfulness, psychoanalysis, and cognitive analytic theories. This article sought to analyze this methods effectiveness on a broad spectrum, including the following:\n\ntraumatic events (PTSD)\nwho have educational needs or disabilities\nchildren with medical conditions\nchildren with none of the former\njuvenile offenders\nArt therapy can be implemented as a holistic therapeutic practice for child cancer patients as well (effecting 1 in 285 children in the US; 15,980 children each year). Given the alleviating effects that are addressed by this method, children were better able to discuss their needs and emotions to their family members and healthcare team. The results of this study conveyed that art therapy lead to improved emotional and mental well-being and improved communication skills.\n\n\n== Parent–child interaction therapy (PCIT) ==\nParent–child interaction therapy is meant to assist parents whom have children ages 2–7 years old who are prone to disruptive behaviors and emotional difficulties. Parent–child therapy utilizing two stages, each possessing their own goals and characteristics to create this approach. Beginning with child-directed interaction (CDI), parents learn skills such as praise, verbal reflection, imitation, behavioral description, and enjoyment, to achieve the goal of warm and secure parenting styles. Parent-Directed interaction (PDI), the second phase, seeks to decrease the original disruptive behaviors exhibited by the child. Both phases are designed to be coached by the therapist via another room while the parent interacts with their child.  This review found that certain cultural values may impede or contribute to the progress of this approach.\n\n\n== Challenges of child psychotherapy ==\n\n\n=== Disregarding suppressed behaviors ===\nTherapeutic interventions among children and adolescents are subject to specific challenges, many of which stem from the reliance of family members as a result of the clients lack of independency at the current stage in their lives. Unlike adult psychotherapy, it is rare for a client to seek treatment themselves in child psychotherapy. The involvement of parents in treatment referral often leads to the frequent disregard suppressed behavioral or emotional problems such as anxiety and depression with the majority of referrals relating to disruptive behaviors.\n\n\n=== Lack of motivation ===\nThe child-parent dynamic in psychotherapy also has the tendency to increase disagreements regarding treatment processes. Children may be hesitant to accept the idea of undergoing psychotherapy if they were forced into it by a third party. This reluctancy to abide by a psychotherapeutic schedule contributes to the challenge of retaining clients in treatment as 40%-60% of children and adolescents end up dropping out due to demotivation.\n\n\n=== Problems of reporting styles ===\nMany challenges associated with child psychotherapy derive from inefficient reports of client symptoms. The methods provided for obtaining information of symptoms typically involve questionnaires and interviews that may affect how the client will answer. Important characteristics of symptoms such as duration and intensity may not be reliable if the client omits crucial information out of fear or risk of embarrassment.\n\n\n== See also ==\nBritish Psychotherapy Foundation\nMichael Fordham\nAnna Freud\nMelanie Klein\nMichael Rutter\nDonald Winnicott\n\n\n== References ==\n\n\n== External links ==\nAssociation of Child Psychotherapists (ACP) the professional body for Psychoanalytic Child and Adolescent Psychotherapists in the UK and a core NHS profession',
  },
  {
    title: "Human resources",
    originalContent:
      'Human resources (HR) is the set of people who make up the workforce of an organization, business sector, industry, or economy. A narrower concept is human capital, the knowledge and skills which the individuals command. Similar terms include manpower, labor, labor-power, or personnel.\nThe Human Resources department (HR department, sometimes just called "Human Resource") of an organization performs human resource management, overseeing various aspects of employment, such as compliance with labor law and employment standards, interviewing and selection, performance management, administration of employee benefits, organizing of employee files with the required documents for future reference, and some aspects of recruitment (also known as talent acquisition), talent management, staff wellbeing, and employee offboarding. They serve as the link between an organization\'s management and its employees.\nThe duties include planning, recruitment and selection process, posting job ads, evaluating the performance of employees, organizing resumes and job applications, scheduling interviews and assisting in the process and ensuring background checks. Another job is payroll and benefits administration which deals with ensuring vacation and sick time are accounted for, reviewing payroll, and participating in benefits tasks, like claim resolutions, reconciling benefits statements, and approving invoices for payment. Human Resources also coordinates employee relations activities and programs including,  but not limited to, employee counseling. The last job is regular maintenance, this job makes sure that the current HR files and databases are up to date, maintaining employee benefits and employment status and performing payroll/benefit-related reconciliations.\n\n\n== Activities ==\nA human resources manager can have various functions in a company, including to: \n\nDetermine the needs of the staff/personnel\nHuman resource accounting, determine whether to use temporary staff or hire employees to fill these needs\nRecruit and/or interview hires\nPrepare employee records and personal policies\nManage employee payroll, benefits, and compensation\nManage employee relations, prepare remote work and hybrid work policy\nEmployee retention, talent management\nDeal with performance issues, motivate employees, monitor staff well-being\nMediate disputes, Establish organizational culture in the organization\nReduce workplace politics, ensure equal opportunities, reduce discrimination\nEnsure that human resources practices conform to various regulations and human resource metrics\nApply HR software to improve HR effectivity.\nHuman Resources can be evaluated empirically using various frameworks such as Stamina Model\n\n\n== History ==\nHuman resource management used to be referred to as "personnel administration". In the 1920s, personnel administration focused mostly on the aspects of hiring, evaluating, and compensating employees. However, they did not focus on any employment relationships at an organizational performance level or on the systematic relationships in any parties. This led to a lacked unifying paradigm in the field during this period.\nAccording to an HR Magazine article, the first personnel management department started at the National Cash Register Co. in 1900. The owner, John Henry Patterson, organized a personnel department to deal with grievances, discharges and safety, and information for supervisors on new laws and practices after several strikes and employee lockouts. This action was followed by other companies; for example, Ford had high turnover ratios of 380 percent in 1913, but just one year later, the line workers of the company had doubled their daily salaries from $2.50 to $5, even though $2.50 was a fair wage at that time. This example clearly shows the importance of effective management which leads to a greater outcome of employee satisfaction as well as encouraging employees to work together in order to achieve better business objectives.\nDuring the 1970s, American businesses began experiencing challenges due to the substantial increase in competitive pressures. Companies experienced globalization, deregulation, and rapid technological change which caused the major companies to enhance their strategic planning – a process of predicting future changes in a particular environment and focus on ways to promote organizational effectiveness. This resulted in developing more jobs and opportunities for people to show their skills which were directed to effectively applying employees toward the fulfillment of individual, group, and organizational goals. Many years later the major/minor of human resource management was created at universities and colleges also known as business administration. It consists of all the activities that companies used to ensure the more effective use of employees.\nNow, human resources focus on the people side of management. There are two real definitions of HRM (Human Resource Management); one is that it is the process of managing people in organizations in a structured and thorough manner. This means that it covers the hiring, firing, pay and perks, and performance management. This first definition is the modern and traditional version more like what a personnel manager would have done back in the 1920s. The second definition is that HRM circles the ideas of management of people in organizations from a macromanagement perspective like customers and competitors in a marketplace. This involves the focus on making the "employment relationship" fulfilling for both management and employees.\nSome research showed that employees can perform at a much higher rate of productivity when their supervisors and managers paid more attention to them. The Father of Human relations, Elton Mayo, was the first person to reinforce the importance of employee communications, cooperation, and involvement. His studies concluded that sometimes the human factors are more important than physical factors, such as quality of lighting and physical workplace conditions. As a result, individuals often place value more on how they feel. For example, a rewarding system in Human resource management, applied effectively, can further encourage employees to achieve their best performance.\n\n\n== Origins of the terminology ==\nPioneering economist John R. Commons mentioned "human resource" in his 1893 book The Distribution of Wealth but did not elaborate. The expression was used during the 1910s to 1930s to promote the idea that human beings are of worth (as in human dignity); by the early 1950s, it meant people as a means to an end (for employers). Among scholars the first use of the phrase in that sense was in a 1958 report by economist E. Wight Bakke.\nIn regard to how individuals respond to the changes in a labor market, the following must be understood:\n\nSkills and qualifications: as industries move from manual to more managerial professions so does the need for more highly skilled staff. If the market is "tight" (i.e. not enough staff for the jobs), employers must compete for employees by offering financial rewards, community investment, etc.\nGeographical spread: how far is the job from the individual? The distance to travel to work should be in line with remuneration, and the transportation and infrastructure of the area also influence who applies for a position.\nOccupational structure: the norms and values of the different careers within an organization. Mahoney 1989 developed 3 different types of occupational structure, namely, craft (loyalty to the profession), organization career path (promotion through the firm), and unstructured (lower/unskilled workers who work when needed).\nGenerational difference: different age categories of employees have certain characteristics, for example, their behavior and their expectations of the organization.\n\n\n== Criticism of its terminology and role ==\n\nOne major concern about considering people as assets or resources is that they will be commoditized, objectified, and abused. Critics of the term human resources would argue that human beings are not "commodities" or "resources", but are creative and social beings in a productive enterprise. The 2000 revision of ISO 9001, in contrast, requires identifying the processes, their sequence, and interaction, and to define and communicate responsibilities and authorities. In general, heavily unionized nations such as France and Germany have adopted and encouraged such approaches. Also, in 2001, the International Labour Organization decided to revisit and revise its 1975 Recommendation 150 on Human Resources Development, resulting in its "Labour is not a commodity" principle. One view of these trends is that a strong social consensus on political economy and a good social welfare system facilitate labor mobility and tend to make the entire economy more productive, as labor can develop skills and experience in various ways, and move from one enterprise to another with little controversy or difficulty in adapting.\nAnother important controversy regards labor mobility and the broader philosophical issue with the usage of the phrase "human resources". Governments of developing nations often regard developed nations that encourage immigration or "guest workers" as appropriating human capital that is more rightfully part of the developing nation and required to further its economic growth. Over time, the United Nations have come to more generally support  the developing nations\' point of view, and have requested significant offsetting "foreign aid" contributions so that a developing nation losing human capital does not lose the capacity to continue to train new people in trades, professions, and the arts. Some businesses and companies are choosing to rename this department using other terms, such as "people operations" or "culture department," in order to erase this stigma.\n\n\n== Development ==\nHuman resource companies play an important part in developing and making a company or organization at the beginning or making a success at the end, due to the labor provided by employees. Human resources are intended to show how to have better employment relations in the workforce. Also, to bring out the best work ethic of the employees and therefore making a move to a better working environment. Moreover, green human resource development is suggested as a paradigm shift from traditional approaches of human resource companies to bring awareness of ways that expertise can be applied to green practices. By integrating the expertise, knowledge, and competencies of human resource development practitioners with industry practitioners, most industries have the potential to be transformed into a sector with ecofriendly and pro-environmental culture.\nHuman resources also deals with essential motivators in the workplace such as payroll, benefits, team morale and workplace harassment. \n\n\n== Planning ==\nAdministration and operations used to be the two role areas of HR. The strategic planning component came into play as a result of companies recognizing the need to consider HR needs in goals and strategies. HR directors commonly sit on company executive teams because of the HR planning function. Numbers and types of employees and the evolution of compensation systems are among elements in the planning role. Various factors affecting Human Resource planning include organizational structure, growth, business location, demographic changes, environmental uncertainties, expansion.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nLibrary resources in your library and in other libraries about Human resources',
  },
  {
    title: "Financial management",
    originalContent:
      "Financial management is the business function concerned with profitability, expenses, cash and credit. These are often grouped together under the rubric of maximizing the value of the firm for stockholders. The discipline is then tasked with the \"efficient acquisition and deployment\" of both short- and long-term financial resources, to ensure the objectives of the enterprise are achieved.\nFinancial managers (FM) are specialized professionals directly reporting to senior management, often the financial director (FD); the function is seen as 'staff', and not 'line'.\n\n\n== Role ==\n\nFinancial management is generally concerned with short term working capital management, focusing on current assets and current liabilities,  and managing fluctuations in foreign currency and product cycles, often through hedging.\nThe function also entails the efficient and effective day-to-day management of funds, and thus overlaps treasury management.\nIt is also involved with long term  strategic financial management, focused on i.a. capital structure management, including capital raising, capital budgeting (capital allocation between business units or products), and dividend policy;\nthese latter, in large corporates, being more the domain of \"corporate finance.\"\nSpecific tasks:\n\nProfit maximization happens when marginal cost is equal to marginal revenue. This is the main objective of financial management.\nMaintaining proper cash flow is a short run objective of financial management. It is necessary for operations to pay the day-to-day expenses e.g. raw material, electricity bills, wages, rent etc. A good cash flow ensures the survival of company; see cashflow forecast.\nMinimization on capital cost in financial management can help operations gain more profit.\nEstimating the requirement of funds: Businesses make forecast on funds needed in both short run and long run, hence, they can improve the efficiency of funding. The estimation is based on the budget e.g. sales budget, production budget; see budget analyst.\nDetermining the capital structure: Capital structure is how a firm finances its overall operations and growth by using different sources of funds. Once the requirement of funds has estimated, the financial manager should decide the mix of debt and equity and also types of debt.\n\n\n== Relationship with other areas of finance ==\nTwo areas of finance directly overlap financial management:\n(i) Managerial finance is the (academic) branch of finance concerned with the managerial application of financial techniques;\n(ii) Corporate finance is mainly concerned with the longer term capital budgeting, and typically is more relevant to large corporations.\nInvestment management, also related, is the professional asset management of various securities (shares, bonds and other securities/assets).\nIn the context of financial management, the function sits with treasury; usually the management of the various short-term financial legal instruments (contractual duties, obligations, or rights) appropriate to the company's cash- and liquidity management requirements. See Treasury management § Functions.\nThe term \"financial management\" refers to a company's financial strategy, while personal finance or financial life management refers to an individual's management strategy. A financial planner, or personal financial planner, is a professional who prepares financial plans here.\n\n\n== Financial management systems ==\nFinancial management systems are the software and technology used by organizations to connect, store, and report on assets, income, and expenses.\nHere, the discipline relies on a range of products, from spreadsheets (invariably as a starting point, and frequently in total) through commercial EPM and BI tools, often BusinessObjects (SAP), OBI EE (Oracle), Cognos (IBM), and Power BI (Microsoft).\nSee Financial modeling § Accounting for discussion.\n\n\n== See also ==\nFinancial management for IT services, financial management of IT assets and resources\nFinancial Management Service, a bureau of the U.S. Treasury which provides financial services for the government.\nFinancial mismanagement\nFinancial risk management § Corporate finance\nFP&A\nManagerial finance\n\n\n== References ==\n\n\n== Further reading ==\nLawrence Gitman and Chad J. Zutter  (2019). Principles of Managerial Finance, 14th edition, Addison-Wesley Publishing, ISBN 978-0133507690.\nClive Marsh (2009). Mastering Financial Management, Financial Times Prentice Hall ISBN 978-0-273-72454-4\nJames Van Horne and John Wachowicz (2009). Fundamentals of Financial Management, 13th ed., Pearson Education Limited. ISBN 9705614229",
  },
  {
    title: "Investment",
    originalContent:
      'Investment is traditionally defined as the "commitment of resources to achieve later benefits". If an investment involves money, then it can be defined as a "commitment of money to receive more money later". From a broader viewpoint, an investment can be defined as "to tailor the pattern of expenditure and receipt of resources to optimise the desirable patterns of these flows". When expenditures and receipts are defined in terms of money, then the net monetary receipt in a time period is termed cash flow, while money received in a series of several time periods is termed cash flow stream.\nIn finance, the purpose of investing is to generate a return on the invested asset. The return may consist of a capital gain (profit) or loss, realised if the  investment is sold, unrealised capital appreciation (or depreciation) if yet unsold. It may also consist of periodic income such as dividends, interest, or rental income. The return may also include currency gains or losses due to changes in foreign currency exchange rates.\nInvestors generally expect higher returns from riskier investments. When a low-risk investment is made, the return is also generally low. Similarly, high risk comes with a chance of high losses. Investors, particularly novices, are often advised to diversify their portfolio. Diversification has the statistical effect of reducing overall risk.\n\n\n== Types of financial investments ==\nIn modern economies, traditional investments include:\n\nStocks - Business ownership, known as equity, in publicly traded companies\nBonds - loans to governments and businesses traded on public markets\nCash - holding a particular currency, whether in anticipation of spending or to take advantage of or hedge against changes in a currency exchange rate\nReal estate, which can be rented to provide ongoing income or resold if it increases in value\nAlternative investments include:\n\nPrivate equity in businesses that are not publicly traded on a stock exchange, often involving venture capital funds, angel investors, or equity crowdfunding\nOther loans, including mortgages\nCommodities, such as precious metals like gold, agricultural products like potatoes, and energy deliveries like natural gas\nCollectables, including art, coins, vintage cars, postage stamps, and wine\nCarbon offsets and credits\nDigital entities like cryptocurrency and non-fungible tokens\nHedge funds that use sophisticated techniques like:\nDerivatives, the value of which is determined by a contract and is derived by calculation from the performance of some other sort of underlying investment; these include forwards, futures, options, swaps, collateralized debt obligations, credit default swaps, and Tax Receivable Agreements\nLeveraged investing, which is the investment of borrowed money\nShort selling, which typically uses leverage and derivatives to bet that the value of a stock will decline\n\n\n== Investment and risk ==\nAn investor may bear a risk of loss of some or all of their capital invested. Investment differs from arbitrage, in which profit is generated without investing capital or bearing risk.\nSavings bear the (normally remote) risk that the financial provider may default.\nForeign currency savings also bear foreign exchange risk: if the currency of a savings account differs from the account holder\'s home currency, then there is the risk that the exchange rate between the two currencies will move unfavourably so that the value of the savings account decreases, measured in the account holder\'s home currency.\nEven investing in tangible assets like property has its risk. And similar to most risks, property buyers can seek to mitigate any potential risk by taking out mortgage and by borrowing at a lower loan to security ratio.\nIn contrast with savings, investments tend to carry more risk, in the form of both a wider variety of risk factors and a greater level of uncertainty.\nIndustry to industry volatility is more or less of a risk depending. In biotechnology, for example, investors look for big profits on companies that have small market capitalizations but can be worth hundreds of millions quite quickly. The risk is high because approximately 90% of biotechnology products researched do not make it to market due to regulations and the complex demands within pharmacology as the average prescription drug takes 10 years and US$2.5 billion worth of capital.\n\n\n== History ==\n\nIn the medieval Islamic world, the qirad was a major financial instrument. This was an arrangement between one or more investors and an agent where the investors entrusted capital to an agent who then traded with it in hopes of making a profit.  Both parties then received a previously settled portion of the profit, though the agent was not liable for any losses. Many will notice that the qirad is similar to the institution of the commenda later used in western Europe, though whether the qirad transformed into the commenda or the two institutions evolved independently cannot be stated with certainty.\nIn the early 1900s, purchasers of stocks, bonds, and other securities were described in media, academia, and commerce as speculators. Since the Wall Street crash of 1929, and particularly by the 1950s, the term "investment" had come to denote the more conservative end of the securities spectrum, while "speculation" was applied by financial brokers and their advertising agencies to higher risk securities much in vogue at that time. Since the last half of the 20th century, the terms "speculation" and "speculator" have specifically referred to higher risk ventures.\n\n\n== Investment strategies ==\n\n\n=== Value investing ===\n\nA value investor buys assets that they believe to be undervalued (and sells overvalued ones). To identify undervalued securities, a value investor uses analysis of the financial reports of the issuer to evaluate the security. Value investors employ accounting ratios, such as earnings per share and sales growth, to identify securities trading at prices below their worth.\nWarren Buffett and Benjamin Graham are notable examples of value investors. Graham and Dodd\'s seminal work, Security Analysis, was written in the wake of the Wall Street Crash of 1929.\nThe price to earnings ratio (P/E), or earnings multiple, is a particularly significant and recognized fundamental ratio, with a function of dividing the share price of the stock, by its earnings per share. This will provide the value representing the sum investors are prepared to expend for each dollar of company earnings. This ratio is an important aspect, due to its capacity as measurement for the comparison of valuations of various companies. A stock with a lower P/E ratio will cost less per share than one with a higher P/E, taking into account the same level of financial performance; therefore, it essentially means a low P/E is the preferred option.\nAn instance in which the price to earnings ratio has a lesser significance is when companies in different industries are compared. For example, although it is reasonable for a telecommunications stock to show a P/E in the low teens, in the case of hi-tech stock, a P/E in the 40s range is not unusual. When making comparisons, the P/E ratio can give you a refined view of a particular stock valuation.\nFor investors paying for each dollar of a company\'s earnings, the P/E ratio is a significant indicator, but the price-to-book ratio (P/B) is also a reliable indication of how much investors are willing to spend on each dollar of company assets. In the process of the P/B ratio, the share price of a stock is divided by its net assets; any intangibles, such as goodwill, are not taken into account. It is a crucial factor of the price-to-book ratio, due to it indicating the actual payment for tangible assets and not the more difficult valuation of intangibles. Accordingly, the P/B could be considered a comparatively conservative metric.\n\n\n=== Growth investing ===\nGrowth investors seek investments they believe are likely to have higher earnings or greater value in the future. To identify such stocks, growth investors often evaluate measures of current stock value as well as predictions of future financial performance. Growth investors seek profits through capital appreciation – the gains earned when a stock is sold at a higher price than what it was purchased for. The price-to-earnings (P/E) multiple is also used for this type of investment; growth stock are likely to have a P/E higher than others in its industry. According to Investopedia author Troy Segal and U.S. Department of State Fulbright fintech research awardee Julius Mansa, growth investing is best suited for investors who prefer relatively shorter investment horizons, higher risks, and are not seeking immediate cash flow through dividends.\nSome investors attribute the introduction of the growth investing strategy to investment banker Thomas Rowe Price Jr., who tested and popularized the method in 1950 by introducing his mutual fund, the T. Rowe Price Growth Stock Fund. Price asserted that investors could reap high returns by "investing in companies that are well-managed in fertile fields."\nA new form of investing that seems to have caught the attention of investors is Venture Capital. Venture Capital is independently managed dedicated pools of capital that focus on equity or equity-linked investments in privately held, high growth companies.\n\n\n=== Momentum investing ===\nMomentum investors generally seek to buy stocks that are currently experiencing a short-term uptrend, and they usually sell them once this momentum starts to decrease. Stocks or securities purchased for momentum investing are often characterized by demonstrating consistently high returns for the past three to twelve months. However, in a bear market, momentum investing also involves short-selling securities of stocks that are experiencing a downward trend, because it is believed that these stocks will continue to decrease in value. Essentially, momentum investing generally relies on the principle that a consistently up-trending stock will continue to grow, while a consistently down-trending stock will continue to fall.\nEconomists and financial analysts have not reached a consensus on the effectiveness of using the momentum investing strategy. Rather than evaluating a company\'s operational performance, momentum investors instead utilize trend lines, moving averages, and the Average Directional Index (ADX) to determine the existence and strength of trends.\n\n\n=== Dollar cost averaging ===\n\nDollar cost averaging (DCA), also known in the UK as pound-cost averaging, is the process of consistently investing a certain amount of money across regular increments of time, and the method can be used in conjunction with value investing, growth investing, momentum investing, or other strategies. For example, an investor who practices dollar-cost averaging could choose to invest $200 a month for the next 3 years, regardless of the share price of their preferred stock(s), mutual funds, or exchange-traded funds.\nMany investors believe that dollar-cost averaging helps minimize short-term volatility by spreading risk out across time intervals and avoiding market timing. Research also shows that DCA can help reduce the total average cost per share in an investment because the method enables the purchase of more shares when their price is lower, and less shares when the price is higher. However, dollar-cost averaging is also generally characterized by more brokerage fees, which could decrease an investor\'s overall returns.\nThe term "dollar-cost averaging" is believed to have first been coined in 1949 by economist and author Benjamin Graham in his book, The Intelligent Investor. Graham asserted that investors that use DCA are "likely to end up with a satisfactory overall price for all [their] holdings."\n\n\n=== Micro-investing ===\nMicro-investing is a type of investment strategy that is designed to make investing regular, accessible and affordable, especially for those who may not have a lot of money to invest or who are new to investing.\n\n\n== Intermediaries and collective investments ==\nInvestments are often made indirectly through intermediary financial institutions. These intermediaries include pension funds, banks, and insurance companies. They may pool money received from a number of individual end investors into funds such as investment trusts, unit trusts, and SICAVs to make large-scale investments. Each individual investor holds an indirect or direct claim on the assets purchased, subject to charges levied by the intermediary, which may be large and varied.\nApproaches to investment sometimes referred to in marketing of collective investments include dollar cost averaging and market timing.\n\n\n== Investment valuation ==\n\nFree cash flow measures the cash a company generates which is available to its debt and equity investors, after allowing for reinvestment in working capital and capital expenditure. High and rising free cash flow, therefore, tend to make a company more attractive to investors.\nThe debt-to-equity ratio is an indicator of capital structure. A high proportion of debt, reflected in a high debt-to-equity ratio, tends to make a company\'s earnings, free cash flow, and ultimately the returns to its investors, riskier or volatile. Investors compare a company\'s debt-to-equity ratio with those of other companies in the same industry, and examine trends in debt-to-equity ratios and free cashflow.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==',
  },
  {
    title: "Financial planning",
    originalContent:
      'In general usage, a financial plan is a comprehensive evaluation of an individual\'s current pay and future financial state by using current known variables to predict future income, asset values and withdrawal plans.  This often includes a budget which organizes an individual\'s finances and sometimes includes a series of steps or specific goals for spending and saving in the future. This plan allocates future income to various types of expenses, such as rent or utilities, and also reserves some income for short-term and long-term savings. A financial plan is sometimes referred to as an investment plan, but in personal finance, a financial plan can focus on other specific areas such as risk management, estates, college, or retirement.\n\n\n== Context of business ==\nIn business, "financial forecast"  or "financial plan" can also refer to an projection across a time horizon, typically an annual one, of income and expenses for a company, division, or department; \nsee Budget § Corporate budget.\nMore specifically, a financial plan can also refer to the three primary financial statements (balance sheet, income statement, and cash flow statement) created within a business plan.\nA financial plan can also be an estimation of cash needs and a decision on how to raise the cash, such as through borrowing or issuing additional shares in a company.\nNote that the financial plan  may then contain prospective financial statements, which are similar, but different, to those of a budget. \nFinancial plans are the entire financial accounting overview of a company. Complete financial plans contain all periods and transaction types. It\'s a combination of the financial statements which independently only reflect a past, present, or future state of the company. Financial plans are the collection of the historical, present, and future financial statements; for example, a (historical & present) costly expense from an operational issue is normally presented prior to the issuance of the prospective financial statements which propose a solution to said operational issue.\nThe confusion surrounding the term "financial plans" might stem from the fact that there are many types of financial statement reports. Individually, financial statements show either the past, present, or future financial results. More specifically, financial statements also only reflect the specific categories which are relevant. For instance, investing activities are not adequately displayed in a balance sheet. A financial plan is a combination of the individual financial statements and reflect all categories of transactions (operations & expenses & investing) over time.\nSome period-specific financial statement examples include pro forma statements (historical period) and prospective statements (current and future period). Compilations are a type of service which involves "presenting, in the form of financial statements, information that is the representation of management".  There are two types of "prospective financial statements": financial  forecasts & financial projections and both relate to the current/future time period.   Prospective financial statements are a time period-type of financial statement which may reflect the current/future financial status of a company using three main reports/financial statements: cash flow statement, income statement, and balance sheet. "Prospective financial statements are of two types- forecasts and projections. Forecasts are based on management\'s expected financial position, results of operations, and cash flows." Pro Forma statements take previously recorded results, the historical financial data, and present a "what-if": "what-if" a transaction had happened sooner.\nWhile the common usage of the term "financial plan" often refers to a formal and defined series of steps or goals, there is some technical confusion about what the term "financial plan" actually means in the industry. For example, one of the industry\'s leading professional organizations, the Certified Financial Planner Board of Standards, lacks any definition for the term "financial plan" in its Standards of Professional Conduct publication. This publication outlines the professional financial planner\'s job, and explains the process of financial planning, but the term "financial plan" never appears in the publication\'s text.\nThe accounting and finance industries have distinct responsibilities and roles. When the products of their work are combined, it produces a complete picture, a financial plan. A financial analyst studies the data and facts (regulations/standards), which are processed, recorded, and presented by accountants. Normally, finance personnel study the data results - meaning what has happened or what might happen - and propose a solution to an inefficiency. Investors and financial institutions must see both the issue and the solution to make an informed decision. Accountants and financial planners are both involved with presenting issues and resolving inefficiencies, so together, the results and explanation are provided in a financial plan.\n\n\n== Issues of definition ==\nTextbooks used in universities offering financial planning-related courses also generally do not define the term \'financial plan\'. For example, Sid Mittra, Anandi P. Sahu, and Robert A Crane, authors of Practicing Financial Planning for Professionals do not define what a financial plan is, but merely defer to the Certified Financial Planner Board of Standards\' definition of \'financial planning\'.\n\n\n== Planning ==\nWhen drafting a financial plan, the company should establish the planning horizon, which is the time period of the plan, whether it be on a short-term (usually 12 months) or long-term (two to five years) basis. Also, the individual projects and investment proposals of each operational unit within the company should be totaled and treated as one large project. This process is called aggregation.\n\n\n== See also ==\nCapital budgeting\nFinancial planning (business)\nFinancial plan (budget)\nOptimism bias\nPersonal budget\nReference class forecasting\n\n\n== References ==\n\n\n== External links ==\nProspective Analysis: Guidelines for Forecasting Financial Statements, Ignacio Velez-Pareja, Joseph Tham, 2008\nTo Plug or Not to Plug, that is the Question: No Plugs, No Circularity: A Better Way to Forecast Financial Statements, Ignacio Velez-Pareja, 2008\nA Step by Step Guide to Construct a Financial Model Without Plugs and Without Circularity for Valuation Purposes, Ignacio Velez-Pareja, 2008\nLong-Term Financial Statements Forecasting: Reinvesting Retained Earnings, Sergei Cheremushkin, 2008\nThe Plan for Financial Comfort, Phronesis Wealth Management, 2023',
  },
  {
    title: "Econometrics",
    originalContent:
      'Econometrics is an application of statistical methods to economic data in order to give empirical content to economic relationships. More precisely, it is "the quantitative analysis of actual economic phenomena based on the concurrent development of theory and observation, related by appropriate methods of inference." An introductory economics textbook describes econometrics as allowing economists "to sift through mountains of data to extract simple relationships." Jan Tinbergen is one of the two founding fathers of econometrics. The other, Ragnar Frisch, also coined the term in the sense in which it is used today.\nA basic tool for econometrics is the multiple linear regression model. Econometric theory uses statistical theory and mathematical statistics to evaluate and develop econometric methods. Econometricians try to find estimators that have desirable statistical properties including unbiasedness, efficiency, and consistency. Applied econometrics uses theoretical econometrics and real-world data for assessing economic theories, developing econometric models, analysing economic history, and forecasting.\n\n\n== Basic models: linear regression ==\nA basic tool for econometrics is the multiple linear regression model. In modern econometrics, other statistical tools are frequently used, but linear regression is still the most frequently used starting point for an analysis. Estimating a linear regression on two variables can be visualized as fitting a line through data points representing paired values of the independent and dependent variables.\n\nFor example, consider Okun\'s law, which relates GDP growth to the unemployment rate. This relationship is represented in a linear regression where the change in unemployment rate (\n  \n    \n      \n        Δ\n         \n        \n          Unemployment\n        \n      \n    \n    {\\displaystyle \\Delta \\ {\\text{Unemployment}}}\n  \n) is a function of an intercept (\n  \n    \n      \n        \n          β\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\beta _{0}}\n  \n), a given value of GDP growth multiplied by a slope coefficient \n  \n    \n      \n        \n          β\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n  \n and an error term, \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n:\n\n  \n    \n      \n        Δ\n         \n        \n          Unemployment\n        \n        =\n        \n          β\n          \n            0\n          \n        \n        +\n        \n          β\n          \n            1\n          \n        \n        \n          Growth\n        \n        +\n        ε\n        .\n      \n    \n    {\\displaystyle \\Delta \\ {\\text{Unemployment}}=\\beta _{0}+\\beta _{1}{\\text{Growth}}+\\varepsilon .}\n  \n\nThe unknown parameters \n  \n    \n      \n        \n          β\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\beta _{0}}\n  \n and \n  \n    \n      \n        \n          β\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n  \n can be estimated. Here \n  \n    \n      \n        \n          β\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\beta _{0}}\n  \n is estimated to be 0.83 and \n  \n    \n      \n        \n          β\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n  \n is estimated to be -1.77. This means that if GDP growth increased by one percentage point, the unemployment rate would be predicted to drop by 1.77\t\t* 1 points, other things held constant. The model could then be tested for statistical significance as to whether an increase in GDP growth is associated with a decrease in the unemployment, as hypothesized. If the estimate of \n  \n    \n      \n        \n          β\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n  \n were not significantly different from 0, the test would fail to find evidence that changes in the growth rate and unemployment rate were related. The variance in a prediction of the dependent variable (unemployment) as a function of the independent variable (GDP growth) is given in polynomial least squares.\n\n\n== Theory ==\n\nEconometric theory uses statistical theory and mathematical statistics to evaluate and develop econometric methods. Econometricians try to find estimators that have desirable statistical properties including unbiasedness, efficiency, and consistency. An estimator is unbiased if its expected value is the true value of the parameter; it is consistent if it converges to the true value as the sample size gets larger, and it is efficient if the estimator has lower standard error than other unbiased estimators for a given sample size. Ordinary least squares (OLS) is often used for estimation since it provides the BLUE or "best linear unbiased estimator" (where "best" means most efficient, unbiased estimator) given the Gauss-Markov assumptions. When these assumptions are violated or other statistical properties are desired, other estimation techniques such as maximum likelihood estimation, generalized method of moments, or generalized least squares are used. Estimators that incorporate prior beliefs are advocated by those who favour Bayesian statistics over traditional, classical or "frequentist" approaches.\n\n\n== Methods ==\n\nApplied econometrics uses theoretical econometrics and real-world data for assessing economic theories, developing econometric models, analysing economic history, and forecasting.\nEconometrics uses standard statistical models to study economic questions, but most often these are based on observational data, rather than data from controlled experiments. In this, the design of observational studies in econometrics is similar to the design of studies in other observational disciplines, such as astronomy, epidemiology, sociology and political science. Analysis of data from an observational study is guided by the study protocol, although exploratory data analysis may be useful for generating new hypotheses. Economics often analyses systems of equations and inequalities, such as supply and demand hypothesized to be in equilibrium. Consequently, the field of econometrics has developed methods for identification and estimation of simultaneous equations models. These methods are analogous to methods used in other areas of science, such as the field of system identification in systems analysis and control theory. Such methods may allow researchers to estimate models and investigate their empirical consequences, without directly manipulating the system.\nIn the absence of evidence from controlled experiments, econometricians often seek illuminating natural experiments or apply quasi-experimental methods to draw credible causal inference. The methods include regression discontinuity design, instrumental variables, and difference-in-differences.\n\n\n== Example ==\nA simple example of a relationship in econometrics from the field of labour economics is:\n\n  \n    \n      \n        ln\n        ⁡\n        (\n        \n          wage\n        \n        )\n        =\n        \n          β\n          \n            0\n          \n        \n        +\n        \n          β\n          \n            1\n          \n        \n        (\n        \n          years of education\n        \n        )\n        +\n        ε\n        .\n      \n    \n    {\\displaystyle \\ln({\\text{wage}})=\\beta _{0}+\\beta _{1}({\\text{years of education}})+\\varepsilon .}\n  \n\nThis example assumes that the natural logarithm of a person\'s wage is a linear function of the number of years of education that person has acquired. The parameter \n  \n    \n      \n        \n          β\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n  \n measures the increase in the natural log of the wage attributable to one more year of education. The term \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n is a random variable representing all other factors that may have direct influence on wage. The econometric goal is to estimate the parameters, \n  \n    \n      \n        \n          β\n          \n            0\n          \n        \n        \n          \n             and \n          \n        \n        \n          β\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{0}{\\mbox{ and }}\\beta _{1}}\n  \n under specific assumptions about the random variable \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n. For example, if \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n is uncorrelated with years of education, then the equation can be estimated with ordinary least squares.\nIf the researcher could randomly assign people to different levels of education, the data set thus generated would allow estimation of the effect of changes in years of education on wages. In reality, those experiments cannot be conducted. Instead, the econometrician observes the years of education of and the wages paid to people who differ along many dimensions. Given this kind of data, the estimated coefficient on years of education in the equation above reflects both the effect of education on wages and the effect of other variables on wages, if those other variables were correlated with education. For example, people born in certain places may have higher wages and higher levels of education. Unless the econometrician controls for place of birth in the above equation, the effect of birthplace on wages may be falsely attributed to the effect of education on wages.\nThe most obvious way to control for birthplace is to include a measure of the effect of birthplace in the equation above. Exclusion of birthplace, together with the assumption that \n  \n    \n      \n        ϵ\n      \n    \n    {\\displaystyle \\epsilon }\n  \n is uncorrelated with education produces a misspecified model. Another technique is to include in the equation additional set of measured covariates which are not instrumental variables, yet render \n  \n    \n      \n        \n          β\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n  \n identifiable. An overview of econometric methods used to study this problem were provided by Card (1999).\n\n\n== Journals ==\nThe main journals that publish work in econometrics are:\n\nEconometrica, which is published by Econometric Society.\nThe Review of Economics and Statistics, which is over 100 years old.\nThe Econometrics Journal, which was established by the Royal Economic Society.\nThe Journal of Econometrics, which also publishes the supplement Annals of Econometrics.\nEconometric Theory, which is a theoretical journal.\nThe Journal of Applied Econometrics, which applies econometrics to a wide various problems.\nEconometric Reviews, which includes reviews on econometric books and software as well.\nThe Journal of Business & Economic Statistics, which is published by the American Statistical Association.\n\n\n== Limitations and criticisms ==\n\nLike other forms of statistical analysis, badly specified econometric models may show a spurious relationship where two variables are correlated but causally unrelated. In a study of the use of econometrics in major economics journals, McCloskey concluded that some economists report p-values (following the Fisherian tradition of tests of significance of point null-hypotheses) and neglect concerns of type II errors; some economists fail to report estimates of the size of effects (apart from statistical significance) and to discuss their economic importance. She also argues that some economists also fail to use economic reasoning for model selection, especially for deciding which variables to include in a regression.\nIn some cases, economic variables cannot be experimentally manipulated as treatments randomly assigned to subjects. In such cases, economists rely on observational studies, often using data sets with many strongly associated covariates, resulting in enormous numbers of models with similar explanatory ability but different covariates and regression estimates. Regarding the plurality of models compatible with observational data-sets, Edward Leamer urged that "professionals ... properly withhold belief until an inference can be shown to be adequately insensitive to the choice of assumptions".\n\n\n== See also ==\n\n\n== Further reading ==\nEconometric Theory book on Wikibooks\nGiovannini, Enrico Understanding Economic Statistics, OECD Publishing, 2008, ISBN 978-92-64-03312-2\n\n\n== References ==\n\n\n== External links ==\n\nJournal of Financial Econometrics\nEconometric Society\nThe Econometrics Journal\nEconometric Links\nTeaching Econometrics (Index by the Economics Network (UK))\nApplied Econometric Association\nThe Society for Financial Econometrics\nThe interview with Clive Granger – Nobel winner in 2003, about econometrics',
  },
  {
    title: "Financial analysis",
    originalContent:
      "Financial analysis (also known as financial statement analysis, accounting analysis, or analysis of finance) refers to an assessment of the viability, stability, and profitability of a business, sub-business, project or investment. \nIt is performed by professionals who prepare reports using ratios and other techniques, that make use of information taken from financial statements and other reports. These reports are usually presented to top management as one of their bases in making business decisions.  \nFinancial analysis may determine if a business will:\n\nContinue or discontinue its main operation or part of its business;\nMake or purchase certain materials in the manufacture of its product;\nAcquire or rent/lease certain machineries and equipment in the production of its goods;\nIssue  shares or negotiate for a bank loan to increase its working capital;\nMake decisions regarding investing or lending capital;\nMake other decisions that allow management to make an informed selection on various alternatives in the conduct of its business.\n\n\n== Firm-level analysis ==\nFinancial analysts often assess the following elements of a firm:\n\nProfitability - its ability to earn income and sustain growth in both the short- and long-term. A company's degree of profitability is usually based on the income statement, which reports on the company's results of operations;\nSolvency - its ability to pay its obligation to creditors and other third parties in the long-term;\nLiquidity - its ability to maintain positive cash flow, while satisfying immediate obligations;\nStability - the firm's ability to remain in business in the long run, without having to sustain significant losses in the conduct of its business. Assessing a company's stability requires the use of both the income statement and the balance sheet, as well as other financial and non-financial indicators.\nBoth 2 and 3 are based on the company's balance sheet, which indicates the financial condition of a business as of a given point in time.\n\n\n== Techniques ==\nFinancial analysts often compare financial ratios (of solvency, profitability, growth, etc.):\n\nPast Performance - Across historical time periods for the same firm (the last 5 years for example),\nFuture Performance - Using historical figures and certain mathematical and statistical techniques, including present and future values, This extrapolation method is the main source of errors in financial analysis as past statistics can be poor predictors of future prospects.\nComparative Performance - Comparison between similar firms\nComparing financial ratios is merely one way of conducting financial analysis. \nFinancial analysts can also use percentage analysis which involves reducing a series of figures as a percentage of some base amount.  For example, a group of items can be expressed as a percentage of net income. When proportionate changes in the same figure over a given time period expressed as a percentage is known as horizontal analysis.\nVertical or common-size analysis reduces all items on a statement to a \"common size\" as a percentage of some base value which assists in comparability with other companies of different sizes. As a result, all Income Statement items are divided by Sales, and all Balance Sheet items are divided by Total Assets.\nAnother method is comparative analysis. This provides a better way to determine trends. Comparative analysis presents the same information for two or more time periods and is presented side-by-side to allow for easy analysis.\n\n\n== Theoretical challenges ==\nFinancial ratios face several theoretical challenges:\n\nThey say little about the firm's prospects in an absolute sense.  Their insights about relative performance require a reference point from other time periods or similar firms.\nOne ratio holds little meaning.  As indicators, ratios can be logically interpreted in at least two ways. One can partially overcome this problem by combining several related ratios to paint a more comprehensive picture of the firm's performance.\nSeasonal factors may prevent year-end values from being representative. A ratio's values may be distorted as account balances change from the beginning to the end of an accounting period. Use average values for such accounts whenever possible.\nFinancial ratios are no more objective than the accounting methods employed. Changes in accounting policies or choices can yield drastically different ratio values.\n\n\n== See also ==\nBusiness valuation\nEconomic base analysis\nFinancial accounting\nFinancial forecast & Cash flow forecast\nFinancial modeling  § Accounting\nFinancial risk management § Corporate finance\nFinancial statement\nGoing concern (Fundamental basis of financial statements)\nManagerial risk accounting\n\n\n== Notes ==\n\n\n== External links ==\n\nSFAF – the French Society of Financial Analysts\nACIIA – Association of Certified International Investment Analysts\nEFFAS – European Federation of Financial Analysts Societies",
  },
  {
    title: "International trade",
    originalContent:
      "International trade is the exchange of capital, goods, and services across international borders or territories because there is a need or want of goods or services. (See: World economy.)\nIn most countries, such trade represents a significant share of gross domestic product (GDP). While international trade has existed throughout history (for example Uttarapatha, Silk Road, Amber Road, salt roads), its economic, social, and political importance has been on the rise in recent centuries.\nCarrying out trade at an international level is a complex process when compared to domestic trade. When trade takes place between two or more states, factors like currency, government policies, economy, judicial system, laws, and markets influence trade.\nTo ease and justify the process of trade between countries of different economic standing in the modern era, some international economic organizations were formed, such as the World Trade Organization. These organizations work towards the facilitation and growth of international trade. Statistical services of intergovernmental and supranational organizations and governmental statistical agencies publish official statistics on international trade.\n\n\n== Characteristics of global trade ==\nA product that is transferred or sold from a party in one country to a party in another country is an export from the originating country, and an import to the country receiving that product.  Imports and exports are accounted for in a country's current account in the balance of payments.\nTrading globally may give consumers and countries the opportunity to be exposed to new markets and  products. Almost every kind of product can be found in the international market, for example: food, clothes, spare parts, oil, jewellery, wine, stocks, currencies, and water. Services are also traded, such as in tourism, banking, consulting, and transportation.\n\nAdvanced technology (including transportation), globalization, industrialization, outsourcing and multinational corporations have major impacts on the international trade systems.\n\n\n== Differences from domestic trade ==\n\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not.\nHowever, in practical terms, carrying out trade at an international level is typically a more complex process than domestic trade. The main difference is that international trade is typically more costly than domestic trade. This is due to the fact that cross-border trade typically incurs additional costs such as explicit tariffs as well as explicit or implicit non-tariff barriers such as time costs (due to border delays), language and cultural differences, product safety, the legal system, and so on.\nAnother difference between domestic and international trade is that factors of production such as capital and labor are often more mobile within a country than across countries. Thus, international trade is mostly restricted to trade in goods and services, and only to a lesser extent to trade in capital, labour, or other factors of production. Trade in goods and services can serve as a substitute for trade in factors of production. Instead of importing a factor of production, a country can import goods that make intensive use of that factor of production and thus embody it. An example of this is the import of labor-intensive goods by the United States from China. Instead of importing Chinese labor, the United States imports goods that were produced with Chinese labor. One report in 2010, suggested that international trade was increased when a country hosted a network of immigrants, but the trade effect was weakened when the immigrants became assimilated into their new country.\n\n\n== History ==\n\nThe history of international trade chronicles notable events that have affected trading among various economies.\n\n\n== Theories and models ==\n\nThere are several models that seek to explain the factors behind international trade, the welfare consequences of trade and the pattern of trade.\n\n\n== Most traded export products ==\n\n\n== Largest countries or regions by total international trade ==\n\nThe following table is a list of the 25 largest trading states according to the World Trade Organization in 2021 and 2022.\n\n\n== Top traded commodities by value (exports) ==\n\nSource: International Trade Centre\n\n\n== Observances ==\nIn the US, the various U.S. Presidents have held observances to promote big and small companies to be more involved with the export and import of goods and services. President George W. Bush observed World Trade Week on May 18, 2001, and May 17, 2002. On May 13, 2016, President Barack Obama proclaimed May 15 through May 21, 2016, World Trade Week, 2016. On May 19, 2017, President Donald Trump proclaimed May 21 through May 27, 2017, World Trade Week, 2017. World Trade Week is the third week of May. Every year the President declares that week to be World Trade Week.\n\n\n== International trade versus local production ==\n\n\n=== Food security ===\n\nThe trade-offs between local food production and distant food production are controversial, with limited studies comparing environmental impact and scientists cautioning that regionally specific environmental impacts should be considered. A 2020 study indicated that local food crop production alone cannot meet the demand for most food crops with \"current production and consumption patterns\" and the locations of food production at the time of the study for 72–89% of the global population and 100 km radiuses as of early 2020. Studies found that food miles are a relatively minor factor for carbon emissions, albeit increased food localization may also enable additional, more significant, environmental benefits such as recycling of energy, water, and nutrients. For specific foods regional differences in harvest seasons may make it more environmentally friendly to import from distant regions than more local production and storage or local production in greenhouses.\n\n\n=== Qualitative differences and economic aspects ===\nQualitative differences between substitutive products of different production regions may exist due to different legal requirements and quality standards or different levels of controllability by local production- and governance-systems which may have aspects of security beyond resource security, environmental protection, product quality and product design and health. The process of transforming supply as well as labor rights may differ as well.\nLocal production has been reported to increase local employment in many cases. A 2018 study claimed that international trade can increase local employment. A 2016 study found that local employment and total labor income in both manufacturing and nonmanufacturing were negatively affected by rising exposure to imports.\nLocal production in high-income countries, rather than distant regions may require higher wages for workers. Higher wages incentivize automation which could allow for automated workers' time to be reallocated by society and its economic mechanisms or be converted into leisure-like time.\n\n\n==== Specialization, production efficiency and regional differences ====\nLocal production may require knowledge transfer, technology transfer and may not be able to compete in efficiency initially with specialized, established industries and businesses, or in consumer demand without policy measures such as eco-tariffs. Regional differences may cause specific regions to be more suitable for a specific production, thereby increasing the advantages of specific trade over specific local production. Forms of local products that are highly localized may not be able to meet the efficiency of more large-scale, highly consolidated production in terms of efficiency, including environmental impact.\n\n\n=== Resource security ===\n\nA systematic, and possibly first large-scale, cross-sectoral analysis of water, energy and land in security in 189 countries that links total and sectorial consumption to sources showed that countries and sectors are highly exposed to over-exploited, insecure, and degraded such resources with economic globalization having decreased security of global supply chains. The 2020 study finds that most countries exhibit greater exposure to resource risks via international trade – mainly from remote production sources – and that diversifying trading partners is unlikely to help countries and sectors to reduce these or to improve their resource self-sufficiency.\n\n\n== Illicit trade ==\n\n\n=== Illegal gold trade ===\nA number of people in Africa, including children, were using informal or \"artisanal\" methods to produce gold. While millions were making a livelihood through this small-scale mining, governments of Ghana, Tanzania and Zambia complained about the increase in illegal production and gold smuggling. Sometimes the procedure involved criminal operations and even human and environmental cost. Investigative reports based on Africa's export data revealed that gold in large quantities is smuggled out of the country through the United Arab Emirates, without any taxes being paid to the producing states. Analysis also reflected discrepancies in the amount exported from Africa and the total gold imported into the UAE.\nIn July 2020, a report by Swissaid highlighted that the Dubai-based precious metal refining firms, including Kaloti Jewellery International Group and Trust One Financial Services (T1FS), received most of their gold from poor African states like Sudan. The gold mines in Sudan were seldom under the militias involved in war crimes and human rights abuses. The Swissaid report also highlighted that the illicit gold coming into Dubai from Africa is imported in large quantities by the world's largest refinery in Switzerland, Valcambi. \nAnother report in March 2022 revealed the contradiction between the lucrative gold trade of West African countries and the illicit dealings. Like Sudan, Democratic Republic of Congo (DRC), Ghana and other states, discrepancies were recorded between the gold production in Mali and its trade with Dubai, UAE. The third largest gold exporter in Africa, Mali imposed taxes only on the first 50 kg of gold exports per month, which allowed several small-scale miners to enjoy tax exemptions and smuggle gold worth millions. In 2014, Mali's gold production was 45.8 tonnes, while the UAE's gold imports were 59.9 tonnes. \n\n\n== See also ==\n\n\n=== Lists ===\nList of countries by current account balance\nList of countries by imports\nList of countries by exports\nList of countries by merchandise exports\nList of countries by service exports\nList of international trade topics\n\n\n== References ==\n\n\n== Further reading ==\nNelson, Scott Reynolds. Oceans of Grain: How American Wheat Remade the World (2022) excerpt\nLinsi, Lukas; Burgoon, Brian; Mügge, Daniel K (2023). \"The Problem with Trade Measurement in International Relations\". International Studies Quarterly. 67 (2).\n\n\n== Sources ==\n\n\n== External links ==\n\n\n=== Data ===\n\n\n==== Statistics from intergovernmental sources ====\nData on the value of exports and imports and their quantities often broken down by detailed lists of products are available in statistical collections on international trade published by the statistical services of intergovernmental and supranational organisations and national statistical institutes. The definitions and methodological concepts applied for the various statistical collections on international trade often differ in terms of definition (e.g. special trade vs. general trade) and coverage (reporting thresholds, inclusion of trade in services, estimates for smuggled goods and cross-border provision of illegal services). Metadata providing information on definitions and methods are often published along with the data.\n\nUnited Nations Commodity Trade Database\nTrade Map, trade statistics for international business development\nWTO Statistics Portal\nStatistical Portal: OECD\nEuropean Union International Trade in Goods Data\nFood and Agricultural Trade Data Archived 2010-07-10 at the Wayback Machine by FAO\n\n\n=== Other external links ===\nThe MIT Observatory of Economic Complexity\nThe McGill Faculty of Law runs a Regional Trade Agreements Database that contains the text of almost all preferential and regional trade agreements in the world.  ptas.mcgill.ca\nHistorical documents on international trade available on FRASER (St Louis Fed)",
  },
  {
    title: "Environmental economics",
    originalContent:
      'Environmental economics is a sub-field of economics concerned with environmental issues. It has become a widely studied subject due to growing environmental concerns in the twenty-first century. Environmental economics "undertakes theoretical or empirical studies of the economic effects of national or local environmental policies around the world. ... Particular issues include the costs and benefits of alternative environmental policies to deal with air pollution, water quality, toxic substances, solid waste, and global warming."\nEnvironmental economics is distinguished from ecological economics in that ecological economics emphasizes the economy as a subsystem of the ecosystem with its focus upon preserving natural capital. One survey of German economists found that ecological and environmental economics are different schools of economic thought, with ecological economists emphasizing "strong" sustainability and rejecting the proposition that human-made ("physical") capital can substitute for natural capital.\n\n\n== History ==\nThe modern field of environmental economics has been traced to the 1960s with significant contribution from Post-Keynesian economist Paul Davidson, who had just completed a management position with the Continental Oil Company.\n\n\n== Topics and concepts ==\n\n\n=== Market failure ===\n\nCentral to environmental economics is the concept of market failure. Market failure means that markets fail to allocate resources efficiently. As stated by Hanley, Shogren, and White (2007): "A market failure occurs when the market does not allocate scarce resources to generate the greatest social welfare. A wedge exists between what a private person does given market prices and what society might want him or her to do to protect the environment. Such a wedge implies wastefulness or economic inefficiency; resources can be reallocated to make at least one person better off without making anyone else worse off." This results in a inefficient market that needs to be corrected through avenues such as government intervention. Common forms of market failure include externalities, non-excludability and non-rivalry.\n\n\n=== Externality ===\nAn externality exists when a person makes a choice that affects other people in a way that is not accounted for in the market price. An externality can be positive or negative but is usually associated with negative externalities in environmental economics. For instance, water seepage in residential buildings occurring in upper floors affect the lower floors. Another example concerns how the sale of Amazon timber disregards the amount of carbon dioxide released in the cutting. Or a firm emitting pollution will typically not take into account the costs that its pollution imposes on others.  As a result, pollution may occur in excess of the \'socially efficient\' level, which is the level that would exist if the market was required to account for the pollution. A classic definition influenced by Kenneth Arrow and James Meade is provided by Heller and Starrett (1976), who define an externality as "a situation in which the private economy lacks sufficient incentives to create a potential market in some good and the nonexistence of this market results in losses of Pareto efficiency".  In economic terminology, externalities are examples of market failures, in which the unfettered market does not lead to an efficient outcome.\n\n\n=== Common goods and public goods ===\nWhen it is too costly to exclude some people from access to an environmental resource, the resource is either called a common property resource (when there is rivalry for the resource, such that one person\'s use of the resource reduces others\' opportunity to use the resource) or a public good (when use of the resource is non-rivalrous). In either case of non-exclusion, market allocation is likely to be inefficient.\nThese challenges have long been recognized. Hardin\'s (1968) concept of the tragedy of the commons popularized the challenges involved in non-exclusion and common property. "Commons" refers to the environmental asset itself, "common property resource" or "common pool resource" refers to a property right regime that allows for some collective body to devise schemes to exclude others, thereby allowing the capture of future benefit streams; and "open-access" implies no ownership in the sense that property everyone owns nobody owns.\nThe basic problem is that if people ignore the scarcity value of the commons, they can end up expending too much effort, over harvesting a resource (e.g., a fishery). Hardin theorizes that in the absence of restrictions, users of an open-access resource will use it more than if they had to pay for it and had exclusive rights, leading to environmental degradation.  See, however, Ostrom\'s (1990) work on how people using real common property resources have worked to establish self-governing rules to reduce the risk of the tragedy of the commons.\nThe mitigation of climate change effects is an example of a public good, where the social benefits are not reflected completely in the market price. Because the personal marginal benefits are less than the social benefits the market under-provides climate change mitigation. This is a public good since the risks of climate change are both non-rival and non-excludable.  Such efforts are non-rival since climate mitigation provided to one does not reduce the level of mitigation that anyone else enjoys.  They are non-excludable actions as they will have global consequences from which no one can be excluded. A country\'s incentive to invest in carbon abatement is reduced because it can "free ride" off the efforts of other countries. Over a century ago, Swedish economist Knut Wicksell (1896) first discussed how public goods can be under-provided by the market because people might conceal their preferences for the good, but still enjoy the benefits without paying for them.\n\n\tGlobal biochemical cycles\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n=== Valuation ===\nAssessing the economic value of the environment is a major topic within the field. The values of natural resources often are not reflected in prices that markets set and, in fact, many of them are available at no monetary charge. This mismatch frequently causes distortions in pricing of natural assets: both overuse of them and underinvestment in them. Economic value or tangible benefits of ecosystem services and, more generally, of natural resources, include both use and indirect (see the nature section of ecological economics). Non-use values include existence, option, and bequest values. For example, some people may value the existence of a diverse set of species, regardless of the effect of the loss of a species on ecosystem services. The existence of these species may have an option value, as there may be the possibility of using it for some human purpose. For example, certain plants may be researched for drugs. Individuals may value the ability to leave a pristine environment for their children.\nUse and indirect use values can often be inferred from revealed behavior, such as the cost of taking recreational trips or using hedonic methods in which values are estimated based on observed prices.  Non-use values are usually estimated using stated preference methods such as contingent valuation or choice modelling. Contingent valuation typically takes the form of surveys in which people are asked how much they would pay to observe and recreate in the environment (willingness to pay) or their willingness to accept (WTA) compensation for the destruction of the environmental good. Hedonic pricing examines the effect the environment has on economic decisions through housing prices, traveling expenses, and payments to visit parks.\n\n\n=== State subsidy ===\nAlmost all governments and states magnify environmental harm by providing various types of subsidies that have the effect of paying companies and other economic actors more to exploit natural resources than to protect them. The damage to nature of such public subsidies has been conservatively estimated at $4-$6 trillion U.S. dollars per year.\n\n\n== Solutions ==\nSolutions advocated to correct such externalities include:\n\nEnvironmental regulations. Under this plan, the economic impact has to be estimated by the regulator. Usually, this is done using cost–benefit analysis.  There is a growing realization that regulations (also known as "command and control" instruments) are not so distinct from economic instruments as is commonly asserted by proponents of environmental economics. E.g.1 regulations are enforced by fines, which operate as a form of tax if pollution rises above the threshold prescribed. E.g.2 pollution must be monitored and laws enforced, whether under a pollution tax regime or a regulatory regime.  The main difference an environmental economist would argue exists between the two methods, however, is the total cost of the regulation.  "Command and control" regulation often applies uniform emissions limits on polluters, even though each firm has different costs for emissions reductions, i.e., some firms, in this system, can abate pollution inexpensively, while others can only abate it at high cost.  Because of this, the total abatement in the system comprises some expensive and some inexpensive efforts.  Consequently, modern "Command and control" regulations are oftentimes designed in a way that addresses these issues by incorporating utility parameters. For instance, CO2 emission standards for specific manufacturers in the automotive industry are either linked to the average vehicle footprint (US system) or average vehicle weight (EU system) of their entire vehicle fleet. Environmental economic regulations find the cheapest emission abatement efforts first, and then move on to the more expensive methods.  E.g. as said earlier, trading, in the quota system, means a firm only abates pollution if doing so would cost less than paying someone else to make the same reduction.  This leads to a lower cost for the total abatement effort as a whole.\nQuotas on pollution. Often it is advocated that pollution reductions should be achieved by way of tradeable emissions permits, which if freely traded may ensure that reductions in pollution are achieved at least cost. In theory, if such tradeable quotas are allowed, then a firm would reduce its own pollution load only if doing so would cost less than paying someone else to make the same reduction, i.e., only if buying tradeable permits from another firm(s) is costlier. In practice, tradeable permits approaches have had some success, such as the U.S.\'s sulphur dioxide trading program or the EU Emissions Trading Scheme, and interest in its application is spreading to other environmental problems.\nTaxes and tariffs on pollution. Increasing the costs of polluting will discourage polluting, and will provide a "dynamic incentive", that is, the disincentive continues to operate even as pollution levels fall.  A pollution tax that reduces pollution to the socially "optimal" level would be set at such a level that pollution occurs only if the benefits to society (for example, in form of greater production) exceeds the costs. This concept was introduced by Arthur Pigou, a British economist active in the late nineteenth through the mid-twentieth century. He showed that these externalities occur when markets fail, meaning they do not naturally produce the socially optimal amount of a good or service. He argued that “a tax on the production of paint would encourage the [polluting] factory to reduce production to the amount best for society as a whole.” These taxes are known amongst economists as Pigouvian Taxes, and they regularly implemented where negative externalities are present. Some advocate a major shift from taxation from income and sales taxes to tax on pollution – the so-called "green tax shift".\nBetter defined property rights. The Coase Theorem states that assigning property rights will lead to an optimal solution, regardless of who receives them, if transaction costs are trivial and the number of parties negotiating is limited. For example, if people living near a factory had a right to clean air and water, or the factory had the right to pollute, then either the factory could pay those affected by the pollution or the people could pay the factory not to pollute. Or, citizens could take action themselves as they would if other property rights were violated. The US River Keepers Law of the 1880s was an early example, giving citizens downstream the right to end pollution upstream themselves if the government itself did not act (an early example of bioregional democracy).  Many markets for "pollution rights" have been created in the late twentieth century—see emissions trading. According to the Coase Theorem, the involved parties will bargain with each other, which results in an efficient solution. However, modern economic theory has shown that the presence of asymmetric information may lead to inefficient bargaining outcomes. Specifically, Rob (1989) has shown that pollution claim settlements will not lead to the socially optimal outcome when the individuals that will be affected by pollution have learned private information about their disutility already before the negotiations take place. Goldlücke and Schmitz (2018) have shown that inefficiencies may also result if the parties learn their private information only after the negotiations, provided that the feasible transfer payments are bounded. Using cooperative game theory, Gonzalez, Marciano and Solal (2019) have shown that in social cost problems involving more than three agents, the Coase theorem suffers from many counterexamples and that only two types of property rights lead to an optimal solution.\nAccounting for environmental externalities in the final price. In fact, the world\'s largest industries burn about $7.3 trillion of free natural capital per year. Thus, the world\'s largest industries would hardly be profitable if they had to pay for this destruction of natural capital. Trucost has assessed over 100 direct environmental impacts and condensed them into 6 key environmental performance indicators (EKPIs). The assessment of environmental impacts is derived from different sources (academic journals, governments, studies, etc.) due to the lack of market prices. The table below gives an overview of the 5 regional sectors per EKPI with the highest impact on the overall EKPI:\n\nIf companies are allowed to include some of these externalities in their final prices, this could undermine the Jevons paradox and provide enough revenue to help companies innovate.\n\n\n== Relationship to other fields ==\nEnvironmental economics is related to ecological economics but there are differences. Most environmental economists have been trained as economists. They apply the tools of economics to address environmental problems, many of which are related to so-called market failures—circumstances wherein the "invisible hand" of economics is unreliable.  Most ecological economists have been trained as ecologists, but have expanded the scope of their work to consider the impacts of humans and their economic activity on ecological systems and services, and vice versa. This field takes as its premise that economics is a strict subfield of ecology.  Ecological economics is sometimes described as taking a more pluralistic approach to environmental problems and focuses more explicitly on long-term environmental sustainability and issues of scale.\nEnvironmental economics is viewed as more idealistic in a price system; ecological economics as more realistic in its attempts to integrate elements outside of the price system as primary arbiters of decisions. These two groups of specialisms sometimes have conflicting views which may be traced to the different philosophical underpinnings.\nAnother context in which externalities apply is when globalization permits one player in a market who is unconcerned with biodiversity to undercut prices of another who is – creating a race to the bottom in regulations and conservation. This, in turn, may cause loss of natural capital with consequent erosion, water purity problems, diseases, desertification, and other outcomes that are not efficient in an economic sense. This concern is related to the subfield of sustainable development and its political relation, the anti-globalization movement.\n\nEnvironmental economics was once distinct from resource economics. Natural resource economics as a subfield began when the main concern of researchers was the optimal commercial exploitation of natural resource stocks. But resource managers and policy-makers eventually began to pay attention to the broader importance of natural resources (e.g. values of fish and trees beyond just their commercial exploitation). It is now difficult to distinguish "environmental" and "natural resource" economics as separate fields as the two became associated with sustainability.  Many of the more radical green economists split off to work on an alternate political economy.\nEnvironmental economics was a major influence on the theories of natural capitalism and environmental finance, which could be said to be two sub-branches of environmental economics concerned with resource conservation in production, and the value of biodiversity to humans, respectively.  The theory of natural capitalism (Hawken, Lovins, Lovins) goes further than traditional environmental economics by envisioning a world where natural services are considered on par with physical capital.\nThe more radical green economists reject neoclassical economics in favour of a new political economy beyond capitalism or communism that gives a greater emphasis to the interaction of the human economy and the natural environment, acknowledging that "economy is three-fifths of ecology". This political group is a proponent of a transition to renewable energy.\nThese more radical approaches would imply changes to money supply and likely also a bioregional democracy so that political, economic, and ecological "environmental limits" were all aligned, and not subject to the arbitrage normally possible under capitalism.\nAn emerging sub-field of environmental economics studies its intersection with development economics. Dubbed "envirodevonomics" by Michael Greenstone and B. Kelsey Jack in their paper "Envirodevonomics: A Research Agenda for a Young Field", the sub-field is primarily interested in studying "why environmental quality [is] so poor in developing countries." A strategy for better understanding this correlation between a country\'s GDP and its environmental quality involves analyzing how many of the central concepts of environmental economics, including market failures, externalities, and willingness to pay, may be complicated by the particular problems facing developing countries, such as political issues, lack of infrastructure, or inadequate financing tools, among many others.\nIn the field of law and economics, environmental law is studied from an economic perspective. The economic analysis of environmental law studies instruments such as zoning, expropriation, licensing, third party liability, safety regulation, mandatory insurance, and criminal sanctions. A book by Michael Faure (2003) surveys this literature.\n\n\n== Professional bodies ==\nThe main academic and professional organizations for the discipline of Environmental Economics are the Association of Environmental and Resource Economists (AERE) and the European Association for Environmental and Resource Economics (EAERE).  The main academic and professional organization for the discipline of Ecological Economics is the International Society for Ecological Economics (ISEE). The main organization for Green Economics is the Green Economics Institute.\n\n\n== By country ==\n\n\n=== India ===\nThe Indian government promotes the Bharatiya model of development considered different from western models. The Economic Survey for the year 2024, noted that often solutions to address climate change “are fuelled by a market society, which seeks to substitute the means to achieve overconsumption rather than addressing overconsumption itself”. The report argued that India needs a different approach and a “Bharatiya Model of Development”, linked to the principles of sustainability and to the Indian philosophy, can help.\n\n\n== See also ==\n\n\n=== Hypotheses and theorems ===\nCoase theorem\nPorter hypothesis\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==',
  },
  {
    title: "Sports economics",
    originalContent:
      'Sports economics is a discipline of economics focused on its relationship to sports. It covers both the ways in which economists can study the distinctive institutions of sports, and the ways in which sports can allow economists to research many topics, including discrimination and antitrust law. The theoretical foundations of the discipline are heavily based on microeconomics. As of 2006, about 100 to 120 college professors taught sports economics courses. As of 2024 there are a number of important locations where sports economics is taught and researched by a group of faculty. This includes Bielefeld, Cork, Liverpool, Reading and Zurich in Europe, as well as Michigan and West Virginia in the United States. The community normally meets annually with prestigious events including the North American Association of Sports Economists (NAASE) Conference and the European Sport Economics Association (ESEA) Conference.\n\n\n== The beginning of sports economics ==\nSimon Rottenberg is credited with likely being the first to pen a scholarly article in the field of sports economics when he wrote his 1956 article on the Uncertainty of Outcome Hypothesis. In this article, Rottenberg highlights the relationship of attendance at baseball games with things such as price, alternate activities, how good the team is, how large of a market the team is in, and so on. Importantly, Rottenberg made mention of “dispersion of games won by team in the league.”\n\n\n== Key components ==\nCompetitive balance is one of the most important ideas within sports economics. This idea, in general, refers to the comparison of wins between all teams in a league. Rottenberg effectively built this seminal idea with his interest in "dispersion of games won." Related to competitive balance is the understanding of different leagues and different team within those leagues objectives. Understanding the ownership structure and motives of front office personnel through their financial, read economic, decisions will reveal whether a team is looking to only generate profit, attempt to win a championship, or something entirely different. Making sense of human behavior through data is the central idea of economics and certainly applies to Sports Economics as well. Sports leagues look to promote competitive balance to make more games appealing to fans to watch, both in person and on TV. Many European leagues achieve competitive balance through promotion and relegation systems where multiple leagues are intertwined with teams moving between leagues based upon performance.  Most sports leagues in the United States are standalone league and work towards competitive balance through other measures. These can include salary caps and roster size limits.\n\n\n== Importance ==\nSometimes sports economics is dismissed as a side hobby for number crunchers. However, the proliferation of globalized sports markets as well as the extreme rise in sports media demonstrates its importance. In the United States, the Super Bowl regularly commands the attention of millions. In the EU, over 15 million people are employed in the sports world. On a larger scale, sports and sporting outlets provide health benefits as well as general satisfaction for a citizenry which are both prominent state concerns the world over. Another feature of sports economics is the data-rich environment in sports, from which economists can apply and investigate common economic models or problems, thus contributing to the field of economics at large.\n\n\n== References ==',
  },
  {
    title: "Urban economics",
    originalContent:
      "Urban economics is broadly the economic study of urban areas; as such, it involves using the tools of economics to analyze urban issues such as crime, education, public transit, housing, and local government finance. More specifically, it is a branch of microeconomics that studies the urban spatial structure and the location of households and firms (Quigley 2008).\nHistorically, much like economics generally, urban economics was influenced by multiple schools of thought, including original institutional economics and Marxist economics. These heterodox economic currents continue to be used  in contemporary political-economic analyses of cities. But, most urban economics today is neoclassical in orientation and centred largely around urban experiences in the Global North. This dominant urban economics also influences mainstream media like The Economist. Today, much urban economic analysis relies on a particular model of urban spatial structure, the monocentric city model pioneered in the 1960s by William Alonso, Richard Muth, and Edwin Mills. While most other forms of neoclassical economics do not account for spatial relationships between individuals and organizations, urban economics focuses on these spatial relationships to understand the economic motivations underlying the formation, functioning, and development of cities.\nSince its formulation in 1964, Alonso's monocentric city model of a disc-shaped Central Business District (CBD) and the surrounding residential region has served as a starting point for urban economic analysis. Monocentricity has weakened over time because of changes in technology, particularly, faster and cheaper transportation (which makes it possible for commuters to live farther from their jobs in the CBD) and communications (which allow back-office operations to move out of the CBD).\nAdditionally, recent research has sought to explain the polycentricity described in Joel Garreau's Edge City. Several explanations for polycentric expansion have been proposed and summarized in models that account for factors such as utility gains from lower average land rents and increasing (or constant) returns due to economies of agglomeration (Strange 2008).\n\n\n== Introduction ==\nUrban economics is rooted in the location theories of von Thünen, Alonso, Christaller, and Lösch that began the process of spatial economic analysis (Capello & Nijkamp 2004:3–4). Economics is the study of the allocation of scarce resources, and as all economic phenomena take place within a geographical space, urban economics focuses on the allocation of resources across space in relation to urban areas (Arnott & McMillen 2006:7) (McCann 2001:1). Other branches of economics ignore the spatial aspects of decision making but urban economics focuses not only on the location decisions of firms but also of cities themselves as cities themselves represent centers of economic activity (O'Sullivan 2003:1).\nMany spatial economic topics can be analyzed within either an urban or regional economics framework as some economic phenomena primarily affect localized urban areas while others are felt over much larger regional areas (McCann 2001:3). Arthur O'Sullivan believes urban economics is divided into six related themes: market forces in the development of cities, land use within cities, urban transportation, urban problems and public policy, housing and public policy, and local government expenditures and taxes. (O'Sullivan 2003:13–14).\n\n\n== Market forces in the development of cities ==\nMarket forces in the development of cities relate to how the location decision of firms and households causes the development of cities. The nature and behavior of markets depend somewhat on their locations therefore market performance partly depends on geography.(McCann 2001:1). If a firm locates in a geographically isolated region, its market performance will be different than a firm located in a concentrated region. The location decisions of both firms and households create cities that differ in size and economic structure. When industries cluster, like in Silicon Valley in California, they create urban areas with dominant firms and distinct economies.\nBy looking at location decisions of firms and households, the urban economist is able to address why cities develop where they do, why some cities are large and others small, what causes economic growth and decline, and how local governments affect urban growth (O'Sullivan 2003:14). Because urban economics is concerned with asking questions about the nature and workings of the economy of a city, models and techniques developed within the field are primarily designed to analyze phenomena that are confined within the limits of a single city (McCann 2001:2).\n\n\n== Land use ==\nLooking at land use within metropolitan areas, the urban economist seeks to analyze the spatial organization of activities within cities. In attempts to explain observed patterns of land use, the urban economist examines the intra-city location choices of firms and households. Considering the spatial organization of activities within cities, urban economics addresses questions in terms of what determines the price of land and why those prices vary across space, the economic forces that caused the spread of employment from the central core of cities outward, identifying land-use controls, such as zoning, and interpreting how such controls affect the urban economy (O'Sullivan 2003:14).\n\n\n== Economic policy ==\nEconomic policy is often implemented at the urban level thus economic policy is often tied to urban policy (McCann 2001:3). Urban problems and public policy tie into urban economics as the theme relates urban problems, such as poverty or crime, to economics by seeking to answer questions with economic guidance. For example, does the tendency for the poor to live close to one another make them even poorer? (O'Sullivan 2003:15).\n\n\n== Transportation and economics ==\nUrban transportation is a theme of urban economics because it affects land-use patterns as transportation affects the relative accessibility of different sites. Issues that tie urban transportation to urban economics include the deficit that most transit authorities have and efficiency questions about proposed transportation developments such as light-rail (O'Sullivan 2003:14).\n\n\n== Housing and public policy ==\nHousing and public policy relate to urban economics as housing is a unique type of commodity. Because housing is immobile, when a household chooses a dwelling, it is also choosing a location. Urban economists analyze the location choices of households in conjunction with the market effects of housing policies (O'Sullivan 2003:15).\nIn analyzing housing policies, we make use of market structures e.g., perfect market structure. There are however problems encountered in making this analysis such as funding, uncertainty, space, etc.\n\n\n== Government expenditures and taxes ==\nThe final theme of local government expenditures and taxes relates to urban economics as it analyzes the efficiency of the fragmented local governments presiding in metropolitan areas (O'Sullivan 2003:15).\n\n\n== See also ==\n\n\n== References ==\n\n\n== Literature ==\nArnott, Richard; McMillen, Daniel P., eds. (2006). A Companion to Urban Economics. Blackwell Publishing. ISBN 1-4051-0629-8.\nCapello, Roberta; Nijkamp, Peter, eds. (2004). Urban Dynamics and Growth: Advances in Urban Economics. Elsevier Inc.\nMcCann, Philip (2001). Urban and Regional Economics. Oxford University Press. ISBN 978-0-19-877645-1.\nObeng-Odoom, Franklin (2016). Reconstructing Urban Economics: Towards a Political Economy of the Built Environment.. Zed Books  ISBN 978-1-7836-0659-7\nObeng-Odoom, Franklin (2023). \"Urban Economics in the Global South: A Study of The Economist\", Urban Challenge Journal, vol. 34, no. 1, pp. 107 -118.\nO'Sullivan, Arthur (2003). Urban economics. Boston, Mass: McGraw-Hill/Irwin. ISBN 0-07-248784-4.\nQuigley, John M. (2008). \"Urban economics\". The New Palgrave Dictionary of Economics (2nd ed.).\nStrange, William C. (2008). \"Urban agglomeration\". The New Palgrave Dictionary of Economics (2nd ed.).\n\n\n== Further reading ==\nGarreau, Joel. Edge City: Life on the New Frontier. 1992. Anchor. ISBN 978-0-385-42434-9.\nGoldstein, Gerald S.; Moses, Leon N. (1973). \"A Survey of Urban Economics\". Journal of Economic Literature. 11 (2): 471–515.\nKahn, Matthew. Green Cities: Urban Growth and the Environment. 2006. Brookings ISBN 978-0-8157-4816-8.\nObeng-Odoom, Franklin. Reconstructing Urban Economics: Towards a Political Economy of the Built Environment. 2016. Zed. ISBN 978-1-7836-0659-7\nStilwell, Frank. Understanding Cities & Regions: Spatial Political Economy. 1993 ISBN 978-0-9491-3888-0.",
  },
  {
    title: "Trade policy",
    originalContent:
      "A commercial policy (also referred to as a trade policy or international trade policy) is a government's policy governing international trade. Commercial policy is an all encompassing term that is used to cover topics which involve international trade.  Trade policy is often described in terms of a scale between the extremes of free trade (no restrictions on trade) on one side and protectionism (high restrictions to protect local producers) on the other. A common commercial policy can sometimes be agreed by treaty within a customs union, as with the European Union's common commercial policy and in Mercosur.\nA nation's commercial policy will include and take into account the policies adopted by that nation's government while negotiating international trade. There are several factors that can affect a nation's commercial policy, all of which can affect international trade policies.\n\n\n== Theories on international trade policy ==\nTrade policy has been controversial since the days of mercantilism. Economics (or political economy) has developed in major part as an effort to make clear various effects of trade policies. See International trade theory. The hottest topic in economic policy is upgrading in Global Value Chains.\n\n\n== Types and aspects of Commercial policy ==\n\n\n=== Regionalism ===\nRegionalism, or Regional Trade Agreements (RTA), are trade policies and agreements that are crafted by the nations in a region for the purposes of increasing international trade in the area. RTAs have been described by supporters as a means of increasing free trade with the goal of eventually merging into larger, either bilateral or multilateral, trade deals. The more relatively local area of RTAs are useful in resolving trade issues as well without causing gridlock in other trade agreements. Critics of RTAs say that they are a hindrance to the negotiation of trade because they can be lopsided or unfairly beneficial to one side over the other sides, particularly if some of the participants are nations that are still in development.\nAs China was rising in economic power and prominence, they turned to regionalism as a strategic method of leveling the playing field with Europe and the United States. In 2000, China signed the Bangkok agreement with the Association of Southeast Asian Nations (ASEAN) to reduce tariffs in the region. The signing of the agreement also began the push for a formal Free Trade Agreement between China and ASEAN. However, strained relations between China and other Asian nations such as Japan have prevented the same level of regional FTAs to be put in place with Northeast Asia.\n\n\n=== Bilateral Free Trade Agreements ===\nA bilateral Free Trade Agreement is when two countries agree to exchange goods to promote trade and investments elimination barriers such as tariffs, import quotas, and export restrains. The United States has signed such treaties as the North American Free Trade Agreement in 1994 as well as with Israel in the 1980s. Experts who support such free trade agreements argue that these agreements increase competition while offering business the ability to reach larger markets. Critics of bilateral agreements claim that a larger nation, such as the United States, can use these agreements to unfairly push smaller states into much harsher work loads than the World Trade Organization already requires.\nRelations between the European Union and South Korea have led to both parties signing several bilateral agreements regarding trade policy. In 2009, South Korea and the EU signed the EU-Korea Free Trade Agreement. The signing of the agreement created an FTA that is second only to NAFTA in size. The agreement held the benefits of increased free trade between the participants in the FTA as well as increased challenge to the United States.\n\n\n=== Preferential Trade Agreements ===\nPreferential agreements are trade deals that involve nations making deals with specific countries that can aid the interests of one another as opposed to the nondiscriminatory deals that are pushed by the WTO. Nations have been increasingly preferring such deals since the 1950s as they are quicker to show gains for the parties involved in the agreements. A common argument that has been made is that it allows businesses to open up markets that would otherwise be considered closed and therefore falls into the free trade idea that most countries will push for. Countries that have similar levels of GDP and a higher scope in their economies as well as their relative position to one another and the rest of the world are more likely to have preferential trade agreements. PTAs can also be applied to regional areas with unions such as NAFTA, the European Union, and ASEAN being examples of regional PTAs.\nThose who opposer PTAs argue that these deals have increased the importance of where a product is made so that tariffs can be applied accordingly. The certification of a product's origin also unfairly holds back smaller countries that have less resources to spend. Others argue that PTAs can hinder negotiations of trade disputes and places an emphasis of which country has more power.\n\n\n== Ways in which commercial policy is affected ==\n\n\n=== Tariffs ===\nTrade tariffs are a tax that are placed on the import of foreign goods. Tariffs increase the price of imports and are usually levied onto the country the goods are being imported from. Governments will use tariffs as a way to promote competition within their own country with businesses of the foreign country that wishes to sell their goods or services. In some instances, a country's government will use them as a means of protectionism for their own interests. In modern history, generally starting at the mid-20th century, the use of tariffs has been largely diminished in favor of the rise of international trade. Beginning in 2017, the Trump administration began to impose tariffs on several of nations that were involved in trade deals with the United States. The countries targeted by the Trump Tariffs then retaliated with their own tariffs on American goods.\n\n\n=== Import Quotas ===\nImport quotas are the limitations of the amount of goods that can be imported into the country from foreign businesses. Generally, an import quota is set for a specific period of time with one year being the most common metric. Some versions of the quotas limits the quantity of specific goods being imported into a country while other versions place the limit on the value of those goods. The objectives of quotas can include: the protections of a nations interests, ensuring a balance of trade so as not to create deficits, retaliation to restrictive trade policies of other countries that do business on the international playing field.\n\n\n== References ==\n\n\n== External links ==\n\n Media related to Commercial policy at Wikimedia Commons",
  },
  {
    title: "Sovereign debt",
    originalContent:
      "A country's gross government debt (also called public debt or sovereign debt) is the financial liabilities of the government sector.: 81  Changes in government debt over time reflect primarily borrowing due to past government deficits. A deficit occurs when a government's expenditures exceed revenues.: 79–82  Government debt may be owed to domestic residents, as well as to foreign residents. If owed to foreign residents, that quantity is included in the country's external debt.\nIn 2020, the value of government debt worldwide was $87.4 US trillion, or 99% measured as a share of gross domestic product (GDP). Government debt accounted for almost 40% of all debt (which includes corporate and household debt), the highest share since the 1960s. The rise in government debt since 2007 is largely attributable to stimulus measures during the Great Recession, and the COVID-19 recession.\nThe ability of government to issue debt has been central to state formation and to state building. Public debt has been linked to the rise of democracy, private financial markets, and modern economic growth.\n\n\n== Measuring government debt ==\n\nGovernment debt is typically measured as the gross debt of the general government sector that is in the form of liabilities that are debt instruments.: 207  A debt instrument is a financial claim that requires payment of interest and/or principal by the debtor to the creditor in the future. Examples include debt securities (such as bonds and bills), loans, and government employee pension obligations.: 207 \nInternational comparisons usually focus on general government debt because the level of government responsible for programs (for example, health care) differs across countries and the general government comprises central, state, provincial, regional, local governments, and social security funds.: 18, s2.58, s2.59  The debt of public corporations (such as post offices that provide goods or services on a market basis) is not included in general government debt, following the International Monetary Fund's Government Finance Statistics Manual 2014 (GFSM), which describes recommended methodologies for compiling debt statistics to ensure international comparability.: 33, s2.127 \nThe gross debt of the general government sector is the total liabilities that are debt instruments. An alternative debt measure is net debt, which is gross debt minus financial assets in the form of debt instruments.: 208, s7.243  Net debt estimates are not always available since some government assets may be difficult to value, such as loans made at concessional rates.: 208–209, s7.246 \nDebt can be measured at market value or nominal value. As a general rule, the GFSM says debt should be valued at market value, the value at which the asset could be exchanged for cash.: 55, s3.107  However, the nominal value is useful for a debt-issuing government, as it is the amount that the debtor owes to the creditor.: 191, ft28  If market and nominal values are not available, face value (the undiscounted amount of principal to be repaid at maturity): 56  is used.: 208, s7.238 \nA country's general government debt-to-GDP ratio is an indicator of its debt burden since GDP measures the value of goods and services produced by an economy during a period (usually a year). As well, debt measured as a percentage of GDP facilitates comparisons across countries of different size. The OECD views the general government debt-to-GDP ratio as a key indicator of the sustainability of government finance.\n\n\n== Causes of government debt accumulation ==\nAn important reason governments borrow is to act as an economic \"shock absorber\". For example, deficit financing can be used to maintain government services during a recession when tax revenues fall and expenses rise for say unemployment benefits. Government debt created to cover costs from major shock events can be particularly beneficial. Such events would include \n\na major war, like World War II;\na public health emergency like the COVID-19 recession; or\na severe economic downturn as with the Great Recession.\nIn the absence of debt financing, when revenues decline during a downturn, a government would need to raise taxes or reduce spending, which would exacerbate the negative event.\nWhile government borrowing may be desirable at times, a \"deficits bias\" can arise when there is disagreement among groups in society over government spending. To counter deficit bias, many countries have adopted balanced budget rules or restrictions on government debt. Examples include the \"debt anchor\" in Sweden; a \"debt brake\" in Germany and Switzerland; and the European Union's Stability and Growth Pact agreement to maintain a general government gross debt of no more than 60% of GDP.\n\n\n== Historic benchmarks ==\n\nThe ability of government to issue debt has been central to state formation and to state building. Public debt has been linked to the rise of democracy, private financial markets, and modern economic growth. For example, in the 17th and 18th centuries England established a parliament that included creditors, as part of a larger coalition, whose authorization had to be secured for the country to borrow or raise taxes. This institution improved England's ability to borrow because lenders were more willing to hold the debt of a state with democratic institutions that would support debt repayment, versus a state where the monarch could not be compelled to repay debt.\nAs public debt came to be recognized as a safe and liquid investment, it could be used as collateral for private loans. This created a complementarity between the development of public debt markets and private financial markets. Government borrowing to finance public goods, such as urban infrastructure, has been associated with modern economic growth.: 6 \nWritten records point to public borrowing as long as two thousand years ago when Greek city-states such as Syracuse borrowed from their citizens.: 10–16  But the founding of the Bank of England in 1694 revolutionised public finance and put an end to defaults such as the Great Stop of the Exchequer of 1672, when Charles II had suspended payments on his bills. From then on, the British Government would never fail to repay its creditors. In the following centuries, other countries in Europe and later around the world adopted similar financial institutions to manage their government debt.\n\nIn 1815, at the end of the Napoleonic Wars, British government debt reached a peak of more than 200% of GDP, nearly 887 million pounds sterling. The debt was paid off over 90 years by running primary budget surpluses (that is, revenues were greater than spending after payment of interest).\nIn 1900, the country with the most total debt was France (£1,086,215,525), followed by Russia (£656,000,000) then the United Kingdom (£628,978,782); on a per-capita basis, the highest-debt countries were New Zealand (£58 12s. per person), the Australian colonies (£52 13s.) and Portugal (£35).\nIn 2018, global government debt reached the equivalent of $66 trillion, or about 80% of global GDP, and by 2020, global government debt reached $87US trillion, or 99% of global GDP. The COVID-19 pandemic caused public debt to soar in 2020, particularly in advanced economies that put in place sweeping fiscal measures.\n\n\n== Impacts of government debt ==\n\nGovernment debt accumulation may lead to a rising interest rate, which can crowd out private investment as governments compete with private firms for limited investment funds. Some evidence suggests growth rates are lower for countries with government debt greater than around 80 percent of GDP. A World Bank Group report that analyzed debt levels of 100 developed and developing countries from 1980 to 2008 found that debt-to-GDP ratios above 77% for developed countries (64% for developing countries) reduced future annual economic growth by 0.017 (0.02 for developing countries) percentage points for each percentage point of debt above the threshold.\nExcessive debt levels may make governments more vulnerable to a debt crisis, where a country is unable to make payments on its debt, and it cannot borrow more. Crises can be costly, particularly if a debt crisis is combined with a financial/banking crisis which leads to economy-wide deleveraging. As firms sell assets to pay off debt, asset prices fall which risks an even greater fall in incomes, further depressing tax revenue and requiring governments to drastically cut government services. Examples of debt crises include the Latin American debt crisis of the early 1980s, and Argentina's debt crisis in 2001. To help avoid a crisis, governments may want to maintain a \"fiscal breathing space\". Historical experience shows that room to double the level of government debt when needed is an approximate guide.\nGovernment debt is built up by borrowing when expenditure exceeds revenue, so government debt generally creates an intergenerational transfer. This is because the beneficiaries of the government's expenditure on goods and services when the debt is created typically differ from the individuals responsible for repaying the debt in the future.\nAn alternative view of government debt, sometimes called the Ricardian equivalence proposition, is that government debt has no impact on the economy if individuals are altruistic and internalize the impact of the debt on future generations. According to this proposition, while the quantity of government purchases affects the economy, debt financing will have the same impact as tax financing because with debt financing individuals will anticipate the future taxes needed to repay the debt, and so increase their saving and bequests by the amount of government debt. Such higher individual saving means, for example, that private consumption falls one-for-one with the rise in government debt, so the interest rate would not rise and private investment is not crowded out.\n\n\n== Risk ==\n\n\n=== Credit (Default) risk ===\n\nHistorically, there have been many cases where governments have defaulted on their debts, including Spain in the 16th and 17th centuries, which nullified its government debt several times; the Confederate States of America, whose debt was not repaid after the American Civil War; and revolutionary Russia after 1917, which refused to accept responsibility for Imperial Russia's foreign debt.\nIf government debt is issued in a country's own fiat money, it is sometimes considered risk free because the debt and interest can be repaid by money creation. However, not all governments issue their own currency. Examples include sub-national governments, like municipal, provincial, and state governments; and countries in the eurozone. In the Greek government-debt crisis, one proposed solution was for Greece to leave the eurozone and go back to issuing the drachma (although this would have addressed only future debt issuance, leaving substantial existing debt denominated in what would then be a foreign currency).\nDebt of a sub-national government is generally viewed as less risky for a lender if it is explicitly or implicitly guaranteed by a regional or national level of government. When New York City declined into what would have been bankrupt status during the 1970s, a bailout came from New York State and the United States national government. U.S. state and local government debt is substantial — in 2016 their debt amounted to $3 trillion, plus another $5 trillion in unfunded liabilities.\n\n\n=== Inflation risk ===\nA country that issues its own currency may be at low risk of default in local currency, but if a central bank provides finance by buying government bonds (sometimes referred to as debt monetization), this can lead to price inflation. In an extreme case, in the 1920s Weimar Germany suffered from hyperinflation when the government used money creation to pay off the national debt following World War I.\n\n\n=== Exchange rate risk ===\nWhile U.S. Treasury bonds denominated in U.S. dollars may be considered risk-free to an American purchaser, a foreign investor bears the risk of a fall in the value of the U.S. dollar relative to their home currency. A government can issue debt in foreign currency to eliminate exchange rate risk for foreign lenders, but that means the borrowing government then bears the exchange rate risk. Also, by issuing debt in foreign currency, a country cannot erode the value of the debt by means of inflation. Almost 70% of all debt in a sample of developing countries from 1979 through 2006 was denominated in U.S. dollars.\n\n\n== Implicit and contingent liabilities ==\nMost governments have contingent liabilities, which are obligations that do not arise unless a particular event occurs in the future.: 76  An example of an explicit contingent liability is a public sector loan guarantee, where the government is required to make payments only if the debtor defaults.: 210, s.7.252  Examples of implicit contingent liabilities include ensuring the payment of future social security pension benefits, covering the obligations of subnational governments in the event of a default, and spending for natural disaster relief.: 209–210 \nExplicit contingent liabilities and net implicit social security obligations should be included as memorandum items to a government's balance sheet,: 69, 76–77, 209–212  but they are not included in government debt because they are not contractual obligations.: 210, s.7.252  Indeed, it is not uncommon for governments to change unilaterally the benefit structure of social security schemes, for example (e.g., by changing the circumstances under which the benefits become payable, or the amount of the benefit).: 76, s4.49  In the U.S. and in many countries, there is no money earmarked for future social insurance payments — the system is called a pay-as-you-go scheme. According to the 2018 annual reports from the trustees for the U.S. Social Security and Medicare trust funds, Medicare is facing a $37 trillion unfunded liability over the next 75 years, and Social Security is facing a $13 trillion unfunded liability over the same time frame. Neither of these amounts are included in the U.S. gross general government debt, which in 2024 was $34 trillion.\nIn 2010 the European Commission required EU Member Countries to publish their debt information in standardized methodology, explicitly including debts that were previously hidden in a number of ways to satisfy minimum requirements on local (national) and European (Stability and Growth Pact) level.\n\n\n== See also ==\nGovernment finance:\n\nSpecific:\n\nGeneral:\n\n\n== References ==\n\n\n== External links ==\n\nThe IMF Public Financial Management Blog\nOECD government debt statistics\nJapan's Central Government Debt\nRiksgäldskontoret – Swedish national debt office\nWhat is Sovereign Debt\nUnited States Treasury, Bureau of Public Debt – The Debt to the Penny and Who Holds It Archived 2011-04-18 at the Wayback Machine\nSlaying the Dragon of Debt, Regional Oral History Office, The Bancroft Library, University of California, Berkeley\nA historical collection of documents on or referring to government spending and fiscal policy, available on FRASER\nEisner, Robert (1993). \"Federal Debt\". In David R. Henderson (ed.). Concise Encyclopedia of Economics (1st ed.). Library of Economics and Liberty. OCLC 317650570, 50016270, 163149563\n\"Government's Borrowing Power\". DebatedWisdom. 3IVIS GmbH. Retrieved 29 October 2016.\nDatabases\nCLYPS dataset on public debt level and composition in Latin America",
  },
  {
    title: "Fiscal policy",
    originalContent:
      'In economics and political science, fiscal policy is the use of government revenue collection (taxes or tax cuts) and expenditure to influence a country\'s economy. The use of government revenue expenditures to influence macroeconomic variables developed in reaction to the Great Depression of the 1930s, when the previous laissez-faire approach to economic management became unworkable. Fiscal policy is based on the theories of the British economist John Maynard Keynes, whose Keynesian economics theorised that government changes in the levels of taxation and government spending influence aggregate demand and the level of economic activity. Fiscal and monetary policy are the key strategies used by a country\'s government and central bank to advance its economic objectives. The combination of these policies enables these authorities to target inflation and to increase employment. In modern economies, inflation is conventionally considered "healthy" in the range of 2%–3%. Additionally, it is designed to try to keep GDP growth at 2%–3% and the unemployment rate near the natural unemployment rate of 4%–5%. This implies that fiscal policy is used to stabilise the economy over the course of the business cycle.\nChanges in the level and composition of taxation and government spending can affect macroeconomic variables, including:\n\naggregate demand and the level of economic activity\nsaving and investment\nincome distribution\nallocation of resources.\nFiscal policy can be distinguished from monetary policy, in that fiscal policy deals with taxation and government spending and is often administered by a government department; while monetary policy deals with the money supply, interest rates and is often administered by a country\'s central bank. Both fiscal and monetary policies influence a country\'s economic performance.\n\n\n== Monetary or fiscal policy? ==\n\nSince the 1970s, it became clear that monetary policy performance has some benefits over fiscal policy due to the fact that it reduces political influence, as it is set by the central bank (to have an expanding economy before the general election, politicians might cut the interest rates). Additionally, fiscal policy can potentially have more supply-side effects on the economy: to reduce inflation, the measures of increasing taxes and lowering spending would not be preferred, so the government might be reluctant to use these. Monetary policy is generally quicker to implement as interest rates can be set every month, while the decision to increase government spending might take time to figure out which area the money should be spent on.\nThe recession of the 2000s decade shows that monetary policy also has certain limitations. A liquidity trap occurs when interest rate cuts are insufficient as a demand booster as banks do not want to lend and the consumers are reluctant to increase spending due to negative expectations for the economy. Government spending is responsible for creating the demand in the economy and can provide a kick-start to get the economy out of the recession. When a deep recession takes place, it is not sufficient to rely just on monetary policy to restore the economic equilibrium.\nEach side of these two policies has its differences, therefore, combining aspects of both policies to deal with economic problems has become a solution that is now used by the US. These policies have limited effects; however, fiscal policy seems to have a greater effect over the long-run period, while monetary policy tends to have a short-run success.\nIn 2000, a survey of 298 members of the American Economic Association (AEA) found that while 84 percent generally agreed with the statement "Fiscal policy has a significant stimulative impact on a less than fully employed economy", 71 percent also generally agreed with the statement "Management of the business cycle should be left to the Federal Reserve; activist fiscal policy should be avoided." In 2011, a follow-up survey of 568 AEA members found that the previous consensus about the latter proposition had dissolved and was by then roughly evenly disputed.\n\n\n== Stances ==\n\nDepending on the state of the economy, fiscal policy may reach for different objectives: its focus can be to restrict economic growth by mediating inflation or, in turn, increase economic growth by decreasing taxes, encouraging spending on different projects that act as stimuli to economic growth and enabling borrowing and spending.\nThe three stances of fiscal policy are the following: \n\n Neutral fiscal policy is usually undertaken when an economy is in neither a recession nor an expansion. The amount of government deficit spending (the excess not financed by tax revenue) is roughly the same as it has been on average over time, so no changes to it are occurring that would have an effect on the level of economic activity.\nExpansionary fiscal policy is used by the government when trying to balance the contraction phase in the business cycle. It involves government spending exceeding tax revenue by more than it has tended to, and is usually undertaken during recessions. Examples of expansionary fiscal policy measures include increased government spending on public works (e.g., building schools) and providing the residents of the economy with tax cuts to increase their purchasing power (in order to fix a decrease in the demand).\nContractionary fiscal policy, on the other hand, is a measure to increase tax rates and decrease government spending. It occurs when government deficit spending is lower than usual. This has the potential to slow economic growth if inflation, which was caused by a significant increase in aggregate demand and the supply of money, is excessive. By reducing the economy\'s amount of aggregate income, the available amount for consumers to spend is also reduced. So, contractionary fiscal policy measures are employed when unsustainable growth takes place, leading to inflation, high prices of investment, recession and unemployment above the "healthy" level of 3%–4%.\nHowever, these definitions can be misleading because, even with no changes in spending or tax laws at all, cyclic fluctuations of the economy cause cyclic fluctuations of tax revenues and of some types of government spending, altering the deficit situation; these are not considered to be policy changes. Therefore, for purposes of the above definitions, "government spending" and "tax revenue" are normally replaced by "cyclically adjusted government spending" and "cyclically adjusted tax revenue". Thus, for example, a government budget that is balanced over the course of the business cycle is considered to represent a neutral and effective fiscal policy stance.\n\n\n== Methods of fiscal policy funding ==\nGovernments spend money on a wide variety of things, from the military and police to services such as education and health care, as well as transfer payments such as welfare benefits. This expenditure can be funded in a number of different ways:\n\nTaxation\nSeigniorage, the benefit from printing money\nBorrowing money from the population or from abroad\nDipping into fiscal reserves\nSale of fixed assets (e.g., land)\nSelling equity to the population\n\n\n=== Borrowing ===\nA fiscal deficit is often funded by issuing bonds such as Treasury bills or and gilt-edged securities but can also be funded by issuing equity. Bonds pay interest, either for a fixed period or indefinitely that is funded by taxpayers as a whole. Equity offers returns on investment (interest) that can only be realized in discharging a future tax liability by an individual taxpayer. If available government revenue is insufficient to support the interest payments on bonds, a nation may default on its debts, usually to foreign creditors. Public debt or borrowing refers to the government borrowing from the public. It is impossible for a government to "default" on its equity since the total returns available to all investors (taxpayers) are limited at any point by the total current year tax liability of all investors.\n\n\n=== Dipping into prior surpluses ===\nA fiscal surplus is often saved for future use, and may be invested in either local currency or any financial instrument that may be traded later once resources are needed and the additional debt is not needed.\n\n\n=== Fiscal straitjacket ===\nThe concept of a fiscal straitjacket is a general economic principle that suggests strict constraints on government spending and public sector borrowing, to limit or regulate the budget deficit over a time period. Most US states have balanced budget rules that prevent them from running a deficit. The United States federal government technically has a legal cap on the total amount of money it can borrow, but it is not a meaningful constraint because the cap can be raised as easily as spending can be authorized, and the cap is almost always raised before the debt gets that high.\n\n\n== Economic effects ==\n\nGovernments use fiscal policy to influence the level of aggregate demand in the economy, so that certain economic goals can be achieved:\n\nPrice stability;\nFull employment;\nEconomic growth.\nThe Keynesian view of economics suggests that increasing government spending and decreasing the rate of taxes are the best ways to have an influence on aggregate demand, stimulate it, while decreasing spending and increasing taxes after the economic expansion has already taken place. Additionally, Keynesians argue that expansionary fiscal policy should be used in times of recession or low economic activity as an essential tool for building the framework for strong economic growth and working towards full employment. In theory, the resulting deficits would be paid for by an expanded economy during the expansion that would follow; this was the reasoning behind the New Deal.\n\nThe IS-LM model is another way of understanding the effects of fiscal expansion. As the government increases spending, there will be a shift in the IS curve up and to the right. In the short run, this increases the real interest rate, which then reduces private investment and increases aggregate demand, placing upward pressure on supply. To meet the short-run increase in aggregate demand, firms increase full-employment output. The increase in short-run price levels reduces the money supply, which shifts the LM curve back, and thus, returning the general equilibrium to the original full employment (FE) level. Therefore, the IS-LM model shows that there will be an overall increase in the price level and real interest rates in the long run due to fiscal expansion.\nGovernments can use a budget surplus to do two things: \n\nto slow the pace of strong economic growth;\nto stabilise prices when inflation is too high.\nKeynesian theory posits that removing spending from the economy will reduce levels of aggregate demand and contract the economy, thus stabilizing prices.\nBut economists still debate the effectiveness of fiscal stimulus. The argument mostly centers on crowding out: whether government borrowing leads to higher interest rates that may offset the stimulative impact of spending. When the government runs a budget deficit, funds will need to come from public borrowing (the issue of government bonds), overseas borrowing, or monetizing the debt. When governments fund a deficit with the issuing of government bonds, interest rates can increase across the market, because government borrowing creates higher demand for credit in the financial markets. This decreases aggregate demand for goods and services, either partially or entirely offsetting the direct expansionary impact of the deficit spending, thus diminishing or eliminating the achievement of the objective of a fiscal stimulus. Neoclassical economists generally emphasize crowding out while Keynesians argue that fiscal policy can still be effective, especially in a liquidity trap where, they argue, crowding out is minimal.\nIn the classical view, expansionary fiscal policy also decreases net exports, which has a mitigating effect on national output and income. When government borrowing increases interest rates it attracts foreign capital from foreign investors. This is because, all other things being equal, the bonds issued from a country executing expansionary fiscal policy now offer a higher rate of return. In other words, companies wanting to finance projects must compete with their government for capital so they offer higher rates of return. To purchase bonds originating from a certain country, foreign investors must obtain that country\'s currency. Therefore, when foreign capital flows into the country undergoing fiscal expansion, demand for that country\'s currency increases. The increased demand, in turn, causes the currency to appreciate, reducing the cost of imports and making exports from that country more expensive to foreigners. Consequently, exports decrease and imports increase, reducing demand from net exports.\nSome economists oppose the discretionary use of fiscal stimulus because of the inside lag (the time lag involved in implementing it), which is almost inevitably long because of the substantial legislative effort involved. Further, the outside lag between the time of implementation and the time that most of the effects of the stimulus are felt could mean that the stimulus hits an already-recovering economy and overheats the ensuing h rather than stimulating the economy when it needs it.\nSome economists are concerned about potential inflationary effects driven by increased demand engendered by a fiscal stimulus. In theory, fiscal stimulus does not cause inflation when it uses resources that would have otherwise been idle. For instance, if a fiscal stimulus employs a worker who otherwise would have been unemployed, there is no inflationary effect; however, if the stimulus employs a worker who otherwise would have had a job, the stimulus is increasing labor demand while labor supply remains fixed, leading to wage inflation and therefore price inflation.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nSimonsen, M.H. The Econometrics and The State Brasilia University Editor, 1960–1964.\nHeyne, P. T., Boettke, P. J., Prychitko, D. L. (2002). The Economic Way of Thinking (10th ed). Prentice Hall.\nLarch, M. and J. Nogueira Martins (2009). Fiscal Policy Making in the European Union: An Assessment of Current Practice and Challenges. Routledge.\nHansen, Bent (2003). The Economic Theory of Fiscal Policy, Volume 3. Routledge.\nAnderson, J. E. (2005). Fiscal Reform and its Firm-Level Effects in Eastern Europe and Central Asia, Working Papers Series wp800, William Davidson Institute at the University of Michigan.\nD. Harries. Roger Fenton and the Crimean War\nSchmidt, M (2018). "A Look at Fiscal and Monetary Policy", Dotdash\nPettinger, T. (2017). "Difference between monetary and fiscal policy", EconomicsHelp.org\nAmadeo, K. (2018). "Fiscal Policy Types, Objectives, and Tools", Dotdash\nKramer, L. (2019). "What Is Fiscal Policy?", Dotdash\nMacek, R; Janků, J. (2015) "The Impact of Fiscal Policy on Economic Growth Depending on Institutional Conditions"\n\n\n== External links ==\n\nFiscal Policy topic page from Encyclopædia Britannica',
  },
  {
    title: "Inflation targeting",
    originalContent:
      'In macroeconomics, inflation targeting is a monetary policy where a central bank follows an explicit target for the inflation rate for the medium-term and announces this inflation target to the public. The assumption is that the best that monetary policy can do to support long-term growth of the economy is to maintain price stability, and price stability is achieved by controlling inflation. The central bank uses interest rates as its main short-term monetary instrument.\nAn inflation-targeting central bank will raise or lower interest rates based on above-target or below-target inflation, respectively. The conventional wisdom is that raising interest rates usually cools the economy to rein in inflation; lowering interest rates usually accelerates the economy, thereby boosting inflation. The first three countries to implement fully-fledged inflation targeting were New Zealand, Canada and the United Kingdom in the early 1990s, although Germany had adopted many elements of inflation targeting earlier.\n\n\n== History ==\nEarly proposals of monetary systems targeting the price level or the inflation rate, rather than the exchange rate, followed the general crisis of the gold standard after World War I. Irving Fisher proposed a "compensated dollar" system in which the gold content in paper money would vary with the price of goods in terms of gold, so that the price level in terms of paper money would stay fixed. Fisher\'s proposal was a first attempt to target prices while retaining the automatic functioning of the gold standard. In his Tract on Monetary Reform (1923), John Maynard Keynes advocated what we would now call an inflation targeting scheme. In the context of sudden inflations and deflations in the international economy right after World War I, Keynes recommended a policy of exchange-rate flexibility, appreciating the currency as a response to international inflation and depreciating it when there are international deflationary forces, so that internal prices remained more or less stable. Interest in inflation targeting waned during the Bretton Woods era (1944–1971), as they were inconsistent with the exchange rate pegs that prevailed during three decades after World War II.\n\n\n=== New Zealand, Canada, United Kingdom ===\nInflation targeting was pioneered in New Zealand in 1990. Canada was the second country to formally adopt inflation targeting in February 1991.\nThe United Kingdom adopted inflation targeting in October 1992 after exiting the European Exchange Rate Mechanism. The Bank of England\'s Monetary Policy Committee was given sole responsibility in 1998 for setting interest rates to meet the Government\'s Retail Prices Index (RPI) inflation target of 2.5%. The target changed to 2% in December 2003 when the Consumer Price Index (CPI) replaced the Retail Prices Index as the UK Treasury\'s inflation index. If inflation overshoots or undershoots the target by more than 1%, the Governor of the Bank of England is required to write a letter to the Chancellor of the Exchequer explaining why, and how he will remedy the situation. The success of inflation targeting in the United Kingdom has been attributed to the Bank\'s focus on transparency. The Bank of England has been a leader in producing innovative ways of communicating information to the public, especially through its Inflation Report, which have been emulated by many other central banks.\nInflation targeting then spread to other advanced economies in the 1990s and began to spread to emerging markets beginning in the 2000s.\n\n\n=== European Central Bank ===\nAlthough the ECB does not consider itself to be an inflation-targeting central bank, after the inception of the euro in January 1999, the objective of the European Central Bank (ECB) has been to maintain price stability within the Eurozone. The Governing Council of the ECB in October 1998 defined price stability as inflation of under 2%, "a year-on-year increase in the Harmonised Index of Consumer Prices (HICP) for the euro area of below 2%" and added that price stability "was to be maintained over the medium term". The Governing Council confirmed this definition in May 2003 following a thorough evaluation of the ECB\'s monetary policy strategy. On that occasion, the Governing Council clarified that "in the pursuit of price stability, it aims to maintain inflation rates below, but close to, 2% over the medium term". Since then, the numerical target of 2% has become common for major developed economies, including the United States (since January 2012) and Japan (since January 2013).\nIn 8 July 2021, the ECB changed its inflation target to a symmetrical 2% over the medium term. Symmetry in the inflation target means that the Governing Council considers negative and positive deviations of inflation from the target to be equally undesirable.\n\n\n=== Emerging markets ===\nIn 2000, Frederic S. Mishkin concluded that "although inflation targeting is not a panacea and may not be appropriate for many emerging market countries, it can be a highly useful monetary policy strategy in a number of them".\n\n\n==== Armenia ====\nThe Central Bank of Armenia (CBA) announced in 2006 that it will implement an inflation targeting strategy. The process of full transition to inflation targeting was supposed to end in 2008. Operational, macroeconomic and institutional preconditions for inflation targeting should have been met to ensure a full transition. CBA believes that it has managed to meet all the preconditions successfully and should concentrate on building a public trust in the new monetary policy regime. A specific model has been developed to estimate CBA\'s reaction function and the results showed that the inertia of inflation rate and interest rate are most vital in the reaction function. This can be an evidence that the announcement of the strategy is a trustworthy commitment. There are people who claim that inflation targeting is too restrictive for dealing with positive supply shocks. On the other hand, the IMF claims that inflation targeting strategy is good for developing economies, however it requires a lot of information for forecasting.\nThe Central Bank continued to pursue a policy of tightening monetary conditions during the reporting period, increasing the policy interest rate by a total of 2.75 percentage points.  At the same time, about half of the tightening, 1.25 percentage points, was carried out in 2022  in March, reacting to the high inflation situation formed in the case of unprecedented uncertainties.\nBeing constantly hit by external shocks to the national economy over the past three years, Armenia is still on the path of recovery thanks to economic management efforts. According to the 3-year Stand-By Arrangement, which came to its end on May 16, 2022, important structural and institutional reforms have been implemented. Those include improvement of tax compliance, budget process refinement, strengthening the stability of financial sector and most importantly fostering the inflation targeting framework.\n\n\n==== Chile ====\n\nIn Chile, a 20% inflation rate pushed the Central Bank of Chile to announce at the end of 1990 an inflation objective for the annual inflation rate for the year ending in December 1991. However, Chile was not regarded as a fully-fledged inflation targeter until October 1999. According to Pablo García Silva, member of the board of the Central Bank of Chile, this has allowed to attenuate inflation. García Silva exemplifies this with the limited inflation seen in Chile during the 2002 Brazilian general election and the Great Recession of 2008–2009.\n\n\n==== Czech Republic ====\nThe Czech National Bank (CNB) is an example of an inflation targeting central bank in a small open economy with a recent history of economic transition and real convergence to its Western European peers. Since 2010 the CNB uses 2 percent with a +/- 1pp range around it as the inflation target. The CNB places a lot of emphasis on transparency and communication; indeed, a recent study of more than 100 central banks found the CNB to be among the four most transparent ones.\nIn 2012, inflation was expected to fall well below the target, leading the CNB to gradually reduce the level of its basic monetary policy instrument, the 2-week repo rate, until the zero lower bound (actually 0.05 percent) was reached in late 2012. In light of the threat of a further fall in inflation and possibly even of a protracted period of deflation, on 7 November 2013 the CNB declared an immediate commitment to weaken the exchange rate to the level of 27 Czech korunas per 1 euro (day-on-day weakening by about 5 percent) and to keep the exchange rate from getting stronger than this value until at least the end of 2014 (later on this was changed to the second half of 2016). The CNB thus decided to use the exchange rate as a supplementary tool to make sure that inflation returns to the 2 percent target level. Such a use of the exchange rate as tool within the regime of inflation targeting should not be confused with a fixed exchange-rate system or with a currency war.\n\n\n=== United States ===\nIn a historic shift on 25 January 2012, U.S. Federal Reserve Chairman Ben Bernanke set a 2% target inflation rate, bringing the Fed in line with many of the world\'s other major central banks. Until then, the Fed\'s policy committee, the Federal Open Market Committee (FOMC), did not have an explicit inflation target but regularly announced a desired target range for inflation (usually between 1.7% and 2%) measured by the personal consumption expenditures price index.\nPrior to adoption of the target, some people argued that an inflation target would give the Fed too little flexibility to stabilise growth and/or employment in the event of an external economic shock. Another criticism was that an explicit target might turn central bankers into what Mervyn King, former Governor of the Bank of England, had in 1997 colorfully termed "inflation nutters"—that is, central bankers who concentrate on the inflation target to the detriment of stable growth, employment, and/or exchange rates. King went on to help design the Bank\'s inflation targeting policy, and asserts that the buffoonery has not actually happened, as did Chairman of the U.S. Federal Reserve Ben Bernanke, who stated in 2003 that all inflation targeting at the time was of a flexible variety, in theory and practice.\nFormer Chairman Alan Greenspan, as well as other former FOMC members such as Alan Blinder, typically agreed with the benefits of inflation targeting, but were reluctant to accept the loss of freedom involved; Bernanke, however, was a well-known advocate.\nIn August 2020, the FOMC released a revised Statement on Longer-Run Goals and Monetary Policy Strategy. The review announced the FED would seek to achieve inflation that \'averages\' 2% over time. In practice this means that following periods when inflation has been running persistently below 2 percent, appropriate monetary policy will likely aim to achieve inflation moderately above 2 percent for some time. This way, the fed hopes to better anchor longer-term inflation expectations, which they say would foster price stability and moderate long-term interest rates and enhance the Committee\'s ability to promote maximum employment in the face of significant economic disturbances.\n\n\n== Theoretical questions ==\n\nNew classical macroeconomics and rational expectations hypothesis can explain how and why inflation targeting works.  Expectations of firms (or the subjective probability distribution of outcomes) will be around the prediction of the theory itself (the objective probability distribution of those outcomes) for the same information set. So, rational agents expect the most probable outcome to emerge.  However, there is limited success at specifying the relevant model, and the full and perfect knowledge of a given macroeconomic system can be regarded as a comfortable presumption at best. Knowledge of the relevant model is not feasible, even if high-level econometrical techniques were accessible or adequate identification of the relevant explanatory variables were performed. So, estimation bias depends on the quantity and quality of information to which the modeller has access. In other words, estimations are asymptotically unbiased with respect to the exploited information.\nMeanwhile, consistency can be interpreted similarly. On the basis of asymptotical unbiasedness, a moderated version of the rational expectations hypothesis can be suggested in which familiarity with the theoretical parameters is not a requirement for the relevant model. An agent with access to sufficiently vast, quality information and high-level methodological skills could specify its own quasi-relevant model describing a specific macroeconomic system.  By increasing the amount of information processed, this agent could further reduce its bias.  If this agent were also focal, such as a central bank, then other agents would likely accept the proposed model and adjust their expectations accordingly.  In this way, individual expectations become unbiased as much as possible, albeit against a background of considerable passivity. According to some researches, this is the theoretical background of the functionality of inflation targeting regimes.\n\n\n== Empirical issues ==\n\n\n=== Target band size ===\nWhile most inflation targeting countries set their target band at 2 percentage points, the band sizes are wide-ranging across countries and inflation targeters frequently update their target bands.\n\n\n=== Track record ===\nInflation targeting countries\' track records in maintaining inflation within the central banks\' target bands differ substantially and financial markets differentiate inflation targeters by behaviors.\n\n\n== Debate ==\nThere is some empirical evidence that inflation targeting does what its advocates claim, that is, making the outcomes, if not the process, of monetary policy more transparent. A 2021 study in the American Political Science Review found that independent central banks with rigid inflation targeting policies produce worse outcomes in banking crises than independent central banks whose policy mandate does not rigidly prioritize inflation.\n\n\n=== Benefits ===\nInflation targeting allows monetary policy to "focus on domestic considerations and to respond to shocks to the domestic economy", which is not possible under a fixed exchange-rate system. Also, as a result of better inflation control and stability of economic growth, investors may more easily factor in likely interest rate changes into their investment decisions. Inflation expectations that are better anchored "allow monetary authorities to cut policy interest rates countercyclically".\nTransparency is another key benefit of inflation targeting. Central banks in developed countries that have successfully implemented inflation targeting tend to "maintain regular channels of communication with the public". For example, the Bank of England pioneered the "Inflation Report" in 1993, which outlines the bank\'s "views about the past and future performance of inflation and monetary policy". Although it was not an inflation-targeting country until January 2012, up until then, the United States\' "Statement on Longer-Run Goals and Monetary Policy Strategy" enumerated the benefits of clear communication—it "facilitates well-informed decisionmaking by households and businesses, reduces economic and financial uncertainty, increases the effectiveness of monetary policy, and enhances transparency and accountability, which are essential in a democratic society".\nAn explicit numerical inflation target increases a central bank\'s accountability, and thus it is less likely that the central bank falls prey to the time-inconsistency trap. This accountability is especially significant because even countries with weak institutions can build public support for an independent central bank. Institutional commitment can also insulate the bank from political pressure to undertake an overly expansionary monetary policy.\nAn econometric analysis found that although inflation targeting results in higher economic growth, it does not necessarily guarantee stability based on their study of 36 emerging economies from 1979 to 2009.\n\n\n=== Shortcomings ===\nSupporters of a nominal income target criticize the propensity of inflation targeting to neglect output shocks by focusing solely on the price level. Adherents of market monetarism, led by Scott Sumner, argue that in the United States, the Federal Reserve\'s mandate is to stabilize both output and the price level, and that consequently a nominal income target would better suit the Fed\'s mandate. Australian economist John Quiggin, who also endorses nominal income targeting, stated that it "would maintain or enhance the transparency associated with a system based on stated targets, while restoring the balance missing from a monetary policy based solely on the goal of price stability". Quiggin blamed the late-2000s recession on inflation targeting in an economic environment in which low inflation is a "drag on growth". In practice, many central banks conduct "flexible inflation targeting" where the central bank strives to keep inflation near the target except when such an effort would imply too much output volatility.\nQuiggin also criticized former Fed Chair Alan Greenspan and former European Central Bank President Jean-Claude Trichet for "ignor[ing] or even applaud[ing] the unsustainable bubbles in speculative real estate that produced the crisis, and to react[ing] too slowly as the evidence emerged".\nIn a 2012 op-ed, University of Nottingham economist Mohammed Farhaan Iqbal suggested that inflation targeting "evidently passed away in September 2008", referencing the 2007–2008 financial crisis. Frankel suggested "that central banks that had been relying on [inflation targeting] had not paid enough attention to asset-price bubbles", and also criticized inflation targeting for "inappropriate responses to supply shocks and terms-of-trade shocks". In turn, Iqbal suggested that nominal income targeting or product-price targeting would succeed inflation targeting as the dominant monetary policy regime. The debate continues and many observers expect that inflation targeting will continue to be the dominant monetary policy regime, perhaps after certain modifications.\nEmpirically, it is not so obvious that inflation targeteers have better inflation control. Some economists argue that better institutions increase a country\'s chances of successfully targeting inflation. John Williams, a high-ranking Federal Reserve official, concluded that "when gauged by the behavior of inflation since the crisis, inflation targeting delivered on its promise".\nIn an article written since the COVID-19 pandemic, critics have pointed out that the Bank of Canada’s inflation-targeting has had unintended consequences, with persistently low interest rates over the last 12 years fuelling an increase in home prices by encouraging borrowing; and contributing to wealth inequalities by supporting higher equity values.\n\n\n=== Choosing a positive, zero, or negative inflation target ===\nChoosing a positive inflation target has at least two drawbacks.\n\nOver time, the compounded effect of small annual price increases will significantly reduce a currency\'s purchasing power. (For example, successfully hitting a target of +2% each year for 40 years would cause the price of a $100 basket of goods to rise to $220.80.) This drawback would be minimized or reversed by choosing a zero inflation target or a negative target.\nVendors must expend resources more frequently to reprice their goods and services. This drawback would be minimized by choosing a zero inflation target.\nHowever, policymakers feel the drawbacks are outweighed by the fact that a positive inflation target reduces the chance of an economy falling into a period of deflation.\nSome economists argue that fear of deflation is unfounded, citing studies that show inflation is more likely than deflation to cause an economic contraction. Andrew Atkeson and Patrick J. Kehoe wrote,\n\nAccording to standard economic theory, deflation is the necessary consequence of optimal monetary policy. In 1969, Milton Friedman argued that under the optimal policy, the nominal interest rate should be zero and the price level should fall steadily at the real rate of interest. Since then, Friedman’s argument has been confirmed in a formal setting. (See, for example, V. V. Chari, Lawrence Christiano, and Patrick Kehoe 1996 and Harold Cole and Narayana Kocherlakota 1998.)\nEffectively, Friedman was arguing for a negative (moderately deflationary) inflation target.\n\n\n=== Numerical target ===\nThe typical numerical target of 2% has come under debate since the period of rapid inflation experienced following the monetary expansion during the COVID-19 pandemic.\nMohamed El-Erian has suggested the Federal Reserve raise its inflation target to a (stable) 3% rate of inflation, saying "There\'s nothing scientific about 2%".\n\n\n== Variations ==\nIn contrast to the usual inflation rate targeting, Laurence M. Ball proposed targeting long-run inflation using a monetary conditions index. In his proposal, the monetary conditions index is a weighted average of the interest rate and exchange rate. It will be easy to put many other things into this monetary conditions index.\nIn the "constrained discretion" framework, inflation targeting combines two contradicting monetary policies—a rule-based approach and a discretionary approach—as a precise numerical target is given for inflation in the medium term and a response to economic shocks in the short term. Some inflation targeters associate this with more economic stability.\n\n\n== Countries ==\nThere were 27 countries regarded by the Bank of England\'s Centre for Central Banking Studies as fully fledged inflation\ntargeters at the beginning of 2012. Other lists count 26 or 28 countries as of 2010. Since then, the United States and Japan have also adopted inflation targets although the Federal Reserve, like the European Central Bank, does not consider itself to be an inflation-targeting central bank.\n\nIn addition, South Korea (Bank of Korea) and Iceland (Central Bank of Iceland) and others.\n\n\n== See also ==\n\nInflationism\nMonetarism\nNominal income target\nOutput gap\nPhillips curve\nTaylor rule\n\n\n== References ==\n\n\n== External links ==\nTable of Central Bank Inflation Targets',
  },
  {
    title: "Central bank digital currency",
    originalContent:
      'A central bank digital currency (CBDC; also called digital fiat currency or digital base money) is a digital currency issued by a central bank, rather than by a commercial bank. It is also a liability of the central bank and denominated in the sovereign currency, as is the case with physical banknotes and coins.\n\nThe two primary categories of CBDCs are retail and wholesale. Retail CBDCs are designed for households and businesses to make payments for everyday transactions, whereas wholesale CBDCs are designed for financial institutions and operate similarly to central bank reserves.\nRetail CBDCs can be distributed through various models. In the intermediated model, the central bank issues the CBDC and manages core infrastructures, while financial intermediaries offer customer services. The ECB and the Federal Reserve have proposed intermediated CBDCs. Alternatively, the central bank could either provide the full service or delegate responsibilities further.\nWhile CBDCs may share some properties with virtual currency and cryptocurrency, such as programmability, they differ in that a CBDC is issued by a state. However, most retail CBDC implementations will likely not use any sort of distributed ledger such as a blockchain.\nAs of 2023, over 120 different jurisdictions, including major economies like the ECB, UK, and the US, were evaluating national digital currencies. As it currently stands, 9 countries and the 8 islands making up the Eastern Caribbean Currency Union have launched CBDCs; 38 countries and Hong Kong have CBDC pilot programmes; and 67 countries and 2 currency unions are researching CBDCs. In the United States, some states have introduced legislation to ban state payments using CBDCs with Florida being the first state to pass such a law citing privacy concerns.\nCBDCs have faced a plethora of criticisms, including concerns about privacy and the potential for them to be used as a "tool for coercion and control". Their implementation could also have a displacement effect on the private sector, affecting bank balance sheets and private payment methods, necessitating carefully calibrated policies.\n\n\n== History ==\n\nAlthough the term "CBDC" did not become widely used until after 2019, central banks have researched and launched digital currency projects for decades. For example, Finland\'s central bank issued the Avant stored value e-money card in the 1990s. In 2014, the Chinese central bank began researching the idea of issuing a CBDC. Elsewhere, the Ecuadorian central bank operated a mobile payment system from 2014 to 2018. In 2021, Australia\'s central bank conducted a proof of concept for a wholesale CBDC using Ethereum to tokenize syndicated loans, aiming to automate and secure high-value transactions in the banking sector.\n\n\n== Implementation ==\nA central bank digital currency would likely be implemented using a database run by the central bank, government, or approved private-sector entities. The database would keep a record (with appropriate privacy and cryptographic protections) of the amount of money held by every entity, such as people and corporations.\nIn contrast to cryptocurrency, a central bank digital currency would be centrally controlled (even if it was on a distributed database), and so a blockchain or other distributed ledger would likely not be required or useful - even as they were the original inspiration for the concept.\nBy March 2024, the central banks of 134 countries accounting for 98% of the world\'s GDP were said to be in various stages of evaluating the launch of a national digital currency. These included the ECB, the UK, and the US. China\'s digital RMB was the first digital currency to be issued by a major economy. Six central banks have launched a CBDC: the Central Bank of The Bahamas (Sand Dollar), the Eastern Caribbean Central Bank (DCash), the Central Bank of Nigeria (e-Naira), the Bank of Jamaica (JamDex),  People\'s Bank of China (Digital renminbi), the Reserve Bank of India (Digital Rupee), and Bank of Russia (Digital Ruble). The Central Bank of Brazil has been rolling out tests of a digital Brazilian currency (Drex) since March 2023. The ECB/Eurozone decided in October 2023 to move forward to the preparation phase for the potential issuance of a digital euro after a two-year study phase.\nSome states have also issued, or have considered issuing, cryptocurrencies: these include Venezuela (Petro) and the Marshall Islands (Sovereign). These cryptocurrencies are often considered with the intent of increasing a state\'s independence from global financial systems, such as by reducing dependence on a foreign currency or by evading international sanctions.\nContrasting attitudes towards digital currencies were demonstrated by developments in the UK and Switzerland in February 2023. The UK Treasury and the Bank of England said a state-backed digital pound was likely to be launched some time after 2025. Two weeks later, a Swiss lobby group triggered a national vote on maintaining a "sufficient quantity" of cash in circulation over fears that electronic payments make it easier for the state to monitor its citizens\' actions. In a comment on the British government\'s plans, the BBC\'s Faisal Islam said the issue was about access to the data attached to every spending transaction, and whether people might choose to trust a global company more than the state: "The eye here is on maintaining UK monetary sovereignty against upheaval from the likes of Big Tech."\nA major problem with central bank digital currencies is deciding whether the currency should be easily traceable. If it\'s traceable, the government has more control than it currently does. Additionally, there\'s a technical aspect to consider: whether CBDCs should be based on tokens or accounts and how much anonymity users should have.\n\n\n== Characteristics ==\nA CBDC is a digital counterpart to fiat money, issued by central banks. Like paper banknotes, it is a means of payment, a unit of account, and a store of value. And like paper currency, each unit is uniquely identifiable to prevent counterfeiting. CBDC will have implications for commercial banks, probably in the field of lowering banks\' commissions, no big customer data-selling ability, accumulating the deposits and deposit policies and credit policies due to higher funding costs for banks. \nDigital fiat currency is part of the base money supply, together with other forms of the currency. As such, DFC is a liability of the central bank just as physical currency is. It is a digital bearer instrument that can be stored, transferred and transmitted by all kinds of digital payment systems and services. The validity of the digital fiat currency is independent of the digital payment systems storing and transferring the digital fiat currency.\nProposals for CBDC implementation often involve the provision of universal bank accounts at the central banks for all citizens.\n\n\n== Benefits and impacts ==\nGovernments and central banks are studying CBDCs and their implications for financial inclusion, economic growth, technology innovation, and the efficiency of bank transactions. Potential advantages include:\n\nTechnological efficiency: instead of relying on intermediaries such as banks and clearing houses, money transfers and payments could be made in real time, directly from the payer to the payee.  Being real time has some advantages:\nReduces risk: payment for goods and services often needs to be done in a timely manner and when payment verification is slow, merchants usually accept the risk of some payments not succeeding in exchange for faster service to customers. When these risks are eliminated with instant payment verifications, merchants no longer need to use intermediaries to handle the risk or to absorb the risk cost themselves.\nReduces complexity: merchants will not need to separately keep track of transactions that are slow (where the customer claims to have paid but the money has not arrived yet), therefore eliminate the waiting queue, which could simplify the transaction process from payment to rendition of goods/services.\nReduces (or eliminates) transaction fees: current payment systems like Visa, Mastercard, American Express etc. have a fee attached to each transaction and lowering or eliminating these fees could lead to widespread price drops and increased adoption of digital payments.\nFinancial inclusion: safe money accounts at the central banks could constitute a strong instrument of financial inclusion, allowing any legal resident or citizen to be provided with a free or low-cost basic bank account.\nPreventing illicit activity: A CBDC makes it feasible for a central bank to keep track of the exact location of every unit of the currency (assuming the more probable centralized, database form)\nTax collection: It makes tax avoidance and tax evasion much more difficult, since it would become impossible to use methods such as offshore banking and unreported employment to hide financial activity from the central bank or government. In contrast, cryptocurrencies risk undermining effort to crack down on corporate tax avoidance.\nCombating crime: It makes it much easier to spot criminal activity (by observing financial activity), and thus put an end to it. Furthermore, in cases where criminal activity has already occurred, tracking makes it much harder to successfully launder money, and it would often be straightforward to instantly reverse a transaction and return money to the victim of the crime.\nProof of transaction: a digital record exists to prove that money changed hands between two parties which avoids problems inherent to cash such as short-changing, cash theft and conflicting testimonies.\nProtection of money as a public utility: digital currencies issued by central banks would provide a modern alternative to physical cash – whose abolition is currently being envisaged.\nSafety of payments systems: A secure and standard interoperable digital payment instrument issued and governed by a Central Bank and used as the national digital payment instruments boosts confidence in privately controlled money systems and increases trust in the entire national payment system while also boosting competition in payment systems.\nPreservation of seigniorage income: public digital currency issuance would avoid a predictable reduction of seigniorage income for governments in the event of a disappearance of physical cash.\nBanking competition: the provision of free bank accounts at the central bank offering complete safety of money deposits could strengthen competition between banks to attract bank deposits, for example by offering once again remunerated sight deposits.\nMonetary policy transmission: the issuance of central bank base money through transfers to the public could constitute a new channel for monetary policy transmission (i.e. helicopter money), which would allow more direct control of the money supply than indirect tools such as quantitative easing and interest rates, and possibly lead the way towards a full reserve banking system. In digital Yuan trial in Shenzhen, the CBDC was programmed with an expiration date, which encouraged spending and discouraged money from sitting in a saving account. In the end, 90% of vouchers were spent in shops. Demurrage could be implemented, such as by shaving off fractions of the value on a scheduled basis, as a supplement to traditional inflation targets.\nFinancial safety: CBDC would provide an alternative to fractional reserve banking for daily uses, for those who want to avoid all risk of bank runs, despite the relative safety provided by deposit insurance.\n\n\n== Risks ==\nDespite having potential advantages, CBDCs remain a controversial topic, and there are risks associated with their implementation.\n\nBanking system disintermediation: With the ability to provide digital currency directly to its citizens, one concern is that depositors would shift out of the banking system. Customers may deem the safety, liquidity, solvency, and publicity of CBDCs to be more attractive, weakening the balance sheet position of commercial banks. In the extreme, this could precipitate potential bank runs and thus make banks\' funding positions weaker. However, the Bank of England found that if the introduction of CBDC follows a set of core principles, the risk of a system-wide run from bank deposits to CBDC is addressed. A central bank could also limit the demand of CBDCs by setting a ceiling on the amount of holdings.\nCentralization: Since most central bank digital currencies are centralized, rather than decentralized like most cryptocurrencies, the controllers of the issuance of CBDCs can add or remove money from anyone\'s account with a flip of a switch.\nDigital dollarization: A well-run foreign digital currency could become a replacement for a local currency for the same reasons as those described in dollarization. The announcement of Facebook\'s Libra contributed to the increased attention to CBDCs by central bankers, as well as China\'s progress with DCEP to that of several Asian economies.\nPrivacy:\n"Governments have direct visibility of financial transactions", an "eagle-eyed view on the spending of everyone".\nDigital currency would give a country "broad new powers when it comes to surveillance and controlling its population."\nData from tracing money routes could lead to losing financial privacy if the CBDC implementation does not have adequate privacy protections. This could lead to encouraging of self-censorship, deterioration of freedom of expression and association, and ultimately to stalling social developments.\nCybersecurity: Cybersecurity is an important risk to any payment infrastructure. While CBDCs offer resiliency by providing a new payment method, they would also represent a critical infrastructure, potentially making them a high-value target for cyber attacks.\nGovernment social manipulation:\nDigital currency "will simply become an extension of the surveillance state" and "it could see citizens fined in a split second for behaviors deemed undesirable. Dissidents and activists could see their wallets emptied or taken offline."\nLimiting individual freedom: "Digital currencies could also empower the state to make it impossible to donate to a vocal NGO"\nLimiting or prohibiting purchases of products: Digital currency could prohibit a "purchase alcohol on a weekday. "\nDigital currency "is also programmable. The government could theoretically give out money that expires within a certain period of time or money that could only be used on certain items, which could be used to induce behaviour that the government is seeking."\n\n\n== See also ==\nBank for International Settlements\nENaira\nDigital renminbi\nDigital rupee\nDigital currency\nmBridge\nM-Pesa\nE-Cedi\nDigital euro\nHistory of CBDCs by country\nCentral bank\n\n\n== References ==\n\n\n== External links ==\n\nAtlantic Council CBDC Tracker',
  },
  {
    title: "World economy",
    originalContent:
      'The world economy or global economy is the economy of all humans in the world, referring to the global economic system, which includes all economic activities conducted both within and between nations, including production, consumption, economic management, work in general, financial transactions and trade of goods and services. In some contexts, the two terms are distinct: the "international" or "global economy" is measured separately and distinguished from national economies, while the "world economy" is simply an aggregate of the separate countries\' measurements. Beyond the minimum standard concerning value in production, use and exchange, the definitions, representations, models and valuations of the world economy vary widely. It is inseparable from the geography and ecology of planet Earth.\nIt is common to limit questions of the world economy exclusively to human economic activity, and the world economy is typically judged in monetary terms, even in cases in which there is no efficient market to help valuate certain goods or services, or in cases in which a lack of independent research, genuine data or government cooperation makes calculating figures difficult. Typical examples are illegal drugs and other black market goods, which by any standard are a part of the world economy, but for which there is, by definition, no legal market of any kind.\nHowever, even in cases in which there is a clear and efficient market to establish monetary value, economists do not typically use the current or official exchange rate to translate the monetary units of this market into a single unit for the world economy since exchange rates typically do not closely reflect worldwide value – for example, in cases where the volume or price of transactions is closely regulated by the government.\nRather, market valuations in a local currency are typically translated to a single monetary unit using the idea of purchasing power. This is the method used below, which is used for estimating worldwide economic activity in terms of real United States dollars or euros. However, the world economy can be evaluated and expressed in many more ways. It is unclear, for example, how many of the world\'s 7.8 billion people (as of March 2020) have most of their economic activity reflected in these valuations.\nAccording to Angus Maddison–a distinguished British economist–until the middle of the 19th century, global output was dominated by China and India, with the Indian subcontinent being the world\'s largest economy from 1 C.E to 17 C.E. Waves of the Industrial Revolution in Western Europe and Northern America shifted the shares to the Western Hemisphere. As of 2024, the following 20 countries or collectives have reached an economy of at least US$2 trillion by Gross Domestic Product (GDP) in nominal or Purchasing Power Parity (PPP) terms: Brazil, Canada, China, Egypt, France, Germany, India, Indonesia, Italy, Japan, Mexico, South Korea, Russia, Saudi Arabia, Spain, Turkey, the United Kingdom, the United States, the European Union and the African Union.\nDespite high levels of government investment, the global economy decreased by 3.4% in 2020 in the midst of the COVID-19 pandemic, an improvement from the World Bank\'s initial prediction of a 5.2 percent decrease. Cities account for 80% of global GDP, thus they faced the brunt of this decline. The world economy increased again in 2021 with an estimated 5.5 percent rebound.\n\n\n== Overview ==\n\n\n=== World economy by country groups ===\n\n\n=== World economy by continent ===\n\n\n=== Current world economic league table of largest economies in the world by GDP and share of global economic growth ===\n\n\n=== Twenty largest economies in the world by nominal GDP ===\n\n\n=== Twenty largest economies in the world by GDP (PPP) ===\n\n\n== Statistical indicators ==\n\n\n=== Finance ===\n\nGDP (GWP) (gross world product): (purchasing power parity exchange rates) – $59.38 trillion (2005 est.), $51.48 trillion (2004), $23 trillion (2002). The GWP is the combined gross national income of all the countries in the world. When calculating the GWP, add GDP of all countries. Also, GWP shows that imports and exports are equal. Because imports and exports balance exactly when considering the whole world:, this also equals the total global gross domestic product (GDP). According to the World Bank, the 2013 nominal GWP was approximately US$75.59 trillion. In 2017, according to the CIA\'s World Factbook, the GWP was around US$80.27 trillion in nominal terms and totaled approximately 127.8 trillion international dollars in terms of purchasing power parity (PPP). The per capita PPP GWP in 2017 was approximately Int$17,500 according to the World Factbook.\nGDP (GWP) (gross world product): (market exchange rates) – $60.69 trillion (2008). The market exchange rates increased from 1990 to 2008. The reason for this increase is the world\'s advancement in terms of technology.\nGDP (real growth rate): The following part shows the GDP growth rate and the expected value after one year.\nDeveloped Economies. A developed country, industrialized country, more developed country (MDC), or more economically developed country (MEDC), is a sovereign state that has a developed economy and advanced technological infrastructure relative to other less industrialized nations. Most commonly, the criteria for evaluating the degree of economic development are gross domestic product (GDP), gross national product (GNP), the per capita income, level of industrialization, amount of widespread infrastructure and general standard of living. Which criteria are to be used and which countries can be classified as being developed are subjects of debate. The GDP of the developed countries is predicted to fall from 2.2% in 2017 to 2.0% in 2018 due to the fall in dollar value.\nDeveloping Countries. A developing country is a country with a less developed industrial base (industries) and a low Human Development Index (HDI) relative to other countries. However, this definition is not universally agreed upon. There is also no clear agreement on which countries fit this category. A nation\'s GDP per capita, compared with other nations, can also be a reference point. In general, the United Nations accepts any country\'s claim of itself being "developing". The GDP of the developing countries is expected to rise from 4.3% in 2017 to 4.6% in 2018 due to political stability in those countries and advancement in technology.\nLeast developed countries. The least developed countries (LDCs) is a list of developing countries that, according to the United Nations, exhibit the lowest indicators of socioeconomic development, with the lowest Human Development Index ratings of all countries in the world. The concept of LDCs originated in the late 1960s and the first group of LDCs was listed by the UN in its resolution 2768 (XXVI) of 18 November 1971. This is a group of countries that are expected to improve their GDP from 4.8% in 2017 to 5.4% in 2018. The predicted growth is associated advancement in technology and industrialization of those countries for the past decade.\nGDP – per capita: purchasing power parity – $9,300, €7,500 (2005 est.), $8,200, €6,800 (92) (2003), $7,900, €5,000 (2002)\nWorld median income: purchasing power parity $1,041, €950 (1993)\nGDP – composition by sector: agriculture: 4%; industry: 32%; services: 64% (2004 est.)\nInflation rate (consumer prices); In economics, inflation is a general rise in the price level in an economy over a period of time, resulting in a sustained drop in the purchasing power of money. When the general price level rises, each unit of currency buys fewer goods and services; consequently, inflation reflects a reduction in the purchasing power per unit of money – a loss of real value in the medium of exchange and unit of account within the economy. The opposite of inflation is deflation, a sustained decrease in the general price level of goods and services. The common measure of inflation is the inflation rate, the annualized percentage change in a general price index, usually the consumer price index, over time. national inflation rates vary widely in individual cases, from declining prices in Japan to hyperinflation (In economics, hyperinflation is very high and typically accelerating inflation) in several Third World countries (2003):\nWorld 2.6% (2017), 2.8% (predicted 2018);\nDeveloped Economies 1% to 4% typically\nDeveloping Countries 5% to 60% typically\nLeast developed countries 11.4% (2017), 8.3% (predicted 2018)\nDerivatives OTC outstanding notional amount: $601 trillion (Dec 2010) ([1])\nDerivatives exchange traded outstanding notional amount: $82 trillion (June 2011) ([2])\nGlobal debt issuance: $5.187 trillion, €3 trillion (2004), $4.938 trillion, €3.98 trillion (2003), $3.938 trillion (2002) (Thomson Financial League Tables)\nGlobal equity issuance: $505 billion, €450 billion (2004), $388 billion. €320 billion (2003), $319 billion, €250 trillion (2002) (Thomson Financial League Tables)\n\n\n=== Employment ===\n\nUnemployment rate: 8.7% (2009 est.). 30% (2007 est.) combined unemployment and underemployment in many non-industrialized countries; developed countries typically 4%–12% unemployment.\n\n\n=== Industries ===\nIndustrial production growth rate: 3% (2002 est.)\n\n\n=== Energy ===\n\nYearly electricity – production: 21,080,878 GWh (2011 est.), 15,850,000 GWh (2003 est.), 14,850,000 GWh (2001 est.)\nYearly electricity – consumption: 14,280,000 GWh (2003 est.), 13,930,000 GWh (2001 est.)\nOil – production: 79,650,000 bbl/d (12,663,000 m3/d) (2003 est.), 75,460,000 barrels per day (11,997,000 m3/d) (2001)\nOil – consumption: 80,100,000 bbl/d (12,730,000 m3/d) (2003 est.), 76,210,000 barrels per day (12,116,000 m3/d) (2001)\nOil – proved reserves: 1.025 trillion barrel (163 km3 (39 cu mi) (2001 est.)\nNatural gas – production: 3,366 km3 (808 cu mi) (2012 est.), 2,569 km3 (616 cu mi) (2001 est.)\nNatural gas – consumption: 2,556 km3 (613 cu mi) (2001 est.)\nNatural gas – proved reserves: 161,200 km3 (38,700 cu mi) (1 January 2002)\n\n\n=== Cross-border ===\nYearly exports: $12.4 trillion, €11.05 trillion (2009 est.)\nExports – commodities: the whole range of industrial and agricultural goods and services\nExports – partners: US 12.7%, Germany 7.1%, China 6.2%, France 4.4%, Japan 4.2%, UK 4.1% (2008)\nYearly imports: $12.29 trillion, €10.95 trillion (2009 est.)\nImports – commodities: the whole range of industrial and agricultural goods and services\nImports – partners: China 10.3%, Germany 8.6%, US 8.1%, Japan 5% (2008)\nDebt – external: $56.9 trillion, €40 trillion (31 December 2009 est.)\n\n\n=== Gift economy ===\nAnnual international aid: Official Development Assistance (ODA) of $204 billion (2022)\n\n\n=== Communications ===\nTelephones – main lines in use: 843,923,500 (2007)4,263,367,600 (2008)\n\nTelephones – mobile cellular: 3,300,000,000 (Nov. 2007)\nInternet Service Providers (ISPs): 10,350 (2000 est.)\nInternet users: 3,079,339,857 (31 December 2014 [3]), 360,985,492 (31 December 2000)\n\n\n=== Transport ===\n\nTransportation infrastructure worldwide includes:\n\nAirports\nTotal: 41,821 (2013)\nRoadways\nTotal: 32,345,165 km (20,098,354 mi)\nPaved: 19,403,061 km (12,056,503 mi)\nUnpaved: 12,942,104 km (8,041,851 mi) (2002)\nRailways\nTotal: 1,122,650 km (697,580 mi) includes about 190,000 to 195,000 km (118,000 to 121,000 mi) of electrified routes of which 147,760 km (91,810 mi) are in Europe, 24,509 km (15,229 mi) in the Far East, 11,050 km (6,870 mi) in Africa, 4,223 km (2,624 mi) in South America, and 4,160 km (2,580 mi) in North America.\n\n\n=== Military ===\n\nWorld military expenditure in 2018: estimated to $1.822 trillion\nMilitary expenditures – percent of GDP: roughly 2% of gross world product (1999).\n\n\n=== Science, research and development ===\n\nThe Royal Society in a 2011 report stated that in terms of number of papers the share of English-language scientific research papers the United States was first followed by China, the UK, Germany, Japan, France, and Canada. In 2015, research and development constituted an average 2.2% of the global GDP according to the UNESCO Institute for Statistics. Metrics and rankings of innovation include the Bloomberg Innovation Index, the Global Innovation Index and the share of Nobel laureates per capita.\n\n\n=== Resources and environment ===\n\nForests (carbon sinks, wood, ecosystem services, ...)\nEstimated number of trees that are net lost annually as of 2021: 10 billion\nGlobal annual deforested land in 2015–2020: 10 million hectares\nGlobal annual net forest area loss in 2000–2010 : 4.7 million hectares\nOther land degradation and land- and organisms-related ecosystem disturbances\nSoils (carbon sink, ecosystem services, food production, ...)\nSoil erosion by water in 2012: almost 36 billion tons (based on a high resolution global potential soil erosion model developed in 2017)\nEstimated annual loss of agricultural productivity due to soil erosion: 8 billion US dollars (based on the soil erosion data)\nSoil erosion by water in 2015: approximately 43 billion tons (according to a 2020 study)\nEnvironmental impact of pesticides\nPesticide use in tonnes of active ingredient in Australia in 2016: ca. 62,500 tonnes\nOceans (ecosystem services, food production, ...): Blue economy\nWaste and pollution (effects of economic mechanisms, effects on ecosystem services)\nAs of 2018, about 380 million tonnes of plastic is produced worldwide each year. From the 1950s up to 2018, an estimated 6.3 billion tonnes of plastic has been produced worldwide, of which an estimated 9% has been recycled and another 12% has been incinerated with the rest reportedly being "dumped in landfills or the natural environment".\nAir pollution\nNumber of human deaths caused annually by air pollution worldwide: ca. 7 million\nEstimated global annual cost of air pollution: $5 trillion\nMicroplastic pollution\nEstimated accumulated number of microplastic particles in the North Atlantic Ocean in 2014: 15 to 51 trillion particles, weighing between 93,000 and 236,000 metric tons\nEstimated accumulated number of microplastic particles in the North Atlantic Ocean in 2020: 3700 microplastics per cubic meter\n\nFrom the scientific perspective, economic activities are embedded in a web of dynamic, interrelated, and interdependent activities that constitute the natural system of Earth. Novel application of cybernetics in decision-making (such as in decision-making related to process- and product-design and related laws) and direction of human activity (such as economic activity) may make it easier to control modern ecological problems.\n\n\n== Historical development ==\n\nOne example for a comparable metric other than GDP are the OECD Better Life Index rankings for different aggregative domains.\n\nThe index includes 11 comparable "dimensions" of well-being:\n\nHousing: housing conditions and spendings (e.g. real estate pricing)\nIncome: household income (after taxes and transfers) and net financial wealth\nJobs: earnings, job security and unemployment\nCommunity: quality of social support network\nEducation: education and what one gets out of it\nEnvironment: quality of environment (e.g. environmental health)\nGovernance: involvement in democracy\nHealth\nLife Satisfaction: level of happiness\nSafety: murder and assault rates\nWork-life balance\n\n\n== Economic studies ==\nTo promote exports, many government agencies publish on the web economic studies by sector and country. Among these agencies include the USCS (US DoC) and FAS (USDA) in the United States, the EDC and AAFC in Canada, Ubifrance in France, the UKTI in the United Kingdom, the HKTDC and JETRO in Asia, Austrade and the NZTE in Oceania. Through Partnership Agreements, the Federation of International Trade Associations publishes studies from several of these agencies (USCS, FAS, AAFC, UKTI, and HKTDC) as well as other non-governmental organizations on its website globaltrade.net.\n\n\n== See also ==\n\nRegional economies:\n\nEvents:\n\nLists:\n\n\n== References ==\n\n\n== External links ==\n\nOECD – Economic Outlook\nUS Bureau of Labor and Statistics, Major Economic Indicators\nIMF – World Economic Outlook\nUN DESA – World Economy publications\nCIA – The World Factbook – World\nCareer Education for a Global Economy\nBBC News Special Report – Global Economy\nGuardian Special Report – Global Economy\nWorld Bank Summary Trade Statistics for World',
  },
  {
    title: "Urban planning",
    originalContent:
      'Urban planning, also known as town planning, city planning, regional planning, or rural planning in specific contexts, is a technical and political process that is focused on the development and design of land use and the built environment, including air, water, and the infrastructure passing into and out of urban areas, such as transportation, communications, and distribution networks, and their accessibility. Traditionally, urban planning followed a top-down approach in master planning the physical layout of human settlements. The primary concern was the public welfare, which included considerations of efficiency, sanitation, protection and use of the environment, as well as effects of the master plans on the social and economic activities. Over time, urban planning has adopted a focus on the social and environmental bottom lines that focus on planning as a tool to improve the health and well-being of people, maintaining sustainability standards. Similarly, in the early 21st century, Jane Jacobs\'s writings on legal and political perspectives to emphasize the interests of residents, businesses and communities effectively influenced urban planners to take into broader consideration of resident experiences and needs while planning.\nUrban planning answers questions about how people will live, work, and play in a given area and thus, guides orderly development in urban, suburban and rural areas. Although predominantly concerned with the planning of settlements and communities, urban planners are also responsible for planning the efficient transportation of goods, resources, people, and waste; the distribution of basic necessities such as water and electricity; a sense of inclusion and opportunity for people of all kinds, culture and needs; economic growth or business development; improving health and conserving areas of natural environmental significance that actively contributes to reduction in CO2 emissions as well as protecting heritage structures and built environments. Since most urban planning teams consist of highly educated individuals that work for city governments, recent debates focus on how to involve more community members in city planning processes.\nUrban planning is an interdisciplinary field that includes civil engineering, architecture, human geography, politics, social science and design sciences. Practitioners of urban planning are concerned with research and analysis, strategic thinking, engineering architecture, urban design, public consultation, policy recommendations, implementation and management. It is closely related to the field of urban design and some urban planners provide designs for streets, parks, buildings and other urban areas. Urban planners work with the cognate fields of civil engineering, landscape architecture, architecture, and public administration to achieve strategic, policy and sustainability goals. Early urban planners were often members of these cognate fields though today, urban planning is a separate, independent professional discipline. The discipline of urban planning is the broader category that includes different sub-fields such as land-use planning, zoning, economic development, environmental planning, and transportation planning. Creating the plans requires a thorough understanding of penal codes and zonal codes of planning.\nAnother important aspect of urban planning is that the range of urban planning projects include the large-scale master planning of empty sites or Greenfield projects as well as small-scale interventions and refurbishments of existing structures, buildings and public spaces. Pierre Charles L\'Enfant in Washington, D.C., Daniel Burnham in Chicago, Lúcio Costa in Brasília and Georges-Eugene Haussmann in Paris planned cities from scratch, and Robert Moses and Le Corbusier refurbished and transformed cities and neighborhoods to meet their ideas of urban planning.\n\n\n== History ==\n\nThere is evidence of urban planning and designed communities dating back to the Mesopotamian, Indus Valley, Minoan, and Egyptian civilizations in the third millennium BCE. Archaeologists studying the ruins of cities in these areas find paved streets that were laid out at right angles in a grid pattern. The idea of a planned out urban area evolved as different civilizations adopted it. Beginning in the 8th century BCE, Greek city states primarily used orthogonal (or grid-like) plans. Hippodamus of Miletus (498–408 BC), the ancient Greek architect and urban planner, is considered to be "the father of European urban planning", and the namesake of the "Hippodamian plan" (grid plan) of city layout.\nThe ancient Romans also used orthogonal plans for their cities. City planning in the Roman world was developed for military defense and public convenience. The spread of the Roman Empire subsequently spread the ideas of urban planning. As the Roman Empire declined, these ideas slowly disappeared. However, many cities in Europe still held onto the planned Roman city center. Cities in Europe from the 9th to 14th centuries, often grew organically and sometimes chaotically. But in the following centuries with the coming of the Renaissance many new cities were enlarged with newly planned extensions. From the 15th century on, much more is recorded of urban design and the people that were involved. In this period, theoretical treatises on architecture and urban planning start to appear in which theoretical questions around planning the main lines, ensuring plans meet the needs of the given population and so forth are addressed and designs of towns and cities are described and depicted. During the Enlightenment period, several European rulers ambitiously attempted to redesign capital cities. During the Second French Empire, Baron Georges-Eugène Haussmann, under the direction of Napoleon III, redesigned the city of Paris into a more modern capital, with long, straight, wide boulevards.\nPlanning and architecture went through a paradigm shift at the turn of the 20th century. The industrialized cities of the 19th century grew at a tremendous rate. The evils of urban life for the working poor were becoming increasingly evident as a matter of public concern. The laissez-faire style of government management of the economy, in fashion for most of the Victorian era, was starting to give way to a New Liberalism that championed intervention on the part of the poor and disadvantaged. Around 1900, theorists began developing urban planning models to mitigate the consequences of the industrial age, by providing citizens, especially factory workers, with healthier environments. The following century would therefore be globally dominated by a central planning approach to urban planning, not representing an increment in the overall quality of the urban realm.\nAt the beginning of the 20th century, urban planning began to be recognized as a separate profession. The Town and Country Planning Association was founded in 1899 and the first academic course in Great Britain on urban planning was offered by the University of Liverpool in 1909. In the 1920s, the ideas of modernism and uniformity began to surface in urban planning, and lasted until the 1970s. In 1933, Le Corbusier presented the Radiant City, a city that grows up in the form of towers, as a solution to the problem of pollution and over-crowding. But many planners started to believe that the ideas of modernism in urban planning led to higher crime rates and social problems.\nIn the second half of the 20th century, urban planners gradually shifted their focus to individualism and diversity in urban centers.\n\n\n== 21st century practices ==\n\nUrban planners studying the effects of increasing congestion in urban areas began to address the externalities, the negative impacts caused by induced demand from larger highway systems in western countries such as in the United States. The United Nations Department of Economic and Social Affairs predicted in 2018 that around 2.5 billion more people occupy urban areas by 2050 according to population elements of global migration. New planning theories have adopted non-traditional concepts such as Blue Zones and Innovation Districts to incorporate geographic areas within the city that allow for novel business development and the prioritization of infrastructure that would assist with improving the quality of life of citizens by extending their potential lifespan.\nPlanning practices have incorporated policy changes to help address anthropogenic (human caused) climate change. London began to charge a congestion charge for cars trying to access already crowded places in the city. Cities nowadays stress the importance of public transit and cycling by adopting such policies.\n\n\n== Theories ==\n\nPlanning theory is the body of scientific concepts, definitions, behavioral relationships, and assumptions that define the body of knowledge of urban planning. There are eight procedural theories of planning that remain the principal theories of planning procedure today: the rational-comprehensive approach, the incremental approach, the transactive approach, the communicative approach, the advocacy approach, the equity approach, the radical approach, and the humanist or phenomenological approach. Some other conceptual planning theories include Ebenezer Howard\'s The Three Magnets theory that he envisioned for the future of British settlement, also his Garden Cities, the Concentric Model Zone also called the Burgess Model by sociologist Ernest Burgess, the Radburn Superblock that encourages pedestrian movement, the Sector Model and the Multiple Nuclei Model among others.\n\n\n=== Participatory urban planning ===\nParticipatory planning is an urban planning approach that involves the entire community in the planning process. Participatory planning in the United States emerged during the 1960s and 1970s.\n\n\n== Technical aspects ==\n\nTechnical aspects of urban planning involve the application of scientific, technical processes, considerations and features that are involved in planning for land use, urban design, natural resources, transportation, and infrastructure. Urban planning includes techniques such as: predicting population growth, zoning, geographic mapping and analysis, analyzing park space, surveying the water supply, identifying transportation patterns, recognizing food supply demands, allocating healthcare and social services, and analyzing the impact of land use.\nIn order to predict how cities will develop and estimate the effects of their interventions, planners use various models. These models can be used to indicate relationships and patterns in demographic, geographic, and economic data. They might deal with short-term issues such as how people move through cities, or long-term issues such as land use and growth. One such model is the Geographic Information System (GIS) that is used to create a model of the existing planning and then to project future impacts on the society, economy and environment.\nBuilding codes and other regulations dovetail with urban planning by governing how cities are constructed and used from the individual level. Enforcement methodologies include governmental zoning, planning permissions, and building codes, as well as private easements and restrictive covenants.\nWith recent advances in information and communication technologies and the Internet of Things, an increasing number of cities are adopting technologies such as crowdsorced mobile phone sensing and machine learning to collect data and extract useful information to help make informed urban planning decisions.\n\n\n== Urban planners ==\n\nAn urban planner is a professional who works in the field of urban planning for the purpose of optimizing the effectiveness of a community\'s land use and infrastructure. They formulate plans for the development and management of urban and suburban areas. They typically analyze land use compatibility as well as economic, environmental, and social trends. In developing any plan for a community (whether commercial, residential, agricultural, natural or recreational), urban planners must consider a wide array of issues including sustainability, existing and potential pollution, transport including potential congestion, crime, land values, economic development, social equity, zoning codes, and other legislation.\nThe importance of the urban planner is increasing in the 21st century, as modern society begins to face issues of increased population growth, climate change and unsustainable development. An urban planner could be considered a green collar professional.\nSome researchers suggest that urban planners, globally, work in different "planning cultures", adapted to their cities and cultures. However, professionals have identified skills, abilities, and basic knowledge sets that are common to urban planners across regional and national boundaries.\n\n\n== Criticisms and debates ==\nThe school of neoclassical economics argues that planning is unnecessary, or even harmful, as it market efficiency allows for effective land use. A pluralist strain of political thinking argues in a similar vein that the government should not intrude in the political competition between different interest groups which decides how land is used. The traditional justification for urban planning has in response been that the planner does to the city what the engineer or architect does to the home, that is, make it more amenable to the needs and preferences of its inhabitants.\nThe widely adopted consensus-building model of planning, which seeks to accommodate different preferences within the community has been criticized for being based upon, rather than challenging, the power structures of the community. Instead, agonism has been proposed as a framework for urban planning decision-making.\nAnother debate within the urban planning field is about who is included and excluded in the urban planning decision-making process. Most urban planning processes use a top-down approach which fails to include the residents of the places where urban planners and city officials are working. Sherry Arnstein\'s "ladder of citizen participation" is often used by many urban planners and city governments to determine the degree of inclusivity or exclusivity of their urban planning. One main source of engagement between city officials and residents are city council meetings that are open to the residents and that welcome public comments. Additionally, in US there are some federal requirements for citizen participation in government-funded infrastructure projects.\nParticipatory urban planning has been criticized for contributing to the housing crisis in parts of the world.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nPennington, Mark (2008). "Urban planning". In Hamowy, Ronald (ed.). The Encyclopedia of Libertarianism. Thousand Oaks, CA: SAGE; Cato Institute. pp. 517–18. doi:10.4135/9781412965811.n316. ISBN 978-1-4129-6580-4. LCCN 2008009151. OCLC 750831024. S2CID 243497795.\nKnox, P. L. (2020) Better by Design?: Architecture, Urban Planning, and the Good City. Blacksburg: Virginia Tech Publishing. DOI: https://doi.org/10.21061/better-by-design\nPaul Waterhouse; Raymond Unwin (1912), Old Towns and New Needs; also the Town Extension Plan, Manchester: Victoria University of Manchester, OCLC 225676578, Wikidata Q18606907\n\n\n== External links ==\n\nAmerican Planning Association\n\n\n=== Library guides for urban planning ===\n"Urban Planning Resources". US: LibGuides at Arizona State University.\n"Urban Planning". Research Guides. University of California, Los Angeles Library. Archived from the original on 30 March 2014. Retrieved 21 March 2015.\nAvery Architectural and Fine Arts Library (September 2009). "Urban Planning: Basic Resources for Research". Research Guides. New York: Columbia University Libraries. Archived from the original on 2 April 2015. Retrieved 21 March 2015.\n"Urban and Regional Policy: Finding Articles". Research Guides. US: Georgia Tech. Archived from the original on 1 June 2022.{{cite web}}:  CS1 maint: unfit URL (link)\nHarvard University Graduate School of Design. "Urban Planning and Design". Research Guides. Massachusetts: Harvard Library.\n"Urban Affairs & Planning". Topic Guides. New York City: CUNY Hunter College Libraries. Archived from the original on 22 February 2014. Retrieved 21 March 2015.\n"City Planning". LibGuides. University of Illinois Library at University of Illinois at Urbana–Champaign. Archived from the original on 23 August 2015.\n"Urban Studies & Planning". Research Guides. Massachusetts Institute of Technology Libraries. Archived from the original on 7 February 2014.\n"Urban and Regional Planning". Research Guides. US: University of Michigan Library.\n"Urban Studies & Planning". Oregon, US: LibGuides at Portland State University. Archived from the original on 2 April 2015. Retrieved 21 March 2015.',
  },
  {
    title: "Housing market",
    originalContent:
      "Housing market can refer to:\n\nThe economics of real-estate used for residential purposes; see Real estate economics.\nReal estate business - buying, selling, or renting real estate (land, buildings, or housing).\nThe problem of assigning indivisible items (such as houses) to people with different preferences such that each person receives a single item; see House allocation problem.",
  },
  {
    title: "Real estate",
    originalContent:
      'Real estate is property consisting of land and the buildings on it, along with its natural resources such as growing crops (e.g. timber), minerals or water, and wild animals; immovable property of this nature; an interest vested in this (also) an item of real property, (more generally) buildings or housing in general. In terms of law, real relates to land property and is different from personal property, while estate means the "interest" a person has in that land property.\nReal estate is different from personal property, which is not permanently attached to the land (or comes with the land), such as vehicles, boats, jewelry, furniture, tools, and the rolling stock of a farm and farm animals.\nIn the United States, the transfer, owning, or acquisition of real estate can be through business corporations, individuals, nonprofit corporations, fiduciaries, or any legal entity as seen within the law of each U.S. state.\n\n\n== History of real estate ==\nThe natural right of a person to own property as a concept can be seen as having roots in Roman law as well as Greek philosophy. The profession of appraisal can be seen as beginning in England during the 1500s, as agricultural needs required land clearing and land preparation. Textbooks on the subject of surveying began to be written and the term "surveying" was used in England, while the term "appraising" was more used in North America. Natural law which can be seen as "universal law" was discussed among writers of the 15th and 16th century as it pertained to "property theory" and the inter-state relations dealing with foreign investments and the protection of citizens private property abroad. Natural law can be seen as having an influence in Emerich de Vattel\'s 1758 treatise The Law of Nations which conceptualized the idea of private property.\nOne of the largest initial real estate deals in history known as the "Louisiana Purchase" happened in 1803 when the Louisiana Purchase Treaty was signed. This treaty paved the way for western expansion and made the U.S. the owners of the "Louisiana Territory" as the land was bought from France for fifteen million, making each acre roughly 4 cents. The oldest real estate brokerage firm was established in 1855 in Chicago, Illinois, and was initially known as "L. D. Olmsted & Co." but is now known as "Baird & Warner". In 1908, the National Association of Realtors was founded in Chicago and in 1916, the name was changed to the National Association of Real Estate Boards and this was also when the term "realtor" was coined to identify real estate professionals.\nThe stock market crash of 1929 and the Great Depression in the U.S. caused a major drop in real estate worth and prices and ultimately resulted in depreciation of 50% for the four years after 1929. Housing financing in the U.S. was greatly affected by the Banking Act of 1933 and the National Housing Act in 1934 because it allowed for mortgage insurance for home buyers and this system was implemented by the Federal Deposit Insurance as well as the Federal Housing Administration. In 1938, an amendment was made to the National Housing Act and Fannie Mae, a government agency, was established to serve as a secondary market for mortgages and to give lenders more money in order for new homes to be funded.\nTitle VIII of the Civil Rights Act in the U.S., which is also known as the Fair Housing Act, was put into place in 1968 and dealt with the incorporation of African Americans into neighborhoods as the issues of discrimination were analyzed with the renting, buying, and financing of homes. Internet real estate as a concept began with the first appearance of real estate platforms on the World Wide Web (www) and occurred in 1999.\n\n\n== Residential real estate ==\nResidential real estate may contain either a single family or multifamily structure that is available for occupation or for non-business purposes.\nResidences can be classified by and how they are connected to neighbouring residences and land. Different types of housing tenure can be used for the same physical type. For example, connected residences might be owned by a single entity and leased out, or owned separately with an agreement covering the relationship between units and common areas and concerns.\nAccording to the Congressional Research Service, in 2021, 65% of homes in the U.S. are owned by the occupier.\n\nMajor categories\nAttached / multi-unit dwellings\nApartment (American English) or Flat (British English) – An individual unit in a multi-unit building. The boundaries of the apartment are generally defined by a perimeter of locked or lockable doors.  Often seen in multi-story apartment buildings.\nMulti-family house – Often seen in multi-story detached buildings, where each floor is a separate apartment or unit.\nTerraced house (a.k.a. townhouse or rowhouse) – A number of single or multi-unit buildings in a continuous row with shared walls and no intervening space.\nCondominium (American English) – A building or complex, similar to apartments, owned by individuals. Common grounds and common areas within the complex are owned and shared jointly. In North America, there are townhouse or rowhouse style condominiums as well. The British equivalent is a block of flats.\nHousing cooperative (a.k.a. co-op) – A type of multiple ownership in which the residents of a multi-unit housing complex own shares in the cooperative corporation that owns the property, giving each resident the right to occupy a specific apartment or unit. Majority of housing in Indian metro cities are of these types.\nTenement – A type of building shared by multiple dwellings, typically with flats or apartments on each floor and with shared entrance stairway access found in Britain.\nSemi-detached dwellings\nDuplex – Two units with one shared wall.\nDetached dwellings\nBungalows\nSplit-level home\nMansions\nVillas\nDetached house or single-family detached house\nCottages\nPortable dwellings\nMobile homes, tiny homes, or residential caravans – A full-time residence that can be (although might not in practice be) movable on wheels.\nHouseboats – A floating home\nTents – Usually temporary, with roof and walls consisting only of fabric-like material.\nOther categories\n\nChawls\nHavelis\nIgloos\nHuts\nThe size of havelis and chawls is measured in Gaz (square yards), Quila, Marla, Beegha, and acre.\nSee List of house types for a complete listing of housing types and layouts, real estate trends for shifts in the market, and house or home for more general information.\n\n\n== Real estate and the environment ==\nReal estate can be valued or devalued based on the amount of environmental degradation that has occurred. Environmental degradation can cause extreme health and safety risks. There is a growing demand for the use of site assessments (ESAs) when valuing a property for both private and commercial real estate.\nEnvironmental surveying is made possible by environmental surveyors who examine the environmental factors present within the development of real estate as well as the impacts that development and real estate has on the environment.\nGreen development is a concept that has grown since the 1970s with the environmental movement and the World Commission on Environment and Development. Green development examines social and environmental impacts with real estate and building. There are 3 areas of focus, being the environmental responsiveness, resource efficiency, and the sensitivity of cultural and societal aspects. Examples of Green development are green infrastructure, LEED, conservation development, and sustainability developments.\nReal estate in itself has been measured as a contributing factor to the rise in green house gases. According to the International Energy Agency, real estate in 2019 was responsible for 39 percent of total emissions worldwide and 11 percent of those emissions were due to the manufacturing of materials used in buildings.\n\n\n== Investment and development ==\n\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\nInvestment in real estate can be categorized by financial risk into core, value-added, and opportunistic.\nReal estate development can be less cyclical than real estate investing.\nIn markets where land and building prices are rising, real estate is often purchased as an investment, whether or not the owner intends to use the property. Often investment properties are rented out, but "flipping" involves quickly reselling a property, sometimes taking advantage of arbitrage or quickly rising value, and sometimes after repairs are made that substantially raise the value of the property. Luxury real estate is sometimes used as a way to store value, especially by wealthy foreigners, without any particular attempt to rent it out. Some luxury units in London and New York City have been used as a way for corrupt foreign government officials and businesspeople from countries without strong rule of law to launder money or to protect it from seizure.\n\n\n== Professionals ==\nReal estate agent – North America\nEstate agent – United Kingdom\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n The dictionary definition of real estate at Wiktionary\n Quotations related to Real estate at Wikiquote',
  },
  {
    title: "Civil engineering",
    originalContent:
      "Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewage systems, pipelines, structural components of buildings, and railways.\nCivil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. Civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to Fortune Global 500 companies.\n\n\n== History ==\n\n\n=== Civil engineering as a discipline ===\nCivil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in the understanding of physics and mathematics throughout history. Because civil engineering is a broad profession, including several specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environmental science, mechanics, project management, and other fields.\nThroughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads, and infrastructure that existed were repetitive, and increases in scale were incremental.\nOne of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes' principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.\n\n\n=== Civil engineering profession ===\n\nEngineering has been an aspect of life since the beginnings of human existence. The earliest practice of civil engineering may have commenced between 4000 and 2000 BC in ancient Egypt, the Indus Valley civilization, and Mesopotamia (ancient Iraq) when humans started to abandon a nomadic existence, creating a need for the construction of shelter. During this time, transportation became increasingly important leading to the development of the wheel and sailing.\nUntil modern times there was no clear distinction between civil engineering and architecture, and the term engineer and architect were mainly geographical variations referring to the same occupation, and often used interchangeably. The constructions of pyramids in Egypt (c. 2700–2500 BC) constitute some of the first instances of large structure constructions in history. Other ancient historic civil engineering constructions include the Qanat water management system in modern-day Iran (the oldest is older than 3000 years and longer than 71 kilometres (44 mi)), the Parthenon by Iktinos in Ancient Greece (447–438 BC), the Appian Way by Roman engineers (c. 312 BC), the Great Wall of China by General Meng T'ien under orders from Ch'in Emperor Shih Huang Ti (c. 220 BC) and the stupas constructed in ancient Sri Lanka like the Jetavanaramaya and the extensive irrigation works in Anuradhapura. The Romans developed civil structures throughout their empire, including especially aqueducts, insulae, harbors, bridges, dams and roads.\n\nIn the 18th century, the term civil engineering was coined to incorporate all things civilian as opposed to military engineering. In 1747, the first institution for the teaching of civil engineering, the École Nationale des Ponts et Chaussées, was established in France; and more examples followed in other European countries, like Spain. The first self-proclaimed civil engineer was John Smeaton, who constructed the Eddystone Lighthouse. In 1771 Smeaton and some of his colleagues formed the Smeatonian Society of Civil Engineers, a group of leaders of the profession who met informally over dinner. Though there was evidence of some technical meetings, it was little more than a social society.\n\nIn 1818 the Institution of Civil Engineers was founded in London, and in 1820 the eminent engineer Thomas Telford became its first president. The institution received a Royal charter in 1828, formally recognising civil engineering as a profession. Its charter defined civil engineering as:the art of directing the great sources of power in nature for the use and convenience of man, as the means of production and of traffic in states, both for external and internal trade, as applied in the construction of roads, bridges, aqueducts, canals, river navigation and docks for internal intercourse and exchange, and in the construction of ports, harbours, moles, breakwaters and lighthouses, and in the art of navigation by artificial power for the purposes of commerce, and in the construction and application of machinery, and in the drainage of cities and towns.\n\n\n=== Civil engineering education ===\nThe first private college to teach civil engineering in the United States was Norwich University, founded in 1819 by Captain Alden Partridge. The first degree in civil engineering in the United States was awarded by Rensselaer Polytechnic Institute in 1835. The first such degree to be awarded to a woman was granted by Cornell University to Nora Stanton Blatch in 1905.\nIn the UK during the early 19th century, the division between civil engineering and military engineering (served by the Royal Military Academy, Woolwich), coupled with the demands of the Industrial Revolution, spawned new engineering education initiatives: the Class of Civil Engineering and Mining was founded at King's College London in 1838, mainly as a response to the growth of the railway system and the need for more qualified engineers, the private College for Civil Engineers in Putney was established in 1839, and the UK's first Chair of Engineering was established at the University of Glasgow in 1840.\n\n\n== Education ==\nCivil engineers typically possess an academic degree in civil engineering. The length of study is three to five years, and the completed degree is designated as a bachelor of technology, or a bachelor of engineering. The curriculum generally includes classes in physics, mathematics, project management, design and specific topics in civil engineering. After taking basic courses in most sub-disciplines of civil engineering, they move on to specialize in one or more sub-disciplines at advanced levels. While an undergraduate degree (BEng/BSc) normally provides successful students with industry-accredited qualifications, some academic institutions offer post-graduate degrees (MEng/MSc), which allow students to further specialize in their particular area of interest.\n\n\n== Practicing engineers ==\nIn most countries, a bachelor's degree in engineering represents the first step towards professional certification, and a professional body certifies the degree program. After completing a certified degree program, the engineer must satisfy a range of requirements including work experience and exam requirements before being certified. Once certified, the engineer is designated as a professional engineer (in the United States, Canada and South Africa), a chartered engineer (in most Commonwealth countries), a chartered professional engineer (in Australia and New Zealand), or a European engineer (in most countries of the European Union). There are international agreements between relevant professional bodies to allow engineers to practice across national borders.\nThe benefits of certification vary depending upon location. For example, in the United States and Canada, \"only a licensed professional engineer may prepare, sign and seal, and submit engineering plans and drawings to a public authority for approval, or seal engineering work for public and private clients.\" This requirement is enforced under provincial law such as the Engineers Act in Quebec. No such legislation has been enacted in other countries including the United Kingdom. In Australia, state licensing of engineers is limited to the state of Queensland. Almost all certifying bodies maintain a code of ethics which all members must abide by.\nEngineers must obey contract law in their contractual relationships with other parties. In cases where an engineer's work fails, they may be subject to the law of tort of negligence, and in extreme cases, criminal charges. An engineer's work must also comply with numerous other rules and regulations such as building codes and environmental law.\n\n\n== Sub-disciplines ==\n\nThere are a number of sub-disciplines within the broad field of civil engineering. General civil engineers work closely with surveyors and specialized civil engineers to design grading, drainage, pavement, water supply, sewer service, dams, electric and communications supply. General civil engineering is also referred to as site engineering, a branch of civil engineering that primarily focuses on converting a tract of land from one usage to another. Site engineers spend time visiting project sites, meeting with stakeholders, and preparing construction plans. Civil engineers apply the principles of geotechnical engineering, structural engineering, environmental engineering, transportation engineering and construction engineering to residential, commercial, industrial and public works projects of all sizes and levels of construction.\n\n\n=== Coastal engineering ===\n\nCoastal engineering is concerned with managing coastal areas. In some jurisdictions, the terms sea defense and coastal protection mean defense against flooding and erosion, respectively. Coastal defense is the more traditional term, but coastal management has become popular as well.\n\n\n=== Construction engineering ===\n\nConstruction engineering involves planning and execution, transportation of materials, and site development based on hydraulic, environmental, structural, and geotechnical engineering. As construction firms tend to have higher business risk than other types of civil engineering firms, construction engineers often engage in more business-like transactions, such as drafting and reviewing contracts, evaluating logistical operations, and monitoring supply prices.\n\n\n=== Earthquake engineering ===\n\nEarthquake engineering involves designing structures to withstand hazardous earthquake exposures. Earthquake engineering is a sub-discipline of structural engineering. The main objectives of earthquake engineering are to understand interaction of structures on the shaky ground; foresee the consequences of possible earthquakes; and design, construct and maintain structures to perform at earthquake in compliance with building codes.\n\n\n=== Environmental engineering ===\n\nEnvironmental engineering is the contemporary term for sanitary engineering, though sanitary engineering traditionally had not included much of the hazardous waste management and environmental remediation work covered by environmental engineering. Public health engineering and environmental health engineering are other terms being used.\nEnvironmental engineering deals with treatment of chemical, biological, or thermal wastes, purification of water and air, and remediation of contaminated sites after waste disposal or accidental contamination. Among the topics covered by environmental engineering are pollutant transport, water purification, waste water treatment, air pollution, solid waste treatment, recycling, and hazardous waste management. Environmental engineers administer pollution reduction, green engineering, and industrial ecology. Environmental engineers also compile information on environmental consequences of proposed actions.\n\n\n=== Forensic engineering ===\n\nForensic engineering is the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property. The consequences of failure are dealt with by the law of product liability. The field also deals with retracing processes and procedures leading to accidents in operation of vehicles or machinery. The subject is applied most commonly in civil law cases, although it may be of use in criminal law cases. Generally the purpose of a Forensic engineering investigation is to locate cause or causes of failure with a view to improve performance or life of a component, or to assist a court in determining the facts of an accident. It can also involve investigation of intellectual property claims, especially patents.\n\n\n=== Geotechnical engineering ===\n\nGeotechnical engineering studies rock and soil supporting civil engineering systems. Knowledge from the field of soil science, materials science, mechanics, and hydraulics is applied to safely and economically design foundations, retaining walls, and other structures. Environmental efforts to protect groundwater and safely maintain landfills have spawned a new area of research called geo-environmental engineering.\nIdentification of soil properties presents challenges to geotechnical engineers. Boundary conditions are often well defined in other branches of civil engineering, but unlike steel or concrete, the material properties and behavior of soil are difficult to predict due to its variability and limitation on investigation. Furthermore, soil exhibits nonlinear (stress-dependent) strength, stiffness, and dilatancy (volume change associated with application of shear stress), making studying soil mechanics all the more difficult. Geotechnical engineers frequently work with professional geologists,  Geological Engineering professionals and soil scientists.\n\n\n=== Materials science and engineering ===\n\nMaterials science is closely related to civil engineering. It studies fundamental characteristics of materials, and deals with ceramics such as concrete and mix asphalt concrete, strong metals such as aluminum and steel, and thermosetting polymers including polymethylmethacrylate (PMMA) and carbon fibers.\nMaterials engineering involves protection and prevention (paints and finishes). Alloying combines two types of metals to produce another metal with desired properties. It incorporates elements of applied physics and chemistry. With recent media attention on nanoscience and nanotechnology, materials engineering has been at the forefront of academic research. It is also an important part of forensic engineering and failure analysis.\n\n\n=== Site development and planning ===\n\nSite development, also known as site planning, is focused on the planning and development potential of a site as well as addressing possible impacts from permitting issues and environmental challenges.\n\n\n=== Structural engineering ===\n\nStructural engineering is concerned with the structural design and structural analysis of buildings, bridges, towers, flyovers (overpasses), tunnels, off shore structures like oil and gas fields in the sea, aerostructure and other structures. This involves identifying the loads which act upon a structure and the forces and stresses which arise within that structure due to those loads, and then designing the structure to successfully support and resist those loads. The loads can be self weight of the structures, other dead load, live loads, moving (wheel) load, wind load, earthquake load, load from temperature change etc. The structural engineer must design structures to be safe for their users and to successfully fulfill the function they are designed for (to be serviceable). Due to the nature of some loading conditions, sub-disciplines within structural engineering have emerged, including wind engineering and earthquake engineering.\nDesign considerations will include strength, stiffness, and stability of the structure when subjected to loads which may be static, such as furniture or self-weight, or dynamic, such as wind, seismic, crowd or vehicle loads, or transitory, such as temporary construction loads or impact. Other considerations include cost, constructibility, safety, aesthetics and sustainability.\n\n\n=== Surveying ===\n\nSurveying is the process by which a surveyor measures certain dimensions that occur on or near the surface of the Earth. Surveying equipment such as levels and theodolites are used for accurate measurement of angular deviation, horizontal, vertical and slope distances. With computerization, electronic distance measurement (EDM), total stations, GPS surveying and laser scanning have to a large extent supplanted traditional instruments. Data collected by survey measurement is converted into a graphical representation of the Earth's surface in the form of a map. This information is then used by civil engineers, contractors and realtors to design from, build on, and trade, respectively. Elements of a structure must be sized and positioned in relation to each other and to site boundaries and adjacent structures.\n\nAlthough surveying is a distinct profession with separate qualifications and licensing arrangements, civil engineers are trained in the basics of surveying and mapping, as well as geographic information systems. Surveyors also lay out the routes of railways, tramway tracks, highways, roads, pipelines and streets as well as position other infrastructure, such as harbors, before construction.\n\nLand surveying\n\nIn the United States, Canada, the United Kingdom and most Commonwealth countries land surveying is considered to be a separate and distinct profession. Land surveyors are not considered to be engineers, and have their own professional associations and licensing requirements. The services of a licensed land surveyor are generally required for boundary surveys (to establish the boundaries of a parcel using its legal description) and subdivision plans (a plot or map based on a survey of a parcel of land, with boundary lines drawn inside the larger parcel to indicate the creation of new boundary lines and roads), both of which are generally referred to as Cadastral surveying. They collect data on important geological features below and on the land.\nConstruction surveying\nConstruction surveying is generally performed by specialized technicians. Unlike land surveyors, the resulting plan does not have legal status. Construction surveyors perform the following tasks:\n\nSurveying existing conditions of the future work site, including topography, existing buildings and infrastructure, and underground infrastructure when possible;\n\"lay-out\" or \"setting-out\": placing reference points and markers that will guide the construction of new structures such as roads or buildings;\nVerifying the location of structures during construction;\nAs-Built surveying: a survey conducted at the end of the construction project to verify that the work authorized was completed to the specifications set on plans.\n\n\n=== Transportation engineering ===\nTransportation engineering is concerned with moving people and goods efficiently, safely, and in a manner conducive to a vibrant community. This involves specifying, designing, constructing, and maintaining transportation infrastructure which includes streets, canals, highways, rail systems, airports, ports, and mass transit. It includes areas such as transportation design, transportation planning, traffic engineering, some aspects of urban engineering, queueing theory, pavement engineering, Intelligent Transportation System (ITS), and infrastructure management.\n\n\n=== Municipal or urban engineering ===\n\nMunicipal engineering is concerned with municipal infrastructure. This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and cycling infrastructure. In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services. It can also include the optimization of waste collection and bus service networks. Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously, and managed by the same municipal authority.  Municipal engineers may also design the site civil works for large buildings, industrial plants or campuses (i.e. access roads, parking lots, potable water supply, treatment or pretreatment of waste water, site drainage, etc.)\n\n\n=== Water resources engineering ===\n\nWater resources engineering is concerned with the collection and management of water (as a natural resource). As a discipline, it therefore combines elements of hydrology, environmental science, meteorology, conservation, and resource management. This area of civil engineering relates to the prediction and management of both the quality and the quantity of water in both underground (aquifers) and above ground (lakes, rivers, and streams) resources. Water resource engineers analyze and model very small to very large areas of the earth to predict the amount and content of water as it flows into, through, or out of a facility. However, the actual design of the facility may be left to other engineers.\n\nHydraulic engineering concerns the flow and conveyance of fluids, principally water. This area of civil engineering is intimately related to the design of pipelines, water supply network, drainage facilities (including bridges, dams, channels, culverts, levees, storm sewers), and canals. Hydraulic engineers design these facilities using the concepts of fluid pressure, fluid statics, fluid dynamics, and hydraulics, among others.\n\n\n=== Civil engineering systems ===\nCivil engineering systems is a discipline that promotes using systems thinking to manage complexity and change in civil engineering within its broader public context. It posits that the proper development of civil engineering infrastructure requires a holistic, coherent understanding of the relationships between all of the crucial factors that contribute to successful projects while at the same time emphasizing the importance of attention to technical detail. Its purpose is to help integrate the entire civil engineering project life cycle from conception, through planning, designing, making, operating to decommissioning.\n\n\n== See also ==\n\n\n=== Associations ===\n\n\n== References ==\n\n\n== Further reading ==\nBlockley, David (2014). Structural Engineering: a very short introduction. New York: Oxford University Press. ISBN 978-0-19-967193-9.\nChen, W.F.; Liew, J.Y. Richard, eds. (2002). The Civil Engineering Handbook. CRC Press. ISBN 978-0-8493-0958-8.\nMuir Wood, David (2012). Civil Engineering: a very short introduction. New York: Oxford University Press. ISBN 978-0-19-957863-4.\nRicketts, Jonathan T.; Loftin, M. Kent; Merritt, Frederick S., eds. (2004). Standard handbook for civil engineers (5 ed.). McGraw Hill. ISBN 978-0-07-136473-7.\n\n\n== External links ==\n\nThe Institution of Civil Engineers\nCivil Engineering Software Database\nThe Institution of Civil Engineering Surveyors\nCivil engineering classes, from MIT OpenCourseWare",
  },
  {
    title: "Structural engineering",
    originalContent:
      "Structural engineering is a sub-discipline of civil engineering in which structural engineers are trained to design the 'bones and joints' that create the form and shape of human-made structures. Structural engineers also must understand and calculate the stability, strength, rigidity and earthquake-susceptibility of built structures for buildings and nonbuilding structures. The structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site. They can also be involved in the design of machinery, medical equipment, and vehicles where structural integrity affects functioning and safety.  See glossary of structural engineering.\nStructural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. Structural engineering design uses a number of relatively simple structural concepts to build complex structural systems. Structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.\n\n\n== History ==\n\nStructural engineering dates back to 2700 B.C. when the step pyramid for Pharaoh Djoser was built by Imhotep, the first engineer in history known by name. Pyramids were the most common major structures built by ancient civilizations because the structural form of a pyramid is inherently stable and can be almost infinitely scaled (as opposed to most other structural forms, which cannot be linearly increased in size in proportion to increased loads).\nThe structural stability of the pyramid, whilst primarily gained from its shape, relies also on the strength of the stone from which it is constructed, and its ability to support the weight of the stone above it. The limestone blocks were often taken from a quarry near the building site and have a compressive strength from 30 to 250 MPa (MPa = Pa × 106). Therefore, the structural strength of the pyramid stems from the material properties of the stones from which it was built rather than the pyramid's geometry.\nThroughout ancient and medieval history most architectural design and construction were carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. No theory of structures existed, and understanding of how structures stood up was extremely limited, and based almost entirely on empirical evidence of 'what had worked before' and intuition. Knowledge was retained by guilds and seldom supplanted by advances. Structures were repetitive, and increases in scale were incremental.\nNo record exists of the first calculations of the strength of structural members or the behavior of structural material, but the profession of a structural engineer only really took shape with the Industrial Revolution and the re-invention of concrete (see History of Concrete). The physical sciences underlying structural engineering began to be understood in the Renaissance and have since developed into computer-based applications pioneered in the 1970s.\n\n\n=== Timeline ===\n\n1452–1519 Leonardo da Vinci made many contributions.\n1638: Galileo Galilei published the book Two New Sciences in which he examined the failure of simple structures.\n1660: Hooke's law by Robert Hooke.\n1687: Isaac Newton published Philosophiæ Naturalis Principia Mathematica, which contains his laws of motion.\n1750: Euler–Bernoulli beam equation.\n1700–1782: Daniel Bernoulli introduced the principle of virtual work.\n1707–1783: Leonhard Euler developed the theory of buckling of columns.\n1826: Claude-Louis Navier published a treatise on the elastic behaviors of structures.\n1873: Carlo Alberto Castigliano presented his dissertation \"Intorno ai sistemi elastici\", which contains his theorem for computing displacement as the partial derivative of the strain energy. This theorem includes the method of \"least work\" as a special case.\n1874: Otto Mohr formalized the idea of a statically indeterminate structure.\n1922: Timoshenko corrects the Euler–Bernoulli beam equation.\n1936: Hardy Cross' publication of the moment distribution method, an important innovation in the design of continuous frames.\n1941: Alexander Hrennikoff solved the discretization of plane elasticity problems using a lattice framework.\n1942: Richard Courant divided a domain into finite subregions.\n1956: J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp's paper on the \"Stiffness and Deflection of Complex Structures\" introduces the name \"finite-element method\" and is widely recognized as the first comprehensive treatment of the method as it is known today.\n\n\n=== Structural failure ===\n\nThe history of structural engineering contains many collapses and failures.  Sometimes this is due to obvious negligence, as in the case of the Pétion-Ville school collapse, in which Rev. Fortin Augustin \" constructed the building all by himself, saying he didn't need an engineer as he had good knowledge of construction\" following a partial collapse of the three-story schoolhouse that sent neighbors fleeing. The final collapse killed 94 people, mostly children.\nIn other cases structural failures require careful study, and the results of these inquiries have resulted in improved practices and a greater understanding of the science of structural engineering.  Some such studies are the result of forensic engineering investigations where the original engineer seems to have done everything in accordance with the state of the profession and acceptable practice yet a failure still eventuated.  A famous case of structural knowledge and practice being advanced in this manner can be found in a series of failures involving box girders which collapsed in Australia during the 1970s.\n\n\n== Theory ==\n\nStructural engineering depends upon a detailed knowledge of applied mechanics, materials science, and applied mathematics to understand and predict how structures support and resist self-weight and imposed loads. To apply the knowledge successfully a structural engineer generally requires detailed knowledge of relevant empirical and theoretical design codes, the techniques of structural analysis, as well as some knowledge of the corrosion resistance of the materials and structures, especially when those structures are exposed to the external environment. Since the 1990s, specialist software has become available to aid in the design of structures, with the functionality to assist in the drawing, analyzing and designing of structures with maximum precision; examples include AutoCAD, StaadPro, ETABS, Prokon, Revit Structure, Inducta RCB, etc. Such software may also take into consideration environmental loads, such as earthquakes and winds.\n\n\n== Profession ==\n\nStructural engineers are responsible for engineering design and structural analysis. Entry-level structural engineers may design the individual structural elements of a structure, such as the beams and columns of a building. More experienced engineers may be responsible for the structural design and integrity of an entire system, such as a building.\nStructural engineers often specialize in particular types of structures, such as buildings, bridges, pipelines, industrial, tunnels, vehicles, ships, aircraft, and spacecraft.  Structural engineers who specialize in buildings may specialize in particular construction materials such as concrete, steel, wood, masonry, alloys and composites.\nStructural engineering has existed since humans first started to construct their structures. It became a more defined and formalized profession with the emergence of architecture as a distinct profession from engineering during the industrial revolution in the late 19th century. Until then, the architect and the structural engineer were usually one and the same thing – the master builder. Only with the development of specialized knowledge of structural theories that emerged during the 19th and early 20th centuries, did the professional structural engineers come into existence.\nThe role of a structural engineer today involves a significant understanding of both static and dynamic loading and the structures that are available to resist them. The complexity of modern structures often requires a great deal of creativity from the engineer in order to ensure the structures support and resist the loads they are subjected to. A structural engineer will typically have a four or five-year undergraduate degree, followed by a minimum of three years of professional practice before being considered fully qualified.\nStructural engineers are licensed or accredited by different learned societies and regulatory bodies around the world (for example, the Institution of Structural Engineers in the UK). Depending on the degree course they have studied and/or the jurisdiction they are seeking licensure in, they may be accredited (or licensed) as just structural engineers, or as civil engineers, or as both civil and structural engineers.\nAnother international organisation is IABSE(International Association for Bridge and Structural Engineering). The aim of that association is to exchange knowledge and to advance the practice of structural engineering worldwide in the service of the profession and society.\n\n\n== Specializations ==\n\n\n=== Building structures ===\n\nStructural building engineering is primarily driven by the creative manipulation of materials and forms and the underlying mathematical and scientific ideas to achieve an end that fulfills its functional requirements and is structurally safe when subjected to all the loads it could reasonably be expected to experience. This is subtly different from architectural design, which is driven by the creative manipulation of materials and forms, mass, space, volume, texture, and light to achieve an end which is aesthetic, functional, and often artistic.\nThe structural design for a building must ensure that the building can stand up safely, able to function without excessive deflections or movements which may cause fatigue of structural elements, cracking or failure of fixtures, fittings or partitions, or discomfort for occupants. It must account for movements and forces due to temperature, creep, cracking, and imposed loads. It must also ensure that the design is practically buildable within acceptable manufacturing tolerances of the materials. It must allow the architecture to work, and the building services to fit within the building and function (air conditioning, ventilation, smoke extract, electrics, lighting, etc.). The structural design of a modern building can be extremely complex and often requires a large team to complete.\nStructural engineering specialties for buildings include:\n\nEarthquake engineering\nFaçade engineering\nFire engineering\nRoof engineering\nTower engineering\nWind engineering\n\n\n=== Earthquake engineering structures ===\n\nEarthquake engineering structures are those engineered to withstand earthquakes.\nThe main objectives of earthquake engineering are to understand the interaction of structures with the shaking ground, foresee the consequences of possible earthquakes, and design and construct the structures to perform during an earthquake.\nEarthquake-proof structures are not necessarily extremely strong like the El Castillo pyramid at Chichen Itza shown above.\nOne important tool of earthquake engineering is base isolation, which allows the base of a structure to move freely with the ground.\n\n\n=== Civil engineering structures ===\nCivil structural engineering includes all structural engineering related to the built environment. It includes:\n\nThe structural engineer is the lead designer on these structures, and often the sole designer. In the design of structures such as these, structural safety is of paramount importance (in the UK, designs for dams, nuclear power stations and bridges must be signed off by a chartered engineer).\nCivil engineering structures are often subjected to very extreme forces, such as large variations in temperature, dynamic loads such as waves or traffic, or high pressures from water or compressed gases. They are also often constructed in corrosive environments, such as at sea, in industrial facilities, or below ground.\n\nresisted and significant deflections of structures.\n\nThe forces which parts of a machine are subjected to can vary significantly and can do so at a great rate. The forces which a boat or aircraft are subjected to vary enormously and will do so thousands of times over the structure's lifetime. The structural design must ensure that such structures can endure such loading for their entire design life without failing.\nThese works can require mechanical structural engineering:\n\nBoilers and pressure vessels\nCoachworks and carriages\nCranes\nElevators\nEscalators\nMarine vessels and hulls\n\n\n=== Aerospace structures ===\n\nAerospace structure types include launch vehicles, (Atlas, Delta, Titan), missiles (ALCM, Harpoon), Hypersonic vehicles (Space Shuttle), military aircraft (F-16, F-18) and commercial aircraft (Boeing 777, MD-11). Aerospace structures typically consist of thin plates with stiffeners for the external surfaces, bulkheads, and frames to support the shape and fasteners such as welds, rivets, screws, and bolts to hold the components together.\n\n\n=== Nanoscale structures ===\nA nanostructure is an object of intermediate size between molecular and microscopic (micrometer-sized) structures. In describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale. Nanotextured surfaces have one dimension on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. Nanotubes have two dimensions on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. Finally, spherical nanoparticles have three dimensions on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometer range. The term 'nanostructure' is often used when referring to magnetic technology.\n\n\n=== Structural engineering for medical science ===\n\nMedical equipment (also known as armamentarium) is designed to aid in the diagnosis, monitoring or treatment of medical conditions. There are several basic types: diagnostic equipment includes medical imaging machines, used to aid in diagnosis; equipment includes infusion pumps, medical lasers, and LASIK surgical machines; medical monitors allow medical staff to measure a patient's medical state. Monitors may measure patient vital signs and other parameters including ECG, EEG, blood pressure, and dissolved gases in the blood; diagnostic medical equipment may also be used in the home for certain purposes, e.g. for the control of diabetes mellitus. A biomedical equipment technician (BMET) is a vital component of the healthcare delivery system. Employed primarily by hospitals, BMETs are the people responsible for maintaining a facility's medical equipment.\n\n\n== Structural elements ==\n\nAny structure is essentially made up of only a small number of different types of elements:\n\nColumns\nBeams\nPlates\nArches\nShells\nCatenaries\nMany of these elements can be classified according to form (straight, plane / curve) and dimensionality (one-dimensional / two-dimensional):\n\n\n=== Columns ===\n\nColumns are elements that carry only axial force (compression) or both axial force and bending (which is technically called a beam-column but practically, just a column). The design of a column must check the axial capacity of the element and the buckling capacity.\nThe buckling capacity is the capacity of the element to withstand the propensity to buckle. Its capacity depends upon its geometry, material, and the effective length of the column, which depends upon the restraint conditions at the top and bottom of the column. The effective length is \n  \n    \n      \n        K\n        ∗\n        l\n      \n    \n    {\\displaystyle K*l}\n  \n where \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \n is the real length of the column and K is the factor dependent on the restraint conditions.\nThe capacity of a column to carry axial load depends on the degree of bending it is subjected to, and vice versa. This is represented on an interaction chart and is a complex non-linear relationship.\n\n\n=== Beams ===\n\nA beam may be defined as an element in which one dimension is much greater than the other two and the applied loads are usually normal to the main axis of the element. Beams and columns are called line elements and are often represented by simple lines in structural modeling.\n\ncantilevered (supported at one end only with a fixed connection)\nsimply supported (fixed against vertical translation at each end and horizontal translation at one end only,  and able to rotate at the supports)\nfixed (supported in all directions for translation and rotation at each end)\ncontinuous (supported by three or more supports)\na combination of the above (ex. supported at one end and in the middle)\nBeams are elements that carry pure bending only. Bending causes one part of the section of a beam (divided along its length) to go into compression and the other part into tension. The compression part must be designed to resist buckling and crushing, while the tension part must be able to adequately resist the tension.\n\n\n=== Trusses ===\n\nA truss is a structure comprising members and connection points or nodes. When members are connected at nodes and forces are applied at nodes members can act in tension or compression. Members acting in compression are referred to as compression members or struts while members acting in tension are referred to as tension members or ties. Most trusses use gusset plates to connect intersecting elements.  Gusset plates are relatively flexible and unable to transfer bending moments. The connection is usually arranged so that the lines of force in the members are coincident at the joint thus allowing the truss members to act in pure tension or compression.\nTrusses are usually used in large-span structures, where it would be uneconomical to use solid beams.\n\n\n=== Plates ===\n\nPlates carry bending in two directions. A concrete flat slab is an example of a plate. Plates are understood by using continuum mechanics, but due to the complexity involved they are most often designed using a codified empirical approach, or computer analysis.\nThey can also be designed with yield line theory, where an assumed collapse mechanism is analyzed to give an upper bound on the collapse load. This technique is used in practice but because the method provides an upper-bound (i.e. an unsafe prediction of the collapse load) for poorly conceived collapse mechanisms, great care is needed to ensure that the assumed collapse mechanism is realistic.\n\n\n=== Shells ===\n\nShells derive their strength from their form and carry forces in compression in two directions. A dome is an example of a shell. They can be designed by making a hanging-chain model, which will act as a catenary in pure tension and inverting the form to achieve pure compression.\n\n\n=== Arches ===\n\nArches carry forces in compression in one direction only, which is why it is appropriate to build arches out of masonry. They are designed by ensuring that the line of thrust of the force remains within the depth of the arch. It is mainly used to increase the bountifulness of any structure.\n\n\n=== Catenaries ===\n\nCatenaries derive their strength from their form and carry transverse forces in pure tension by deflecting (just as a tightrope will sag when someone walks on it). They are almost always cable or fabric structures. A fabric structure acts as a catenary in two directions.\n\n\n== Materials ==\n\nStructural engineering depends on the knowledge of materials and their properties, in order to understand how different materials support and resist loads. It also involves a knowledge of Corrosion engineering to avoid for example galvanic coupling of dissimilar materials.\nCommon structural materials are:\n\nIron: wrought iron, cast iron\nConcrete: reinforced concrete, prestressed concrete\nAlloy: steel, stainless steel\nMasonry\nTimber: hardwood, softwood\nAluminium\nComposite materials: plywood\nOther structural materials: adobe, bamboo, carbon fibre, fiber reinforced plastic, mudbrick, roofing materials\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\nHibbeler, R. C. (2010). Structural Analysis. Prentice-Hall.\nBlank, Alan; McEvoy, Michael; Plank, Roger (1993). Architecture and Construction in Steel. Taylor & Francis. ISBN 0-419-17660-8.\nHewson, Nigel R. (2003). Prestressed Concrete Bridges: Design and Construction. Thomas Telford. ISBN 0-7277-2774-5.\nHeyman, Jacques (1999). The Science of Structural Engineering. Imperial College Press. ISBN 1-86094-189-3.\nHosford, William F. (2005). Mechanical Behavior of Materials. Cambridge University Press. ISBN 0-521-84670-6.\n\n\n== Further reading ==\nBlockley, David (2014). A Very Short Introduction to Structural Engineering. Oxford University Press ISBN 978-0-19967193-9.\nBradley, Robert E.; Sandifer, Charles Edward (2007). Leonhard Euler: Life, Work, and Legacy. Elsevier. ISBN 0-444-52728-1.\nChapman, Allan. (2005). England's Leornardo: Robert Hooke and the Seventeenth Century's Scientific Revolution. CRC Press. ISBN 0-7503-0987-3.\nDugas, René (1988). A History of Mechanics. Courier Dover Publications. ISBN 0-486-65632-2.\nFeld, Jacob; Carper, Kenneth L. (1997). Construction Failure. John Wiley & Sons. ISBN 0-471-57477-5.\nGalilei, Galileo. (translators: Crew, Henry; de Salvio, Alfonso) (1954). Dialogues Concerning Two New Sciences. Courier Dover Publications. ISBN 0-486-60099-8\nKirby, Richard Shelton (1990). Engineering in History. Courier Dover Publications. ISBN 0-486-26412-2.\nHeyman, Jacques (1998). Structural Analysis: A Historical Approach. Cambridge University Press. ISBN 0-521-62249-2.\nLabrum, E.A. (1994). Civil Engineering Heritage. Thomas Telford. ISBN 0-7277-1970-X.\nLewis, Peter R. (2004). Beautiful Bridge of the Silvery Tay. Tempus.\nMir, Ali (2001). Art of the Skyscraper: the Genius of Fazlur Khan. Rizzoli International Publications. ISBN 0-8478-2370-9.\nRozhanskaya, Mariam; Levinova, I. S. (1996). \"Statics\" in Morelon, Régis & Rashed, Roshdi (1996). Encyclopedia of the History of Arabic Science, vol. 2–3, Routledge. ISBN 0-415-02063-8\nWhitbeck, Caroline (1998). Ethics in Engineering Practice and Research. Cambridge University Press. ISBN 0-521-47944-4.\nHoogenboom P.C.J. (1998). \"Discrete Elements and Nonlinearity in Design of Structural Concrete Walls\", Section 1.3 Historical Overview of Structural Concrete Modelling, ISBN 90-901184-3-8.\nNedwell, P.J.; Swamy, R.N.(ed) (1994). Ferrocement:Proceedings of the Fifth International Symposium. Taylor & Francis. ISBN 0-419-19700-1.\n\n\n== External links ==\n\nStructural Engineering Association – International\nNational Council of Structural Engineers Associations\nStructural Engineering Institute, an institute of the American Society of Civil Engineers\nStructurae database of structures\nStructural Engineering Association – International\nThe EN Eurocodes are a series of 10 European Standards, EN 1990 – EN 1999, providing a common approach for the design of buildings and other civil engineering works and construction products",
  },
  {
    title: "Environmental engineering",
    originalContent:
      "Environmental engineering is a professional engineering discipline related to environmental science. It encompasses broad scientific topics like chemistry, biology, ecology, geology, hydraulics, hydrology, microbiology, and mathematics to create solutions that will protect and also improve the health of living organisms and improve the quality of the environment. Environmental engineering is a sub-discipline of civil engineering and chemical engineering. While on the part of civil engineering, the Environmental Engineering is focused mainly on Sanitary Engineering.\nEnvironmental engineering applies scientific and engineering principles to improve and maintain the environment to protect human health, protect nature's beneficial ecosystems, and improve environmental-related enhancement of the quality of human life.\nEnvironmental engineers devise solutions for wastewater management, water and air pollution control, recycling, waste disposal, and public health.  They design municipal water supply and industrial wastewater treatment systems, and design plans to prevent waterborne diseases and improve sanitation in urban, rural and recreational areas. They evaluate hazardous-waste management systems to evaluate the severity of such hazards, advise on treatment and containment, and develop regulations to prevent mishaps. They implement environmental engineering law, as in assessing the environmental impact of proposed construction projects.\nEnvironmental engineers study the effect of technological advances on the environment, addressing local and worldwide environmental issues such as acid rain, global warming, ozone depletion, water pollution and air pollution from automobile exhausts and industrial sources.\nMost jurisdictions impose licensing and registration requirements for qualified environmental engineers.\n\n\n== Etymology ==\nThe word environmental has its root in the late 19th-century French word environ (verb), meaning to encircle or to encompass. The word environment was used by Carlyle in 1827 to refer to the aggregate of conditions in which a person or thing lives. The meaning shifted again in 1956 when it was used in the ecological sense, where Ecology is the branch of science dealing with the relationship of living things to their environment.\nThe second part of the phrase environmental engineer originates from Latin roots and was used in the 14th century French as engignour, meaning a constructor of military engines such as trebuchets, harquebuses, longbows, cannons, catapults, ballistas, stirrups, armour as well as other deadly or bellicose contraptions. The word engineer was not used to reference public works until the 16th century; and it likely entered the popular vernacular as meaning a contriver of public works during John Smeaton's time.\n\n\n== History ==\n\n\n=== Ancient civilizations ===\nEnvironmental engineering is a name for work that has been done since early civilizations, as people learned to modify and control the environmental conditions to meet needs. As people recognized that their health was related to the quality of their environment, they built systems to improve it. The ancient Indus Valley Civilization (3300 B.C.E. to 1300 B.C.E.) had advanced control over their water resources. The public work structures found at various sites in the area include wells, public baths, water storage tanks, a drinking water system, and a city-wide sewage collection system. They also had an early canal irrigation system enabling large-scale agriculture.\nFrom 4000 to 2000 B.C.E., many civilizations had drainage systems and some had sanitation facilities, including the Mesopotamian Empire, Mohenjo-Daro, Egypt, Crete, and the Orkney Islands in Scotland. The Greeks also had aqueducts and sewer systems that used rain and wastewater to irrigate and fertilize fields.\nThe first aqueduct in Rome was constructed in 312 B.C.E., and the Romans continued to construct aqueducts for irrigation and safe urban water supply during droughts. They also built an underground sewer system as early as the 7th century B.C.E. that fed into the Tiber River, draining marshes to create farmland as well as removing sewage from the city.\n\n\n=== Modern era ===\nVery little change was seen from the decline of the Roman Empire until the 19th century, where improvements saw increasing efforts focused on public health. Modern environmental engineering began in London in the mid-19th century when Joseph Bazalgette designed the first major sewerage system following the Great Stink. The city's sewer system conveyed raw sewage to the River Thames, which also supplied the majority of the city's drinking water, leading to an outbreak of cholera. The introduction of drinking water treatment and sewage treatment in industrialized countries reduced waterborne diseases from leading causes of death to rarities.\nThe field emerged as a separate academic discipline during the middle of the 20th century in response to widespread public concern about water and air pollution and other environmental degradation. As society and technology grew more complex, they increasingly produced unintended effects on the natural environment. One example is the widespread application of the pesticide DDT to control agricultural pests in the years following World War II. The story of DDT as vividly told in Rachel Carson's Silent Spring (1962) is considered to be the birth of the modern environmental movement, which led to the modern field of \"environmental engineering.\"\n\n\n== Education ==\n\nMany universities offer environmental engineering programs through either the department of civil engineering or chemical engineering and also including electronic projects to develop and balance the environmental\nconditions.  Environmental engineers in a civil engineering program often focus on hydrology, water resources management, bioremediation, and water and wastewater treatment plant design. Environmental engineers in a chemical engineering program tend to focus on environmental chemistry, advanced air and water treatment technologies, and separation processes. Some subdivisions of environmental engineering include natural resources engineering and agricultural engineering.\nCourses for students fall into a few broad classes:\n\nMechanical engineering courses oriented towards designing machines and mechanical systems for environmental use such as water and wastewater treatment facilities, pumping stations, garbage segregation plants, and other mechanical facilities.\nEnvironmental engineering or environmental systems courses oriented towards a civil engineering approach in which structures and the landscape are constructed to blend with or protect the environment.\nEnvironmental chemistry, sustainable chemistry or environmental chemical engineering courses oriented towards understanding the effects of chemicals in the environment, including any mining processes, pollutants, and also biochemical processes.\nEnvironmental technology courses oriented towards producing electronic or electrical graduates capable of developing devices and artifacts able to monitor, measure, model and control environmental impact, including monitoring and managing energy generation from renewable sources.\n\n\n=== Curriculum ===\nThe following topics make up a typical curriculum in environmental engineering:\n\nMass and Energy transfer\nEnvironmental chemistry\nInorganic chemistry\nOrganic Chemistry\nNuclear Chemistry\nGrowth models\nResource consumption\nPopulation growth\nEconomic growth\nRisk assessment\nHazard identification\nDose-response Assessment\nExposure assessment\nRisk characterization\nComparative risk analysis\nWater pollution\nWater resources and pollutants\nOxygen demand\nPollutant transport\nWater and waste water treatment\nAir pollution\nIndustry, transportation, commercial and residential emissions\nCriteria and toxic air pollutants\nPollution modelling (e.g. Atmospheric dispersion modeling)\nPollution control\nAir pollution and meteorology\nGlobal change\nGreenhouse effect and global temperature\nCarbon, nitrogen, and oxygen cycle\nIPCC emissions scenarios\nOceanic changes (ocean acidification, other effects of global warming on oceans) and changes in the stratosphere (see Physical impacts of climate change)\nSolid waste management and resource recovery\nLife cycle assessment\nSource reduction\nCollection and transfer operations\nRecycling\nWaste-to-energy conversion\nLandfill\n\n\n== Applications ==\n\n\n=== Water supply and treatment ===\nEnvironmental engineers evaluate the water balance within a watershed and determine the available water supply, the water needed for various needs in that watershed, the seasonal cycles of water movement through the watershed and they develop systems to store, treat, and convey water for various uses.\nWater is treated to achieve water quality objectives for the end uses. In the case of a potable water supply, water is treated to minimize the risk of infectious disease transmission, the risk of non-infectious illness, and to create a palatable water flavor. Water distribution systems are designed and built to provide adequate water pressure and flow rates to meet various end-user needs such as domestic use, fire suppression, and irrigation.\n\n\n=== Wastewater treatment ===\nThere are numerous wastewater treatment technologies. A wastewater treatment train can consist of a primary clarifier system to remove solid and floating materials, a secondary treatment system consisting of an aeration basin followed by flocculation and sedimentation or an activated sludge system and a secondary clarifier, a tertiary biological nitrogen removal system, and a final disinfection process. The aeration basin/activated sludge system removes organic material by growing bacteria (activated sludge).  The secondary clarifier removes the activated sludge from the water. The tertiary system, although not always included due to costs, is becoming more prevalent to remove nitrogen and phosphorus and to disinfect the water before discharge to a surface water stream or ocean outfall.\n\n\n=== Air pollution management ===\nScientists have developed air pollution dispersion models to evaluate the concentration of a pollutant at a receptor or the impact on overall air quality from vehicle exhausts and industrial flue gas stack emissions. To some extent, this field overlaps the desire to decrease carbon dioxide and other greenhouse gas emissions from combustion processes.\n\n\n=== Environmental impact assessment and mitigation ===\n\nEnvironmental engineers apply scientific and engineering principles to evaluate if there are likely to be any adverse impacts to water quality, air quality, habitat quality, flora and fauna, agricultural capacity, traffic, ecology, and noise. If impacts are expected, they then develop mitigation measures to limit or prevent such impacts. An example of a mitigation measure would be the creation of wetlands in a nearby location to mitigate the filling in of wetlands necessary for a road development if it is not possible to reroute the road.\nIn the United States, the practice of environmental assessment was formally initiated on January 1, 1970, the effective date of the National Environmental Policy Act (NEPA). Since that time, more than 100 developing and developed nations either have planned specific analogous laws or have adopted procedure used elsewhere. NEPA is applicable to all federal agencies in the United States.\n\n\n== Regulatory agencies ==\n\n\n=== Environmental Protection Agency ===\nThe U.S. Environmental Protection Agency (EPA) is one of the many agencies that work with environmental engineers to solve critical issues. An essential component of EPA's mission is to protect and improve air, water, and overall environmental quality to avoid or mitigate the consequences of harmful effects.\n\n\n== See also ==\n\n\n=== Associations ===\n\n\n== References ==\n\n\n== Further reading ==\nDavis, M. L. and D. A. Cornwell, (2006) Introduction to environmental engineering (4th ed.)  McGraw-Hill ISBN 978-0072424119\nNational Academies of Sciences, Engineering, and Medicine (2019). Environmental Engineering for the 21st Century: Addressing Grand Challenges (Report). Washington, DC: The National Academies Press. doi:10.17226/25121. ISBN 978-0-309-47652-2.{{cite report}}:  CS1 maint: multiple names: authors list (link)",
  },
];

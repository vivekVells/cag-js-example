export const jsonForSmallDataset = [
  {
    title: "Data science",
    originalContent:
      "Data science is an interdisciplinary academic field that uses statistics scientific computing scientific methods processing scientific visualization algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy structured or unstructured data Data science also integrates domain knowledge from the underlying application domain eg natural sciences information technology and medicine Data science is multifaceted and can be described as a science a research paradigm a research method a discipline a workflow and a profession Data science is a concept to unify statistics data analysis informatics and their related methods to understand and analyze actual phenomena with data It uses techniques and theories drawn from many fields within the context of mathematics statistics computer science information science and domain knowledge However data science is different from computer science and information science Turing Award winner Jim Gray imagined data science as a fourth paradigm of science empirical theoretical computational and now datadriven and asserted that everything about science is changing because of the impact of information technology and the data deluge A data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data Foundations Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains The field encompasses preparing data for analysis formulating data science problems analyzing data developing datadriven solutions and presenting findings to inform highlevel decisions in a broad range of application domains As such it incorporates skills from computer science statistics information science mathematics data visualization information visualization data sonification data integration graphic design complex systems communication and business Statistician Nathan Yau drawing on Ben Fry also links data science to humancomputer interaction users should be able to intuitively control and explore data In 2015 the American Statistical Association identified database management statistics and machine learning and distributed and parallel systems as the three emerging foundational professional communities Relationship to statistics Many statisticians including Nate Silver have argued that data science is not a new field but rather another name for statistics Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data Vasant Dhar writes that statistics emphasizes quantitative data and description In contrast data science deals with quantitative and qualitative data eg from images text sensors transactions customer information etc and emphasizes prediction and action Andrew Gelman of Columbia University has described statistics as a nonessential part of data science Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a datascience program He describes data science as an applied field growing out of traditional statistics Etymology Early usage In 1962 John Tukey described a field he called data analysis which resembles modern data science In 1985 in a lecture given to the Chinese Academy of Sciences in Beijing C F Jeff Wu used the term data science for the first time as an alternative name for statistics Later attendees at a 1992 statistics symposium at the University of Montpellier II acknowledged the emergence of a new discipline focused on data of various origins and forms combining established concepts and principles of statistics and data analysis with computing The term data science has been traced back to 1974 when Peter Naur proposed it as an alternative name to computer science In 1996 the International Federation of Classification Societies became the first conference to specifically feature data science as a topic However the definition was still in flux After the 1985 lecture at the Chinese Academy of Sciences in Beijing in 1997 C F Jeff Wu again suggested that statistics should be renamed data science He reasoned that a new name would help statistics shed inaccurate stereotypes such as being synonymous with accounting or limited to describing data In 1998 Hayashi Chikio argued for data science as a new interdisciplinary concept with three aspects data design collection and analysis During the 1990s popular terms for the process of finding patterns in datasets which were increasingly large included knowledge discovery and data mining Modern usage In 2012 technologists Thomas H Davenport and DJ Patil declared Data Scientist The Sexiest Job of the 21st Century a catchphrase that was picked up even by majorcity newspapers like the New York Times and the Boston Globe A decade later they reaffirmed it stating that the job is more in demand than ever with employers The modern conception of data science as an independent discipline is sometimes attributed to William S Cleveland In a 2001 paper he advocated an expansion of statistics beyond theory into technical areas because this would significantly change the field it warranted a new name Data science became more widely used in the next few years in 2002 the Committee on Data for Science and Technology launched the Data Science Journal In 2003 Columbia University launched The Journal of Data Science In 2014 the American Statistical Associations Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science reflecting the ascendant popularity of data science The professional title of data scientist has been attributed to DJ Patil and Jeff Hammerbacher in 2008 Though it was used by the National Science Board in their 2005 report LongLived Digital Data Collections Enabling Research and Education in the 21st Century it referred broadly to any key role in managing a digital data collection There is still no consensus on the definition of data science and it is considered by some to be a buzzword Big data is a related marketing term Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations Data science and data analysis Data science and data analysis are both important disciplines in the field of data management and analysis but they differ in several key ways While both fields involve working with data data science is more of an interdisciplinary field that involves the application of statistical computational and machine learning methods to extract insights from data and make predictions while data analysis is more focused on the examination and interpretation of data to identify patterns and trends Data analysis typically involves working with smaller structured datasets to answer specific questions or solve specific problems This can involve tasks such as data cleaning data visualization and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data For example a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies Data science on the other hand is a more complex and iterative process that involves working with larger more complex datasets that often require advanced computational and statistical methods to analyze Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make datadriven decisions In addition to statistical analysis data science often involves tasks such as data preprocessing feature engineering and model selection For instance a data scientist might develop a recommendation system for an ecommerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences While data analysis focuses on extracting insights from existing data data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions Data scientists are often responsible for collecting and cleaning data selecting appropriate analytical techniques and deploying models in realworld scenarios They work at the intersection of mathematics computer science and domain expertise to solve complex problems and uncover hidden patterns in large datasets Despite these differences data science and data analysis are closely related fields and often require similar skill sets Both fields require a solid foundation in statistics programming and data visualization as well as the ability to communicate findings effectively to both technical and nontechnical audiences Both fields benefit from critical thinking and domain knowledge as understanding the context and nuances of the data is essential for accurate analysis and modeling In summary data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis Data analysis focuses on extracting insights and drawing conclusions from structured data while data science involves a more comprehensive approach that combines statistical analysis computational methods and machine learning to extract insights build predictive models and drive datadriven decisionmaking Both fields use data to understand patterns make informed decisions and solve complex problems across various domains Data Science as an Academic Discipline As illustrated in the previous sections there is substantially some considerable differences between data science data analysis and statistics Consequently just like statistics grew into an independent field from applied mathematics similarly data science has emerged as a independent field and has gained traction over the recent years The unique demand for professional skills on computerized data analysis skills has exploded due to the increasing amounts of data emanating from a variety of independent sources Whereas some of these highly sought skills can be provided by statisticians the lack of high algorithmic writing skills makes them less preferred than trained data scientists who provide unique expertise on skills such as NoSQL Apache Hadoop Cloud Computing platforms and use of complex networks This paradigm shift has seen various institution craft academic programmes to prepare skilled labor for the market Some of the institutions offering degree programmes in data science include Stanford University Harvard University University of Oxford ETH Zurich Meru University1 among many others Cloud computing for data science Cloud computing can offer access to large amounts of computational power and storage In big data where volumes of information are continually generated and processed these platforms can be used to handle complex and resourceintensive analytical tasks Some distributed computing frameworks are designed to handle big data workloads These frameworks can enable data scientists to process and analyze large datasets in parallel which can reducing processing times Ethical consideration in data science Data science involve collecting processing and analyzing data which often including personal and sensitive information Ethical concerns include potential privacy violations bias perpetuation and negative societal impacts Machine learning models can amplify existing biases present in training data leading to discriminatory or unfair outcomes See also Python programming language R programming language Data engineering Big data Machine learning References",
  },
  {
    title: "Relativity",
    originalContent:
      "Relativity may refer to Physics Galilean relativity Galileos conception of relativity Numerical relativity a subfield of computational physics that aims to establish numerical solutions to Einsteins field equations in general relativity Principle of relativity used in Einsteins theories and derived from Galileos principle Theory of relativity a general treatment that refers to both special relativity and general relativity General relativity Albert Einsteins theory of gravitation Special relativity a theory formulated by Albert Einstein Henri Poincar and Hendrik Lorentz Relativity The Special and the General Theory a 1920 book by Albert Einstein Social sciences Linguistic relativity Cultural relativity Moral relativity Arts and entertainment Music Relativity Music Group a Universal subsidiary record label for releasing film soundtracks Relativity Records an American record label Relativity band a ScotsIrish traditional music quartet 19851987 Relativity Emarosa album 2008 Relativity Indecent Obsession album 1993 Relativity Walt Dickerson album or the title song 1962 Relativity an EP by Grafton Primary 2007 Television Relativity TV series a 19961997 American drama series Relativity Farscape an episode Relativity Star Trek Voyager an episode Other Relativity M C Escher a 1953 lithograph print by M C Escher Relativity Media an American film production company Business Relativity Space an American aerospace manufacturing company See also Relative disambiguation Relativism a family of philosophical religious and social views",
  },
  {
    title: "RNA",
    originalContent:
      "Ribonucleic acid RNA is a polymeric molecule that is essential for most biological functions either by performing the function itself noncoding RNA or by forming a template for the production of proteins messenger RNA RNA and deoxyribonucleic acid DNA are nucleic acids The nucleic acids constitute one of the four major macromolecules essential for all known forms of life RNA is assembled as a chain of nucleotides Cellular organisms use messenger RNA mRNA to convey genetic information using the nitrogenous bases of guanine uracil adenine and cytosine denoted by the letters G U A and C that directs synthesis of specific proteins Many viruses encode their genetic information using an RNA genome Some RNA molecules play an active role within cells by catalyzing biological reactions controlling gene expression or sensing and communicating responses to cellular signals One of these active processes is protein synthesis a universal function in which RNA molecules direct the synthesis of proteins on ribosomes This process uses transfer RNA tRNA molecules to deliver amino acids to the ribosome where ribosomal RNA rRNA then links amino acids together to form coded proteins It has become widely accepted in science that early in the history of life on Earth prior to the evolution of DNA and possibly of proteinbased enzymes as well an RNA world existed in which RNA served as both living organisms storage method for genetic informationa role fulfilled today by DNA except in the case of RNA virusesand potentially performed catalytic functions in cellsa function performed today by protein enzymes with the notable and important exception of the ribosome which is a ribozyme Chemical structure of RNA Basic chemical composition Each nucleotide in RNA contains a ribose sugar with carbons numbered 1 through 5 A base is attached to the 1 position in general adenine A cytosine C guanine G or uracil U Adenine and guanine are purines and cytosine and uracil are pyrimidines A phosphate group is attached to the 3 position of one ribose and the 5 position of the next The phosphate groups have a negative charge each making RNA a charged molecule polyanion The bases form hydrogen bonds between cytosine and guanine between adenine and uracil and between guanine and uracil However other interactions are possible such as a group of adenine bases binding to each other in a bulge or the GNRA tetraloop that has a guanineadenine basepair Differences between DNA and RNA Like DNA most biologically active RNAs including mRNA tRNA rRNA snRNAs and other noncoding RNAs contain selfcomplementary sequences that allow parts of the RNA to fold and pair with itself to form double helices Analysis of these RNAs has revealed that they are highly structured Unlike DNA their structures do not consist of long double helices but rather collections of short helices packed together into structures akin to proteins In this fashion RNAs can achieve chemical catalysis like enzymes For instance determination of the structure of the ribosomean RNAprotein complex that catalyzes the assembly of proteinsrevealed that its active site is composed entirely of RNA An important structural component of RNA that distinguishes it from DNA is the presence of a hydroxyl group at the 2 position of the ribose sugar The presence of this functional group causes the helix to mostly take the Aform geometry although in single strand dinucleotide contexts RNA can rarely also adopt the Bform most commonly observed in DNA The Aform geometry results in a very deep and narrow major groove and a shallow and wide minor groove A second consequence of the presence of the 2hydroxyl group is that in conformationally flexible regions of an RNA molecule that is not involved in formation of a double helix it can chemically attack the adjacent phosphodiester bond to cleave the backbone Secondary and tertiary structures The functional form of singlestranded RNA molecules just like proteins frequently requires a specific spatial tertiary structure The scaffold for this structure is provided by secondary structural elements that are hydrogen bonds within the molecule This leads to several recognizable domains of secondary structure like hairpin loops bulges and internal loops In order to create ie design RNA for any given secondary structure two or three bases would not be enough but four bases are enough This is likely why nature has chosen a four base alphabet fewer than four would not allow the creation of all structures while more than four bases are not necessary to do so Since RNA is charged metal ions such as Mg2 are needed to stabilise many secondary and tertiary structures The naturally occurring enantiomer of RNA is DRNA composed of Dribonucleotides All chirality centers are located in the Dribose By the use of Lribose or rather Lribonucleotides LRNA can be synthesized LRNA is much more stable against degradation by RNase Like other structured biopolymers such as proteins one can define topology of a folded RNA molecule This is often done based on arrangement of intrachain contacts within a folded RNA termed as circuit topology Chemical modifications RNA is transcribed with only four bases adenine cytosine guanine and uracil but these bases and attached sugars can be modified in numerous ways as the RNAs mature Pseudouridine in which the linkage between uracil and ribose is changed from a CN bond to a CC bond and ribothymidine T are found in various places the most notable ones being in the TC loop of tRNA Another notable modified base is hypoxanthine a deaminated adenine base whose nucleoside is called inosine I Inosine plays a key role in the wobble hypothesis of the genetic code There are more than 100 other naturally occurring modified nucleosides The greatest structural diversity of modifications can be found in tRNA while pseudouridine and nucleosides with 2Omethylribose often present in rRNA are the most common The specific roles of many of these modifications in RNA are not fully understood However it is notable that in ribosomal RNA many of the posttranscriptional modifications occur in highly functional regions such as the peptidyl transferase center and the subunit interface implying that they are important for normal function Types of RNA Messenger RNA mRNA is the type of RNA that carries information from DNA to the ribosome the sites of protein synthesis translation in the cell cytoplasm The coding sequence of the mRNA determines the amino acid sequence in the protein that is produced However many RNAs do not code for protein about 97 of the transcriptional output is nonproteincoding in eukaryotes These socalled noncoding RNAs ncRNA can be encoded by their own genes RNA genes but can also derive from mRNA introns The most prominent examples of noncoding RNAs are transfer RNA tRNA and ribosomal RNA rRNA both of which are involved in the process of translation There are also noncoding RNAs involved in gene regulation RNA processing and other roles Certain RNAs are able to catalyse chemical reactions such as cutting and ligating other RNA molecules and the catalysis of peptide bond formation in the ribosome these are known as ribozymes According to the length of RNA chain RNA includes small RNA and long RNA Usually small RNAs are shorter than 200 nt in length and long RNAs are greater than 200 nt long Long RNAs also called large RNAs mainly include long noncoding RNA lncRNA and mRNA Small RNAs mainly include 58S ribosomal RNA rRNA 5S rRNA transfer RNA tRNA microRNA miRNA small interfering RNA siRNA small nucleolar RNA snoRNAs Piwiinteracting RNA piRNA tRNAderived small RNA tsRNA and small rDNAderived RNA srRNA There are certain exceptions as in the case of the 5S rRNA of the members of the genus Halococcus Archaea which have an insertion thus increasing its size RNAs involved in protein synthesis Messenger RNA mRNA carries information about a protein sequence to the ribosomes the protein synthesis factories in the cell It is coded so that every three nucleotides a codon corresponds to one amino acid In eukaryotic cells once precursor mRNA premRNA has been transcribed from DNA it is processed to mature mRNA This removes its intronsnoncoding sections of the premRNA The mRNA is then exported from the nucleus to the cytoplasm where it is bound to ribosomes and translated into its corresponding protein form with the help of tRNA In prokaryotic cells which do not have nucleus and cytoplasm compartments mRNA can bind to ribosomes while it is being transcribed from DNA After a certain amount of time the message degrades into its component nucleotides with the assistance of ribonucleases Transfer RNA tRNA is a small RNA chain of about 80 nucleotides that transfers a specific amino acid to a growing polypeptide chain at the ribosomal site of protein synthesis during translation It has sites for amino acid attachment and an anticodon region for codon recognition that binds to a specific sequence on the messenger RNA chain through hydrogen bonding Ribosomal RNA rRNA is the catalytic component of the ribosomes The rRNA is the component of the ribosome that hosts translation Eukaryotic ribosomes contain four different rRNA molecules 18S 58S 28S and 5S rRNA Three of the rRNA molecules are synthesized in the nucleolus and one is synthesized elsewhere In the cytoplasm ribosomal RNA and protein combine to form a nucleoprotein called a ribosome The ribosome binds mRNA and carries out protein synthesis Several ribosomes may be attached to a single mRNA at any time Nearly all the RNA found in a typical eukaryotic cell is rRNA Transfermessenger RNA tmRNA is found in many bacteria and plastids It tags proteins encoded by mRNAs that lack stop codons for degradation and prevents the ribosome from stalling Regulatory RNA The earliest known regulators of gene expression were proteins known as repressors and activators regulators with specific short binding sites within enhancer regions near the genes to be regulated Later studies have shown that RNAs also regulate genes There are several kinds of RNAdependent processes in eukaryotes regulating the expression of genes at various points such as RNAi repressing genes posttranscriptionally long noncoding RNAs shutting down blocks of chromatin epigenetically and enhancer RNAs inducing increased gene expression Bacteria and archaea have also been shown to use regulatory RNA systems such as bacterial small RNAs and CRISPR Fire and Mello were awarded the 2006 Nobel Prize in Physiology or Medicine for discovering microRNAs miRNAs specific short RNA molecules that can basepair with mRNAs MicroRNA miRNA and small interfering RNA siRNA Posttranscriptional expression levels of many genes can be controlled by RNA interference in which miRNAs specific short RNA molecules pair with mRNA regions and target them for degradation This antisensebased process involves steps that first process the RNA so that it can basepair with a region of its target mRNAs Once the base pairing occurs other proteins direct the mRNA to be destroyed by nucleases Long noncoding RNAs Next to be linked to regulation were Xist and other long noncoding RNAs associated with X chromosome inactivation Their roles at first mysterious were shown by Jeannie T Lee and others to be the silencing of blocks of chromatin via recruitment of Polycomb complex so that messenger RNA could not be transcribed from them Additional lncRNAs currently defined as RNAs of more than 200 base pairs that do not appear to have coding potential have been found associated with regulation of stem cell pluripotency and cell division Enhancer RNAs The third major group of regulatory RNAs is called enhancer RNAs It is not clear at present whether they are a unique category of RNAs of various lengths or constitute a distinct subset of lncRNAs In any case they are transcribed from enhancers which are known regulatory sites in the DNA near genes they regulate They upregulate the transcription of the genes under control of the enhancer from which they are transcribed Small RNA in prokaryotes Small RNA At first regulatory RNA was thought to be a eukaryotic phenomenon a part of the explanation for why so much more transcription in higher organisms was seen than had been predicted But as soon as researchers began to look for possible RNA regulators in bacteria they turned up there as well termed as small RNA sRNA Currently the ubiquitous nature of systems of RNA regulation of genes has been discussed as support for the RNA World theory There are indications that the enterobacterial sRNAs are involved in various cellular processes and seem to have significant role in stress responses such as membrane stress starvation stress phosphosugar stress and DNA damage Also it has been suggested that sRNAs have been evolved to have important role in stress responses because of their kinetic properties that allow for rapid response and stabilisation of the physiological state Bacterial small RNAs generally act via antisense pairing with mRNA to downregulate its translation either by affecting stability or affecting cisbinding ability Riboswitches have also been discovered They are cisacting regulatory RNA sequences acting allosterically They change shape when they bind metabolites so that they gain or lose the ability to bind chromatin to regulate expression of genes CRISPR RNA Archaea also have systems of regulatory RNA The CRISPR system recently being used to edit DNA in situ acts via regulatory RNAs in archaea and bacteria to provide protection against virus invaders RNA synthesis and processing Synthesis Synthesis of RNA typically occurs in the cell nucleus and is usually catalyzed by an enzymeRNA polymeraseusing DNA as a template a process known as transcription Initiation of transcription begins with the binding of the enzyme to a promoter sequence in the DNA usually found upstream of a gene The DNA double helix is unwound by the helicase activity of the enzyme The enzyme then progresses along the template strand in the 3 to 5 direction synthesizing a complementary RNA molecule with elongation occurring in the 5 to 3 direction The DNA sequence also dictates where termination of RNA synthesis will occur Primary transcript RNAs are often modified by enzymes after transcription For example a polyA tail and a 5 cap are added to eukaryotic premRNA and introns are removed by the spliceosome There are also a number of RNAdependent RNA polymerases that use RNA as their template for synthesis of a new strand of RNA For instance a number of RNA viruses such as poliovirus use this type of enzyme to replicate their genetic material Also RNAdependent RNA polymerase is part of the RNA interference pathway in many organisms RNA processing Many RNAs are involved in modifying other RNAs Introns are spliced out of premRNA by spliceosomes which contain several small nuclear RNAs snRNA or the introns can be ribozymes that are spliced by themselves RNA can also be altered by having its nucleotides modified to nucleotides other than A C G and U In eukaryotes modifications of RNA nucleotides are in general directed by small nucleolar RNAs snoRNA 60300 nt found in the nucleolus and cajal bodies snoRNAs associate with enzymes and guide them to a spot on an RNA by basepairing to that RNA These enzymes then perform the nucleotide modification rRNAs and tRNAs are extensively modified but snRNAs and mRNAs can also be the target of base modification RNA can also be methylated RNA in genetics RNA genomes Like DNA RNA can carry genetic information RNA viruses have genomes composed of RNA that encodes a number of proteins The viral genome is replicated by some of those proteins while other proteins protect the genome as the virus particle moves to a new host cell Viroids are another group of pathogens but they consist only of RNA do not encode any protein and are replicated by a host plant cells polymerase Reverse transcription Reverse transcribing viruses replicate their genomes by reverse transcribing DNA copies from their RNA these DNA copies are then transcribed to new RNA Retrotransposons also spread by copying DNA and RNA from one another and telomerase contains an RNA that is used as template for building the ends of eukaryotic chromosomes Doublestranded RNA Doublestranded RNA dsRNA is RNA with two complementary strands similar to the DNA found in all cells but with the replacement of thymine by uracil and the adding of one oxygen atom dsRNA forms the genetic material of some viruses doublestranded RNA viruses Doublestranded RNA such as viral RNA or siRNA can trigger RNA interference in eukaryotes as well as interferon response in vertebrates In eukaryotes doublestranded RNA dsRNA plays a role in the activation of the innate immune system against viral infections Circular RNA In the late 1970s it was shown that there is a single stranded covalently closed ie circular form of RNA expressed throughout the animal and plant kingdom see circRNA circRNAs are thought to arise via a backsplice reaction where the spliceosome joins a upstream 3 acceptor to a downstream 5 donor splice site So far the function of circRNAs is largely unknown although for few examples a microRNA sponging activity has been demonstrated Key discoveries in RNA biology Research on RNA has led to many important biological discoveries and numerous Nobel Prizes Nucleic acids were discovered in 1868 by Friedrich Miescher who called the material nuclein since it was found in the nucleus It was later discovered that prokaryotic cells which do not have a nucleus also contain nucleic acids The role of RNA in protein synthesis was suspected already in 1939 Severo Ochoa won the 1959 Nobel Prize in Medicine shared with Arthur Kornberg after he discovered an enzyme that can synthesize RNA in the laboratory However the enzyme discovered by Ochoa polynucleotide phosphorylase was later shown to be responsible for RNA degradation not RNA synthesis In 1956 Alex Rich and David Davies hybridized two separate strands of RNA to form the first crystal of RNA whose structure could be determined by Xray crystallography The sequence of the 77 nucleotides of a yeast tRNA was found by Robert W Holley in 1965 winning Holley the 1968 Nobel Prize in Medicine shared with Har Gobind Khorana and Marshall Nirenberg In the early 1970s retroviruses and reverse transcriptase were discovered showing for the first time that enzymes could copy RNA into DNA the opposite of the usual route for transmission of genetic information For this work David Baltimore Renato Dulbecco and Howard Temin were awarded a Nobel Prize in 1975 In 1976 Walter Fiers and his team determined the first complete nucleotide sequence of an RNA virus genome that of bacteriophage MS2 In 1977 introns and RNA splicing were discovered in both mammalian viruses and in cellular genes resulting in a 1993 Nobel to Philip Sharp and Richard Roberts Catalytic RNA molecules ribozymes were discovered in the early 1980s leading to a 1989 Nobel award to Thomas Cech and Sidney Altman In 1990 it was found in Petunia that introduced genes can silence similar genes of the plants own now known to be a result of RNA interference At about the same time 22 nt long RNAs now called microRNAs were found to have a role in the development of C elegans Studies on RNA interference earned a Nobel Prize for Andrew Fire and Craig Mello in 2006 and another Nobel for studies on the transcription of RNA to Roger Kornberg in the same year The discovery of gene regulatory RNAs has led to attempts to develop drugs made of RNA such as siRNA to silence genes Adding to the Nobel prizes for research on RNA in 2009 it was awarded for the elucidation of the atomic structure of the ribosome to Venki Ramakrishnan Thomas A Steitz and Ada Yonath In 2023 the Nobel Prize in Physiology or Medicine was awarded to Katalin Karik and Drew Weissman for their discoveries concerning modified nucleosides that enabled the development of effective mRNA vaccines against COVID19 Relevance for prebiotic chemistry and abiogenesis In 1968 Carl Woese hypothesized that RNA might be catalytic and suggested that the earliest forms of life selfreplicating molecules could have relied on RNA both to carry genetic information and to catalyze biochemical reactionsan RNA world In May 2022 scientists discovered that RNA can form spontaneously on prebiotic basalt lava glass presumed to have been abundant on the early Earth In March 2015 DNA and RNA nucleobases including uracil cytosine and thymine were reportedly formed in the laboratory under outer space conditions using starter chemicals such as pyrimidine an organic compound commonly found in meteorites Pyrimidine like polycyclic aromatic hydrocarbons PAHs is one of the most carbonrich compounds found in the universe and may have been formed in red giants or in interstellar dust and gas clouds In July 2022 astronomers reported massive amounts of prebiotic molecules including possible RNA precursors in the galactic center of the Milky Way Galaxy Medical applications RNA initially deemed unsuitable for therapeutics due to its short halflife has been made useful through advances in stabilization Therapeutic applications arise as RNA folds into complex conformations and binds proteins nucleic acids and small molecules to form catalytic centers RNAbased vaccines are thought to be easier to produce than traditional vaccines derived from killed or altered pathogens because it can take months or years to grow and study a pathogen and determine which molecular parts to extract inactivate and use in a vaccine Small molecules with conventional therapeutic properties can target RNA and DNA structures thereby treating novel diseases However research is scarce on small molecules targeting RNA and approved drugs for human illness Ribavirin branaplam and ataluren are currently available medications that stabilize doublestranded RNA structures and control splicing in a variety of disorders Proteincoding mRNAs have emerged as new therapeutic candidates with RNA replacement being particularly beneficial for brief but torrential protein expression In vitro transcribed mRNAs IVTmRNA have been used to deliver proteins for bone regeneration pluripotency and heart function in animal models SiRNAs short RNA molecules play a crucial role in innate defense against viruses and chromatin structure They can be artificially introduced to silence specific genes making them valuable for gene function studies therapeutic target validation and drug development mRNA vaccines have emerged as an important new class of vaccines using mRNA to manufacture proteins which provoke an immune response Their first successful largescale application came in the form of COVID19 vaccines during the COVID19 pandemic See also References External links RNA World website Link collection structures sequences tools journals Nucleic Acid Database Images of DNA RNA and complexes Anna Marie Pyles Seminar RNA Structure Function and Recognition Archived 20180621 at the Wayback Machine",
  },
  {
    title: "Behavior",
    originalContent:
      "Behavior American English or behaviour British English is the range of actions and mannerisms made by individuals organisms systems or artificial entities in some environment These systems can include other systems or organisms as well as the inanimate physical environment It is the computed response of the system or organism to various stimuli or inputs whether internal or external conscious or subconscious overt or covert and voluntary or involuntary While some behavior is produced in response to an organisms environment extrinsic motivation behavior can also be the product of intrinsic motivation also referred to as agency or free will Taking a behavior informatics perspective a behavior consists of actor operation interactions and their properties This can be represented as a behavior vector Models Biology Although disagreement exists as to how to precisely define behavior in a biological context one common interpretation based on a metaanalysis of scientific literature states that behavior is the internally coordinated responses actions or inactions of whole living organisms individuals or groups to internal or external stimuli A broader definition of behavior applicable to plants and other organisms is similar to the concept of phenotypic plasticity It describes behavior as a response to an event or environment change during the course of the lifetime of an individual differing from other physiological or biochemical changes that occur more rapidly and excluding changes that are a result of development ontogeny Behaviors can be either innate or learned from the environment Behaviour can be regarded as any action of an organism that changes its relationship to its environment Behavior provides outputs from the organism to the environment Human behavior The endocrine system and the nervous system likely influence human behavior Complexity in the behavior of an organism may be correlated to the complexity of its nervous system Generally organisms with more complex nervous systems have a greater capacity to learn new responses and thus adjust their behavior Animal behavior Ethology is the scientific and objective study of animal behavior usually with a focus on behavior under natural conditions and viewing behavior as an evolutionarily adaptive trait Behaviorism is a term that also describes the scientific and objective study of animal behavior usually referring to measured responses to stimuli or trained behavioral responses in a laboratory context without a particular emphasis on evolutionary adaptivity Consumer behavior Consumers behavior Consumer behavior involves the processes consumers go through and reactions they have towards products or services It has to do with consumption and the processes consumers go through around purchasing and consuming goods and services Consumers recognize needs or wants and go through a process to satisfy these needs Consumer behavior is the process they go through as customers which includes types of products purchased amount spent frequency of purchases and what influences them to make the purchase decision or not Circumstances that influence consumer behaviour are varied with contributions from both internal and external factors Internal factors include attitudes needs motives preferences and perceptual processes whilst external factors include marketing activities social and economic factors and cultural aspects Doctor Lars Perner of the University of Southern California claims that there are also physical factors that influence consumer behavior for example if a consumer is hungry then this physical feeling of hunger will influence them so that they go and purchase a sandwich to satisfy the hunger Consumer decision making Lars Perner presents a model that outlines the decisionmaking process involved in consumer behaviour The process initiates with the identification of a problem wherein the consumer acknowledges an unsatisfied need or desire Subsequently the consumer proceeds to seek information whereas for lowinvolvement products the search tends to rely on internal resources retrieving alternatives from memory Conversely for highinvolvement products the search is typically more extensive involving activities like reviewing reports reading reviews or seeking recommendations from friends The consumer will then evaluate his or her alternatives comparing price and quality doing tradeoffs between products and narrowing down the choice by eliminating the less appealing products until there is one left After this has been identified the consumer will purchase the product Finally the consumer will evaluate the purchase decision and the purchased product bringing in factors such as value for money quality of goods and purchase experience However this logical process does not always happen this way people are emotional and irrational creatures People make decisions with emotion and then justify them with logic according to Robert Cialdini PhD Psychology How the 4Ps influence consumer behavior The Marketing mix 4 Ps are a marketing tool and stand for Price Promotion Product and Placement Due to the significant impact of businesstoconsumer marketing on consumer behavior the four elements of the marketing mix known as the 4 Ps product price place and promotion exert a notable influence on consumer behavior The price of a good or service is largely determined by the market as businesses will set their prices to be similar to that of other businesses so as to remain competitive whilst making a profit When market prices for a product are high it will cause consumers to purchase less and use purchased goods for longer periods of time meaning they are purchasing the product less often Alternatively when market prices for a product are low consumers are more likely to purchase more of the product and more often The way that promotion influences consumer behavior has changed over time In the past large promotional campaigns and heavy advertising would convert into sales for a business but nowadays businesses can have success on products with little or no advertising This is due to the Internet and in particular social media They rely on word of mouth from consumers using social media and as products trend online so sales increase as products effectively promote themselves Thus promotion by businesses does not necessarily result in consumer behavior trending towards purchasing products The way that product influences consumer behavior is through consumer willingness to pay and consumer preferences This means that even if a company were to have a long history of products in the market consumers will still pick a cheaper product over the company in questions product if it means they will pay less for something that is very similar This is due to consumer willingness to pay or their willingness to part with the money they have earned The product also influences consumer behavior through customer preferences For example take Pepsi vs CocaCola a Pepsidrinker is less likely to purchase CocaCola even if it is cheaper and more convenient This is due to the preference of the consumer and no matter how hard the opposing company tries they will not be able to force the customer to change their mind Product placement in the modern era has little influence on consumer behavior due to the availability of goods online If a customer can purchase a good from the comfort of their home instead of purchasing instore then the placement of products is not going to influence their purchase decision In management Behavior outside of psychology includes Organizational In management behaviors are associated with desired or undesired focuses Managers generally note what the desired outcome is but behavioral patterns can take over These patterns are the reference to how often the desired behavior actually occurs Before a behavior actually occurs antecedents focus on the stimuli that influence the behavior that is about to happen After the behavior occurs consequences fall into place Consequences consist of rewards or punishments Social behavior Social behavior is behavior among two or more organisms within the same species and encompasses any behavior in which one member affects the other This is due to an interaction among those members Social behavior can be seen as similar to an exchange of goods with the expectation that when one gives one will receive the same This behavior can be affected by both the qualities of the individual and the environmental situational factors Therefore social behavior arises as a result of an interaction between the twothe organism and its environment This means that in regards to humans social behavior can be determined by both the individual characteristics of the person and the situation they are in Behavior informatics Behavior informatics also called behavior computing explores behavior intelligence and behavior insights from the informatics and computing perspectives Different from applied behavior analysis from the psychological perspective BI builds computational theories systems and tools to qualitatively and quantitatively model represent analyze and manage behaviors of individuals groups andor organizations Health Health behavior refers to a persons beliefs and actions regarding their health and wellbeing Health behaviors are direct factors in maintaining a healthy lifestyle Health behaviors are influenced by the social cultural and physical environments in which we live They are shaped by individual choices and external constraints Positive behaviors help promote health and prevent disease while the opposite is true for risk behaviors Health behaviors are early indicators of population health Because of the time lag that often occurs between certain behaviors and the development of disease these indicators may foreshadow the future burdens and benefits of healthrisk and healthpromoting behaviors Correlates A variety of studies have examined the relationship between health behaviors and health outcomes eg Blaxter 1990 and have demonstrated their role in both morbidity and mortality These studies have identified seven features of lifestyle which were associated with lower morbidity and higher subsequent longterm survival Belloc and Breslow 1972 Avoiding snacks Eating breakfast regularly Exercising regularly Maintaining a desirable body weight Moderate alcohol intake Not smoking Sleeping 78hrs per night Health behaviors impact upon individuals quality of life by delaying the onset of chronic disease and extending active lifespan Smoking alcohol consumption diet gaps in primary care services and low screening uptake are all significant determinants of poor health and changing such behaviors should lead to improved health For example in US Healthy People 2000 United States Department of Health and Human Services lists increased physical activity changes in nutrition and reductions in tobacco alcohol and drug use as important for health promotion and disease prevention Treatment approach Any interventions done are matched with the needs of each individual in an ethical and respected manner Health belief model encourages increasing individuals perceived susceptibility to negative health outcomes and making individuals aware of the severity of such negative health behavior outcomes Eg through health promotion messages In addition the health belief model suggests the need to focus on the benefits of health behaviors and the fact that barriers to action are easily overcome The theory of planned behavior suggests using persuasive messages for tackling behavioral beliefs to increase the readiness to perform a behavior called intentions The theory of planned behavior advocates the need to tackle normative beliefs and control beliefs in any attempt to change behavior Challenging the normative beliefs is not enough but to follow through the intention with selfefficacy from individuals mastery in problem solving and task completion is important to bring about a positive change Self efficacy is often cemented through standard persuasive techniques See also References General Cao L 2014 Behavior Informatics A New Perspective IEEE Intelligent Systems Trends and Controversies 294 6280 Clemons E K 2008 How Information Changes Consumer Behavior and How Consumer Behavior Determines Corporate Strategy Journal of Management Information Systems 25 2 1340 doi102753mis07421222250202 S2CID 16370526 Dowhan D 2013 Hitting Your Target Marketing Insights 35 2 3238 Perner L 2008 Consumer behavior University of Southern California Marshall School of Business Retrieved from httpwwwconsumerpsychologistcomintro_Consumer_Behaviorhtml SzwackaMokrzycka J 2015 TRENDS IN CONSUMER behavior CHANGES OVERVIEW OF CONCEPTS Acta Scientiarum Polonorum Oeconomia 14 3 149156 Further reading Bateson P 2017 behavior Development and Evolution Open Book Publishers Cambridge ISBN 9781783742509 Plomin Robert DeFries John C Knopik Valerie S Neiderhiser Jenae M 24 September 2012 Behavioral Genetics Shaun Purcell Appendix Statistical Methods in Behavioral Genetics Worth Publishers ISBN 9781429242158 Retrieved 4 September 2013 Flint Jonathan Greenspan Ralph J Kendler Kenneth S 28 January 2010 How Genes Influence Behavior Oxford University Press ISBN 9780199559909 External links What is behavior Baby dont ask me dont ask me no more at Earthling Nature behaviorinformaticsorg Links to review articles by Eric Turkheimer and coauthors on behavior research Links to IJCAI2013 tutorial on behavior informatics and computing",
  },
  {
    title: "Probability",
    originalContent:
      "Probability is the branch of mathematics and statistics concerning events and numerical descriptions of how likely they are to occur The probability of an event is a number between 0 and 1 the larger the probability the more likely an event is to occur A simple example is the tossing of a fair unbiased coin Since the coin is fair the two outcomes heads and tails are both equally probable the probability of heads equals the probability of tails and since no other outcomes are possible the probability of either heads or tails is 12 which could also be written as 05 or 50 These concepts have been given an axiomatic mathematical formalization in probability theory which is used widely in areas of study such as statistics mathematics science finance gambling artificial intelligence machine learning computer science game theory and philosophy to for example draw inferences about the expected frequency of events Probability theory is also used to describe the underlying mechanics and regularities of complex systems Interpretations When dealing with random experiments ie experiments that are random and welldefined in a purely theoretical setting like tossing a coin probabilities can be numerically described by the number of desired outcomes divided by the total number of all outcomes This is referred to as theoretical probability in contrast to empirical probability dealing with probabilities in the context of real experiments For example tossing a coin twice will yield headhead headtail tailhead and tailtail outcomes The probability of getting an outcome of headhead is 1 out of 4 outcomes or in numerical terms 14 025 or 25 However when it comes to practical application there are two major competing categories of probability interpretations whose adherents hold different views about the fundamental nature of probability Objectivists assign numbers to describe some objective or physical state of affairs The most popular version of objective probability is frequentist probability which claims that the probability of a random event denotes the relative frequency of occurrence of an experiments outcome when the experiment is repeated indefinitely This interpretation considers probability to be the relative frequency in the long run of outcomes A modification of this is propensity probability which interprets probability as the tendency of some experiment to yield a certain outcome even if it is performed only once Subjectivists assign numbers per subjective probability that is as a degree of belief The degree of belief has been interpreted as the price at which you would buy or sell a bet that pays 1 unit of utility if E 0 if not E although that interpretation is not universally agreed upon The most popular version of subjective probability is Bayesian probability which includes expert knowledge as well as experimental data to produce probabilities The expert knowledge is represented by some subjective prior probability distribution These data are incorporated in a likelihood function The product of the prior and the likelihood when normalized results in a posterior probability distribution that incorporates all the information known to date By Aumanns agreement theorem Bayesian agents whose prior beliefs are similar will end up with similar posterior beliefs However sufficiently different priors can lead to different conclusions regardless of how much information the agents share Etymology The word probability derives from the Latin probabilitas which can also mean probity a measure of the authority of a witness in a legal case in Europe and often correlated with the witnesss nobility In a sense this differs much from the modern meaning of probability which in contrast is a measure of the weight of empirical evidence and is arrived at from inductive reasoning and statistical inference History The scientific study of probability is a modern development of mathematics Gambling shows that there has been an interest in quantifying the ideas of probability throughout history but exact mathematical descriptions arose much later There are reasons for the slow development of the mathematics of probability Whereas games of chance provided the impetus for the mathematical study of probability fundamental issues are still obscured by superstitions According to Richard Jeffrey Before the middle of the seventeenth century the term probable Latin probabilis meant approvable and was applied in that sense univocally to opinion and to action A probable action or opinion was one such as sensible people would undertake or hold in the circumstances However in legal contexts especially probable could also apply to propositions for which there was good evidence The sixteenthcentury Italian polymath Gerolamo Cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes Aside from the elementary work by Cardano the doctrine of probabilities dates to the correspondence of Pierre de Fermat and Blaise Pascal 1654 Christiaan Huygens 1657 gave the earliest known scientific treatment of the subject Jakob Bernoullis Ars Conjectandi posthumous 1713 and Abraham de Moivres Doctrine of Chances 1718 treated the subject as a branch of mathematics See Ian Hackings The Emergence of Probability and James Franklins The Science of Conjecture for histories of the early development of the very concept of mathematical probability The theory of errors may be traced back to Roger Cotess Opera Miscellanea posthumous 1722 but a memoir prepared by Thomas Simpson in 1755 printed 1756 first applied the theory to the discussion of errors of observation The reprint 1757 of this memoir lays down the axioms that positive and negative errors are equally probable and that certain assignable limits define the range of all errors Simpson also discusses continuous errors and describes a probability curve The first two laws of error that were proposed both originated with PierreSimon Laplace The first law was published in 1774 and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the error disregarding sign The second law of error was proposed in 1778 by Laplace and stated that the frequency of the error is an exponential function of the square of the error The second law of error is called the normal distribution or the Gauss law It is difficult historically to attribute that law to Gauss who in spite of his wellknown precocity had probably not made this discovery before he was two years old Daniel Bernoulli 1778 introduced the principle of the maximum product of the probabilities of a system of concurrent errors AdrienMarie Legendre 1805 developed the method of least squares and introduced it in his Nouvelles mthodes pour la dtermination des orbites des comtes New Methods for Determining the Orbits of Comets In ignorance of Legendres contribution an IrishAmerican writer Robert Adrain editor of The Analyst 1808 first deduced the law of facility of error x c e h 2 x 2 displaystyle phi xceh2x2 where h displaystyle h is a constant depending on precision of observation and c displaystyle c is a scale factor ensuring that the area under the curve equals 1 He gave two proofs the second being essentially the same as John Herschels 1850 Gauss gave the first proof that seems to have been known in Europe the third after Adrains in 1809 Further proofs were given by Laplace 1810 1812 Gauss 1823 James Ivory 1825 1826 Hagen 1837 Friedrich Bessel 1838 WF Donkin 1844 1856 and Morgan Crofton 1870 Other contributors were Ellis 1844 De Morgan 1864 Glaisher 1872 and Giovanni Schiaparelli 1875 Peterss 1856 formula for r the probable error of a single observation is well known In the nineteenth century authors on the general theory included Laplace Sylvestre Lacroix 1816 Littrow 1833 Adolphe Quetelet 1853 Richard Dedekind 1860 Helmert 1872 Hermann Laurent 1873 Liagre Didion and Karl Pearson Augustus De Morgan and George Boole improved the exposition of the theory In 1906 Andrey Markov introduced the notion of Markov chains which played an important role in stochastic processes theory and its applications The modern theory of probability based on measure theory was developed by Andrey Kolmogorov in 1931 On the geometric side contributors to The Educational Times included Miller Crofton McColl Wolstenholme Watson and Artemas Martin See integral geometry for more information Theory Like other theories the theory of probability is a representation of its concepts in formal terms that is in terms that can be considered separately from their meaning These formal terms are manipulated by the rules of mathematics and logic and any results are interpreted or translated back into the problem domain There have been at least two successful attempts to formalize probability namely the Kolmogorov formulation and the Cox formulation In Kolmogorovs formulation see also probability space sets are interpreted as events and probability as a measure on a class of sets In Coxs theorem probability is taken as a primitive ie not further analyzed and the emphasis is on constructing a consistent assignment of probability values to propositions In both cases the laws of probability are the same except for technical details There are other methods for quantifying uncertainty such as the DempsterShafer theory or possibility theory but those are essentially different and not compatible with the usuallyunderstood laws of probability Applications Probability theory is applied in everyday life in risk assessment and modeling The insurance industry and markets use actuarial science to determine pricing and make trading decisions Governments apply probabilistic methods in environmental regulation entitlement analysis and financial regulation An example of the use of probability theory in equity trading is the effect of the perceived probability of any widespread Middle East conflict on oil prices which have ripple effects in the economy as a whole An assessment by a commodity trader that a war is more likely can send that commoditys prices up or down and signals other traders of that opinion Accordingly the probabilities are neither assessed independently nor necessarily rationally The theory of behavioral finance emerged to describe the effect of such groupthink on pricing on policy and on peace and conflict In addition to financial assessment probability can be used to analyze trends in biology eg disease spread as well as ecology eg biological Punnett squares As with finance risk assessment can be used as a statistical tool to calculate the likelihood of undesirable events occurring and can assist with implementing protocols to avoid encountering such circumstances Probability is used to design games of chance so that casinos can make a guaranteed profit yet provide payouts to players that are frequent enough to encourage continued play Another significant application of probability theory in everyday life is reliability Many consumer products such as automobiles and consumer electronics use reliability theory in product design to reduce the probability of failure Failure probability may influence a manufacturers decisions on a products warranty The cache language model and other statistical language models that are used in natural language processing are also examples of applications of probability theory Mathematical treatment Consider an experiment that can produce a number of results The collection of all possible results is called the sample space of the experiment sometimes denoted as displaystyle Omega The power set of the sample space is formed by considering all different collections of possible results For example rolling a die can produce six possible results One collection of possible results gives an odd number on the die Thus the subset 135 is an element of the power set of the sample space of dice rolls These collections are called events In this case 135 is the event that the die falls on some odd number If the results that actually occur fall in a given event the event is said to have occurred A probability is a way of assigning every event a value between zero and one with the requirement that the event made up of all possible results in our example the event 123456 is assigned a value of one To qualify as a probability the assignment of values must satisfy the requirement that for any collection of mutually exclusive events events with no common results such as the events 16 3 and 24 the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events The probability of an event A is written as P A displaystyle PA p A displaystyle pA or Pr A displaystyle textPrA This mathematical definition of probability can extend to infinite sample spaces and even uncountable sample spaces using the concept of a measure The opposite or complement of an event A is the event not A that is the event of A not occurring often denoted as A A c displaystyle AAc A A A displaystyle overline AAcomplement neg A or A displaystyle sim A its probability is given by Pnot A 1 PA As an example the chance of not rolling a six on a sixsided die is 1 chance of rolling a six 1 16 56 For a more comprehensive treatment see Complementary event If two events A and B occur on a single performance of an experiment this is called the intersection or joint probability of A and B denoted as P A B displaystyle PAcap B Independent events If two events A and B are independent then the joint probability is P A and B P A B P A P B displaystyle PAmbox and BPAcap BPAPB For example if two coins are flipped then the chance of both being heads is 1 2 1 2 1 4 displaystyle tfrac 12times tfrac 12tfrac 14 Mutually exclusive events If either event A or event B can occur but never both simultaneously then they are called mutually exclusive events If two events are mutually exclusive then the probability of both occurring is denoted as P A B displaystyle PAcap B and P A and B P A B 0 displaystyle PAmbox and BPAcap B0 If two events are mutually exclusive then the probability of either occurring is denoted as P A B displaystyle PAcup B and P A or B P A B P A P B P A B P A P B 0 P A P B displaystyle PAmbox or BPAcup BPAPBPAcap BPAPB0PAPB For example the chance of rolling a 1 or 2 on a sixsided die is P 1 or 2 P 1 P 2 1 6 1 6 1 3 displaystyle P1mbox or 2P1P2tfrac 16tfrac 16tfrac 13 Not necessarily mutually exclusive events If the events are not necessarily mutually exclusive then P A or B P A B P A P B P A and B displaystyle PleftAhbox or BrightPAcup BPleftArightPleftBrightPleftAmbox and Bright Rewritten P A B P A P B P A B displaystyle PleftAcup BrightPleftArightPleftBrightPleftAcap Bright For example when drawing a card from a deck of cards the chance of getting a heart or a face card J Q K or both is 13 52 12 52 3 52 11 26 displaystyle tfrac 1352tfrac 1252tfrac 352tfrac 1126 since among the 52 cards of a deck 13 are hearts 12 are face cards and 3 are both here the possibilities included in the 3 that are both are included in each of the 13 hearts and the 12 face cards but should only be counted once This can be expanded further for multiple not necessarily mutually exclusive events For three events this proceeds as follows P A B C P A B C P A B P C P A B C P A P B P A B P C P A C B C P A P B P C P A B P A C P B C P A C B C P A B C P A P B P C P A B P A C P B C P A B C displaystyle beginalignedPleftAcup Bcup CrightPleftleftAcup Brightcup CrightPleftAcup BrightPleftCrightPleftleftAcup Brightcap CrightPleftArightPleftBrightPleftAcap BrightPleftCrightPleftleftAcap Crightcup leftBcap CrightrightPleftArightPleftBrightPleftCrightPleftAcap BrightleftPleftAcap CrightPleftBcap CrightPleftleftAcap Crightcap leftBcap CrightrightrightPleftAcup Bcup CrightPleftArightPleftBrightPleftCrightPleftAcap BrightPleftAcap CrightPleftBcap CrightPleftAcap Bcap Crightendaligned It can be seen then that this pattern can be repeated for any number of events Conditional probability Conditional probability is the probability of some event A given the occurrence of some other event B Conditional probability is written P A B displaystyle PAmid B and is read the probability of A given B It is defined by P A B P A B P B displaystyle PAmid Bfrac PAcap BPB If P B 0 displaystyle PB0 then P A B displaystyle PAmid B is formally undefined by this expression In this case A displaystyle A and B displaystyle B are independent since P A B P A P B 0 displaystyle PAcap BPAPB0 However it is possible to define a conditional probability for some zeroprobability events for example by using a algebra of such events such as those arising from a continuous random variable For example in a bag of 2 red balls and 2 blue balls 4 balls in total the probability of taking a red ball is 1 2 displaystyle 12 however when taking a second ball the probability of it being either a red ball or a blue ball depends on the ball previously taken For example if a red ball was taken then the probability of picking a red ball again would be 1 3 displaystyle 13 since only 1 red and 2 blue balls would have been remaining And if a blue ball was taken previously the probability of taking a red ball will be 2 3 displaystyle 23 Inverse probability In probability theory and applications Bayes rule relates the odds of event A 1 displaystyle A_1 to event A 2 displaystyle A_2 before prior to and after posterior to conditioning on another event B displaystyle B The odds on A 1 displaystyle A_1 to event A 2 displaystyle A_2 is simply the ratio of the probabilities of the two events When arbitrarily many events A displaystyle A are of interest not just two the rule can be rephrased as posterior is proportional to prior times likelihood P A B P A P B A displaystyle PABpropto PAPBA where the proportionality symbol means that the left hand side is proportional to ie equals a constant times the right hand side as A displaystyle A varies for fixed or given B displaystyle B Lee 2012 Bertsch McGrayne 2012 In this form it goes back to Laplace 1774 and to Cournot 1843 see Fienberg 2005 Summary of probabilities Relation to randomness and probability in quantum mechanics In a deterministic universe based on Newtonian concepts there would be no probability if all conditions were known Laplaces demon but there are situations in which sensitivity to initial conditions exceeds our ability to measure them ie know them In the case of a roulette wheel if the force of the hand and the period of that force are known the number on which the ball will stop would be a certainty though as a practical matter this would likely be true only of a roulette wheel that had not been exactly levelled as Thomas A Bass Newtonian Casino revealed This also assumes knowledge of inertia and friction of the wheel weight smoothness and roundness of the ball variations in hand speed during the turning and so forth A probabilistic description can thus be more useful than Newtonian mechanics for analyzing the pattern of outcomes of repeated rolls of a roulette wheel Physicists face the same situation in the kinetic theory of gases where the system while deterministic in principle is so complex with the number of molecules typically the order of magnitude of the Avogadro constant 6021023 that only a statistical description of its properties is feasible Probability theory is required to describe quantum phenomena A revolutionary discovery of early 20th century physics was the random character of all physical processes that occur at subatomic scales and are governed by the laws of quantum mechanics The objective wave function evolves deterministically but according to the Copenhagen interpretation it deals with probabilities of observing the outcome being explained by a wave function collapse when an observation is made However the loss of determinism for the sake of instrumentalism did not meet with universal approval Albert Einstein famously remarked in a letter to Max Born I am convinced that God does not play dice Like Einstein Erwin Schrdinger who discovered the wave function believed quantum mechanics is a statistical approximation of an underlying deterministic reality In some modern interpretations of the statistical mechanics of measurement quantum decoherence is invoked to account for the appearance of subjectively probabilistic experimental outcomes See also Contingency Equiprobability Fuzzy logic Heuristic psychology Notes References Bibliography Kallenberg O 2005 Probabilistic Symmetries and Invariance Principles SpringerVerlag New York 510 pp ISBN 0387251154 Kallenberg O 2002 Foundations of Modern Probability 2nd ed Springer Series in Statistics 650 pp ISBN 0387953132 Olofsson Peter 2005 Probability Statistics and Stochastic Processes WileyInterscience 504 pp ISBN 0471679690 External links Virtual Laboratories in Probability and Statistics Univ of AlaHuntsville Probability on In Our Time at the BBC Probability and Statistics EBook Edwin Thompson Jaynes Probability Theory The Logic of Science Preprint Washington University 1996 HTML index with links to PostScript files and PDF first three chapters People from the History of Probability and Statistics Univ of Southampton Probability and Statistics on the Earliest Uses Pages Univ of Southampton Earliest Uses of Symbols in Probability and Statistics on Earliest Uses of Various Mathematical Symbols A tutorial on probability and Bayes theorem devised for firstyear Oxford University students U B U W E B La Monte Young pdf file of An Anthology of Chance Operations 1963 at UbuWeb Introduction to Probability eBook Archived 27 July 2011 at the Wayback Machine by Charles Grinstead Laurie Snell Source Archived 25 March 2012 at the Wayback Machine GNU Free Documentation License in English and Italian Bruno de Finetti Probabilit e induzione Bologna CLUEB 1993 ISBN 8880911767 digital version Richard Feynmans Lecture on probability",
  },
  {
    title: "Neural network",
    originalContent:
      "A neural network is a group of interconnected units called neurons that send signals to one another Neurons can be either biological cells or mathematical models While individual neurons are simple many of them together in a network can perform complex tasks There are two main types of neural network In neuroscience a biological neural network is a physical structure found in brains and complex nervous systems a population of nerve cells connected by synapses In machine learning an artificial neural network is a mathematical model used to approximate nonlinear functions Artificial neural networks are used to solve artificial intelligence problems In biology In the context of biology a neural network is a population of biological neurons chemically connected to each other by synapses A given neuron can be connected to hundreds of thousands of synapses Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors A neuron can serve an excitatory role amplifying and propagating signals it receives or an inhibitory role suppressing signals instead Populations of interconnected neurons that are smaller than neural networks are called neural circuits Very large interconnected networks are called large scale brain networks and many of these together form brains and nervous systems Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells where they cause contraction and thereby motion In machine learning In machine learning a neural network is an artificial mathematical model used to approximate nonlinear functions While early artificial neural networks were physical machines today they are almost always implemented in software Neurons in an artificial neural network are usually arranged into layers with information passing from the first layer the input layer through one or more intermediate layers the hidden layers to the final layer the output layer The signal input to each neuron is a number specifically a linear combination of the outputs of the connected neurons in the previous layer The signal each neuron outputs is calculated from this number according to its activation function The behavior of the network depends on the strengths or weights of the connections between neurons A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset Neural networks are used to solve problems in artificial intelligence and have thereby found applications in many disciplines including predictive modeling adaptive control facial recognition handwriting recognition general game playing and generative AI History The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890 Both posited that human thought emerged from interactions among large numbers of neurons inside the brain In 1949 Donald Hebb described Hebbian learning the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism However starting with the invention of the perceptron a simple artificial neural network by Warren McCulloch and Walter Pitts in 1943 followed by the implementation of one in hardware by Frank Rosenblatt in 1957 artificial neural networks became increasingly used for machine learning applications instead and increasingly different from their biological counterparts See also Emergence Biological cybernetics Biologicallyinspired computing References",
  },
  {
    title: "Decision tree",
    originalContent:
      "A decision tree is a decision support recursive partitioning structure that uses a treelike model of decisions and their possible consequences including chance event outcomes resource costs and utility It is one way to display an algorithm that only contains conditional control statements Decision trees are commonly used in operations research specifically in decision analysis to help identify a strategy most likely to reach a goal but are also a popular tool in machine learning Overview A decision tree is a flowchartlike structure in which each internal node represents a test on an attribute eg whether a coin flip comes up heads or tails each branch represents the outcome of the test and each leaf node represents a class label decision taken after computing all attributes The paths from root to leaf represent classification rules In decision analysis a decision tree and the closely related influence diagram are used as a visual and analytical decision support tool where the expected values or expected utility of competing alternatives are calculated A decision tree consists of three types of nodes Decision nodes typically represented by squares Chance nodes typically represented by circles End nodes typically represented by triangles Decision trees are commonly used in operations research and operations management If in practice decisions have to be taken online with no recall under incomplete knowledge a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm Another use of decision trees is as a descriptive means for calculating conditional probabilities Decision trees influence diagrams utility functions and other decision analysis tools and methods are taught to undergraduate students in schools of business health economics and public health and are examples of operations research or management science methods These tools are also used to predict decisions of householders in normal and emergency scenarios Decisiontree building blocks Decisiontree elements Drawn from left to right a decision tree has only burst nodes splitting paths but no sink nodes converging paths So used manually they can grow very big and are then often hard to draw fully by hand Traditionally decision trees have been created manually as the aside example shows although increasingly specialized software is employed Decision rules The decision tree can be linearized into decision rules where the outcome is the contents of the leaf node and the conditions along the path form a conjunction in the if clause In general the rules have the form if condition1 and condition2 and condition3 then outcome Decision rules can be generated by constructing association rules with the target variable on the right They can also denote temporal or causal relations Decision tree using flowchart symbols Commonly a decision tree is drawn using flowchart symbols as it is easier for many to read and understand Note there is a conceptual error in the Proceed calculation of the tree shown below the error relates to the calculation of costs awarded in a legal action Analysis example Analysis can take into account the decision makers eg the companys preference or utility function for example The basic interpretation in this situation is that the company prefers Bs risk and payoffs under realistic risk preference coefficients greater than 400Kin that range of risk aversion the company would need to model a third strategy Neither A nor B Another example commonly used in operations research courses is the distribution of lifeguards on beaches aka the Lifes a Beach example The example describes two beaches with lifeguards to be distributed on each beach There is maximum budget B that can be distributed among the two beaches in total and using a marginal returns table analysts can decide how many lifeguards to allocate to each beach In this example a decision tree can be drawn to illustrate the principles of diminishing returns on beach 1 The decision tree illustrates that when sequentially distributing lifeguards placing a first lifeguard on beach 1 would be optimal if there is only the budget for 1 lifeguard But if there is a budget for two guards then placing both on beach 2 would prevent more overall drownings Influence diagram Much of the information in a decision tree can be represented more compactly as an influence diagram focusing attention on the issues and relationships between events Association rule induction Decision trees can also be seen as generative models of induction rules from empirical data An optimal decision tree is then defined as a tree that accounts for most of the data while minimizing the number of levels or questions Several algorithms to generate such optimal trees have been devised such as ID345 CLS ASSISTANT and CART Advantages and disadvantages Among decision support tools decision trees and influence diagrams have several advantages Decision trees Are simple to understand and interpret People are able to understand decision tree models after a brief explanation Have value even with little hard data Important insights can be generated based on experts describing a situation its alternatives probabilities and costs and their preferences for outcomes Help determine worst best and expected values for different scenarios Use a white box model If a given result is provided by a model Can be combined with other decision techniques The action of more than one decisionmaker can be considered Disadvantages of decision trees They are unstable meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree They are often relatively inaccurate Many other predictors perform better with similar data This can be remedied by replacing a single decision tree with a random forest of decision trees but a random forest is not as easy to interpret as a single decision tree For data including categorical variables with different numbers of levels information gain in decision trees is biased in favor of those attributes with more levels Calculations can get very complex particularly if many values are uncertain andor if many outcomes are linked Optimizing a decision tree A few things should be considered when improving the accuracy of the decision tree classifier The following are some possible optimizations to consider when looking to make sure the decision tree model produced makes the correct decision or classification Note that these things are not the only things to consider but only some Increasing the number of levels of the tree The accuracy of the decision tree can change based on the depth of the decision tree In many cases the trees leaves are pure nodes When a node is pure it means that all the data in that node belongs to a single class For example if the classes in the data set are Cancer and NonCancer a leaf node would be considered pure when all the sample data in a leaf node is part of only one class either cancer or noncancer It is important to note that a deeper tree is not always better when optimizing the decision tree A deeper tree can influence the runtime in a negative way If a certain classification algorithm is being used then a deeper tree could mean the runtime of this classification algorithm is significantly slower There is also the possibility that the actual algorithm building the decision tree will get significantly slower as the tree gets deeper If the treebuilding algorithm being used splits pure nodes then a decrease in the overall accuracy of the tree classifier could be experienced Occasionally going deeper in the tree can cause an accuracy decrease in general so it is very important to test modifying the depth of the decision tree and selecting the depth that produces the best results To summarize observe the points below we will define the number D as the depth of the tree Possible advantages of increasing the number D Accuracy of the decisiontree classification model increases Possible disadvantages of increasing D Runtime issues Decrease in accuracy in general Pure node splits while going deeper can cause issues The ability to test the differences in classification results when changing D is imperative We must be able to easily change and test the variables that could affect the accuracy and reliability of the decision treemodel The choice of nodesplitting functions The node splitting function used can have an impact on improving the accuracy of the decision tree For example using the informationgain function may yield better results than using the phi function The phi function is known as a measure of goodness of a candidate split at a node in the decision tree The information gain function is known as a measure of the reduction in entropy In the following we will build two decision trees One decision tree will be built using the phi function to split the nodes and one decision tree will be built using the information gain function to split the nodes The main advantages and disadvantages of information gain and phi function One major drawback of information gain is that the feature that is chosen as the next node in the tree tends to have more unique values An advantage of information gain is that it tends to choose the most impactful features that are close to the root of the tree It is a very good measure for deciding the relevance of some features The phi function is also a good measure for deciding the relevance of some features based on goodness This is the information gain function formula The formula states the information gain is a function of the entropy of a node of the decision tree minus the entropy of a candidate split at node t of a decision tree I gain s H t H s t displaystyle I_textrm gainsHtHst This is the phi function formula The phi function is maximized when the chosen feature splits the samples in a way that produces homogenous splits and have around the same number of samples in each split s t 2 P L P R Q s t displaystyle Phi st2P_LP_RQst We will set D which is the depth of the decision tree we are building to three D 3 We also have the following data set of cancer and noncancer samples and the mutation features that the samples either have or do not have If a sample has a feature mutation then the sample is positive for that mutation and it will be represented by one If a sample does not have a feature mutation then the sample is negative for that mutation and it will be represented by zero To summarize C stands for cancer and NC stands for noncancer The letter M stands for mutation and if a sample has a particular mutation it will show up in the table as a one and otherwise zero Now we can use the formulas to calculate the phi function values and information gain values for each M in the dataset Once all the values are calculated the tree can be produced The first thing to be done is to select the root node In information gain and the phi function we consider the optimal split to be the mutation that produces the highest value for information gain or the phi function Now assume that M1 has the highest phi function value and M4 has the highest information gain value The M1 mutation will be the root of our phi function tree and M4 will be the root of our information gain tree You can observe the root nodes below Now once we have chosen the root node we can split the samples into two groups based on whether a sample is positive or negative for the root node mutation The groups will be called group A and group B For example if we use M1 to split the samples in the root node we get NC2 and C2 samples in group A and the rest of the samples NC4 NC3 NC1 C1 in group B Disregarding the mutation chosen for the root node proceed to place the next best features that have the highest values for information gain or the phi function in the left or right child nodes of the decision tree Once we choose the root node and the two child nodes for the tree of depth 3 we can just add the leaves The leaves will represent the final classification decision the model has produced based on the mutations a sample either has or does not have The left tree is the decision tree we obtain from using information gain to split the nodes and the right tree is what we obtain from using the phi function to split the nodes Now assume the classification results from both trees are given using a confusion matrix Information gain confusion matrix Phi function confusion matrix The tree using information gain has the same results when using the phi function when calculating the accuracy When we classify the samples based on the model using information gain we get one true positive one false positive zero false negatives and four true negatives For the model using the phi function we get two true positives zero false positives one false negative and three true negatives The next step is to evaluate the effectiveness of the decision tree using some key metrics that will be discussed in the evaluating a decision tree section below The metrics that will be discussed below can help determine the next steps to be taken when optimizing the decision tree Other techniques The above information is not where it ends for building and optimizing a decision tree There are many techniques for improving the decision tree classification models we build One of the techniques is making our decision tree model from a bootstrapped dataset The bootstrapped dataset helps remove the bias that occurs when building a decision tree model with the same data the model is tested with The ability to leverage the power of random forests can also help significantly improve the overall accuracy of the model being built This method generates many decisions from many decision trees and tallies up the votes from each decision tree to make the final classification There are many techniques but the main objective is to test building your decision tree model in different ways to make sure it reaches the highest performance level possible Evaluating a decision tree It is important to know the measurements used to evaluate decision trees The main metrics used are accuracy sensitivity specificity precision miss rate false discovery rate and false omission rate All these measurements are derived from the number of true positives false positives True negatives and false negatives obtained when running a set of samples through the decision tree classification model Also a confusion matrix can be made to display these results All these main metrics tell something different about the strengths and weaknesses of the classification model built based on your decision tree For example a low sensitivity with high specificity could indicate the classification model built from the decision tree does not do well identifying cancer samples over noncancer samples Let us take the confusion matrix below The confusion matrix shows us the decision tree model classifier built gave 11 true positives 1 false positive 45 false negatives and 105 true negatives We will now calculate the values accuracy sensitivity specificity precision miss rate false discovery rate and false omission rate Accuracy A c c u r a c y T P T N T P T N F P F N displaystyle AccuracyTPTNTPTNFPFN 11 105 162 7160 displaystyle 11105div 1627160 Sensitivity TPR true positive rate T P R T P T P F N displaystyle TPRTPTPFN 11 11 45 1964 displaystyle 11div 11451964 Specificity TNR true negative rate T N R T N T N F P displaystyle TNRTNTNFP 105 105 1 9906 displaystyle 105div 10519906 Precision PPV positive predictive value P P V T P T P F P displaystyle PPVTPTPFP 11 11 1 9166 displaystyle 111119166 Miss Rate FNR false negative rate F N R F N F N T P displaystyle FNRFNFNTP 45 45 11 8035 displaystyle 45div 45118035 False discovery rate FDR F D R F P F P T P displaystyle FDRFPFPTP 1 1 11 830 displaystyle 1div 111830 False omission rate FOR F O R F N F N T N displaystyle FORFNFNTN 45 45 105 3000 displaystyle 45div 451053000 Once we have calculated the key metrics we can make some initial conclusions on the performance of the decision tree model built The accuracy that we calculated was 7160 The accuracy value is good to start but we would like to get our models as accurate as possible while maintaining the overall performance The sensitivity value of 1964 means that out of everyone who was actually positive for cancer tested positive If we look at the specificity value of 9906 we know that out of all the samples that were negative for cancer actually tested negative When it comes to sensitivity and specificity it is important to have a balance between the two values so if we can decrease our specificity to increase the sensitivity that would prove to be beneficial These are just a few examples on how to use these values and the meanings behind them to evaluate the decision tree model and improve upon the next iteration See also Behavior tree artificial intelligence robotics and control Mathematical model of plan execution Boosting machine learning Method in machine learning Decision cycle Sequence of steps for decisionmaking Decision list Decision matrix Decision table Table specifying actions based on conditions Decision tree model Model of computational complexity of computation Design rationale explicit documentation of the reasons behind decisions made when designing a system or artifactPages displaying wikidata descriptions as a fallback DRAKON Algorithm mapping tool Markov chain Random process independent of past history Random forest Treebased ensemble machine learning method Ordinal priority approach Multiplecriteria decision analysis method Odds algorithm Method of computing optimal strategies for lastsuccess problems Topological combinatorics Truth table Mathematical table used in logic References External links Extensive Decision Tree tutorials and examples Gallery of example decision trees Gradient Boosted Decision Trees",
  },
  {
    title: "Chatbot",
    originalContent:
      "A chatbot originally chatterbot is a software application or web interface designed to have textual or spoken conversations Modern chatbots are typically online and use generative artificial intelligence systems that are capable of maintaining a conversation with a user in natural language and simulating the way a human would behave as a conversational partner Such chatbots often use deep learning and natural language processing but simpler chatbots have existed for decades Although chatbots have existed since the late 1960s the field gained widespread attention in the early 2020s due to the popularity of OpenAIs ChatGPT followed by alternatives such as Microsofts Copilot and Googles Gemini Such examples reflect the recent practice of basing such products upon broad foundational large language models such as GPT4 or the Gemini language model that get finetuned so as to target specific tasks or applications ie simulating human conversation in the case of chatbots Chatbots can also be designed or customized to further target even more specific situations andor particular subjectmatter domains A major area where chatbots have long been used is in customer service and support with various sorts of virtual assistants Companies spanning a wide range of industries have begun using the latest generative artificial intelligence technologies to power more advanced developments in such areas History Turing test In 1950 Alan Turings famous article Computing Machinery and Intelligence was published which proposed what is now called the Turing test as a criterion of intelligence This criterion depends on the ability of a computer program to impersonate a human in a realtime written conversation with a human judge to the extent that the judge is unable to distinguish reliablyon the basis of the conversational content alonebetween the program and a real human Eliza The notoriety of Turings proposed test stimulated great interest in Joseph Weizenbaums program ELIZA published in 1966 which seemed to be able to fool users into believing that they were conversing with a real human However Weizenbaum himself did not claim that ELIZA was genuinely intelligent and the introduction to his paper presented it more as a debunking exerciseIn artificial intelligence machines are made to behave in wondrous ways often sufficient to dazzle even the most experienced observer But once a particular program is unmasked once its inner workings are explained its magic crumbles away it stands revealed as a mere collection of procedures The observer says to himself I could have written that With that thought he moves the program in question from the shelf marked intelligent to that reserved for curios The object of this paper is to cause just such a reevaluation of the program about to be explained Few programs ever needed it moreELIZAs key method of operation involves the recognition of clue words or phrases in the input and the output of the corresponding preprepared or preprogrammed responses that can move the conversation forward in an apparently meaningful way eg by responding to any input that contains the word MOTHER with TELL ME MORE ABOUT YOUR FAMILY Thus an illusion of understanding is generated even though the processing involved has been merely superficial ELIZA showed that such an illusion is surprisingly easy to generate because human judges are ready to give the benefit of the doubt when conversational responses are capable of being interpreted as intelligent Interface designers have come to appreciate that humans readiness to interpret computer output as genuinely conversationaleven when it is actually based on rather simple patternmatchingcan be exploited for useful purposes Most people prefer to engage with programs that are humanlike and this gives chatbotstyle techniques a potentially useful role in interactive systems that need to elicit information from users as long as that information is relatively straightforward and falls into predictable categories Thus for example online help systems can usefully employ chatbot techniques to identify the area of help that users require potentially providing a friendlier interface than a more formal search or menu system This sort of usage holds the prospect of moving chatbot technology from Weizenbaums shelf reserved for curios to that marked genuinely useful computational methods Early chatbots Among the most notable early chatbots are ELIZA 1966 and PARRY 1972 More recent notable programs include ALICE Jabberwacky and DUDE Agence Nationale de la Recherche and CNRS 2006 While ELIZA and PARRY were used exclusively to simulate typed conversation many chatbots now include other functional features such as games and web searching abilities In 1984 a book called The Policemans Beard is Half Constructed was published allegedly written by the chatbot Racter though the program as released would not have been capable of doing so From 1978 to some time after 1983 the CYRUS project led by Janet Kolodner constructed a chatbot simulating Cyrus Vance 57th United States Secretary of State It used casebased reasoning and updated its database daily by parsing wire news from United Press International The program was unable to process the news items subsequent to the surprise resignation of Cyrus Vance in April 1980 and the team constructed another chatbot simulating his successor Edmund Muskie One pertinent field of AI research is naturallanguage processing Usually weak AI fields employ specialized software or programming languages created specifically for the narrow function required For example ALICE uses a markup language called AIML which is specific to its function as a conversational agent and has since been adopted by various other developers of socalled Alicebots Nevertheless ALICE is still purely based on pattern matching techniques without any reasoning capabilities the same technique ELIZA was using back in 1966 This is not strong AI which would require sapience and logical reasoning abilities Jabberwacky learns new responses and context based on realtime user interactions rather than being driven from a static database Some more recent chatbots also combine realtime learning with evolutionary algorithms that optimize their ability to communicate based on each conversation held Chatbot competitions focus on the Turing test or more specific goals Two such annual contests are the Loebner Prize and The Chatterbox Challenge the latter has been offline since 2015 however materials can still be found from web archives DBpedia created a chatbot during the GSoC of 2017 It can communicate through Facebook Messenger see Master of Code Global article Modern chatbots based on large language models Modern chatbots like ChatGPT are often based on large language models called generative pretrained transformers GPT They are based on a deep learning architecture called the transformer which contains artificial neural networks They learn how to generate text by being trained on a large text corpus which provides a solid foundation for the model to perform well on downstream tasks with limited amounts of taskspecific data Despite criticism of its accuracy and tendency to hallucinatethat is to confidently output false information and even cite nonexistent sourcesChatGPT has gained attention for its detailed responses and historical knowledge Another example is BioGPT developed by Microsoft which focuses on answering biomedical questions In November 2023 Amazon announced a new chatbot called Q for people to use at work Application Messaging apps Many companies chatbots run on messaging apps or simply via SMS They are used for B2C customer service sales and marketing In 2016 Facebook Messenger allowed developers to place chatbots on their platform There were 30000 bots created for Messenger in the first six months rising to 100000 by September 2017 Since September 2017 this has also been as part of a pilot program on WhatsApp Airlines KLM and Aeromxico both announced their participation in the testing both airlines had previously launched customer services on the Facebook Messenger platform The bots usually appear as one of the users contacts but can sometimes act as participants in a group chat Many banks insurers media companies ecommerce companies airlines hotel chains retailers health care providers government entities and restaurant chains have used chatbots to answer simple questions increase customer engagement for promotion and to offer additional ways to order from them Chatbots are also used in market research to collect short survey responses A 2017 study showed 4 of companies used chatbots According to a 2016 study 80 of businesses said they intended to have one by 2020 As part of company apps and websites Previous generations of chatbots were present on company websites eg Ask Jenn from Alaska Airlines which debuted in 2008 or Expedias virtual customer service agent which launched in 2011 The newer generation of chatbots includes IBM Watsonpowered Rocky introduced in February 2017 by the New York Citybased ecommerce company Rare Carat to provide information to prospective diamond buyers Chatbot sequences Used by marketers to script sequences of messages very similar to an autoresponder sequence Such sequences can be triggered by user optin or the use of keywords within user interactions After a trigger occurs a sequence of messages is delivered until the next anticipated user response Each user response is used in the decision tree to help the chatbot navigate the response sequences to deliver the correct response message Company internal platforms Companies have used chatbots for customer support human resources or in InternetofThings IoT projects Overstockcom for one has reportedly launched a chatbot named Mila to attempt to automate certain processes when customer service employees request sick leave Other large companies such as Lloyds Banking Group Royal Bank of Scotland Renault and Citron are now using chatbots instead of call centres with humans to provide a first point of contact In large companies like in hospitals and aviation organizations chatbots are also used to share information within organizations and to assist and replace service desks Customer service Chatbots have been proposed as a replacement for customer service departments Deep learning techniques can be incorporated into chatbot applications to allow them to map conversations between users and customer service agents especially in social media In 2019 Gartner predicted that by 2021 15 of all customer service interactions globally will be handled completely by AI A study by Juniper Research in 2019 estimates retail sales resulting from chatbotbased interactions will reach 112 billion by 2023 In 2016 Russiabased Tochka Bank launched a chatbot on Facebook for a range of financial services including a possibility of making payments In July 2016 Barclays Africa also launched a Facebook chatbot In 2023 USbased National Eating Disorders Association replaced its human helpline staff with a chatbot but had to take it offline after users reported receiving harmful advice from it Healthcare Chatbots are also appearing in the healthcare industry A study suggested that physicians in the United States believed that chatbots would be most beneficial for scheduling doctor appointments locating health clinics or providing medication information ChatGPT is able to answer user queries related to health promotion and disease prevention such as screening and vaccination WhatsApp has teamed up with the World Health Organization WHO to make a chatbot service that answers users questions on COVID19 In 2020 the Government of India launched a chatbot called MyGov Corona Helpdesk that worked through WhatsApp and helped people access information about the Coronavirus COVID19 pandemic Certain patient groups are still reluctant to use chatbots A mixedmethods 2019 study showed that people are still hesitant to use chatbots for their healthcare due to poor understanding of the technological complexity the lack of empathy and concerns about cybersecurity The analysis showed that while 6 had heard of a health chatbot and 3 had experience of using it 67 perceived themselves as likely to use one within 12 months The majority of participants would use a health chatbot for seeking general health information 78 booking a medical appointment 78 and looking for local health services 80 However a health chatbot was perceived as less suitable for seeking results of medical tests and seeking specialist advice such as sexual health The analysis of attitudinal variables showed that most participants reported their preference for discussing their health with doctors 73 and having access to reliable and accurate health information 93 While 80 were curious about new technologies that could improve their health 66 reported only seeking a doctor when experiencing a health problem and 65 thought that a chatbot was a good idea 30 reported dislike about talking to computers 41 felt it would be strange to discuss health matters with a chatbot and about half were unsure if they could trust the advice given by a chatbot Therefore perceived trustworthiness individual attitudes towards bots and dislike for talking to computers are the main barriers to health chatbots Politics In New Zealand the chatbot SAM short for Semantic Analysis Machine has been developed by Nick Gerritsen of Touchtech It is designed to share its political thoughts for example on topics such as climate change healthcare and education etc It talks to people through Facebook Messenger In 2022 the chatbot Leader Lars or Leder Lars was nominated for The Synthetic Party to run in the Danish parliamentary election and was built by the artist collective Computer Lars Leader Lars differed from earlier virtual politicians by leading a political party and by not pretending to be an objective candidate This chatbot engaged in critical discussions on politics with users from around the world In India the state government has launched a chatbot for its Aaple Sarkar platform which provides conversational access to information regarding public services managed Toys Chatbots have also been incorporated into devices not primarily meant for computing such as toys Hello Barbie is an Internetconnected version of the doll that uses a chatbot provided by the company ToyTalk which previously used the chatbot for a range of smartphonebased characters for children These characters behaviors are constrained by a set of rules that in effect emulate a particular character and produce a storyline The My Friend Cayla doll was marketed as a line of 18inch 46 cm dolls which uses speech recognition technology in conjunction with an Android or iOS mobile app to recognize the childs speech and have a conversation Like the Hello Barbie doll it attracted controversy due to vulnerabilities with the dolls Bluetooth stack and its use of data collected from the childs speech IBMs Watson computer has been used as the basis for chatbotbased educational toys for companies such as CogniToys intended to interact with children for educational purposes Malicious use Malicious chatbots are frequently used to fill chat rooms with spam and advertisements by mimicking human behavior and conversations or to entice people into revealing personal information such as bank account numbers They were commonly found on Yahoo Messenger Windows Live Messenger AOL Instant Messenger and other instant messaging protocols There has also been a published report of a chatbot used in a fake personal ad on a dating services website Tay an AI chatbot designed to learn from previous interaction caused major controversy due to it being targeted by internet trolls on Twitter Soon after its launch the bot was exploited and with its repeat after me capability it started releasing racist sexist and controversial responses to Twitter users This suggests that although the bot learned effectively from experience adequate protection was not put in place to prevent misuse If a textsending algorithm can pass itself off as a human instead of a chatbot its message would be more credible Therefore humanseeming chatbots with wellcrafted online identities could start scattering fake news that seems plausible for instance making false claims during an election With enough chatbots it might be even possible to achieve artificial social proof Data security Data security is one of the major concerns of chatbot technologies Security threats and system vulnerabilities are weaknesses that are often exploited by malicious users Storage of user data and past communication that is highly valuable for training and development of chatbots can also give rise to security threats Chatbots operating on thirdparty networks may be subject to various security issues if owners of the thirdparty applications have policies regarding user data that differ from those of the chatbot Security threats can be reduced or prevented by incorporating protective mechanisms User authentication chat Endtoend encryption and selfdestructing messages are some effective solutions to resist potential security threats Limitations of chatbots Chatbots have difficulty managing nonlinear conversations that must go back and forth on a topic with a user Large language models are more versatile but require a large amount of conversational data to train These modeles generate new responses word by word based on user input are usually trained on a large dataset of naturallanguage phrases They sometimes provide plausiblesounding but incorrect or nonsensical answers They can make up names dates historical events and even simple math problems When large language models produce coherentsounding but inaccurate or fabricated content this is referred to as hallucinations When humans use and apply chatbot content contaminated with hallucinations this results in botshit Given the increasing adoption and use of chatbots for generating content there are concerns that this technology will significantly reduce the cost it takes humans to generate misinformation Impact on jobs Chatbots and technology in general used to automate repetitive tasks But advanced chatbots like ChatGPT are also targeting highpaying creative and knowledgebased jobs raising concerns about workforce disruption and quality tradeoffs in favor of costcutting Chatbots are increasingly used by small and medium enterprises to handle customer interactions efficiently reducing reliance on large call centers and lowering operational costs Prompt engineering the task of designing and refining prompts inputs leading to desired AIgenerated responses has quickly gained significant demand with the advent of large language models although the viability of this job is questioned due to new techniques for automating prompt engineering Impact on the environment Generative AI uses a high amount of electric power Due to reliance on fossil fuels in its generation this increases air pollution water pollution and greenhouse gas emissions In 2023 a question to ChatGPT consumed on average 10 times as much energy as a Google search Data centres in general and those used for AI tasks specifically consume significant amounts of water for cooling See also References Further reading Gertner Jon 2023 Wikipedias Moment of Truth Can the online encyclopedia help teach AI chatbots to get their facts right without destroying itself in the process New York Times Magazine 18 July 2023 online Searle John 1980 Minds Brains and Programs Behavioral and Brain Sciences 3 3 417457 doi101017S0140525X00005756 S2CID 55303721 Shevat Amir 2017 Designing bots Creating conversational experiences First ed Sebastopol CA OReilly Media ISBN 9781491974827 OCLC 962125282 Vincent James Horny Robot Baby Voice James Vincent on AI chatbots London Review of Books vol 46 no 19 10 October 2024 pp 2932 AI chatbot programs are made possible by new technologies but rely on the timelelss human tendency to anthropomorphise p 29 External links Media related to Chatbots at Wikimedia Commons Conversational bots at Wikibooks",
  },
  {
    title: "Supervised learning",
    originalContent:
      "In machine learning supervised learning SL is a paradigm where a model is trained using input objects eg a vector of predictor variables and desired output values also known as a supervisory signal which are often humanmade labels The training process builds a function that maps new data to expected output values An optimal scenario will allow for the algorithm to accurately determine output values for unseen instances This requires the learning algorithm to generalize from the training data to unseen situations in a reasonable way see inductive bias This statistical quality of an algorithm is measured via a generalization error Steps to follow To solve a given problem of supervised learning the following steps must be performed Determine the type of training samples Before doing anything else the user should decide what kind of data is to be used as a training set In the case of handwriting analysis for example this might be a single handwritten character an entire handwritten word an entire sentence of handwriting or a full paragraph of handwriting Gather a training set The training set needs to be representative of the realworld use of the function Thus a set of input objects is gathered together with corresponding outputs either from human experts or from measurements Determine the input feature representation of the learned function The accuracy of the learned function depends strongly on how the input object is represented Typically the input object is transformed into a feature vector which contains a number of features that are descriptive of the object The number of features should not be too large because of the curse of dimensionality but should contain enough information to accurately predict the output Determine the structure of the learned function and corresponding learning algorithm For example one may choose to use supportvector machines or decision trees Complete the design Run the learning algorithm on the gathered training set Some supervised learning algorithms require the user to determine certain control parameters These parameters may be adjusted by optimizing performance on a subset called a validation set of the training set or via crossvalidation Evaluate the accuracy of the learned function After parameter adjustment and learning the performance of the resulting function should be measured on a test set that is separate from the training set Algorithm choice A wide range of supervised learning algorithms are available each with its strengths and weaknesses There is no single learning algorithm that works best on all supervised learning problems see the No free lunch theorem There are four major issues to consider in supervised learning Biasvariance tradeoff A first issue is the tradeoff between bias and variance Imagine that we have available several different but equally good training data sets A learning algorithm is biased for a particular input x displaystyle x if when trained on each of these data sets it is systematically incorrect when predicting the correct output for x displaystyle x A learning algorithm has high variance for a particular input x displaystyle x if it predicts different output values when trained on different training sets The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm Generally there is a tradeoff between bias and variance A learning algorithm with low bias must be flexible so that it can fit the data well But if the learning algorithm is too flexible it will fit each training data set differently and hence have high variance A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance either automatically or by providing a biasvariance parameter that the user can adjust Function complexity and amount of training data The second issue is of the amount of training data available relative to the complexity of the true function classifier or regression function If the true function is simple then an inflexible learning algorithm with high bias and low variance will be able to learn it from a small amount of data But if the true function is highly complex eg because it involves complex interactions among many different input features and behaves differently in different parts of the input space then the function will only be able to learn with a large amount of training data paired with a flexible learning algorithm with low bias and high variance Dimensionality of the input space A third issue is the dimensionality of the input space If the input feature vectors have large dimensions learning the function can be difficult even if the true function only depends on a small number of those features This is because the many extra dimensions can confuse the learning algorithm and cause it to have high variance Hence input data of large dimensions typically requires tuning the classifier to have low variance and high bias In practice if the engineer can manually remove irrelevant features from the input data it will likely improve the accuracy of the learned function In addition there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones This is an instance of the more general strategy of dimensionality reduction which seeks to map the input data into a lowerdimensional space prior to running the supervised learning algorithm Noise in the output values A fourth issue is the degree of noise in the desired output values the supervisory target variables If the desired output values are often incorrect because of human error or sensor errors then the learning algorithm should not attempt to find a function that exactly matches the training examples Attempting to fit the data too carefully leads to overfitting You can overfit even when there are no measurement errors stochastic noise if the function you are trying to learn is too complex for your learning model In such a situation the part of the target function that cannot be modeled corrupts your training data this phenomenon has been called deterministic noise When either type of noise is present it is better to go with a higher bias lower variance estimator In practice there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance Other factors to consider Other factors to consider when choosing and applying a learning algorithm include the following Heterogeneity of the data If the feature vectors include features of many different kinds discrete discrete ordered counts continuous values some algorithms are easier to apply than others Many algorithms including supportvector machines linear regression logistic regression neural networks and nearest neighbor methods require that the input features be numerical and scaled to similar ranges eg to the 11 interval Methods that employ a distance function such as nearest neighbor methods and supportvector machines with Gaussian kernels are particularly sensitive to this An advantage of decision trees is that they easily handle heterogeneous data Redundancy in the data If the input features contain redundant information eg highly correlated features some learning algorithms eg linear regression logistic regression and distancebased methods will perform poorly because of numerical instabilities These problems can often be solved by imposing some form of regularization Presence of interactions and nonlinearities If each of the features makes an independent contribution to the output then algorithms based on linear functions eg linear regression logistic regression supportvector machines naive Bayes and distance functions eg nearest neighbor methods supportvector machines with Gaussian kernels generally perform well However if there are complex interactions among features then algorithms such as decision trees and neural networks work better because they are specifically designed to discover these interactions Linear methods can also be applied but the engineer must manually specify the interactions when using them When considering a new application the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand see crossvalidation Tuning the performance of a learning algorithm can be very timeconsuming Given fixed resources it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms Algorithms The most widely used learning algorithms are Supportvector machines Linear regression Logistic regression Naive Bayes Linear discriminant analysis Decision trees knearest neighbors algorithm Neural networks eg Multilayer perceptron Similarity learning How supervised learning algorithms work Given a set of N displaystyle N training examples of the form x 1 y 1 x N y N displaystyle x_1y_1x_Ny_N such that x i displaystyle x_i is the feature vector of the i displaystyle i th example and y i displaystyle y_i is its label ie class a learning algorithm seeks a function g X Y displaystyle gXto Y where X displaystyle X is the input space and Y displaystyle Y is the output space The function g displaystyle g is an element of some space of possible functions G displaystyle G usually called the hypothesis space It is sometimes convenient to represent g displaystyle g using a scoring function f X Y R displaystyle fXtimes Yto mathbb R such that g displaystyle g is defined as returning the y displaystyle y value that gives the highest score g x arg max y f x y displaystyle gxunderset yarg max fxy Let F displaystyle F denote the space of scoring functions Although G displaystyle G and F displaystyle F can be any space of functions many learning algorithms are probabilistic models where g displaystyle g takes the form of a conditional probability model g x arg max y P y x displaystyle gxunderset yarg max Pyx or f displaystyle f takes the form of a joint probability model f x y P x y displaystyle fxyPxy For example naive Bayes and linear discriminant analysis are joint probability models whereas logistic regression is a conditional probability model There are two basic approaches to choosing f displaystyle f or g displaystyle g empirical risk minimization and structural risk minimization Empirical risk minimization seeks the function that best fits the training data Structural risk minimization includes a penalty function that controls the biasvariance tradeoff In both cases it is assumed that the training set consists of a sample of independent and identically distributed pairs x i y i displaystyle x_iy_i In order to measure how well a function fits the training data a loss function L Y Y R 0 displaystyle LYtimes Yto mathbb R geq 0 is defined For training example x i y i displaystyle x_iy_i the loss of predicting the value y displaystyle hat y is L y i y displaystyle Ly_ihat y The risk R g displaystyle Rg of function g displaystyle g is defined as the expected loss of g displaystyle g This can be estimated from the training data as R e m p g 1 N i L y i g x i displaystyle R_empgfrac 1Nsum _iLy_igx_i Empirical risk minimization In empirical risk minimization the supervised learning algorithm seeks the function g displaystyle g that minimizes R g displaystyle Rg Hence a supervised learning algorithm can be constructed by applying an optimization algorithm to find g displaystyle g When g displaystyle g is a conditional probability distribution P y x displaystyle Pyx and the loss function is the negative log likelihood L y y log P y x displaystyle Lyhat ylog Pyx then empirical risk minimization is equivalent to maximum likelihood estimation When G displaystyle G contains many candidate functions or the training set is not sufficiently large empirical risk minimization leads to high variance and poor generalization The learning algorithm is able to memorize the training examples without generalizing well overfitting Structural risk minimization Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization The regularization penalty can be viewed as implementing a form of Occams razor that prefers simpler functions over more complex ones A wide variety of penalties have been employed that correspond to different definitions of complexity For example consider the case where the function g displaystyle g is a linear function of the form g x j 1 d j x j displaystyle gxsum _j1dbeta _jx_j A popular regularization penalty is j j 2 displaystyle sum _jbeta _j2 which is the squared Euclidean norm of the weights also known as the L 2 displaystyle L_2 norm Other norms include the L 1 displaystyle L_1 norm j j displaystyle sum _jbeta _j and the L 0 displaystyle L_0 norm which is the number of nonzero j displaystyle beta _j s The penalty will be denoted by C g displaystyle Cg The supervised learning optimization problem is to find the function g displaystyle g that minimizes J g R e m p g C g displaystyle JgR_empglambda Cg The parameter displaystyle lambda controls the biasvariance tradeoff When 0 displaystyle lambda 0 this gives empirical risk minimization with low bias and high variance When displaystyle lambda is large the learning algorithm will have high bias and low variance The value of displaystyle lambda can be chosen empirically via crossvalidation The complexity penalty has a Bayesian interpretation as the negative log prior probability of g displaystyle g log P g displaystyle log Pg in which case J g displaystyle Jg is the posterior probability of g displaystyle g Generative training The training methods described above are discriminative training methods because they seek to find a function g displaystyle g that discriminates well between the different output values see discriminative model For the special case where f x y P x y displaystyle fxyPxy is a joint probability distribution and the loss function is the negative log likelihood i log P x i y i displaystyle sum _ilog Px_iy_i a risk minimization algorithm is said to perform generative training because f displaystyle f can be regarded as a generative model that explains how the data were generated Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms In some cases the solution can be computed in closed form as in naive Bayes and linear discriminant analysis Generalizations There are several ways in which the standard supervised learning problem can be generalized Semisupervised learning or weak supervision the desired output values are provided only for a subset of the training data The remaining data is unlabeled or imprecisely labeled Active learning Instead of assuming that all of the training examples are given at the start active learning algorithms interactively collect new examples typically by making queries to a human user Often the queries are based on unlabeled data which is a scenario that combines semisupervised learning with active learning Structured prediction When the desired output value is a complex object such as a parse tree or a labeled graph then standard methods must be extended Learning to rank When the input is a set of objects and the desired output is a ranking of those objects then again the standard methods must be extended Approaches and algorithms Analytical learning Artificial neural network Backpropagation Boosting metaalgorithm Bayesian statistics Casebased reasoning Decision tree learning Inductive logic programming Gaussian process regression Genetic programming Group method of data handling Kernel estimators Learning automata Learning classifier systems Learning vector quantization Minimum message length decision trees decision graphs etc Multilinear subspace learning Naive Bayes classifier Maximum entropy classifier Conditional random field Nearest neighbor algorithm Probably approximately correct learning PAC learning Ripple down rules a knowledge acquisition methodology Symbolic machine learning algorithms Subsymbolic machine learning algorithms Support vector machines Minimum complexity machines MCM Random forests Ensembles of classifiers Ordinal classification Data preprocessing Handling imbalanced datasets Statistical relational learning Proaftn a multicriteria classification algorithm Applications Bioinformatics Cheminformatics Quantitative structureactivity relationship Database marketing Handwriting recognition Information retrieval Learning to rank Information extraction Object recognition in computer vision Optical character recognition Spam detection Pattern recognition Speech recognition Supervised learning is a special case of downward causation in biological systems Landform classification using satellite imagery Spend classification in procurement processes General issues Computational learning theory Inductive bias Overfitting Uncalibrated class membership probabilities Version spaces See also List of datasets for machine learning research Unsupervised learning References External links Machine Learning Open Source Software MLOSS",
  },
  {
    title: "Unsupervised learning",
    originalContent:
      "Unsupervised learning is a framework in machine learning where in contrast to supervised learning algorithms learn patterns exclusively from unlabeled data Other frameworks in the spectrum of supervisions include weak or semisupervision where a small portion of the data is tagged and selfsupervision Some researchers consider selfsupervised learning a form of unsupervised learning Conceptually unsupervised learning divides into the aspects of data training algorithm and downstream applications Typically the dataset is harvested cheaply in the wild such as massive text corpus obtained by web crawling with only minor filtering such as Common Crawl This compares favorably to supervised learning where the dataset such as the ImageNet1000 is typically constructed manually which is much more expensive There were algorithms designed specifically for unsupervised learning such as clustering algorithms like kmeans dimensionality reduction techniques like principal component analysis PCA Boltzmann machine learning and autoencoders After the rise of deep learning most largescale unsupervised learning have been done by training generalpurpose neural network architectures by gradient descent adapted to performing unsupervised learning by designing an appropriate training procedure Sometimes a trained model can be used asis but more often they are modified for downstream applications For example the generative pretraining method trains a model to generate a textual dataset before finetuning it for other applications such as text classification As another example autoencoders are trained to good features which can then be used as a module for other models such as in a latent diffusion model Tasks Tasks are often categorized as discriminative recognition or generative imagination Often but not always discriminative tasks use supervised methods and generative tasks use unsupervised see Venn diagram however the separation is very hazy For example object recognition favors supervised learning but unsupervised learning can also cluster objects into groups Furthermore as progress marches onward some tasks employ both methods and some tasks swing from one to another For example image recognition started off as heavily supervised but became hybrid by employing unsupervised pretraining and then moved towards supervision again with the advent of dropout ReLU and adaptive learning rates A typical generative task is as follows At each step a datapoint is sampled from the dataset and part of the data is removed and the model must infer the removed part This is particularly clear for the denoising autoencoders and BERT Neural network architectures Training During the learning phase an unsupervised network tries to mimic the data its given and uses the error in its mimicked output to correct itself ie correct its weights and biases Sometimes the error is expressed as a low probability that the erroneous output occurs or it might be expressed as an unstable high energy state in the network In contrast to supervised methods dominant use of backpropagation unsupervised learning also employs other methods including Hopfield learning rule Boltzmann learning rule Contrastive Divergence Wake Sleep Variational Inference Maximum Likelihood Maximum A Posteriori Gibbs Sampling and backpropagating reconstruction errors or hidden state reparameterizations See the table below for more details Energy An energy function is a macroscopic measure of a networks activation state In Boltzmann machines it plays the role of the Cost function This analogy with physics is inspired by Ludwig Boltzmanns analysis of a gas macroscopic energy from the microscopic probabilities of particle motion p e E k T displaystyle ppropto eEkT where k is the Boltzmann constant and T is temperature In the RBM network the relation is p e E Z displaystyle peEZ where p displaystyle p and E displaystyle E vary over every possible activation pattern and Z All Patterns e E pattern displaystyle textstyle Zsum _scriptscriptstyle textAll PatternseEtextpattern To be more precise p a e E a Z displaystyle paeEaZ where a displaystyle a is an activation pattern of all neurons visible and hidden Hence some early neural networks bear the name Boltzmann Machine Paul Smolensky calls E displaystyle E the Harmony A network seeks low energy which is high Harmony Networks This table shows connection diagrams of various unsupervised networks the details of which will be given in the section Comparison of Networks Circles are neurons and edges between them are connection weights As network design changes features are added on to enable new capabilities or removed to make learning faster For instance neurons change between deterministic Hopfield and stochastic Boltzmann to allow robust output weights are removed within a layer RBM to hasten learning or connections are allowed to become asymmetric Helmholtz Of the networks bearing peoples names only Hopfield worked directly with neural networks Boltzmann and Helmholtz came before artificial neural networks but their work in physics and physiology inspired the analytical methods that were used History Specific Networks Here we highlight some characteristics of select networks The details of each are given in the comparison table below Hopfield Network Ferromagnetism inspired Hopfield networks A neuron correspond to an iron domain with binary magnetic moments Up and Down and neural connections correspond to the domains influence on each other Symmetric connections enable a global energy formulation During inference the network updates each state using the standard activation step function Symmetric weights and the right energy functions guarantees convergence to a stable activation pattern Asymmetric weights are difficult to analyze Hopfield nets are used as Content Addressable Memories CAM Boltzmann Machine These are stochastic Hopfield nets Their state value is sampled from this pdf as follows suppose a binary neuron fires with the Bernoulli probability p1 13 and rests with p0 23 One samples from it by taking a uniformly distributed random number y and plugging it into the inverted cumulative distribution function which in this case is the step function thresholded at 23 The inverse function 0 if x 23 1 if x 23 Sigmoid Belief Net Introduced by Radford Neal in 1992 this network applies ideas from probabilistic graphical models to neural networks A key difference is that nodes in graphical models have preassigned meanings whereas Belief Net neurons features are determined after training The network is a sparsely connected directed acyclic graph composed of binary stochastic neurons The learning rule comes from Maximum Likelihood on pX wij displaystyle propto sj si pi where pi 1 1 eweighted inputs into neuron i sjs are activations from an unbiased sample of the posterior distribution and this is problematic due to the Explaining Away problem raised by Judea Perl Variational Bayesian methods uses a surrogate posterior and blatantly disregard this complexity Deep Belief Network Introduced by Hinton this network is a hybrid of RBM and Sigmoid Belief Network The top 2 layers is an RBM and the second layer downwards form a sigmoid belief network One trains it by the stacked RBM method and then throw away the recognition weights below the top RBM As of 2009 34 layers seems to be the optimal depth Helmholtz machine These are early inspirations for the Variational Auto Encoders Its 2 networks combined into oneforward weights operates recognition and backward weights implements imagination It is perhaps the first network to do both Helmholtz did not work in machine learning but he inspired the view of statistical inference engine whose function is to infer probable causes of sensory input the stochastic binary neuron outputs a probability that its state is 0 or 1 The data input is normally not considered a layer but in the Helmholtz machine generation mode the data layer receives input from the middle layer and has separate weights for this purpose so it is considered a layer Hence this network has 3 layers Variational autoencoder These are inspired by Helmholtz machines and combines probability network with neural networks An Autoencoder is a 3layer CAM network where the middle layer is supposed to be some internal representation of input patterns The encoder neural network is a probability distribution qz given x and the decoder network is px given z The weights are named phi theta rather than W and V as in Helmholtza cosmetic difference These 2 networks here can be fully connected or use another NN scheme Comparison of networks Hebbian Learning ART SOM The classical example of unsupervised learning in the study of neural networks is Donald Hebbs principle that is neurons that fire together wire together In Hebbian learning the connection is reinforced irrespective of an error but is exclusively a function of the coincidence between action potentials between the two neurons A similar version that modifies synaptic weights takes into account the time between the action potentials spiketimingdependent plasticity or STDP Hebbian Learning has been hypothesized to underlie a range of cognitive functions such as pattern recognition and experiential learning Among neural network models the selforganizing map SOM and adaptive resonance theory ART are commonly used in unsupervised learning algorithms The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a userdefined constant called the vigilance parameter ART networks are used for many pattern recognition tasks such as automatic target recognition and seismic signal processing Probabilistic methods Two of the main methods used in unsupervised learning are principal component and cluster analysis Cluster analysis is used in unsupervised learning to group or segment datasets with shared attributes in order to extrapolate algorithmic relationships Cluster analysis is a branch of machine learning that groups the data that has not been labelled classified or categorized Instead of responding to feedback cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data This approach helps detect anomalous data points that do not fit into either group A central application of unsupervised learning is in the field of density estimation in statistics though unsupervised learning encompasses many other domains involving summarizing and explaining data features It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution conditioned on the label of input data unsupervised learning intends to infer an a priori probability distribution Approaches Some of the most common algorithms used in unsupervised learning include 1 Clustering 2 Anomaly detection 3 Approaches for learning latent variable models Each approach uses several methods as follows Clustering methods include hierarchical clustering kmeans mixture models modelbased clustering DBSCAN and OPTICS algorithm Anomaly detection methods include Local Outlier Factor and Isolation Forest Approaches for learning latent variable models such as Expectationmaximization algorithm EM Method of moments and Blind signal separation techniques Principal component analysis Independent component analysis Nonnegative matrix factorization Singular value decomposition Method of moments One of the statistical approaches for unsupervised learning is the method of moments In the method of moments the unknown parameters of interest in the model are related to the moments of one or more random variables and thus these unknown parameters can be estimated given the moments The moments are usually estimated from samples empirically The basic moments are first and second order moments For a random vector the first order moment is the mean vector and the second order moment is the covariance matrix when the mean is zero Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multidimensional arrays In particular the method of moments is shown to be effective in learning the parameters of latent variable models Latent variable models are statistical models where in addition to the observed variables a set of latent variables also exists which is not observed A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words observed variables in the document based on the topic latent variable of the document In the topic modeling the words in the document are generated according to different statistical parameters when the topic of the document is changed It is shown that method of moments tensor decomposition techniques consistently recover the parameters of a large class of latent variable models under some assumptions The Expectationmaximization algorithm EM is also one of the most practical methods for learning latent variable models However it can get stuck in local optima and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model In contrast for the method of moments the global convergence is guaranteed under some conditions See also Automated machine learning Cluster analysis Modelbased clustering Anomaly detection Expectationmaximization algorithm Generative topographic map Metalearning computer science Multivariate analysis Radial basis function network Weak supervision References Further reading",
  },
  {
    title: "Simulated annealing",
    originalContent:
      "Simulated annealing SA is a probabilistic technique for approximating the global optimum of a given function Specifically it is a metaheuristic to approximate global optimization in a large search space for an optimization problem For large numbers of local optima SA can find the global optimum It is often used when the search space is discrete for example the traveling salesman problem the boolean satisfiability problem protein structure prediction and jobshop scheduling For problems where finding an approximate global optimum is more important than finding a precise local optimum in a fixed amount of time simulated annealing may be preferable to exact algorithms such as gradient descent or branch and bound The name of the algorithm comes from annealing in metallurgy a technique involving heating and controlled cooling of a material to alter its physical properties Both are attributes of the material that depend on their thermodynamic free energy Heating and cooling the material affects both the temperature and the thermodynamic free energy or Gibbs energy Simulated annealing can be used for very hard computational optimization problems where exact algorithms fail even though it usually achieves an approximate solution to the global minimum it could be enough for many practical problems The problems solved by SA are currently formulated by an objective function of many variables subject to several mathematical constraints In practice the constraint can be penalized as part of the objective function Similar techniques have been independently introduced on several occasions including Pincus 1970 Khachaturyan et al 1979 1981 Kirkpatrick Gelatt and Vecchi 1983 and Cerny 1985 In 1983 this approach was used by Kirkpatrick Gelatt Jr Vecchi for a solution of the traveling salesman problem They also proposed its current name simulated annealing This notion of slow cooling implemented in the simulated annealing algorithm is interpreted as a slow decrease in the probability of accepting worse solutions as the solution space is explored Accepting worse solutions allows for a more extensive search for the global optimal solution In general simulated annealing algorithms work as follows The temperature progressively decreases from an initial positive value to zero At each time step the algorithm randomly selects a solution close to the current one measures its quality and moves to it according to the temperaturedependent probabilities of selecting better or worse solutions which during the search respectively remain at 1 or positive and decrease toward zero The simulation can be performed either by a solution of kinetic equations for probability density functions or by using a stochastic sampling method The method is an adaptation of the MetropolisHastings algorithm a Monte Carlo method to generate sample states of a thermodynamic system published by N Metropolis et al in 1953 Overview The state s of some physical systems and the function Es to be minimized is analogous to the internal energy of the system in that state The goal is to bring the system from an arbitrary initial state to a state with the minimum possible energy The basic iteration At each step the simulated annealing heuristic considers some neighboring state s of the current state s and probabilistically decides between moving the system to state s or staying in state s These probabilities ultimately lead the system to move to states of lower energy Typically this step is repeated until the system reaches a state that is good enough for the application or until a given computation budget has been exhausted The neighbors of a state Optimization of a solution involves evaluating the neighbors of a state of the problem which are new states produced through conservatively altering a given state For example in the traveling salesman problem each state is typically defined as a permutation of the cities to be visited and the neighbors of any state are the set of permutations produced by swapping any two of these cities The welldefined way in which the states are altered to produce neighboring states is called a move and different moves give different sets of neighboring states These moves usually result in minimal alterations of the last state in an attempt to progressively improve the solution through iteratively improving its parts such as the city connections in the traveling salesman problem It is even better to reverse the order of an interval of cities This is a smaller move since swapping two cities can be achieved by twice reversing an interval Simple heuristics like hill climbing which move by finding better neighbor after better neighbor and stop when they have reached a solution which has no neighbors that are better solutions cannot guarantee to lead to any of the existing better solutions their outcome may easily be just a local optimum while the actual best solution would be a global optimum that could be different Metaheuristics use the neighbors of a solution as a way to explore the solution space and although they prefer better neighbors they also accept worse neighbors in order to avoid getting stuck in local optima they can find the global optimum if run for a long enough amount of time Acceptance probabilities The probability of making the transition from the current state s displaystyle s to a candidate new state s n e w displaystyle s_mathrm new is specified by an acceptance probability function P e e n e w T displaystyle Pee_mathrm new T that depends on the energies e E s displaystyle eEs and e n e w E s n e w displaystyle e_mathrm new Es_mathrm new of the two states and on a global timevarying parameter T displaystyle T called the temperature States with a smaller energy are better than those with a greater energy The probability function P displaystyle P must be positive even when e n e w displaystyle e_mathrm new is greater than e displaystyle e This feature prevents the method from becoming stuck at a local minimum that is worse than the global one When T displaystyle T tends to zero the probability P e e n e w T displaystyle Pee_mathrm new T must tend to zero if e n e w e displaystyle e_mathrm new e and to a positive value otherwise For sufficiently small values of T displaystyle T the system will then increasingly favor moves that go downhill ie to lower energy values and avoid those that go uphill With T 0 displaystyle T0 the procedure reduces to the greedy algorithm which makes only the downhill transitions In the original description of simulated annealing the probability P e e n e w T displaystyle Pee_mathrm new T was equal to 1 when e n e w e displaystyle e_mathrm new e ie the procedure always moved downhill when it found a way to do so irrespective of the temperature Many descriptions and implementations of simulated annealing still take this condition as part of the methods definition However this condition is not essential for the method to work The P displaystyle P function is usually chosen so that the probability of accepting a move decreases when the difference e n e w e displaystyle e_mathrm new e increasesthat is small uphill moves are more likely than large ones However this requirement is not strictly necessary provided that the above requirements are met Given these properties the temperature T displaystyle T plays a crucial role in controlling the evolution of the state s displaystyle s of the system with regard to its sensitivity to the variations of system energies To be precise for a large T displaystyle T the evolution of s displaystyle s is sensitive to coarser energy variations while it is sensitive to finer energy variations when T displaystyle T is small The annealing schedule The name and inspiration of the algorithm demand an interesting feature related to the temperature variation to be embedded in the operational characteristics of the algorithm This necessitates a gradual reduction of the temperature as the simulation proceeds The algorithm starts initially with T displaystyle T set to a high value or infinity and then it is decreased at each step following some annealing schedulewhich may be specified by the user but must end with T 0 displaystyle T0 towards the end of the allotted time budget In this way the system is expected to wander initially towards a broad region of the search space containing good solutions ignoring small features of the energy function then drift towards lowenergy regions that become narrower and narrower and finally move downhill according to the steepest descent heuristic For any given finite problem the probability that the simulated annealing algorithm terminates with a global optimal solution approaches 1 as the annealing schedule is extended This theoretical result however is not particularly helpful since the time required to ensure a significant probability of success will usually exceed the time required for a complete search of the solution space Pseudocode The following pseudocode presents the simulated annealing heuristic as described above It starts from a state s0 and continues until a maximum of kmax steps have been taken In the process the call neighbours should generate a randomly chosen neighbour of a given state s the call random0 1 should pick and return a value in the range 0 1 uniformly at random The annealing schedule is defined by the call temperaturer which should yield the temperature to use given the fraction r of the time budget that has been expended so far Selecting the parameters In order to apply the simulated annealing method to a specific problem one must specify the following parameters the state space the energy goal function E the candidate generator procedure neighbor the acceptance probability function P and the annealing schedule temperature AND initial temperature init_temp These choices can have a significant impact on the methods effectiveness Unfortunately there are no choices of these parameters that will be good for all problems and there is no general way to find the best choices for a given problem The following sections give some general guidelines Sufficiently near neighbour Simulated annealing may be modeled as a random walk on a search graph whose vertices are all possible states and whose edges are the candidate moves An essential requirement for the neighbor function is that it must provide a sufficiently short path on this graph from the initial state to any state which may be the global optimum the diameter of the search graph must be small In the traveling salesman example above for instance the search space for n 20 cities has n 2432902008176640000 24 quintillion states yet the number of neighbors of each vertex is k 1 n 1 k n n 1 2 190 displaystyle sum _k1n1kfrac nn12190 edges coming from n choose 20 and the diameter of the graph is n 1 displaystyle n1 Transition probabilities To investigate the behavior of simulated annealing on a particular problem it can be useful to consider the transition probabilities that result from the various design choices made in the implementation of the algorithm For each edge s s displaystyle ss of the search graph the transition probability is defined as the probability that the simulated annealing algorithm will move to state s displaystyle s when its current state is s displaystyle s This probability depends on the current temperature as specified by temperature on the order in which the candidate moves are generated by the neighbor function and on the acceptance probability function P Note that the transition probability is not simply P e e T displaystyle PeeT because the candidates are tested serially Acceptance probabilities The specification of neighbour P and temperature is partially redundant In practice its common to use the same acceptance function P for many problems and adjust the other two functions according to the specific problem In the formulation of the method by Kirkpatrick et al the acceptance probability function P e e T displaystyle PeeT was defined as 1 if e e displaystyle ee and exp e e T displaystyle expeeT otherwise This formula was superficially justified by analogy with the transitions of a physical system it corresponds to the MetropolisHastings algorithm in the case where T1 and the proposal distribution of MetropolisHastings is symmetric However this acceptance probability is often used for simulated annealing even when the neighbor function which is analogous to the proposal distribution in MetropolisHastings is not symmetric or not probabilistic at all As a result the transition probabilities of the simulated annealing algorithm do not correspond to the transitions of the analogous physical system and the longterm distribution of states at a constant temperature T displaystyle T need not bear any resemblance to the thermodynamic equilibrium distribution over states of that physical system at any temperature Nevertheless most descriptions of simulated annealing assume the original acceptance function which is probably hardcoded in many implementations of SA In 1990 Moscato and Fontanari and independently Dueck and Scheuer proposed that a deterministic update ie one that is not based on the probabilistic acceptance rule could speedup the optimization process without impacting on the final quality Moscato and Fontanari conclude from observing the analogous of the specific heat curve of the threshold updating annealing originating from their study that the stochasticity of the Metropolis updating in the simulated annealing algorithm does not play a major role in the search of nearoptimal minima Instead they proposed that the smoothening of the cost function landscape at high temperature and the gradual definition of the minima during the cooling process are the fundamental ingredients for the success of simulated annealing The method subsequently popularized under the denomination of threshold accepting due to Dueck and Scheuers denomination In 2001 Franz Hoffmann and Salamon showed that the deterministic update strategy is indeed the optimal one within the large class of algorithms that simulate a random walk on the costenergy landscape Efficient candidate generation When choosing the candidate generator neighbor one must consider that after a few iterations of the simulated annealing algorithm the current state is expected to have much lower energy than a random state Therefore as a general rule one should skew the generator towards candidate moves where the energy of the destination state s displaystyle s is likely to be similar to that of the current state This heuristic which is the main principle of the MetropolisHastings algorithm tends to exclude very good candidate moves as well as very bad ones however the former are usually much less common than the latter so the heuristic is generally quite effective In the traveling salesman problem above for example swapping two consecutive cities in a lowenergy tour is expected to have a modest effect on its energy length whereas swapping two arbitrary cities is far more likely to increase its length than to decrease it Thus the consecutiveswap neighbor generator is expected to perform better than the arbitraryswap one even though the latter could provide a somewhat shorter path to the optimum with n 1 displaystyle n1 swaps instead of n n 1 2 displaystyle nn12 A more precise statement of the heuristic is that one should try the first candidate states s displaystyle s for which P E s E s T displaystyle PEsEsT is large For the standard acceptance function P displaystyle P above it means that E s E s displaystyle EsEs is on the order of T displaystyle T or less Thus in the traveling salesman example above one could use a neighbor function that swaps two random cities where the probability of choosing a citypair vanishes as their distance increases beyond T displaystyle T Barrier avoidance When choosing the candidate generator neighbor one must also try to reduce the number of deep local minimastates or sets of connected states that have much lower energy than all its neighboring states Such closed catchment basins of the energy function may trap the simulated annealing algorithm with high probability roughly proportional to the number of states in the basin and for a very long time roughly exponential on the energy difference between the surrounding states and the bottom of the basin As a rule it is impossible to design a candidate generator that will satisfy this goal and also prioritize candidates with similar energy On the other hand one can often vastly improve the efficiency of simulated annealing by relatively simple changes to the generator In the traveling salesman problem for instance it is not hard to exhibit two tours A displaystyle A B displaystyle B with nearly equal lengths such that 1 A displaystyle A is optimal 2 every sequence of citypair swaps that converts A displaystyle A to B displaystyle B goes through tours that are much longer than both and 3 A displaystyle A can be transformed into B displaystyle B by flipping reversing the order of a set of consecutive cities In this example A displaystyle A and B displaystyle B lie in different deep basins if the generator performs only random pairswaps but they will be in the same basin if the generator performs random segmentflips Cooling schedule The physical analogy that is used to justify simulated annealing assumes that the cooling rate is low enough for the probability distribution of the current state to be near thermodynamic equilibrium at all times Unfortunately the relaxation timethe time one must wait for the equilibrium to be restored after a change in temperaturestrongly depends on the topography of the energy function and on the current temperature In the simulated annealing algorithm the relaxation time also depends on the candidate generator in a very complicated way Note that all these parameters are usually provided as black box functions to the simulated annealing algorithm Therefore the ideal cooling rate cannot be determined beforehand and should be empirically adjusted for each problem Adaptive simulated annealing algorithms address this problem by connecting the cooling schedule to the search progress Other adaptive approaches such as Thermodynamic Simulated Annealing automatically adjusts the temperature at each step based on the energy difference between the two states according to the laws of thermodynamics Restarts Sometimes it is better to move back to a solution that was significantly better rather than always moving from the current state This process is called restarting of simulated annealing To do this we set s and e to sbest and ebest and perhaps restart the annealing schedule The decision to restart could be based on several criteria Notable among these include restarting based on a fixed number of steps based on whether the current energy is too high compared to the best energy obtained so far restarting randomly etc Related methods Interacting MetropolisHasting algorithms aka sequential Monte Carlo combines simulated annealing moves with an acceptancerejection of the bestfitted individuals equipped with an interacting recycling mechanism Quantum annealing uses quantum fluctuations instead of thermal fluctuations to get through high but thin barriers in the target function Stochastic tunneling attempts to overcome the increasing difficulty simulated annealing runs have in escaping from local minima as the temperature decreases by tunneling through barriers Tabu search normally moves to neighbouring states of lower energy but will take uphill moves when it finds itself stuck in a local minimum and avoids cycles by keeping a taboo list of solutions already seen Dualphase evolution is a family of algorithms and processes to which simulated annealing belongs that mediate between local and global search by exploiting phase changes in the search space Reactive search optimization focuses on combining machine learning with optimization by adding an internal feedback loop to selftune the free parameters of an algorithm to the characteristics of the problem of the instance and of the local situation around the current solution Genetic algorithms maintain a pool of solutions rather than just one New candidate solutions are generated not only by mutation as in SA but also by recombination of two solutions from the pool Probabilistic criteria similar to those used in SA are used to select the candidates for mutation or combination and for discarding excess solutions from the pool Memetic algorithms search for solutions by employing a set of agents that both cooperate and compete in the process sometimes the agents strategies involve simulated annealing procedures for obtaining highquality solutions before recombining them Annealing has also been suggested as a mechanism for increasing the diversity of the search Graduated optimization digressively smooths the target function while optimizing Ant colony optimization ACO uses many ants or agents to traverse the solution space and find locally productive areas The crossentropy method CE generates candidate solutions via a parameterized probability distribution The parameters are updated via crossentropy minimization so as to generate better samples in the next iteration Harmony search mimics musicians in improvisation where each musician plays a note to find the best harmony together Stochastic optimization is an umbrella set of methods that includes simulated annealing and numerous other approaches Particle swarm optimization is an algorithm modeled on swarm intelligence that finds a solution to an optimization problem in a search space or models and predicts social behavior in the presence of objectives The runnerroot algorithm RRA is a metaheuristic optimization algorithm for solving unimodal and multimodal problems inspired by the runners and roots of plants in nature Intelligent water drops algorithm IWD which mimics the behavior of natural water drops to solve optimization problems Parallel tempering is a simulation of model copies at different temperatures or Hamiltonians to overcome the potential barriers Multiobjective simulated annealing algorithms have been used in multiobjective optimization See also References Further reading A Das and B K Chakrabarti Eds Quantum Annealing and Related Optimization Methods Lecture Note in Physics Vol 679 Springer Heidelberg 2005 Weinberger E 1990 Correlated and uncorrelated fitness landscapes and how to tell the difference Biological Cybernetics 63 5 325336 doi101007BF00202749 S2CID 851736 Press WH Teukolsky SA Vetterling WT Flannery BP 2007 Section 1012 Simulated Annealing Methods Numerical Recipes The Art of Scientific Computing 3rd ed New York Cambridge University Press ISBN 9780521880688 Archived from the original on 20110811 Retrieved 20110813 Strobl MAR Barker D 2016 On simulated annealing phase transitions in phylogeny reconstruction Molecular Phylogenetics and Evolution 101 4655 Bibcode2016MolPE10146S doi101016jympev201605001 PMC 4912009 PMID 27150349 VVassilev APrahova The Use of Simulated Annealing in the Control of Flexible Manufacturing Systems International Journal INFORMATION THEORIES APPLICATIONS VOLUME 61999 External links Simulated Annealing A Javascript app that allows you to experiment with simulated annealing Source code included General Simulated Annealing Algorithm Archived 20080923 at the Wayback Machine An opensource MATLAB program for general simulated annealing exercises SelfGuided Lesson on Simulated Annealing A Wikiversity project Google in superposition of using not using quantum computer Ars Technica discusses the possibility that the DWave computer being used by Google may in fact be an efficient simulated annealing coprocessor 1 A Simulated AnnealingBased Multiobjective Optimization Algorithm AMOSA",
  },
  {
    title: "Swarm intelligence",
    originalContent:
      "Swarm intelligence SI is the collective behavior of decentralized selforganized systems natural or artificial The concept is employed in work on artificial intelligence The expression was introduced by Gerardo Beni and Jing Wang in 1989 in the context of cellular robotic systems SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment The inspiration often comes from nature especially biological systems The agents follow very simple rules and although there is no centralized control structure dictating how individual agents should behave local and to a certain degree random interactions between such agents lead to the emergence of intelligent global behavior unknown to the individual agents Examples of swarm intelligence in natural systems include ant colonies bee colonies bird flocking hawks hunting animal herding bacterial growth fish schooling and microbial intelligence The application of swarm principles to robots is called swarm robotics while swarm intelligence refers to the more general set of algorithms Swarm prediction has been used in the context of forecasting problems Similar approaches to those proposed for swarm robotics are considered for genetically modified organisms in synthetic collective intelligence Models of swarm behavior Boids Reynolds 1987 Boids is an artificial life program developed by Craig Reynolds in 1986 which simulates flocking It was published in 1987 in the proceedings of the ACM SIGGRAPH conference The name boid corresponds to a shortened version of birdoid object which refers to a birdlike object As with most artificial life simulations Boids is an example of emergent behavior that is the complexity of Boids arises from the interaction of individual agents the boids in this case adhering to a set of simple rules The rules applied in the simplest Boids world are as follows separation steer to avoid crowding local flockmates alignment steer towards the average heading of local flockmates cohesion steer to move toward the average position center of mass of local flockmates More complex rules can be added such as obstacle avoidance and goal seeking Selfpropelled particles Vicsek et al 1995 Selfpropelled particles SPP also referred to as the Vicsek model was introduced in 1995 by Vicsek et al as a special case of the boids model introduced in 1986 by Reynolds A swarm is modelled in SPP by a collection of particles that move with a constant speed but respond to a random perturbation by adopting at each time increment the average direction of motion of the other particles in their local neighbourhood SPP models predict that swarming animals share certain properties at the group level regardless of the type of animals in the swarm Swarming systems give rise to emergent behaviours which occur at many different scales some of which are turning out to be both universal and robust It has become a challenge in theoretical physics to find minimal statistical models that capture these behaviours Metaheuristics Evolutionary algorithms EA particle swarm optimization PSO differential evolution DE ant colony optimization ACO and their variants dominate the field of natureinspired metaheuristics This list includes algorithms published up to circa the year 2000 A large number of more recent metaphorinspired metaheuristics have started to attract criticism in the research community for hiding their lack of novelty behind an elaborate metaphor For algorithms published since that time see List of metaphorbased metaheuristics Metaheuristics lack a confidence in a solution When appropriate parameters are determined and when sufficient convergence stage is achieved they often find a solution that is optimal or near close to optimum nevertheless if one does not know optimal solution in advance a quality of a solution is not known In spite of this obvious drawback it has been shown that these types of algorithms work well in practice and have been extensively researched and developed On the other hand it is possible to avoid this drawback by calculating solution quality for a special case where such calculation is possible and after such run it is known that every solution that is at least as good as the solution a special case had has at least a solution confidence a special case had One such instance is Antinspired Monte Carlo algorithm for Minimum Feedback Arc Set where this has been achieved probabilistically via hybridization of Monte Carlo algorithm with Ant Colony Optimization technique Ant colony optimization Dorigo 1992 Ant colony optimization ACO introduced by Dorigo in his doctoral dissertation is a class of optimization algorithms modeled on the actions of an ant colony ACO is a probabilistic technique useful in problems that deal with finding better paths through graphs Artificial antssimulation agentslocate optimal solutions by moving through a parameter space representing all possible solutions Natural ants lay down pheromones directing each other to resources while exploring their environment The simulated ants similarly record their positions and the quality of their solutions so that in later simulation iterations more ants locate for better solutions Particle swarm optimization Kennedy Eberhart Shi 1995 Particle swarm optimization PSO is a global optimization algorithm for dealing with problems in which a best solution can be represented as a point or surface in an ndimensional space Hypotheses are plotted in this space and seeded with an initial velocity as well as a communication channel between the particles Particles then move through the solution space and are evaluated according to some fitness criterion after each timestep Over time particles are accelerated towards those particles within their communication grouping which have better fitness values The main advantage of such an approach over other global minimization strategies such as simulated annealing is that the large number of members that make up the particle swarm make the technique impressively resilient to the problem of local minima Artificial bee colony algorithm Karaboga 2005 Karaboga introduced ABC metaheuristic in 2005 as an answer to optimize numerical problems Inspired by honey bee foraging behavior Karabogas model had three components The employed onlooker and scout In practice the artificial scout bee would expose all food source positions solutions good or bad The employed bee would search for the shortest route to each position to extract the food amount quality of the source If the food was depleted from the source the employed bee would become a scout and randomly search for other food sources Each source that became abandoned created negative feedback meaning the answers found were poor solutions The onlooker bees wait for employed bees to either abandon a source or give information that the source has a large quantity of food and is worth sending additional resources to The more an onlooker bee is recruited the more positive the feedback is meaning that the answer is likely a good solution Artificial Swarm Intelligence 2015 Artificial Swarm Intelligence ASI is method of amplifying the collective intelligence of networked human groups using control algorithms modeled after natural swarms Sometimes referred to as Human Swarming or Swarm AI the technology connects groups of human participants into realtime systems that deliberate and converge on solutions as dynamic swarms when simultaneously presented with a question ASI has been used for a wide range of applications from enabling business teams to generate highly accurate financial forecasts to enabling sports fans to outperform Vegas betting markets ASI has also been used to enable groups of doctors to generate diagnoses with significantly higher accuracy than traditional methods ASI has been used by the Food and Agriculture Organization FAO of the United Nations to help forecast famines in hotspots around the world Applications Swarm Intelligencebased techniques can be used in a number of applications The US military is investigating swarm techniques for controlling unmanned vehicles The European Space Agency is thinking about an orbital swarm for selfassembly and interferometry NASA is investigating the use of swarm technology for planetary mapping A 1992 paper by M Anthony Lewis and George A Bekey discusses the possibility of using swarm intelligence to control nanobots within the body for the purpose of killing cancer tumors Conversely alRifaie and Aber have used stochastic diffusion search to help locate tumours Swarm intelligence SI is increasingly applied in Internet of Things IoT systems and by association to IntentBased Networking IBN due to its ability to handle complex distributed tasks through decentralized selforganizing algorithms Swarm intelligence has also been applied for data mining and cluster analysis Antbased models are further subject of modern management theory Antbased routing The use of swarm intelligence in telecommunication networks has also been researched in the form of antbased routing This was pioneered separately by Dorigo et al and HewlettPackard in the mid1990s with a number of variants existing Basically this uses a probabilistic routing table rewardingreinforcing the route successfully traversed by each ant a small control packet which flood the network Reinforcement of the route in the forwards reverse direction and both simultaneously have been researched backwards reinforcement requires a symmetric network and couples the two directions together forwards reinforcement rewards a route before the outcome is known but then one would pay for the cinema before one knows how good the film is As the system behaves stochastically and is therefore lacking repeatability there are large hurdles to commercial deployment Mobile media and new technologies have the potential to change the threshold for collective action due to swarm intelligence Rheingold 2002 P175 The location of transmission infrastructure for wireless communication networks is an important engineering problem involving competing objectives A minimal selection of locations or sites are required subject to providing adequate area coverage for users A very different antinspired swarm intelligence algorithm stochastic diffusion search SDS has been successfully used to provide a general model for this problem related to circle packing and set covering It has been shown that the SDS can be applied to identify suitable solutions even for large problem instances Airlines have also used antbased routing in assigning aircraft arrivals to airport gates At Southwest Airlines a software program uses swarm theory or swarm intelligencethe idea that a colony of ants works better than one alone Each pilot acts like an ant searching for the best airport gate The pilot learns from his experience whats the best for him and it turns out that thats the best solution for the airline Douglas A Lawson explains As a result the colony of pilots always go to gates they can arrive at and depart from quickly The program can even alert a pilot of plane backups before they happen We can anticipate that its going to happen so well have a gate available Lawson says Crowd simulation Artists are using swarm technology as a means of creating complex interactive systems or simulating crowds Instances The Lord of the Rings film trilogy made use of similar technology known as Massive software during battle scenes Swarm technology is particularly attractive because it is cheap robust and simple Stanley and Stella in Breaking the Ice was the first movie to make use of swarm technology for rendering realistically depicting the movements of groups of fish and birds using the Boids system Tim Burtons Batman Returns also made use of swarm technology for showing the movements of a group of bats Airlines have used swarm theory to simulate passengers boarding a plane Southwest Airlines researcher Douglas A Lawson used an antbased computer simulation employing only six interaction rules to evaluate boarding times using various boarding methodsMiller 2010 xiixviii Human swarming Networks of distributed users can be organized into human swarms through the implementation of realtime closedloop control systems Developed by Louis Rosenberg in 2015 human swarming also called artificial swarm intelligence allows the collective intelligence of interconnected groups of people online to be harnessed The collective intelligence of the group often exceeds the abilities of any one member of the group Stanford University School of Medicine published in 2018 a study showing that groups of human doctors when connected together by realtime swarming algorithms could diagnose medical conditions with substantially higher accuracy than individual doctors or groups of doctors working together using traditional crowdsourcing methods In one such study swarms of human radiologists connected together were tasked with diagnosing chest xrays and demonstrated a 33 reduction in diagnostic errors as compared to the traditional human methods and a 22 improvement over traditional machinelearning The University of California San Francisco UCSF School of Medicine released a preprint in 2021 about the diagnosis of MRI images by small groups of collaborating doctors The study showed a 23 increase in diagnostic accuracy when using Artificial Swarm Intelligence ASI technology compared to majority voting Swarm grammars Swarm grammars are swarms of stochastic grammars that can be evolved to describe complex properties such as found in art and architecture These grammars interact as agents behaving according to rules of swarm intelligence Such behavior can also suggest deep learning algorithms in particular when mapping of such swarms to neural circuits is considered Swarmic art In a series of works alRifaie et al have successfully used two swarm intelligence algorithmsone mimicking the behaviour of one species of ants Leptothorax acervorum foraging stochastic diffusion search SDS and the other algorithm mimicking the behaviour of birds flocking particle swarm optimization PSOto describe a novel integration strategy exploiting the local search properties of the PSO with global SDS behaviour The resulting hybrid algorithm is used to sketch novel drawings of an input image exploiting an artistic tension between the local behaviour of the birds flockingas they seek to follow the input sketchand the global behaviour of the ants foragingas they seek to encourage the flock to explore novel regions of the canvas The creativity of this hybrid swarm system has been analysed under the philosophical light of the rhizome in the context of Deleuzes Orchid and Wasp metaphor A more recent work of alRifaie et al Swarmic Sketches and Attention Mechanism introduces a novel approach deploying the mechanism of attention by adapting SDS to selectively attend to detailed areas of a digital canvas Once the attention of the swarm is drawn to a certain line within the canvas the capability of PSO is used to produce a swarmic sketch of the attended line The swarms move throughout the digital canvas in an attempt to satisfy their dynamic rolesattention to areas with more detailsassociated with them via their fitness function Having associated the rendering process with the concepts of attention the performance of the participating swarms creates a unique nonidentical sketch each time the artist swarms embark on interpreting the input line drawings In other works while PSO is responsible for the sketching process SDS controls the attention of the swarm In a similar work Swarmic Paintings and Colour Attention nonphotorealistic images are produced using SDS algorithm which in the context of this work is responsible for colour attention The computational creativity of the abovementioned systems are discussed in through the two prerequisites of creativity ie freedom and constraints within the swarm intelligences two infamous phases of exploration and exploitation Michael Theodore and Nikolaus Correll use swarm intelligent art installation to explore what it takes to have engineered systems to appear lifelike Notable researchers See also References Further reading Bonabeau Eric Dorigo Marco Theraulaz Guy 1999 Swarm Intelligence From Natural to Artificial Systems Oup USA ISBN 9780195131598 Kennedy James Eberhart Russell C 20010409 Swarm Intelligence Morgan Kaufmann ISBN 9781558605954 Engelbrecht Andries 20051216 Fundamentals of Computational Swarm Intelligence Wiley Sons ISBN 9780470091913 External links Marco Dorigo and Mauro Birattari 2007 Swarm intelligence in Scholarpedia Antoinette Brown Swarm Intelligence",
  },
  {
    title: "Fuzzy logic",
    originalContent:
      "Fuzzy logic is a form of manyvalued logic in which the truth value of variables may be any real number between 0 and 1 It is employed to handle the concept of partial truth where the truth value may range between completely true and completely false By contrast in Boolean logic the truth values of variables may only be the integer values 0 or 1 The term fuzzy logic was introduced with the 1965 proposal of fuzzy set theory by mathematician Lotfi Zadeh Fuzzy logic had however been studied since the 1920s as infinitevalued logicnotably by ukasiewicz and Tarski Fuzzy logic is based on the observation that people make decisions based on imprecise and nonnumerical information Fuzzy models or fuzzy sets are mathematical means of representing vagueness and imprecise information hence the term fuzzy These models have the capability of recognising representing manipulating interpreting and using data and information that are vague and lack certainty Fuzzy logic has been applied to many fields from control theory to artificial intelligence Overview Classical logic only permits conclusions that are either true or false However there are also propositions with variable answers which one might find when asking a group of people to identify a color In such instances the truth appears as the result of reasoning from inexact or partial knowledge in which the sampled answers are mapped on a spectrum Both degrees of truth and probabilities range between 0 and 1 and hence may seem identical at first but fuzzy logic uses degrees of truth as a mathematical model of vagueness while probability is a mathematical model of ignorance Applying truth values A basic application might characterize various subranges of a continuous variable For instance a temperature measurement for antilock brakes might have several separate membership functions defining particular temperature ranges needed to control the brakes properly Each function maps the same temperature value to a truth value in the 0 to 1 range These truth values can then be used to determine how the brakes should be controlled Fuzzy set theory provides a means for representing uncertainty Linguistic variables In fuzzy logic applications nonnumeric values are often used to facilitate the expression of rules and facts A linguistic variable such as age may accept values such as young and its antonym old Because natural languages do not always contain enough value terms to express a fuzzy value scale it is common practice to modify linguistic values with adjectives or adverbs For example we can use the hedges rather and somewhat to construct the additional values rather old or somewhat young Fuzzy systems Mamdani The most wellknown system is the Mamdani rulebased one It uses the following rules Fuzzify all input values into fuzzy membership functions Execute all applicable rules in the rulebase to compute the fuzzy output functions Defuzzify the fuzzy output functions to get crisp output values Fuzzification Fuzzification is the process of assigning the numerical input of a system to fuzzy sets with some degree of membership This degree of membership may be anywhere within the interval 01 If it is 0 then the value does not belong to the given fuzzy set and if it is 1 then the value completely belongs within the fuzzy set Any value between 0 and 1 represents the degree of uncertainty that the value belongs in the set These fuzzy sets are typically described by words and so by assigning the system input to fuzzy sets we can reason with it in a linguistically natural manner For example in the image below the meanings of the expressions cold warm and hot are represented by functions mapping a temperature scale A point on that scale has three truth valuesone for each of the three functions The vertical line in the image represents a particular temperature that the three arrows truth values gauge Since the red arrow points to zero this temperature may be interpreted as not hot ie this temperature has zero membership in the fuzzy set hot The orange arrow pointing at 02 may describe it as slightly warm and the blue arrow pointing at 08 fairly cold Therefore this temperature has 02 membership in the fuzzy set warm and 08 membership in the fuzzy set cold The degree of membership assigned for each fuzzy set is the result of fuzzification Fuzzy sets are often defined as triangle or trapezoidshaped curves as each value will have a slope where the value is increasing a peak where the value is equal to 1 which can have a length of 0 or greater and a slope where the value is decreasing They can also be defined using a sigmoid function One common case is the standard logistic function defined as S x 1 1 e x displaystyle Sxfrac 11ex which has the following symmetry property S x S x 1 displaystyle SxSx1 From this it follows that S x S x S y S y S z S z 1 displaystyle SxSxcdot SySycdot SzSz1 Fuzzy logic operators Fuzzy logic works with membership values in a way that mimics Boolean logic To this end replacements for basic operators gates AND OR NOT must be available There are several ways to this A common replacement is called the Zadeh operators For TRUE1 and FALSE0 the fuzzy expressions produce the same result as the Boolean expressions There are also other operators more linguistic in nature called hedges that can be applied These are generally adverbs such as very or somewhat which modify the meaning of a set using a mathematical formula However an arbitrary choice table does not always define a fuzzy logic function In the paper Zaitsev et al a criterion has been formulated to recognize whether a given choice table defines a fuzzy logic function and a simple algorithm of fuzzy logic function synthesis has been proposed based on introduced concepts of constituents of minimum and maximum A fuzzy logic function represents a disjunction of constituents of minimum where a constituent of minimum is a conjunction of variables of the current area greater than or equal to the function value in this area to the right of the function value in the inequality including the function value Another set of ANDOR operators is based on multiplication where Given any two of ANDORNOT it is possible to derive the third The generalization of AND is an instance of a tnorm IFTHEN rules IFTHEN rules map input or computed truth values to desired output truth values Example Given a certain temperature the fuzzy variable hot has a certain truth value which is copied to the high variable Should an output variable occur in several THEN parts then the values from the respective IF parts are combined using the OR operator Defuzzification The goal is to get a continuous variable from fuzzy truth values This would be easy if the output truth values were exactly those obtained from fuzzification of a given number Since however all output truth values are computed independently in most cases they do not represent such a set of numbers One has then to decide for a number that matches best the intention encoded in the truth value For example for several truth values of fan_speed an actual speed must be found that best fits the computed truth values of the variables slow moderate and so on There is no single algorithm for this purpose A common algorithm is For each truth value cut the membership function at this value Combine the resulting curves using the OR operator Find the centerofweight of the area under the curve The x position of this center is then the final output TakagiSugenoKang TSK The TSK system is similar to Mamdani but the defuzzification process is included in the execution of the fuzzy rules These are also adapted so that instead the consequent of the rule is represented through a polynomial function usually constant or linear An example of a rule with a constant output would beIn this case the output will be equal to the constant of the consequent eg 2 In most scenarios we would have an entire rule base with 2 or more rules If this is the case the output of the entire rule base will be the average of the consequent of each rule i Yi weighted according to the membership value of its antecedent hi i h i Y i i h i displaystyle frac sum _ih_icdot Y_isum _ih_i An example of a rule with a linear output would be insteadIn this case the output of the rule will be the result of function in the consequent The variables within the function represent the membership values after fuzzification not the crisp values Same as before in case we have an entire rule base with 2 or more rules the total output will be the weighted average between the output of each rule The main advantage of using TSK over Mamdani is that it is computationally efficient and works well within other algorithms such as PID control and with optimization algorithms It can also guarantee the continuity of the output surface However Mamdani is more intuitive and easier to work with by people Hence TSK is usually used within other complex methods such as in adaptive neuro fuzzy inference systems Forming a consensus of inputs and fuzzy rules Since the fuzzy system output is a consensus of all of the inputs and all of the rules fuzzy logic systems can be well behaved when input values are not available or are not trustworthy Weightings can be optionally added to each rule in the rulebase and weightings can be used to regulate the degree to which a rule affects the output values These rule weightings can be based upon the priority reliability or consistency of each rule These rule weightings may be static or can be changed dynamically even based upon the output from other rules Applications Fuzzy logic is used in control systems to allow experts to contribute vague rules such as if you are close to the destination station and moving fast increase the trains brake pressure these vague rules can then be numerically refined within the system Many of the early successful applications of fuzzy logic were implemented in Japan A first notable application was on the Sendai Subway 1000 series in which fuzzy logic was able to improve the economy comfort and precision of the ride It has also been used for handwriting recognition in Sony pocket computers helicopter flight aids subway system controls improving automobile fuel efficiency singlebutton washing machine controls automatic power controls in vacuum cleaners and early recognition of earthquakes through the Institute of Seismology Bureau of Meteorology Japan Artificial intelligence Neural networks based artificial intelligence and fuzzy logic are when analyzed the same thingthe underlying logic of neural networks is fuzzy A neural network will take a variety of valued inputs give them different weights in relation to each other combine intermediate values a certain number of times and arrive at a decision with a certain value Nowhere in that process is there anything like the sequences of eitheror decisions which characterize nonfuzzy mathematics computer programming and digital electronics In the 1980s researchers were divided about the most effective approach to machine learning decision tree learning or neural networks The former approach uses binary logic matching the hardware on which it runs but despite great efforts it did not result in intelligent systems Neural networks by contrast did result in accurate models of complex situations and soon found their way onto a multitude of electronic devices They can also now be implemented directly on analog microchips as opposed to the previous pseudoanalog implementations on digital chips The greater efficiency of these compensates for the intrinsic lesser accuracy of analog in various use cases Medical decision making Fuzzy logic is an important concept in medical decision making Since medical and healthcare data can be subjective or fuzzy applications in this domain have a great potential to benefit a lot by using fuzzylogicbased approaches Fuzzy logic can be used in many different aspects within the medical decision making framework Such aspects include in medical image analysis biomedical signal analysis segmentation of images or signals and feature extraction selection of images or signals The biggest question in this application area is how much useful information can be derived when using fuzzy logic A major challenge is how to derive the required fuzzy data This is even more challenging when one has to elicit such data from humans usually patients As has been said The envelope of what can be achieved and what cannot be achieved in medical diagnosis ironically is itself a fuzzy one How to elicit fuzzy data and how to validate the accuracy of the data is still an ongoing effort strongly related to the application of fuzzy logic The problem of assessing the quality of fuzzy data is a difficult one This is why fuzzy logic is a highly promising possibility within the medical decision making application area but still requires more research to achieve its full potential Imagebased computeraided diagnosis One of the common application areas of fuzzy logic is imagebased computeraided diagnosis in medicine Computeraided diagnosis is a computerized set of interrelated tools that can be used to aid physicians in their diagnostic decisionmaking Fuzzy databases Once fuzzy relations are defined it is possible to develop fuzzy relational databases The first fuzzy relational database FRDB appeared in Maria Zemankovas dissertation 1983 Later some other models arose like the BucklesPetry model the PradeTestemale Model the UmanoFukami model or the GEFRED model by J M Medina M A Vila et al Fuzzy querying languages have been defined such as the SQLf by P Bosc et al and the FSQL by J Galindo et al These languages define some structures in order to include fuzzy aspects in the SQL statements like fuzzy conditions fuzzy comparators fuzzy constants fuzzy constraints fuzzy thresholds linguistic labels etc Logical analysis In mathematical logic there are several formal systems of fuzzy logic most of which are in the family of tnorm fuzzy logics Propositional fuzzy logics The most important propositional fuzzy logics are Monoidal tnormbased propositional fuzzy logic MTL is an axiomatization of logic where conjunction is defined by a left continuous tnorm and implication is defined as the residuum of the tnorm Its models correspond to MTLalgebras that are prelinear commutative bounded integral residuated lattices Basic propositional fuzzy logic BL is an extension of MTL logic where conjunction is defined by a continuous tnorm and implication is also defined as the residuum of the tnorm Its models correspond to BLalgebras ukasiewicz fuzzy logic is the extension of basic fuzzy logic BL where standard conjunction is the ukasiewicz tnorm It has the axioms of basic fuzzy logic plus an axiom of double negation and its models correspond to MValgebras Gdel fuzzy logic is the extension of basic fuzzy logic BL where conjunction is the Gdel tnorm that is minimum It has the axioms of BL plus an axiom of idempotence of conjunction and its models are called Galgebras Product fuzzy logic is the extension of basic fuzzy logic BL where conjunction is the product tnorm It has the axioms of BL plus another axiom for cancellativity of conjunction and its models are called product algebras Fuzzy logic with evaluated syntax sometimes also called Pavelkas logic denoted by EV is a further generalization of mathematical fuzzy logic While the above kinds of fuzzy logic have traditional syntax and manyvalued semantics in EV syntax is also evaluated This means that each formula has an evaluation Axiomatization of EV stems from ukasziewicz fuzzy logic A generalization of the classical Gdel completeness theorem is provable in EV Predicate fuzzy logics Similar to the way predicate logic is created from propositional logic predicate fuzzy logics extend fuzzy systems by universal and existential quantifiers The semantics of the universal quantifier in tnorm fuzzy logics is the infimum of the truth degrees of the instances of the quantified subformula while the semantics of the existential quantifier is the supremum of the same Decidability Issues The notions of a decidable subset and recursively enumerable subset are basic ones for classical mathematics and classical logic Thus the question of a suitable extension of them to fuzzy set theory is a crucial one The first proposal in such a direction was made by E S Santos by the notions of fuzzy Turing machine Markov normal fuzzy algorithm and fuzzy program see Santos 1970 Successively L Biacino and G Gerla argued that the proposed definitions are rather questionable For example in one shows that the fuzzy Turing machines are not adequate for fuzzy language theory since there are natural fuzzy languages intuitively computable that cannot be recognized by a fuzzy Turing Machine Then they proposed the following definitions Denote by the set of rational numbers in 01 Then a fuzzy subset s S displaystyle rightarrow 01 of a set S is recursively enumerable if a recursive map h SN displaystyle rightarrow exists such that for every x in S the function hxn is increasing with respect to n and sx lim hxn We say that s is decidable if both s and its complement s are recursively enumerable An extension of such a theory to the general case of the Lsubsets is possible see Gerla 2006 The proposed definitions are well related to fuzzy logic Indeed the following theorem holds true provided that the deduction apparatus of the considered fuzzy logic satisfies some obvious effectiveness property Any axiomatizable fuzzy theory is recursively enumerable In particular the fuzzy set of logically true formulas is recursively enumerable in spite of the fact that the crisp set of valid formulas is not recursively enumerable in general Moreover any axiomatizable and complete theory is decidable It is an open question to give support for a Church thesis for fuzzy mathematics the proposed notion of recursive enumerability for fuzzy subsets is the adequate one In order to solve this an extension of the notions of fuzzy grammar and fuzzy Turing machine are necessary Another open question is to start from this notion to find an extension of Gdels theorems to fuzzy logic Compared to other logics Probability Fuzzy logic and probability address different forms of uncertainty While both fuzzy logic and probability theory can represent degrees of certain kinds of subjective belief fuzzy set theory uses the concept of fuzzy set membership ie how much an observation is within a vaguely defined set and probability theory uses the concept of subjective probability ie frequency of occurrence or likelihood of some event or condition The concept of fuzzy sets was developed in the midtwentieth century at Berkeley as a response to the lack of a probability theory for jointly modelling uncertainty and vagueness Bart Kosko claims in Fuzziness vs Probability that probability theory is a subtheory of fuzzy logic as questions of degrees of belief in mutuallyexclusive set membership in probability theory can be represented as certain cases of nonmutuallyexclusive graded membership in fuzzy theory In that context he also derives Bayes theorem from the concept of fuzzy subsethood Lotfi A Zadeh argues that fuzzy logic is different in character from probability and is not a replacement for it He fuzzified probability to fuzzy probability and also generalized it to possibility theory More generally fuzzy logic is one of many different extensions to classical logic intended to deal with issues of uncertainty outside of the scope of classical logic the inapplicability of probability theory in many domains and the paradoxes of DempsterShafer theory Ecorithms Computational theorist Leslie Valiant uses the term ecorithms to describe how many less exact systems and techniques like fuzzy logic and less robust logic can be applied to learning algorithms Valiant essentially redefines machine learning as evolutionary In general use ecorithms are algorithms that learn from their more complex environments hence eco to generalize approximate and simplify solution logic Like fuzzy logic they are methods used to overcome continuous variables or systems too complex to completely enumerate or understand discretely or exactly Ecorithms and fuzzy logic also have the common property of dealing with possibilities more than probabilities although feedback and feed forward basically stochastic weights are a feature of both when dealing with for example dynamical systems Gdel G logic Another logical system where truth values are real numbers between 0 and 1 and where AND OR operators are replaced with MIN and MAX is Gdels G logic This logic has many similarities with fuzzy logic but defines negation differently and has an internal implication Negation G displaystyle neg _G and implication G displaystyle xrightarrowG are defined as follows G u 1 if u 0 0 if u 0 u G v 1 if u v v if u v displaystyle beginalignedneg _Gubegincases1textif u00textif u0endcases3ptumathrel xrightarrowG vbegincases1textif uleq vvtextif uvendcasesendaligned which turns the resulting logical system into a model for intuitionistic logic making it particularly wellbehaved among all possible choices of logical systems with real numbers between 0 and 1 as truth values In this case implication may be interpreted as x is less true than y and negation as x is less true than 0 or x is strictly false and for any x displaystyle x and y displaystyle y we have that A N D x x G y A N D x y displaystyle ANDxxmathrel xrightarrowG yANDxy In particular in Gdel logic negation is no longer an involution and double negation maps any nonzero value to 1 Compensatory fuzzy logic Compensatory fuzzy logic CFL is a branch of fuzzy logic with modified rules for conjunction and disjunction When the truth value of one component of a conjunction or disjunction is increased or decreased the other component is decreased or increased to compensate This increase or decrease in truth value may be offset by the increase or decrease in another component An offset may be blocked when certain thresholds are met Proponents claim that CFL allows for better computational semantic behaviors and mimic natural language According to Jess Cejas Montero 2011 The Compensatory fuzzy logic consists of four continuous operators conjunction c disjunction d fuzzy strict order or and negation n The conjunction is the geometric mean and its dual as conjunctive and disjunctive operators Markup language standardization The IEEE 1855 the IEEE STANDARD 18552016 is about a specification language named Fuzzy Markup Language FML developed by the IEEE Standards Association FML allows modelling a fuzzy logic system in a humanreadable and hardware independent way FML is based on eXtensible Markup Language XML The designers of fuzzy systems with FML have a unified and highlevel methodology for describing interoperable fuzzy systems IEEE STANDARD 18552016 uses the W3C XML Schema definition language to define the syntax and semantics of the FML programs Prior to the introduction of FML fuzzy logic practitioners could exchange information about their fuzzy algorithms by adding to their software functions the ability to read correctly parse and store the result of their work in a form compatible with the Fuzzy Control Language FCL described and specified by Part 7 of IEC 61131 See also References Bibliography External links IEC 11317 CD1 Archived 20210304 at the Wayback Machine IEC 11317 CD1 PDF Fuzzy Logic article at Scholarpedia Modeling With Words article at Scholarpedia Fuzzy logic article at Stanford Encyclopedia of Philosophy Fuzzy Math Beginner level introduction to Fuzzy Logic Fuzziness and exactness Fuzziness in everyday life science religion ethics politics etc Fuzzylite A crossplatform free opensource Fuzzy Logic Control Library written in C Also has a very useful graphic user interface in QT4 More Flexible Machine Learning MIT describes one application Semantic Similarity Archived 20151004 at the Wayback Machine MIT provides details about fuzzy semantic similarity",
  },
  {
    title: "Evolutionary computation",
    originalContent:
      "In computer science evolutionary computation is a family of algorithms for global optimization inspired by biological evolution and the subfield of artificial intelligence and soft computing studying these algorithms In technical terms they are a family of populationbased trial and error problem solvers with a metaheuristic or stochastic optimization character In evolutionary computation an initial set of candidate solutions is generated and iteratively updated Each new generation is produced by stochastically removing less desired solutions and introducing small random changes as well as depending on the method mixing parental information In biological terminology a population of solutions is subjected to natural selection or artificial selection mutation and possibly recombination As a result the population will gradually evolve to increase in fitness in this case the chosen fitness function of the algorithm Evolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings making them popular in computer science Many variants and extensions exist suited to more specific families of problems and data structures Evolutionary computation is also sometimes used in evolutionary biology as an in silico experimental procedure to study common aspects of general evolutionary processes History The concept of mimicking evolutionary processes to solve problems originates before the advent of computers such as when Alan Turing proposed a method of genetic search in 1948 Turings Btype umachines resemble primitive neural networks and connections between neurons were learnt via a sort of genetic algorithm His Ptype umachines resemble a method for reinforcement learning where pleasure and pain signals direct the machine to learn certain behaviors However Turings paper went unpublished until 1968 and he died in 1954 so this early work had little to no effect on the field of evolutionary computation that was to develop Evolutionary computing as a field began in earnest in the 1950s and 1960s There were several independent attempts to use the process of evolution in computing at this time which developed separately for roughly 15 years Three branches emerged in different places to attain this goal evolution strategies evolutionary programming and genetic algorithms A fourth branch genetic programming eventually emerged in the early 1990s These approaches differ in the method of selection the permitted mutations and the representation of genetic data By the 1990s the distinctions between the historic branches had begun to blur and the term evolutionary computing was coined in 1991 to denote a field that exists over all four paradigms In 1962 Lawrence J Fogel initiated the research of Evolutionary Programming in the United States which was considered an artificial intelligence endeavor In this system finite state machines are used to solve a prediction problem these machines would be mutated adding or deleting states or changing the state transition rules and the best of these mutated machines would be evolved further in future generations The final finite state machine may be used to generate predictions when needed The evolutionary programming method was successfully applied to prediction problems system identification and automatic control It was eventually extended to handle time series data and to model the evolution of gaming strategies In 1964 Ingo Rechenberg and HansPaul Schwefel introduce the paradigm of evolution strategies in Germany Since traditional gradient descent techniques produce results that may get stuck in local minima Rechenberg and Schwefel proposed that random mutations applied to all parameters of some solution vector may be used to escape these minima Child solutions were generated from parent solutions and the more successful of the two was kept for future generations This technique was first used by the two to successfully solve optimization problems in fluid dynamics Initially this optimization technique was performed without computers instead relying on dice to determine random mutations By 1965 the calculations were performed wholly by machine John Henry Holland introduced genetic algorithms in the 1960s and it was further developed at the University of Michigan in the 1970s While the other approaches were focused on solving problems Holland primarily aimed to use genetic algorithms to study adaptation and determine how it may be simulated Populations of chromosomes represented as bit strings were transformed by an artificial selection process selecting for specific allele bits in the bit string Among other mutation methods interactions between chromosomes were used to simulate the recombination of DNA between different organisms While previous methods only tracked a single optimal organism at a time having children compete with parents Hollands genetic algorithms tracked large populations having many organisms compete each generation By the 1990s a new approach to evolutionary computation that came to be called genetic programming emerged advocated for by John Koza among others In this class of algorithms the subject of evolution was itself a program written in a highlevel programming language there had been some previous attempts as early as 1958 to use machine code but they met with little success For Koza the programs were Lisp Sexpressions which can be thought of as trees of subexpressions This representation permits programs to swap subtrees representing a sort of genetic mixing Programs are scored based on how well they complete a certain task and the score is used for artificial selection Sequence induction pattern recognition and planning were all successful applications of the genetic programming paradigm Many other figures played a role in the history of evolutionary computing although their work did not always fit into one of the major historical branches of the field The earliest computational simulations of evolution using evolutionary algorithms and artificial life techniques were performed by Nils Aall Barricelli in 1953 with first results published in 1954 Another pioneer in the 1950s was Alex Fraser who published a series of papers on simulation of artificial selection As academic interest grew dramatic increases in the power of computers allowed practical applications including the automatic evolution of computer programs Evolutionary algorithms are now used to solve multidimensional problems more efficiently than software produced by human designers and also to optimize the design of systems Techniques Evolutionary computing techniques mostly involve metaheuristic optimization algorithms Broadly speaking the field includes Agentbased modeling Ant colony optimization Artificial immune systems Artificial life also see digital organism Cultural algorithms Coevolutionary algorithm Differential evolution Dualphase evolution Estimation of distribution algorithm Evolutionary algorithm Evolutionary programming Evolution strategy Gene expression programming Genetic algorithm Genetic programming Grammatical evolution Learnable evolution model Learning classifier system Memetic algorithms Neuroevolution Particle swarm optimization Beetle antennae search Selforganization such as selforganizing maps competitive learning Swarm intelligence A thorough catalogue with many other recently proposed algorithms has been published in the Evolutionary Computation Bestiary It is important to note that many recent algorithms however have poor experimental validation Evolutionary algorithms Evolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction mutation recombination natural selection and survival of the fittest Candidate solutions to the optimization problem play the role of individuals in a population and the cost function determines the environment within which the solutions live see also fitness function Evolution of the population then takes place after the repeated application of the above operators In this process there are two main forces that form the basis of evolutionary systems Recombination eg crossover and mutation create the necessary diversity and thereby facilitate novelty while selection acts as a force increasing quality Many aspects of such an evolutionary process are stochastic Changed pieces of information due to recombination and mutation are randomly chosen On the other hand selection operators can be either deterministic or stochastic In the latter case individuals with a higher fitness have a higher chance to be selected than individuals with a lower fitness but typically even the weak individuals have a chance to become a parent or to survive Evolutionary algorithms and biology Genetic algorithms deliver methods to model biological systems and systems biology that are linked to the theory of dynamical systems since they are used to predict the future states of the system This is just a vivid but perhaps misleading way of drawing attention to the orderly wellcontrolled and highly structured character of development in biology However the use of algorithms and informatics in particular of computational theory beyond the analogy to dynamical systems is also relevant to understand evolution itself This view has the merit of recognizing that there is no central control of development organisms develop as a result of local interactions within and between cells The most promising ideas about programdevelopment parallels seem to us to be ones that point to an apparently close analogy between processes within cells and the lowlevel operation of modern computers Thus biological systems are like computational machines that process input information to compute next states such that biological systems are closer to a computation than classical dynamical system Furthermore following concepts from computational theory micro processes in biological organisms are fundamentally incomplete and undecidable completeness logic implying that there is more than a crude metaphor behind the analogy between cells and computers The analogy to computation extends also to the relationship between inheritance systems and biological structure which is often thought to reveal one of the most pressing problems in explaining the origins of life Evolutionary automata a generalization of Evolutionary Turing machines have been introduced in order to investigate more precisely properties of biological and evolutionary computation In particular they allow to obtain new results on expressiveness of evolutionary computation This confirms the initial result about undecidability of natural evolution and evolutionary algorithms and processes Evolutionary finite automata the simplest subclass of Evolutionary automata working in terminal mode can accept arbitrary languages over a given alphabet including nonrecursively enumerable eg diagonalization language and recursively enumerable but not recursive languages eg language of the universal Turing machine Notable practitioners The list of active researchers is naturally dynamic and nonexhaustive A network analysis of the community was published in 2007 Kalyanmoy Deb Kenneth A De Jong Peter J Fleming David B Fogel Stephanie Forrest David E Goldberg John Henry Holland Theo Jansen John Koza Zbigniew Michalewicz Melanie Mitchell Peter Nordin Riccardo Poli Ingo Rechenberg HansPaul Schwefel Journals While articles on or using evolutionary computation permeate the literature several journals are dedicated to evolutionary computation Evolutionary Computation journal founded 1993 Artificial Life journal founded 1993 IEEE Transactions on Evolutionary Computation founded 1997 Genetic Programming and Evolvable Machines founded 2000 Swarm Intelligence founded 2007 Evolutionary Intelligence founded 2008 Journal of Artificial Evolution and Applications 20082010 Memetic Computing founded 2009 International Journal of Applied Evolutionary Computation founded 2010 Swarm and Evolutionary Computation founded 2011 International Journal of Swarm Intelligence and Evolutionary Computation founded 2012 Conferences The main conferences in the evolutionary computation area include ACM Genetic and Evolutionary Computation Conference GECCO IEEE Congress on Evolutionary Computation CEC EvoStar which comprises four conferences EuroGP EvoApplications EvoCOP and EvoMUSART Parallel Problem Solving from Nature PPSN See also External links Article in the Stanford Encyclopedia of Philosophy about Biological Information English Bibliography Th Bck DB Fogel and Z Michalewicz Editors Handbook of Evolutionary Computation 1997 ISBN 0750303921 Th Bck and HP Schwefel An overview of evolutionary algorithms for parameter optimization Archived July 12 2018 at the Wayback Machine Evolutionary Computation 11123 1993 W Banzhaf P Nordin RE Keller and FD Francone Genetic Programming An Introduction Morgan Kaufmann 1998 S Cagnoni et al RealWorld Applications of Evolutionary Computing SpringerVerlag Lecture Notes in Computer Science Berlin 2000 R Chiong Th Weise Z Michalewicz Editors Variants of Evolutionary Algorithms for RealWorld Applications Springer 2012 ISBN 3642234232 K A De Jong Evolutionary computation a unified approach MIT Press Cambridge MA 2006 A E Eiben and JE Smith From evolutionary computation to the evolution of things Nature 521476482 doi101038nature14544 2015 A E Eiben and JE Smith Introduction to Evolutionary Computing Springer First edition 2003 Second edition 2015 D B Fogel Evolutionary Computation Toward a New Philosophy of Machine Intelligence IEEE Press Piscataway NJ 1995 L J Fogel A J Owens and M J Walsh Artificial Intelligence through Simulated Evolution New York John Wiley 1966 D E Goldberg Genetic algorithms in search optimization and machine learning Addison Wesley 1989 J H Holland Adaptation in natural and artificial systems University of Michigan Press Ann Arbor 1975 P Hingston L Barone and Z Michalewicz Editors Design by Evolution Natural Computing Series 2008 Springer ISBN 3540741097 J R Koza Genetic Programming On the Programming of Computers by means of Natural Evolution MIT Press Massachusetts 1992 FJ Lobo CF Lima Z Michalewicz Editors Parameter Setting in Evolutionary Algorithms Springer 2010 ISBN 3642088929 Z Michalewicz Genetic Algorithms Data Structures Evolution Programs 1996 Springer ISBN 3540606769 Z Michalewicz and DB Fogel How to Solve It Modern Heuristics Springer 2004 ISBN 9783540224945 I Rechenberg Evolutionstrategie Optimierung Technischer Systeme nach Prinzipien des Biologischen Evolution FrommanHozlboog Verlag Stuttgart 1973 in German HP Schwefel Numerical Optimization of Computer Models John Wiley Sons NewYork 1981 1995 2nd edition D Simon Evolutionary Optimization Algorithms Archived March 10 2014 at the Wayback Machine Wiley 2013 M Sipper W Fu K Ahuja J H Moore 2018 Investigating the parameter space of evolutionary algorithms BioData Mining 11 2 doi101186s130400180164x PMC 5816380 PMID 29467825 Y Zhang S Li 2017 PSA A novel optimization algorithm based on survival rules of porcellio scaber arXiv170909840 csNE References",
  },
  {
    title: "Supergravity",
    originalContent:
      "In theoretical physics supergravity supergravity theory SUGRA for short is a modern field theory that combines the principles of supersymmetry and general relativity this is in contrast to nongravitational supersymmetric theories such as the Minimal Supersymmetric Standard Model Supergravity is the gauge theory of local supersymmetry Since the supersymmetry SUSY generators form together with the Poincar algebra a superalgebra called the superPoincar algebra supersymmetry as a gauge theory makes gravity arise in a natural way Gravitons Like all covariant approaches to quantum gravity supergravity contains a spin2 field whose quantum is the graviton Supersymmetry requires the graviton field to have a superpartner This field has spin 32 and its quantum is the gravitino The number of gravitino fields is equal to the number of supersymmetries History Gauge supersymmetry The first theory of local supersymmetry was proposed by Dick Arnowitt and Pran Nath in 1975 and was called gauge supersymmetry Supergravity The first model of 4dimensional supergravity without this denotation was formulated by Dmitri Vasilievich Volkov and Vyacheslav A Soroka in 1973 emphasizing the importance of spontaneous supersymmetry breaking for the possibility of a realistic model The minimal version of 4dimensional supergravity with unbroken local supersymmetry was constructed in detail in 1976 by Dan Freedman Sergio Ferrara and Peter van Nieuwenhuizen In 2019 the three were awarded a special Breakthrough Prize in Fundamental Physics for the discovery The key issue of whether or not the spin 32 field is consistently coupled was resolved in the nearly simultaneous paper by Deser and Zumino which independently proposed the minimal 4dimensional model It was quickly generalized to many different theories in various numbers of dimensions and involving additional N supersymmetries Supergravity theories with N1 are usually referred to as extended supergravity SUEGRA Some supergravity theories were shown to be related to certain higherdimensional supergravity theories via dimensional reduction eg N1 11dimensional supergravity is dimensionally reduced on T7 to 4dimensional ungauged N 8 supergravity The resulting theories were sometimes referred to as KaluzaKlein theories as Kaluza and Klein constructed in 1919 a 5dimensional gravitational theory that when dimensionally reduced on a circle its 4dimensional nonmassive modes describe electromagnetism coupled to gravity mSUGRA mSUGRA means minimal SUper GRAvity The construction of a realistic model of particle interactions within the N 1 supergravity framework where supersymmetry SUSY breaks by a super Higgs mechanism carried out by Ali Chamseddine Richard Arnowitt and Pran Nath in 1982 Collectively now known as minimal supergravity Grand Unification Theories mSUGRA GUT gravity mediates the breaking of SUSY through the existence of a hidden sector mSUGRA naturally generates the Soft SUSY breaking terms which are a consequence of the Super Higgs effect Radiative breaking of electroweak symmetry through Renormalization Group Equations RGEs follows as an immediate consequence Due to its predictive power requiring only four input parameters and a sign to determine the low energy phenomenology from the scale of Grand Unification its interest is a widely investigated model of particle physics 11D the maximal SUGRA One of these supergravities the 11dimensional theory generated considerable excitement as the first potential candidate for the theory of everything This excitement was built on four pillars two of which have now been largely discredited Werner Nahm showed 11 dimensions as the largest number of dimensions consistent with a single graviton and more dimensions will show particles with spins greater than 2 However if two of these dimensions are timelike these problems are avoided in 12 dimensions Itzhak Bars gives this emphasis In 1981 Ed Witten showed 11 as the smallest number of dimensions big enough to contain the gauge groups of the Standard Model namely SU3 for the strong interactions and SU2 times U1 for the electroweak interactions Many techniques exist to embed the standard model gauge group in supergravity in any number of dimensions like the obligatory gauge symmetry in type I and heterotic string theories and obtained in type II string theory by compactification on certain CalabiYau manifolds The Dbranes engineer gauge symmetries too In 1978 Eugne Cremmer Bernard Julia and Jol Scherk CJS found the classical action for an 11dimensional supergravity theory This remains today the only known classical 11dimensional theory with local supersymmetry and no fields of spin higher than two Other 11dimensional theories known and quantummechanically inequivalent reduce to the CJS theory when one imposes the classical equations of motion However in the mid1980s Bernard de Wit and Hermann Nicolai found an alternate theory in D11 Supergravity with Local SU8 Invariance While not manifestly Lorentzinvariant it is in many ways superior because it dimensionallyreduces to the 4dimensional theory without recourse to the classical equations of motion In 1980 Peter Freund and M A Rubin showed that compactification from 11 dimensions preserving all the SUSY generators could occur in two ways leaving only 4 or 7 macroscopic dimensions the others compact The noncompact dimensions have to form an antide Sitter space There are many possible compactifications but the FreundRubin compactifications invariance under all of the supersymmetry transformations preserves the action Finally the first two results each appeared to establish 11 dimensions the third result appeared to specify the theory and the last result explained why the observed universe appears to be fourdimensional Many of the details of the theory were fleshed out by Peter van Nieuwenhuizen Sergio Ferrara and Daniel Z Freedman The end of the SUGRA era The initial excitement over 11dimensional supergravity soon waned as various failings were discovered and attempts to repair the model failed as well Problems included The compact manifolds which were known at the time and which contained the standard model were not compatible with supersymmetry and could not hold quarks or leptons One suggestion was to replace the compact dimensions with the 7sphere with the symmetry group SO8 or the squashed 7sphere with symmetry group SO5 times SU2 Until recently the physical neutrinos seen in experiments were believed to be massless and appeared to be lefthanded a phenomenon referred to as the chirality of the Standard Model It was very difficult to construct a chiral fermion from a compactification the compactified manifold needed to have singularities but physics near singularities did not begin to be understood until the advent of orbifold conformal field theories in the late 1980s Supergravity models generically result in an unrealistically large cosmological constant in four dimensions and that constant is difficult to remove and so require finetuning This is still a problem today Quantization of the theory led to quantum field theory gauge anomalies rendering the theory inconsistent In the intervening years physicists have learned how to cancel these anomalies Some of these difficulties could be avoided by moving to a 10dimensional theory involving superstrings However by moving to 10 dimensions one loses the sense of uniqueness of the 11dimensional theory The core breakthrough for the 10dimensional theory known as the first superstring revolution was a demonstration by Michael B Green John H Schwarz and David Gross that there are only three supergravity models in 10 dimensions which have gauge symmetries and in which all of the gauge and gravitational anomalies cancel These were theories built on the groups SO32 and E 8 E 8 displaystyle E_8times E_8 the direct product of two copies of E8 Today we know that using Dbranes for example gauge symmetries can be introduced in other 10dimensional theories as well The second superstring revolution Initial excitement about the 10dimensional theories and the string theories that provide their quantum completion died by the end of the 1980s There were too many CalabiYaus to compactify on many more than Yau had estimated as he admitted in December 2005 at the 23rd International Solvay Conference in Physics None quite gave the standard model but it seemed as though one could get close with enough effort in many distinct ways Plus no one understood the theory beyond the regime of applicability of string perturbation theory There was a comparatively quiet period at the beginning of the 1990s however several important tools were developed For example it became apparent that the various superstring theories were related by string dualities some of which relate weak stringcoupling perturbative physics in one model with strong stringcoupling nonperturbative in another Then the second superstring revolution occurred Joseph Polchinski realized that obscure string theory objects called Dbranes which he discovered six years earlier equate to stringy versions of the pbranes known in supergravity theories String theory perturbation didnt restrict these pbranes Thanks to supersymmetry pbranes in supergravity gained understanding well beyond the limits of string theory Armed with this new nonperturbative tool Edward Witten and many others could show all of the perturbative string theories as descriptions of different states in a single theory that Witten named Mtheory Furthermore he argued that Mtheorys long wavelength limit ie when the quantum wavelength associated to objects in the theory appear much larger than the size of the 11th dimension needs 11dimensional supergravity descriptors that fell out of favor with the first superstring revolution 10 years earlier accompanied by the 2 and 5branes Therefore supergravity comes full circle and uses a common framework in understanding features of string theories Mtheory and their compactifications to lower spacetime dimensions Relation to superstrings The term low energy limits labels some 10dimensional supergravity theories These arise as the massless treelevel approximation of string theories True effective field theories of string theories rather than truncations are rarely available Due to string dualities the conjectured 11dimensional Mtheory is required to have 11dimensional supergravity as a low energy limit However this doesnt necessarily mean that string theoryMtheory is the only possible UV completion of supergravity supergravity research is useful independent of those relations 4D N 1 SUGRA Before we move on to SUGRA proper lets recapitulate some important details about general relativity We have a 4D differentiable manifold M with a Spin31 principal bundle over it This principal bundle represents the local Lorentz symmetry In addition we have a vector bundle T over the manifold with the fiber having four real dimensions and transforming as a vector under Spin31 We have an invertible linear map from the tangent bundle TM to T This map is the vierbein The local Lorentz symmetry has a gauge connection associated with it the spin connection The following discussion will be in superspace notation as opposed to the component notation which isnt manifestly covariant under SUSY There are actually many different versions of SUGRA out there which are inequivalent in the sense that their actions and constraints upon the torsion tensor are different but ultimately equivalent in that we can always perform a field redefinition of the supervierbeins and spin connection to get from one version to another In 4D N1 SUGRA we have a 44 real differentiable supermanifold M ie we have 4 real bosonic dimensions and 4 real fermionic dimensions As in the nonsupersymmetric case we have a Spin31 principal bundle over M We have an R44 vector bundle T over M The fiber of T transforms under the local Lorentz group as follows the four real bosonic dimensions transform as a vector and the four real fermionic dimensions transform as a Majorana spinor This Majorana spinor can be reexpressed as a complex lefthanded Weyl spinor and its complex conjugate righthanded Weyl spinor theyre not independent of each other We also have a spin connection as before We will use the following conventions the spatial both bosonic and fermionic indices will be indicated by M N The bosonic spatial indices will be indicated by the lefthanded Weyl spatial indices by and the righthanded Weyl spatial indices by displaystyle dot alpha displaystyle dot beta The indices for the fiber of T will follow a similar notation except that they will be hatted like this M displaystyle hat Mhat alpha See van der Waerden notation for more details M displaystyle Mmu alpha dot alpha The supervierbein is denoted by e N M displaystyle e_Nhat M and the spin connection by M N P displaystyle omega _hat Mhat NP The inverse supervierbein is denoted by E M N displaystyle E_hat MN The supervierbein and spin connection are real in the sense that they satisfy the reality conditions e N M x e N M x displaystyle e_Nhat Mxoverline theta theta e_Nhat Mxtheta overline theta where displaystyle mu mu displaystyle alpha dot alpha and displaystyle dot alpha alpha and x x displaystyle omega xoverline theta theta omega xtheta overline theta The covariant derivative is defined as D M f E M N N f N f displaystyle D_hat MfE_hat MNleftpartial _Nfomega _Nfright The covariant exterior derivative as defined over supermanifolds needs to be super graded This means that every time we interchange two fermionic indices we pick up a 1 sign factor instead of 1 The presence or absence of R symmetries is optional but if Rsymmetry exists the integrand over the full superspace has to have an Rcharge of 0 and the integrand over chiral superspace has to have an Rcharge of 2 A chiral superfield X is a superfield which satisfies D X 0 displaystyle overline D_hat dot alpha X0 In order for this constraint to be consistent we require the integrability conditions that D D c D displaystyle leftoverline D_hat dot alpha overline D_hat dot beta rightc_hat dot alpha hat dot beta hat dot gamma overline D_hat dot gamma for some coefficients c Unlike nonSUSY GR the torsion has to be nonzero at least with respect to the fermionic directions Already even in flat superspace D e D e 0 displaystyle D_hat alpha e_hat dot alpha overline D_hat dot alpha e_hat alpha neq 0 In one version of SUGRA but certainly not the only one we have the following constraints upon the torsion tensor T _ _ _ 0 displaystyle T_hat underline alpha hat underline beta hat underline gamma 0 T 0 displaystyle T_hat alpha hat beta hat mu 0 T 0 displaystyle T_hat dot alpha hat dot beta hat mu 0 T 2 i displaystyle T_hat alpha hat dot beta hat mu 2isigma _hat alpha hat dot beta hat mu T _ 0 displaystyle T_hat mu hat underline alpha hat nu 0 T 0 displaystyle T_hat mu hat nu hat rho 0 Here _ displaystyle underline alpha is a shorthand notation to mean the index runs over either the left or right Weyl spinors The superdeterminant of the supervierbein e displaystyle lefteright gives us the volume factor for M Equivalently we have the volume 44superform e 0 e 3 e 1 e 2 e 1 e 2 displaystyle ehat mu 0wedge cdots wedge ehat mu 3wedge ehat alpha 1wedge ehat alpha 2wedge ehat dot alpha 1wedge ehat dot alpha 2 If we complexify the superdiffeomorphisms there is a gauge where E 0 displaystyle E_hat dot alpha mu 0 E 0 displaystyle E_hat dot alpha beta 0 and E displaystyle E_hat dot alpha dot beta delta _dot alpha dot beta The resulting chiral superspace has the coordinates x and R is a scalar valued chiral superfield derivable from the supervielbeins and spin connection If f is any superfield D 2 8 R f displaystyle leftbar D28Rrightf is always a chiral superfield The action for a SUGRA theory with chiral superfields X is given by S d 4 x d 2 2 E 3 8 D 2 8 R e K X X 3 W X c c displaystyle Sint d4xd2Theta 2mathcal Eleftfrac 38leftbar D28RrighteKbar XX3WXrightcc where K is the Khler potential and W is the superpotential and E displaystyle mathcal E is the chiral volume factor Unlike the case for flat superspace adding a constant to either the Khler or superpotential is now physical A constant shift to the Khler potential changes the effective Planck constant while a constant shift to the superpotential changes the effective cosmological constant As the effective Planck constant now depends upon the value of the chiral superfield X we need to rescale the supervierbeins a field redefinition to get a constant Planck constant This is called the Einstein frame N 8 supergravity in 4 dimensions N 8 supergravity is the most symmetric quantum field theory which involves gravity and a finite number of fields It can be found from a dimensional reduction of 11D supergravity by making the size of 7 of the dimensions go to zero It has 8 supersymmetries which is the most any gravitational theory can have since there are 8 halfsteps between spin 2 and spin 2 A graviton has the highest spin in this theory which is a spin 2 particle More supersymmetries would mean the particles would have superpartners with spins higher than 2 The only theories with spins higher than 2 which are consistent involve an infinite number of particles such as string theory and higherspin theories Stephen Hawking in his A Brief History of Time speculated that this theory could be the Theory of Everything However in later years this was abandoned in favour of string theory There has been renewed interest in the 21st century with the possibility that this theory may be finite Higherdimensional SUGRA Higherdimensional SUGRA is the higherdimensional supersymmetric generalization of general relativity Supergravity can be formulated in any number of dimensions up to eleven Higherdimensional SUGRA focuses upon supergravity in greater than four dimensions The number of supercharges in a spinor depends on the dimension and the signature of spacetime The supercharges occur in spinors Thus the limit on the number of supercharges cannot be satisfied in a spacetime of arbitrary dimension Some theoretical examples in which this is satisfied are 12dimensional twotime theory 11dimensional maximal supergravity 10dimensional supergravity theories Type IIA supergravity N 1 1 Type IIB supergravity N 2 0 Type I supergravity N 1 0 9d supergravity theories Maximal 9d supergravity from 10d Tduality N 1 Gauged supergravity The supergravity theories that have attracted the most interest contain no spins higher than two This means in particular that they do not contain any fields that transform as symmetric tensors of rank higher than two under Lorentz transformations The consistency of interacting higher spin field theories is however presently a field of very active interest See also References Bibliography Historical Volkov DV Soroka VA 1973 Higgs effect for goldstone particles with spin 12 Supersymmetry and Quantum Field Theory Lecture Notes in Physics Vol 18 pp 529533 Bibcode1973JETPL18312V doi101007BFb0105271 ISBN 9783540646235 cite book journal ignored help Nath P Arnowitt R 1975 Generalized supergauge symmetry as a new framework for unified gauge theories Physics Letters B 56 2 177 Bibcode1975PhLB56177N doi101016037026937590297x Freedman DZ van Nieuwenhuizen P Ferrara S 1976 Progress toward a theory of supergravity Physical Review D 13 12 32143218 Bibcode1976PhRvD133214F doi101103physrevd133214 Cremmer E Julia B Scherk J 1978 Supergravity in theory in 11 dimensions Physics Letters B 76 4 409412 Bibcode1978PhLB76409C doi1010160370269378908948 Freund P Rubin M 1980 Dynamics of dimensional reduction Physics Letters B 97 2 233235 Bibcode1980PhLB97233F doi1010160370269380905900 Chamseddine A H Arnowitt R Nath Pran 1982 Locally supersymmetric grand unification Physical Review Letters 49 14 970974 Bibcode1982PhRvL49970C doi101103PhysRevLett49970 Green Michael B Schwarz John H 1984 Anomaly cancellation in supersymmetric D 10 gauge theory and superstring theory Physics Letters B 149 13 117122 Bibcode1984PhLB149117G doi101016037026938491565x Deser S 2018 A brief history and geography of supergravity The first 3 weeks and after PDF The European Physical Journal H 43 3 281291 arXiv170405886 Bibcode2018EPJH43281D doi101140epjhe2018900053 S2CID 119428513 Duplij S 2019 Supergravity was discovered by DV Volkov and VA Soroka in 1973 wasnt it East European Journal of Physics 3 8182 arXiv191003259 doi1026565231243342019310 General de Wit Bernard 2002 Supergravity arXivhepth0212245 Pran Nath 2017 Supersymmetry Supergravity and Unification Cambridge University Press ISBN 9780521197021 Martin Stephen P 1998 A Supersymmetry Primer In Kane Gordon L ed Perspectives on Supersymmetry Advanced Series on Directions in High Energy Physics Vol 18 World Scientific pp 198 arXivhepph9709356 doi1011429789812839657_0001 ISBN 9789810235536 S2CID 118973381 Drees Manuel Godbole Rohini M Roy Probir 2004 Theory and Phenomenology of Sparticles World Scientific ISBN 9810237391 Bilal Adel 2001 Introduction to Supersymmetry arXivhepth0101055 Brandt Friedemann 2002 Lectures on Supergravity Fortschritte der Physik 50 1011 11261172 arXivhepth0204035 Bibcode2002ForPh501126B doi101002152139782002105010111126AIDPROP112630CO2B S2CID 15471713 Sezgin Ergin 2023 Survey of supergravities arXiv231206754 hepth Further reading DallAgata G Zagermann M Supergravity From First Principles to Modern Applications Springer 2021 ISBN 9783662639788 Freedman D Z Van Proeyen A Supergravity Cambridge University Press Cambridge 2012 ISBN 9780521194013 Lauria E Van Proeyen A N 2 Supergravity in D 4 5 6 Dimensions Springer 2020 ISBN 9783030337551 Nstase H Introduction to Supergravity and Its Applications 2024 ISBN 9781009445597 Nath P Supersymmetry Supergravity and Unification Cambridge University Press Cambridge 2016 ISBN 9780521197021 Tanii Y Introduction to Supergravity Springer 2014 ISBN 9784431548270 Rausch de Traubenberg M Valenzuela M A Supergravity Primer World Scientific Press Singapore 2019 ISBN 9789811210518 Wess P Introduction To Supersymmetry And Supergravity World Scientific Press Singapore 1990 ISBN 9789810200985 Wess P Bagger J Supersymmetry and Supergravity Princeton University Press Princeton 1992 ISBN 9780691025308 External links Quotations related to Supergravity at Wikiquote",
  },
  {
    title: "Black hole thermodynamics",
    originalContent:
      "In physics black hole thermodynamics is the area of study that seeks to reconcile the laws of thermodynamics with the existence of black hole event horizons As the study of the statistical mechanics of blackbody radiation led to the development of the theory of quantum mechanics the effort to understand the statistical mechanics of black holes has had a deep impact upon the understanding of quantum gravity leading to the formulation of the holographic principle Overview The second law of thermodynamics requires that black holes have entropy If black holes carried no entropy it would be possible to violate the second law by throwing mass into the black hole The increase of the entropy of the black hole more than compensates for the decrease of the entropy carried by the object that was swallowed In 1972 Jacob Bekenstein conjectured that black holes should have an entropy proportional to the area of the event horizon where by the same year he proposed nohair theorems In 1973 Bekenstein suggested ln 2 08 0276 displaystyle frac ln 208pi approx 0276 as the constant of proportionality asserting that if the constant was not exactly this it must be very close to it The next year in 1974 Stephen Hawking showed that black holes emit thermal Hawking radiation corresponding to a certain temperature Hawking temperature Using the thermodynamic relationship between energy temperature and entropy Hawking was able to confirm Bekensteins conjecture and fix the constant of proportionality at 1 4 displaystyle 14 S BH k B A 4 P 2 displaystyle S_textBHfrac k_textBA4ell _textP2 where A displaystyle A is the area of the event horizon k B displaystyle k_textB is the Boltzmann constant and P G c 3 displaystyle ell _textPsqrt Ghbar c3 is the Planck length This is often referred to as the BekensteinHawking formula The subscript BH either stands for black hole or BekensteinHawking The black hole entropy is proportional to the area of its event horizon A displaystyle A The fact that the black hole entropy is also the maximal entropy that can be obtained by the Bekenstein bound wherein the Bekenstein bound becomes an equality was the main observation that led to the holographic principle This area relationship was generalized to arbitrary regions via the RyuTakayanagi formula which relates the entanglement entropy of a boundary conformal field theory to a specific surface in its dual gravitational theory Although Hawkings calculations gave further thermodynamic evidence for black hole entropy until 1995 no one was able to make a controlled calculation of black hole entropy based on statistical mechanics which associates entropy with a large number of microstates In fact so called nohair theorems appeared to suggest that black holes could have only a single microstate The situation changed in 1995 when Andrew Strominger and Cumrun Vafa calculated the right BekensteinHawking entropy of a supersymmetric black hole in string theory using methods based on Dbranes and string duality Their calculation was followed by many similar computations of entropy of large classes of other extremal and nearextremal black holes and the result always agreed with the BekensteinHawking formula However for the Schwarzschild black hole viewed as the most farfromextremal black hole the relationship between micro and macrostates has not been characterized Efforts to develop an adequate answer within the framework of string theory continue In loop quantum gravity LQG it is possible to associate a geometrical interpretation with the microstates these are the quantum geometries of the horizon LQG offers a geometric explanation of the finiteness of the entropy and of the proportionality of the area of the horizon It is possible to derive from the covariant formulation of full quantum theory spinfoam the correct relation between energy and area 1st law the Unruh temperature and the distribution that yields Hawking entropy The calculation makes use of the notion of dynamical horizon and is done for nonextremal black holes There seems to be also discussed the calculation of BekensteinHawking entropy from the point of view of loop quantum gravity The current accepted microstate ensemble for black holes is the microcanonical ensemble The partition function for black holes results in a negative heat capacity In canonical ensembles there is limitation for a positive heat capacity whereas microcanonical ensembles can exist at a negative heat capacity The laws of black hole mechanics The four laws of black hole mechanics are physical properties that black holes are believed to satisfy The laws analogous to the laws of thermodynamics were discovered by Jacob Bekenstein Brandon Carter and James Bardeen Further considerations were made by Stephen Hawking Statement of the laws The laws of black hole mechanics are expressed in geometrized units The zeroth law The horizon has constant surface gravity for a stationary black hole The first law For perturbations of stationary black holes the change of energy is related to change of area angular momentum and electric charge by d E 8 d A d J d Q displaystyle dEfrac kappa 8pi dAOmega dJPhi dQ where E displaystyle E is the energy displaystyle kappa is the surface gravity A displaystyle A is the horizon area displaystyle Omega is the angular velocity J displaystyle J is the angular momentum displaystyle Phi is the electrostatic potential and Q displaystyle Q is the electric charge The second law The horizon area is assuming the weak energy condition a nondecreasing function of time d A d t 0 displaystyle frac dAdtgeq 0 This law was superseded by Hawkings discovery that black holes radiate which causes both the black holes mass and the area of its horizon to decrease over time The third law It is not possible to form a black hole with vanishing surface gravity That is 0 displaystyle kappa 0 cannot be achieved Discussion of the laws The zeroth law The zeroth law is analogous to the zeroth law of thermodynamics which states that the temperature is constant throughout a body in thermal equilibrium It suggests that the surface gravity is analogous to temperature T constant for thermal equilibrium for a normal system is analogous to displaystyle kappa constant over the horizon of a stationary black hole The first law The left side d E displaystyle dE is the change in energy proportional to mass Although the first term does not have an immediately obvious physical interpretation the second and third terms on the right side represent changes in energy due to rotation and electromagnetism Analogously the first law of thermodynamics is a statement of energy conservation which contains on its right side the term T d S displaystyle TdS The second law The second law is the statement of Hawkings area theorem Analogously the second law of thermodynamics states that the change in entropy in an isolated system will be greater than or equal to 0 for a spontaneous process suggesting a link between entropy and the area of a black hole horizon However this version violates the second law of thermodynamics by matter losing its entropy as it falls in giving a decrease in entropy However generalizing the second law as the sum of black hole entropy and outside entropy shows that the second law of thermodynamics is not violated in a system including the universe beyond the horizon The generalized second law of thermodynamics GSL was needed to present the second law of thermodynamics as valid This is because the second law of thermodynamics as a result of the disappearance of entropy near the exterior of black holes is not useful The GSL allows for the application of the law because now the measurement of interior common entropy is possible The validity of the GSL can be established by studying an example such as looking at a system having entropy that falls into a bigger nonmoving black hole and establishing upper and lower entropy bounds for the increase in the black hole entropy and entropy of the system respectively One should also note that the GSL will hold for theories of gravity such as Einstein gravity Lovelock gravity or Braneworld gravity because the conditions to use GSL for these can be met However on the topic of black hole formation the question becomes whether or not the generalized second law of thermodynamics will be valid and if it is it will have been proved valid for all situations Because a black hole formation is not stationary but instead moving proving that the GSL holds is difficult Proving the GSL is generally valid would require using quantumstatistical mechanics because the GSL is both a quantum and statistical law This discipline does not exist so the GSL can be assumed to be useful in general as well as for prediction For example one can use the GSL to predict that for a cold nonrotating assembly of N displaystyle N nucleons S B H S 0 displaystyle S_BHS0 where S B H displaystyle S_BH is the entropy of a black hole and S displaystyle S is the sum of the ordinary entropy The third law The third law of black hole thermodynamics is controversial Specific counterexamples called extremal black holes fail to obey the rule The classical third law of thermodynamics known as the Nernst theorem which says the entropy of a system must go to zero as the temperature goes to absolute zero is also not a universal law However the systems that fail the classical third law have not been realized in practice leading to the suggestion that the extremal black holes may not represent the physics of black holes generally A weaker form of the classical third law known as the unattainability principle states that an infinite number of steps are required to put a system in to its ground state This form of the third law does have an analog in black hole physics 10 Interpretation of the laws The four laws of black hole mechanics suggest that one should identify the surface gravity of a black hole with temperature and the area of the event horizon with entropy at least up to some multiplicative constants If one only considers black holes classically then they have zero temperature and by the nohair theorem zero entropy and the laws of black hole mechanics remain an analogy However when quantummechanical effects are taken into account one finds that black holes emit thermal radiation Hawking radiation at a temperature T H 2 displaystyle T_textHfrac kappa 2pi From the first law of black hole mechanics this determines the multiplicative constant of the BekensteinHawking entropy which is in geometrized units S BH A 4 displaystyle S_textBHfrac A4 which is the entropy of the black hole in Einsteins general relativity Quantum field theory in curved spacetime can be utilized to calculate the entropy for a black hole in any covariant theory for gravity known as the Wald entropy Critique While black hole thermodynamics BHT has been regarded as one of the deepest clues to a quantum theory of gravity there remain a philosophical criticism that the analogy is not nearly as good as is commonly supposed that it is often based on a kind of caricature of thermodynamics and its unclear what the systems in BHT are supposed to be These criticisms where reexamined in detail ending with the opposite conclusion stationary black holes are not analogous to thermodynamic systems they are thermodynamic systems in the fullest sense Beyond black holes Gary Gibbons and Hawking have shown that black hole thermodynamics is more general than black holesthat cosmological event horizons also have an entropy and temperature More fundamentally Gerard t Hooft and Leonard Susskind used the laws of black hole thermodynamics to argue for a general holographic principle of nature which asserts that consistent theories of gravity and quantum mechanics must be lowerdimensional Though not yet fully understood in general the holographic principle is central to theories like the AdSCFT correspondence There are also connections between black hole entropy and fluid surface tension See also Joseph Polchinski Robert Wald Notes Citations Bibliography Bardeen J M Carter B Hawking S W 1973 The four laws of black hole mechanics Communications in Mathematical Physics 31 2 161170 Bibcode1973CMaPh31161B doi101007BF01645742 S2CID 54690354 Bekenstein Jacob D April 1973 Black holes and entropy Physical Review D 7 8 23332346 Bibcode1973PhRvD72333B doi101103PhysRevD72333 S2CID 122636624 Hawking Stephen W 1974 Black hole explosions Nature 248 5443 3031 Bibcode1974Natur24830H doi101038248030a0 S2CID 4290107 Hawking Stephen W 1975 Particle creation by black holes Communications in Mathematical Physics 43 3 199220 Bibcode1975CMaPh43199H doi101007BF02345020 S2CID 55539246 Hawking S W Ellis G F R 1973 The Large Scale Structure of SpaceTime New York Cambridge University Press ISBN 9780521099066 Hawking Stephen W 1994 The Nature of Space and Time arXivhepth9409195 t Hooft Gerardus 1985 On the quantum structure of a black hole PDF Nuclear Physics B 256 727745 Bibcode1985NuPhB256727T doi1010160550321385904183 Archived from the original PDF on 20110926 Page Don 2005 Hawking Radiation and Black Hole Thermodynamics New Journal of Physics 7 1 203 arXivhepth0409024 Bibcode2005NJPh7203P doi1010881367263071203 S2CID 119047329 External links BekensteinHawking entropy on Scholarpedia Black Hole Thermodynamics Black hole entropy on arxivorg",
  },
  {
    title: "Electromagnetism",
    originalContent:
      "In physics electromagnetism is an interaction that occurs between particles with electric charge via electromagnetic fields The electromagnetic force is one of the four fundamental forces of nature It is the dominant force in the interactions of atoms and molecules Electromagnetism can be thought of as a combination of electrostatics and magnetism which are distinct but closely intertwined phenomena Electromagnetic forces occur between any two charged particles Electric forces cause an attraction between particles with opposite charges and repulsion between particles with the same charge while magnetism is an interaction that occurs between charged particles in relative motion These two forces are described in terms of electromagnetic fields Macroscopic charged objects are described in terms of Coulombs law for electricity and Ampres force law for magnetism the Lorentz force describes microscopic charged particles The electromagnetic force is responsible for many of the chemical and physical phenomena observed in daily life The electrostatic attraction between atomic nuclei and their electrons holds atoms together Electric forces also allow different atoms to combine into molecules including the macromolecules such as proteins that form the basis of life Meanwhile magnetic interactions between the spin and angular momentum magnetic moments of electrons also play a role in chemical reactivity such relationships are studied in spin chemistry Electromagnetism also plays several crucial roles in modern technology electrical energy production transformation and distribution light heat and sound production and detection fiber optic and wireless communication sensors computation electrolysis electroplating and mechanical motors and actuators Electromagnetism has been studied since ancient times Many ancient civilizations including the Greeks and the Mayans created wideranging theories to explain lightning static electricity and the attraction between magnetized pieces of iron ore However it was not until the late 18th century that scientists began to develop a mathematical basis for understanding the nature of electromagnetic interactions In the 18th and 19th centuries prominent scientists and mathematicians such as Coulomb Gauss and Faraday developed namesake laws which helped to explain the formation and interaction of electromagnetic fields This process culminated in the 1860s with the discovery of Maxwells equations a set of four partial differential equations which provide a complete description of classical electromagnetic fields Maxwells equations provided a sound mathematical basis for the relationships between electricity and magnetism that scientists had been exploring for centuries and predicted the existence of selfsustaining electromagnetic waves Maxwell postulated that such waves make up visible light which was later shown to be true Gammarays xrays ultraviolet visible infrared radiation microwaves and radio waves were all determined to be electromagnetic radiation differing only in their range of frequencies In the modern era scientists continue to refine the theory of electromagnetism to account for the effects of modern physics including quantum mechanics and relativity The theoretical implications of electromagnetism particularly the requirement that observations remain consistent when viewed from various moving frames of reference relativistic electromagnetism and the establishment of the speed of light based on properties of the medium of propagation permeability and permittivity helped inspire Einsteins theory of special relativity in 1905 Quantum electrodynamics QED modifies Maxwells equations to be consistent with the quantized nature of matter In QED changes in the electromagnetic field are expressed in terms of discrete excitations particles known as photons the quanta of light History Ancient world Investigation into electromagnetic phenomena began about 5000 years ago There is evidence that the ancient Chinese Mayan and potentially even Egyptian civilizations knew that the naturally magnetic mineral magnetite had attractive properties and many incorporated it into their art and architecture Ancient people were also aware of lightning and static electricity although they had no idea of the mechanisms behind these phenomena The Greek philosopher Thales of Miletus discovered around 600 BCE that amber could acquire an electric charge when it was rubbed with cloth which allowed it to pick up light objects such as pieces of straw Thales also experimented with the ability of magnetic rocks to attract one other and hypothesized that this phenomenon might be connected to the attractive power of amber foreshadowing the deep connections between electricity and magnetism that would be discovered over 2000 years later Despite all this investigation ancient civilizations had no understanding of the mathematical basis of electromagnetism and often analyzed its impacts through the lens of religion rather than science lightning for instance was considered to be a creation of the gods in many cultures 19th century Electricity and magnetism were originally considered to be two separate forces This view changed with the publication of James Clerk Maxwells 1873 A Treatise on Electricity and Magnetism in which the interactions of positive and negative charges were shown to be mediated by one force There are four main effects resulting from these interactions all of which have been clearly demonstrated by experiments Electric charges attract or repel one another with a force inversely proportional to the square of the distance between them opposite charges attract like charges repel Magnetic poles or states of polarization at individual points attract or repel one another in a manner similar to positive and negative charges and always exist as pairs every north pole is yoked to a south pole An electric current inside a wire creates a corresponding circumferential magnetic field outside the wire Its direction clockwise or counterclockwise depends on the direction of the current in the wire A current is induced in a loop of wire when it is moved toward or away from a magnetic field or a magnet is moved towards or away from it the direction of current depends on that of the movement In April 1820 Hans Christian rsted observed that an electrical current in a wire caused a nearby compass needle to move At the time of discovery rsted did not suggest any satisfactory explanation of the phenomenon nor did he try to represent the phenomenon in a mathematical framework However three months later he began more intensive investigations Soon thereafter he published his findings proving that an electric current produces a magnetic field as it flows through a wire The CGS unit of magnetic induction oersted is named in honor of his contributions to the field of electromagnetism His findings resulted in intensive research throughout the scientific community in electrodynamics They influenced French physicist AndrMarie Ampres developments of a single mathematical form to represent the magnetic forces between currentcarrying conductors rsteds discovery also represented a major step toward a unified concept of energy This unification which was observed by Michael Faraday extended by James Clerk Maxwell and partially reformulated by Oliver Heaviside and Heinrich Hertz is one of the key accomplishments of 19thcentury mathematical physics It has had farreaching consequences one of which was the understanding of the nature of light Unlike what was proposed by the electromagnetic theory of that time light and other electromagnetic waves are at present seen as taking the form of quantized selfpropagating oscillatory electromagnetic field disturbances called photons Different frequencies of oscillation give rise to the different forms of electromagnetic radiation from radio waves at the lowest frequencies to visible light at intermediate frequencies to gamma rays at the highest frequencies rsted was not the only person to examine the relationship between electricity and magnetism In 1802 Gian Domenico Romagnosi an Italian legal scholar deflected a magnetic needle using a Voltaic pile The factual setup of the experiment is not completely clear nor if current flowed across the needle or not An account of the discovery was published in 1802 in an Italian newspaper but it was largely overlooked by the contemporary scientific community because Romagnosi seemingly did not belong to this community An earlier 1735 and often neglected connection between electricity and magnetism was reported by a Dr Cookson The account statedA tradesman at Wakefield in Yorkshire having put up a great number of knives and forks in a large box and having placed the box in the corner of a large room there happened a sudden storm of thunder lightning c The owner emptying the box on a counter where some nails lay the persons who took up the knives that lay on the nails observed that the knives took up the nails On this the whole number was tried and found to do the same and that to such a degree as to take up large nails packing needles and other iron things of considerable weight E T Whittaker suggested in 1910 that this particular event was responsible for lightning to be credited with the power of magnetizing steel and it was doubtless this which led Franklin in 1751 to attempt to magnetize a sewingneedle by means of the discharge of Leyden jars A fundamental force The electromagnetic force is the second strongest of the four known fundamental forces and has unlimited range All other forces known as nonfundamental forces eg friction contact forces are derived from the four fundamental forces At high energy the weak force and electromagnetic force are unified as a single interaction called the electroweak interaction Most of the forces involved in interactions between atoms are explained by electromagnetic forces between electrically charged atomic nuclei and electrons The electromagnetic force is also involved in all forms of chemical phenomena Electromagnetism explains how materials carry momentum despite being composed of individual particles and empty space The forces we experience when pushing or pulling ordinary material objects result from intermolecular forces between individual molecules in our bodies and in the objects The effective forces generated by the momentum of electrons movement is a necessary part of understanding atomic and intermolecular interactions As electrons move between interacting atoms they carry momentum with them As a collection of electrons becomes more confined their minimum momentum necessarily increases due to the Pauli exclusion principle The behavior of matter at the molecular scale including its density is determined by the balance between the electromagnetic force and the force generated by the exchange of momentum carried by the electrons themselves Classical electrodynamics In 1600 William Gilbert proposed in his De Magnete that electricity and magnetism while both capable of causing attraction and repulsion of objects were distinct effects Mariners had noticed that lightning strikes had the ability to disturb a compass needle The link between lightning and electricity was not confirmed until Benjamin Franklins proposed experiments in 1752 were conducted on 10 May 1752 by ThomasFranois Dalibard of France using a 40foottall 12 m iron rod instead of a kite and he successfully extracted electrical sparks from a cloud One of the first to discover and publish a link between humanmade electric current and magnetism was Gian Romagnosi who in 1802 noticed that connecting a wire across a voltaic pile deflected a nearby compass needle However the effect did not become widely known until 1820 when rsted performed a similar experiment rsteds work influenced Ampre to conduct further experiments which eventually gave rise to a new area of physics electrodynamics By determining a force law for the interaction between elements of electric current Ampre placed the subject on a solid mathematical foundation A theory of electromagnetism known as classical electromagnetism was developed by several physicists during the period between 1820 and 1873 when James Clerk Maxwells treatise was published which unified previous developments into a single theory proposing that light was an electromagnetic wave propagating in the luminiferous ether In classical electromagnetism the behavior of the electromagnetic field is described by a set of equations known as Maxwells equations and the electromagnetic force is given by the Lorentz force law One of the peculiarities of classical electromagnetism is that it is difficult to reconcile with classical mechanics but it is compatible with special relativity According to Maxwells equations the speed of light in vacuum is a universal constant that is dependent only on the electrical permittivity and magnetic permeability of free space This violates Galilean invariance a longstanding cornerstone of classical mechanics One way to reconcile the two theories electromagnetism and classical mechanics is to assume the existence of a luminiferous aether through which the light propagates However subsequent experimental efforts failed to detect the presence of the aether After important contributions of Hendrik Lorentz and Henri Poincar in 1905 Albert Einstein solved the problem with the introduction of special relativity which replaced classical kinematics with a new theory of kinematics compatible with classical electromagnetism For more information see History of special relativity In addition relativity theory implies that in moving frames of reference a magnetic field transforms to a field with a nonzero electric component and conversely a moving electric field transforms to a nonzero magnetic component thus firmly showing that the phenomena are two sides of the same coin Hence the term electromagnetism For more information see Classical electromagnetism and special relativity and Covariant formulation of classical electromagnetism Today few problems in electromagnetism remain unsolved These include the lack of magnetic monopoles AbrahamMinkowski controversy the location in space of the electromagnetic field energy and the mechanism by which some organisms can sense electric and magnetic fields Extension to nonlinear phenomena The Maxwell equations are linear in that a change in the sources the charges and currents results in a proportional change of the fields Nonlinear dynamics can occur when electromagnetic fields couple to matter that follows nonlinear dynamical laws This is studied for example in the subject of magnetohydrodynamics which combines Maxwell theory with the NavierStokes equations Another branch of electromagnetism dealing with nonlinearity is nonlinear optics Quantities and units Here is a list of common units related to electromagnetism In the electromagnetic CGS system electric current is a fundamental quantity defined via Ampres law and takes the permeability as a dimensionless quantity relative permeability whose value in vacuum is unity As a consequence the square of the speed of light appears explicitly in some of the equations interrelating quantities in this system Formulas for physical laws of electromagnetism such as Maxwells equations need to be adjusted depending on what system of units one uses This is because there is no onetoone correspondence between electromagnetic units in SI and those in CGS as is the case for mechanical units Furthermore within CGS there are several plausible choices of electromagnetic units leading to different unit subsystems including Gaussian ESU EMU and HeavisideLorentz Among these choices Gaussian units are the most common today and in fact the phrase CGS units is often used to refer specifically to CGSGaussian units Applications The study of electromagnetism informs electric circuits magnetic circuits and semiconductor devices construction See also References Further reading Web sources Textbooks General coverage External links Magnetic Field Strength Converter Electromagnetic Force from Eric Weissteins World of Physics",
  },
  {
    title: "Nuclear physics",
    originalContent:
      "Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions in addition to the study of other forms of nuclear matter Nuclear physics should not be confused with atomic physics which studies the atom as a whole including its electrons Discoveries in nuclear physics have led to applications in many fields This includes nuclear power nuclear weapons nuclear medicine and magnetic resonance imaging industrial and agricultural isotopes ion implantation in materials engineering and radiocarbon dating in geology and archaeology Such applications are studied in the field of nuclear engineering Particle physics evolved out of nuclear physics and the two fields are typically taught in close association Nuclear astrophysics the application of nuclear physics to astrophysics is crucial in explaining the inner workings of stars and the origin of the chemical elements History The history of nuclear physics as a discipline distinct from atomic physics starts with the discovery of radioactivity by Henri Becquerel in 1896 made while investigating phosphorescence in uranium salts The discovery of the electron by J J Thomson a year later was an indication that the atom had internal structure At the beginning of the 20th century the accepted model of the atom was J J Thomsons plum pudding model in which the atom was a positively charged ball with smaller negatively charged electrons embedded inside it In the years that followed radioactivity was extensively investigated notably by Marie Curie a Polish physicist whose maiden name was Sklodowska Pierre Curie Ernest Rutherford and others By the turn of the century physicists had also discovered three types of radiation emanating from atoms which they named alpha beta and gamma radiation Experiments by Otto Hahn in 1911 and by James Chadwick in 1914 discovered that the beta decay spectrum was continuous rather than discrete That is electrons were ejected from the atom with a continuous range of energies rather than the discrete amounts of energy that were observed in gamma and alpha decays This was a problem for nuclear physics at the time because it seemed to indicate that energy was not conserved in these decays The 1903 Nobel Prize in Physics was awarded jointly to Becquerel for his discovery and to Marie and Pierre Curie for their subsequent research into radioactivity Rutherford was awarded the Nobel Prize in Chemistry in 1908 for his investigations into the disintegration of the elements and the chemistry of radioactive substances In 1905 Albert Einstein formulated the idea of massenergy equivalence While the work on radioactivity by Becquerel and Marie Curie predates this an explanation of the source of the energy of radioactivity would have to wait for the discovery that the nucleus itself was composed of smaller constituents the nucleons Rutherford discovers the nucleus In 1906 Ernest Rutherford published Retardation of the Particle from Radium in passing through matter Hans Geiger expanded on this work in a communication to the Royal Society with experiments he and Rutherford had done passing alpha particles through air aluminum foil and gold leaf More work was published in 1909 by Geiger and Ernest Marsden and further greatly expanded work was published in 1910 by Geiger In 19111912 Rutherford went before the Royal Society to explain the experiments and propound the new theory of the atomic nucleus as we now understand it Published in 1909 with the eventual classical analysis by Rutherford published May 1911 the key preemptive experiment was performed during 1909 at the University of Manchester Ernest Rutherfords assistant Professor Johannes Hans Geiger and an undergraduate Marsden performed an experiment in which Geiger and Marsden under Rutherfords supervision fired alpha particles helium 4 nuclei at a thin film of gold foil The plum pudding model had predicted that the alpha particles should come out of the foil with their trajectories being at most slightly bent But Rutherford instructed his team to look for something that shocked him to observe a few particles were scattered through large angles even completely backwards in some cases He likened it to firing a bullet at tissue paper and having it bounce off The discovery with Rutherfords analysis of the data in 1911 led to the Rutherford model of the atom in which the atom had a very small very dense nucleus containing most of its mass and consisting of heavy positively charged particles with embedded electrons in order to balance out the charge since the neutron was unknown As an example in this model which is not the modern one nitrogen14 consisted of a nucleus with 14 protons and 7 electrons 21 total particles and the nucleus was surrounded by 7 more orbiting electrons Eddington and stellar nuclear fusion Around 1920 Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars in his paper The Internal Constitution of the Stars At that time the source of stellar energy was a complete mystery Eddington correctly speculated that the source was fusion of hydrogen into helium liberating enormous energy according to Einsteins equation E mc2 This was a particularly remarkable development since at that time fusion and thermonuclear energy and even that stars are largely composed of hydrogen see metallicity had not yet been discovered Studies of nuclear spin The Rutherford model worked quite well until studies of nuclear spin were carried out by Franco Rasetti at the California Institute of Technology in 1929 By 1925 it was known that protons and electrons each had a spin of 12 In the Rutherford model of nitrogen14 20 of the total 21 nuclear particles should have paired up to cancel each others spin and the final odd particle should have left the nucleus with a net spin of 12 Rasetti discovered however that nitrogen14 had a spin of 1 James Chadwick discovers the neutron In 1932 Chadwick realized that radiation that had been observed by Walther Bothe Herbert Becker Irne and Frdric JoliotCurie was actually due to a neutral particle of about the same mass as the proton that he called the neutron following a suggestion from Rutherford about the need for such a particle In the same year Dmitri Ivanenko suggested that there were no electrons in the nucleus only protons and neutrons and that neutrons were spin 12 particles which explained the mass not due to protons The neutron spin immediately solved the problem of the spin of nitrogen14 as the one unpaired proton and one unpaired neutron in this model each contributed a spin of 12 in the same direction giving a final total spin of 1 With the discovery of the neutron scientists could at last calculate what fraction of binding energy each nucleus had by comparing the nuclear mass with that of the protons and neutrons which composed it Differences between nuclear masses were calculated in this way When nuclear reactions were measured these were found to agree with Einsteins calculation of the equivalence of mass and energy to within 1 as of 1934 Procas equations of the massive vector boson field Alexandru Proca was the first to develop and report the massive vector boson field equations and a theory of the mesonic field of nuclear forces Procas equations were known to Wolfgang Pauli who mentioned the equations in his Nobel address and they were also known to Yukawa Wentzel Taketani Sakata Kemmer Heitler and Frhlich who appreciated the content of Procas equations for developing a theory of the atomic nuclei in Nuclear Physics Yukawas meson postulated to bind nuclei In 1935 Hideki Yukawa proposed the first significant theory of the strong force to explain how the nucleus holds together In the Yukawa interaction a virtual particle later called a meson mediated a force between all nucleons including protons and neutrons This force explained why nuclei did not disintegrate under the influence of proton repulsion and it also gave an explanation of why the attractive strong force had a more limited range than the electromagnetic repulsion between protons Later the discovery of the pi meson showed it to have the properties of Yukawas particle With Yukawas papers the modern model of the atom was complete The center of the atom contains a tight ball of neutrons and protons which is held together by the strong nuclear force unless it is too large Unstable nuclei may undergo alpha decay in which they emit an energetic helium nucleus or beta decay in which they eject an electron or positron After one of these decays the resultant nucleus may be left in an excited state and in this case it decays to its ground state by emitting highenergy photons gamma decay The study of the strong and weak nuclear forces the latter explained by Enrico Fermi via Fermis interaction in 1934 led physicists to collide nuclei and electrons at ever higher energies This research became the science of particle physics the crown jewel of which is the standard model of particle physics which describes the strong weak and electromagnetic forces Modern nuclear physics A heavy nucleus can contain hundreds of nucleons This means that with some approximation it can be treated as a classical system rather than a quantummechanical one In the resulting liquiddrop model the nucleus has an energy that arises partly from surface tension and partly from electrical repulsion of the protons The liquiddrop model is able to reproduce many features of nuclei including the general trend of binding energy with respect to mass number as well as the phenomenon of nuclear fission Superimposed on this classical picture however are quantummechanical effects which can be described using the nuclear shell model developed in large part by Maria Goeppert Mayer and J Hans D Jensen Nuclei with certain magic numbers of neutrons and protons are particularly stable because their shells are filled Other more complicated models for the nucleus have also been proposed such as the interacting boson model in which pairs of neutrons and protons interact as bosons Ab initio methods try to solve the nuclear manybody problem from the ground up starting from the nucleons and their interactions Much of current research in nuclear physics relates to the study of nuclei under extreme conditions such as high spin and excitation energy Nuclei may also have extreme shapes similar to that of Rugby balls or even pears or extreme neutrontoproton ratios Experimenters can create such nuclei using artificially induced fusion or nucleon transfer reactions employing ion beams from an accelerator Beams with even higher energies can be used to create nuclei at very high temperatures and there are signs that these experiments have produced a phase transition from normal nuclear matter to a new state the quarkgluon plasma in which the quarks mingle with one another rather than being segregated in triplets as they are in neutrons and protons Nuclear decay Eighty elements have at least one stable isotope which is never observed to decay amounting to a total of about 251 stable nuclides However thousands of isotopes have been characterized as unstable These radioisotopes decay over time scales ranging from fractions of a second to trillions of years Plotted on a chart as a function of atomic and neutron numbers the binding energy of the nuclides forms what is known as the valley of stability Stable nuclides lie along the bottom of this energy valley while increasingly unstable nuclides lie up the valley walls that is have weaker binding energy The most stable nuclei fall within certain ranges or balances of composition of neutrons and protons too few or too many neutrons in relation to the number of protons will cause it to decay For example in beta decay a nitrogen16 atom 7 protons 9 neutrons is converted to an oxygen16 atom 8 protons 8 neutrons within a few seconds of being created In this decay a neutron in the nitrogen nucleus is converted by the weak interaction into a proton an electron and an antineutrino The element is transmuted to another element with a different number of protons In alpha decay which typically occurs in the heaviest nuclei the radioactive element decays by emitting a helium nucleus 2 protons and 2 neutrons giving another element plus helium4 In many cases this process continues through several steps of this kind including other types of decays usually beta decay until a stable element is formed In gamma decay a nucleus decays from an excited state into a lower energy state by emitting a gamma ray The element is not changed to another element in the process no nuclear transmutation is involved Other more exotic decays are possible see the first main article For example in internal conversion decay the energy from an excited nucleus may eject one of the inner orbital electrons from the atom in a process which produces high speed electrons but is not beta decay and unlike beta decay does not transmute one element to another Nuclear fusion In nuclear fusion two lowmass nuclei come into very close contact with each other so that the strong force fuses them It requires a large amount of energy for the strong or nuclear forces to overcome the electrical repulsion between the nuclei in order to fuse them therefore nuclear fusion can only take place at very high temperatures or high pressures When nuclei fuse a very large amount of energy is released and the combined nucleus assumes a lower energy level The binding energy per nucleon increases with mass number up to nickel62 Stars like the Sun are powered by the fusion of four protons into a helium nucleus two positrons and two neutrinos The uncontrolled fusion of hydrogen into helium is known as thermonuclear runaway A frontier in current research at various institutions for example the Joint European Torus JET and ITER is the development of an economically viable method of using energy from a controlled fusion reaction Nuclear fusion is the origin of the energy including in the form of light and other electromagnetic radiation produced by the core of all stars including our own Sun Nuclear fission Nuclear fission is the reverse process to fusion For nuclei heavier than nickel62 the binding energy per nucleon decreases with the mass number It is therefore possible for energy to be released if a heavy nucleus breaks apart into two lighter ones The process of alpha decay is in essence a special type of spontaneous nuclear fission It is a highly asymmetrical fission because the four particles which make up the alpha particle are especially tightly bound to each other making production of this nucleus in fission particularly likely From several of the heaviest nuclei whose fission produces free neutrons and which also easily absorb neutrons to initiate fission a selfigniting type of neutroninitiated fission can be obtained in a chain reaction Chain reactions were known in chemistry before physics and in fact many familiar processes like fires and chemical explosions are chemical chain reactions The fission or nuclear chainreaction using fissionproduced neutrons is the source of energy for nuclear power plants and fissiontype nuclear bombs such as those detonated in Hiroshima and Nagasaki Japan at the end of World War II Heavy nuclei such as uranium and thorium may also undergo spontaneous fission but they are much more likely to undergo decay by alpha decay For a neutroninitiated chain reaction to occur there must be a critical mass of the relevant isotope present in a certain space under certain conditions The conditions for the smallest critical mass require the conservation of the emitted neutrons and also their slowing or moderation so that there is a greater crosssection or probability of them initiating another fission In two regions of Oklo Gabon Africa natural nuclear fission reactors were active over 15 billion years ago Measurements of natural neutrino emission have demonstrated that around half of the heat emanating from the Earths core results from radioactive decay However it is not known if any of this results from fission chain reactions Production of heavy elements According to the theory as the Universe cooled after the Big Bang it eventually became possible for common subatomic particles as we know them neutrons protons and electrons to exist The most common particles created in the Big Bang which are still easily observable to us today were protons and electrons in equal numbers The protons would eventually form hydrogen atoms Almost all the neutrons created in the Big Bang were absorbed into helium4 in the first three minutes after the Big Bang and this helium accounts for most of the helium in the universe today see Big Bang nucleosynthesis Some relatively small quantities of elements beyond helium lithium beryllium and perhaps some boron were created in the Big Bang as the protons and neutrons collided with each other but all of the heavier elements carbon element number 6 and elements of greater atomic number that we see today were created inside stars during a series of fusion stages such as the protonproton chain the CNO cycle and the triplealpha process Progressively heavier elements are created during the evolution of a star Energy is only released in fusion processes involving smaller atoms than iron because the binding energy per nucleon peaks around iron 56 nucleons Since the creation of heavier nuclei by fusion requires energy nature resorts to the process of neutron capture Neutrons due to their lack of charge are readily absorbed by a nucleus The heavy elements are created by either a slow neutron capture process the socalled sprocess or the rapid or rprocess The s process occurs in thermally pulsing stars called AGB or asymptotic giant branch stars and takes hundreds to thousands of years to reach the heaviest elements of lead and bismuth The rprocess is thought to occur in supernova explosions which provide the necessary conditions of high temperature high neutron flux and ejected matter These stellar conditions make the successive neutron captures very fast involving very neutronrich species which then betadecay to heavier elements especially at the socalled waiting points that correspond to more stable nuclides with closed neutron shells magic numbers See also References Bibliography Introductory Semat H and Albright John R 1972 Introduction to Atomic and Nuclear Physics Springer ISBN 9780412156700 Littlefield TA and Thorley N 1979 Atomic and Nuclear Physics An Introduction Springer US ISBN 9780442301903 Belyaev Alexander Ross Douglas 2021 The Basics of Nuclear and Particle Physics Undergraduate Texts in Physics Cham Springer International Publishing Bibcode2021bnppbookB doi1010079783030801168 ISBN 9783030801151 Retrieved 20230219 Povh Bogdan Rith Klaus Scholz Christoph Zetsche Frank Rodejohann Werner 2015 Particles and Nuclei An Introduction to the Physical Concepts Graduate Texts in Physics Berlin Heidelberg Springer Berlin Heidelberg doi1010079783662463215 ISBN 9783662463208 Retrieved 20240527 Reference works Handbook of Nuclear Physics Isao Tanihata Hiroshi Toki Toshitaka Kajino eds Singapore Springer Nature Singapore 2020 doi1010079789811588181 ISBN 9789811588181 Retrieved 20230531cite book CS1 maint others link Advanced Cohen Bernard L 1971 Concepts of Nuclear Physics McGrawHill Inc Bohr Aage Mottelson Ben R 1998 1969 Nuclear Structure In 2 Volumes World Scientific doi1011423530 ISBN 9789810231972 Archived from the original on 20230120 Retrieved 20230419 Greiner Walter Maruhn Joachim A and Bromley DA 1996 Nuclear Models Springer ISBN 9783540591801 Paetz gen Schieck Hans 2014 Nuclear Reactions An Introduction Lecture Notes in Physics Vol 882 Berlin Heidelberg Springer Berlin Heidelberg doi1010079783642539862 ISBN 9783642539855 Retrieved 20230404 Classics or Historic Fermi E 1950 Nuclear Physics Univ Chicago Press Mott N F Massey H S W 1949 The Theory Of Atomic Collisions The International Series of Monographs on Physics 2 ed Oxford Calrendon Press OUP Blatt John M Weisskopf Victor F 1979 1952 Theoretical Nuclear Physics New York NY Springer New York doi1010079781461299592 ISBN 9781461299615 Retrieved 20230222 Bethe Hans A Morrison Philip 2006 1956 Elementary Nuclear Theory Mineola NY Dover Publications ISBN 9780486450483 External links Ernest Rutherfords biography at the American Institute of Physics Archived 20160730 at the Wayback Machine American Physical Society Division of Nuclear Physics Archived 20170920 at the Wayback Machine American Nuclear Society Archived 20081202 at the Wayback Machine Annotated bibliography on nuclear physics from the Alsos Digital Library for Nuclear Issues Nuclear science wiki Archived 20131021 at the Wayback Machine Nuclear Data Services IAEA Archived 20210318 at the Wayback Machine Nuclear Physics Archived 20171223 at the Wayback Machine BBC Radio 4 discussion with Jim AlKhalili John Gribbin and Catherine Sutton In Our Time Jan 10 2002",
  },
  {
    title: "Particle physics",
    originalContent:
      "Particle physics or highenergy physics is the study of fundamental particles and forces that constitute matter and radiation The field also studies combinations of elementary particles up to the scale of protons and neutrons while the study of combination of protons and neutrons is called nuclear physics The fundamental particles in the universe are classified in the Standard Model as fermions matter particles and bosons forcecarrying particles There are three generations of fermions although ordinary matter is made only from the first fermion generation The first generation consists of up and down quarks which form protons and neutrons and electrons and electron neutrinos The three fundamental interactions known to be mediated by bosons are electromagnetism the weak interaction and the strong interaction Quarks cannot exist on their own but form hadrons Hadrons that contain an odd number of quarks are called baryons and those that contain an even number are called mesons Two baryons the proton and the neutron make up most of the mass of ordinary matter Mesons are unstable and the longestlived last for only a few hundredths of a microsecond They occur after collisions between particles made of quarks such as fastmoving protons and neutrons in cosmic rays Mesons are also produced in cyclotrons or other particle accelerators Particles have corresponding antiparticles with the same mass but with opposite electric charges For example the antiparticle of the electron is the positron The electron has a negative electric charge the positron has a positive charge These antiparticles can theoretically form a corresponding form of matter called antimatter Some particles such as the photon are their own antiparticle These elementary particles are excitations of the quantum fields that also govern their interactions The dominant theory explaining these fundamental particles and fields along with their dynamics is called the Standard Model The reconciliation of gravity to the current particle physics theory is not solved many theories have addressed this problem such as loop quantum gravity string theory and supersymmetry theory Practical particle physics is the study of these particles in radioactive processes and in particle accelerators such as the Large Hadron Collider Theoretical particle physics is the study of these particles in the context of cosmology and quantum theory The two are closely interrelated the Higgs boson was postulated by theoretical particle physicists and its presence confirmed by practical experiments History The idea that all matter is fundamentally composed of elementary particles dates from at least the 6th century BC In the 19th century John Dalton through his work on stoichiometry concluded that each element of nature was composed of a single unique type of particle The word atom after the Greek word atomos meaning indivisible has since then denoted the smallest particle of a chemical element but physicists later discovered that atoms are not in fact the fundamental particles of nature but are conglomerates of even smaller particles such as the electron The early 20th century explorations of nuclear physics and quantum physics led to proofs of nuclear fission in 1939 by Lise Meitner based on experiments by Otto Hahn and nuclear fusion by Hans Bethe in that same year both discoveries also led to the development of nuclear weapons Throughout the 1950s and 1960s a bewildering variety of particles was found in collisions of particles from beams of increasingly high energy It was referred to informally as the particle zoo Important discoveries such as the CP violation by James Cronin and Val Fitch brought new questions to matterantimatter imbalance After the formulation of the Standard Model during the 1970s physicists clarified the origin of the particle zoo The large number of particles was explained as combinations of a relatively small number of more fundamental particles and framed in the context of quantum field theories This reclassification marked the beginning of modern particle physics Standard Model The current state of the classification of all elementary particles is explained by the Standard Model which gained widespread acceptance in the mid1970s after experimental confirmation of the existence of quarks It describes the strong weak and electromagnetic fundamental interactions using mediating gauge bosons The species of gauge bosons are eight gluons W W and Z bosons and the photon The Standard Model also contains 24 fundamental fermions 12 particles and their associated antiparticles which are the constituents of all matter Finally the Standard Model also predicted the existence of a type of boson known as the Higgs boson On 4 July 2012 physicists with the Large Hadron Collider at CERN announced they had found a new particle that behaves similarly to what is expected from the Higgs boson The Standard Model as currently formulated has 61 elementary particles Those elementary particles can combine to form composite particles accounting for the hundreds of other species of particles that have been discovered since the 1960s The Standard Model has been found to agree with almost all the experimental tests conducted to date However most particle physicists believe that it is an incomplete description of nature and that a more fundamental theory awaits discovery See Theory of Everything In recent years measurements of neutrino mass have provided the first experimental deviations from the Standard Model since neutrinos do not have mass in the Standard Model Subatomic particles Modern particle physics research is focused on subatomic particles including atomic constituents such as electrons protons and neutrons protons and neutrons are composite particles called baryons made of quarks that are produced by radioactive and scattering processes such particles are photons neutrinos and muons as well as a wide range of exotic particles All particles and their interactions observed to date can be described almost entirely by the Standard Model Dynamics of particles are also governed by quantum mechanics they exhibit waveparticle duality displaying particlelike behaviour under certain experimental conditions and wavelike behaviour in others In more technical terms they are described by quantum state vectors in a Hilbert space which is also treated in quantum field theory Following the convention of particle physicists the term elementary particles is applied to those particles that are according to current understanding presumed to be indivisible and not composed of other particles Quarks and leptons Ordinary matter is made from firstgeneration quarks up down and leptons electron electron neutrino Collectively quarks and leptons are called fermions because they have a quantum spin of halfintegers 12 12 32 etc This causes the fermions to obey the Pauli exclusion principle where no two particles may occupy the same quantum state Quarks have fractional elementary electric charge 13 or 23 and leptons have wholenumbered electric charge 0 or 1 Quarks also have color charge which is labeled arbitrarily with no correlation to actual light color as red green and blue Because the interactions between the quarks store energy which can convert to other particles when the quarks are far apart enough quarks cannot be observed independently This is called color confinement There are three known generations of quarks up and down strange and charm top and bottom and leptons electron and its neutrino muon and its neutrino tau and its neutrino with strong indirect evidence that a fourth generation of fermions does not exist Bosons Bosons are the mediators or carriers of fundamental interactions such as electromagnetism the weak interaction and the strong interaction Electromagnetism is mediated by the photon the quanta of light 2930 The weak interaction is mediated by the W and Z bosons The strong interaction is mediated by the gluon which can link quarks together to form composite particles Due to the aforementioned color confinement gluons are never observed independently The Higgs boson gives mass to the W and Z bosons via the Higgs mechanism the gluon and photon are expected to be massless All bosons have an integer quantum spin 0 and 1 and can have the same quantum state Antiparticles and color charge Most aforementioned particles have corresponding antiparticles which compose antimatter Normal particles have positive lepton or baryon number and antiparticles have these numbers negative Most properties of corresponding antiparticles and particles are the same with a few gets reversed the electrons antiparticle positron has an opposite charge To differentiate between antiparticles and particles a plus or negative sign is added in superscript For example the electron and the positron are denoted e and e When a particle and an antiparticle interact with each other they are annihilated and convert to other particles Some particles such as the photon or gluon have no antiparticles Quarks and gluons additionally have color charges which influences the strong interaction Quarks color charges are called red green and blue though the particle itself have no physical color and in antiquarks are called antired antigreen and antiblue The gluon can have eight color charges which are the result of quarks interactions to form composite particles gauge symmetry SU3 Composite The neutrons and protons in the atomic nuclei are baryons the neutron is composed of two down quarks and one up quark and the proton is composed of two up quarks and one down quark A baryon is composed of three quarks and a meson is composed of two quarks one normal one anti Baryons and mesons are collectively called hadrons Quarks inside hadrons are governed by the strong interaction thus are subjected to quantum chromodynamics color charges The bounded quarks must have their color charge to be neutral or white for analogy with mixing the primary colors More exotic hadrons can have other types arrangement or number of quarks tetraquark pentaquark An atom is made from protons neutrons and electrons By modifying the particles inside a normal atom exotic atoms can be formed A simple example would be the hydrogen41 which has one of its electrons replaced with a muon Hypothetical The graviton is a hypothetical particle that can mediate the gravitational interaction but it has not been detected or completely reconciled with current theories Many other hypothetical particles have been proposed to address the limitations of the Standard Model Notably supersymmetric particles aim to solve the hierarchy problem axions address the strong CP problem and various other particles are proposed to explain the origins of dark matter and dark energy Experimental laboratories The worlds major particle physics laboratories are Brookhaven National Laboratory Long Island New York United States Its main facility is the Relativistic Heavy Ion Collider RHIC which collides heavy ions such as gold ions and polarized protons It is the worlds first heavy ion collider and the worlds only polarized proton collider Budker Institute of Nuclear Physics Novosibirsk Russia Its main projects are now the electronpositron colliders VEPP2000 operated since 2006 and VEPP4 started experiments in 1994 Earlier facilities include the first electronelectron beambeam collider VEP1 which conducted experiments from 1964 to 1968 the electronpositron colliders VEPP2 operated from 1965 to 1974 and its successor VEPP2M performed experiments from 1974 to 2000 CERN European Organization for Nuclear Research FrancoSwiss border near Geneva Switzerland Its main project is now the Large Hadron Collider LHC which had its first beam circulation on 10 September 2008 and is now the worlds most energetic collider of protons It also became the most energetic collider of heavy ions after it began colliding lead ions Earlier facilities include the Large ElectronPositron Collider LEP which was stopped on 2 November 2000 and then dismantled to give way for LHC and the Super Proton Synchrotron which is being reused as a preaccelerator for the LHC and for fixedtarget experiments DESY Deutsches ElektronenSynchrotron Hamburg Germany Its main facility was the Hadron Elektron Ring Anlage HERA which collided electrons and positrons with protons The accelerator complex is now focused on the production of synchrotron radiation with PETRA III FLASH and the European XFEL Fermi National Accelerator Laboratory Fermilab Batavia Illinois United States Its main facility until 2011 was the Tevatron which collided protons and antiprotons and was the highestenergy particle collider on earth until the Large Hadron Collider surpassed it on 29 November 2009 Institute of High Energy Physics IHEP Beijing China IHEP manages a number of Chinas major particle physics facilities including the Beijing ElectronPositron Collider IIBEPC II the Beijing Spectrometer BES the Beijing Synchrotron Radiation Facility BSRF the International CosmicRay Observatory at Yangbajing in Tibet the Daya Bay Reactor Neutrino Experiment the China Spallation Neutron Source the Hard Xray Modulation Telescope HXMT and the Acceleratordriven Subcritical System ADS as well as the Jiangmen Underground Neutrino Observatory JUNO KEK Tsukuba Japan It is the home of a number of experiments such as the K2K experiment a neutrino oscillation experiment and Belle II an experiment measuring the CP violation of B mesons SLAC National Accelerator Laboratory Menlo Park California United States Its 2milelong linear particle accelerator began operating in 1962 and was the basis for numerous electron and positron collision experiments until 2008 Since then the linear accelerator is being used for the Linac Coherent Light Source Xray laser as well as advanced accelerator design research SLAC staff continue to participate in developing and building many particle detectors around the world Theory Theoretical particle physics attempts to develop the models theoretical framework and mathematical tools to understand current experiments and make predictions for future experiments see also theoretical physics There are several major interrelated efforts being made in theoretical particle physics today One important branch attempts to better understand the Standard Model and its tests Theorists make quantitative predictions of observables at collider and astronomical experiments which along with experimental measurements is used to extract the parameters of the Standard Model with less uncertainty This work probes the limits of the Standard Model and therefore expands scientific understanding of natures building blocks Those efforts are made challenging by the difficulty of calculating high precision quantities in quantum chromodynamics Some theorists working in this area use the tools of perturbative quantum field theory and effective field theory referring to themselves as phenomenologists Others make use of lattice field theory and call themselves lattice theorists Another major effort is in model building where model builders develop ideas for what physics may lie beyond the Standard Model at higher energies or smaller distances This work is often motivated by the hierarchy problem and is constrained by existing experimental data It may involve work on supersymmetry alternatives to the Higgs mechanism extra spatial dimensions such as the RandallSundrum models Preon theory combinations of these or other ideas Vanishingdimensions theory is a particle physics theory suggesting that systems with higher energy have a smaller number of dimensions A third major effort in theoretical particle physics is string theory String theorists attempt to construct a unified description of quantum mechanics and general relativity by building a theory based on small strings and branes rather than particles If the theory is successful it may be considered a Theory of Everything or TOE There are also other areas of work in theoretical particle physics ranging from particle cosmology to loop quantum gravity Practical applications In principle all physics and practical applications developed therefrom can be derived from the study of fundamental particles In practice even if particle physics is taken to mean only highenergy atom smashers many technologies have been developed during these pioneering investigations that later find wide uses in society Particle accelerators are used to produce medical isotopes for research and treatment for example isotopes used in PET imaging or used directly in external beam radiotherapy The development of superconductors has been pushed forward by their use in particle physics The World Wide Web and touchscreen technology were initially developed at CERN Additional applications are found in medicine national security industry computing science and workforce development illustrating a long and growing list of beneficial practical applications with contributions from particle physics Future Major efforts to look for physics beyond the Standard Model include the Future Circular Collider proposed for CERN and the Particle Physics Project Prioritization Panel P5 in the US that will update the 2014 P5 study that recommended the Deep Underground Neutrino Experiment among other experiments See also References External links",
  },
  {
    title: "String cosmology",
    originalContent:
      "String cosmology is a relatively new field that tries to apply equations of string theory to solve the questions of early cosmology A related area of study is brane cosmology Overview This approach can be dated back to a paper by Gabriele Veneziano that shows how an inflationary cosmological model can be obtained from string theory thus opening the door to a description of preBig Bang scenarios The idea is related to a property of the bosonic string in a curve background better known as nonlinear sigma model First calculations from this model showed as the beta function representing the running of the metric of the model as a function of an energy scale is proportional to the Ricci tensor giving rise to a Ricci flow As this model has conformal invariance and this must be kept to have a sensible quantum field theory the beta function must be zero producing immediately the Einstein field equations While Einstein equations seem to appear somewhat out of place nevertheless this result is surely striking showing as a background twodimensional model could produce higherdimensional physics An interesting point here is that such a string theory can be formulated without a requirement of criticality at 26 dimensions for consistency as happens on a flat background This is a serious hint that the underlying physics of Einstein equations could be described by an effective twodimensional conformal field theory Indeed the fact that we have evidence for an inflationary universe is an important support to string cosmology In the evolution of the universe after the inflationary phase the expansion observed today sets in that is well described by Friedmann equations A smooth transition is expected between these two different phases String cosmology appears to have difficulties in explaining this transition This is known in the literature as the graceful exit problem An inflationary cosmology implies the presence of a scalar field that drives inflation In string cosmology this arises from the socalled dilaton field This is a scalar term entering into the description of the bosonic string that produces a scalar field term into the effective theory at low energies The corresponding equations resemble those of a BransDicke theory Analysis has been worked out from a critical number of dimension 26 down to four In general one gets Friedmann equations in an arbitrary number of dimensions The other way round is to assume that a certain number of dimensions is compactified producing an effective fourdimensional theory to work with Such a theory is a typical KaluzaKlein theory with a set of scalar fields arising from compactified dimensions Such fields are called moduli Technical details This section presents some of the relevant equations entering into string cosmology The starting point is the Polyakov action which can be written as S 2 1 4 d 2 z a b G X a X b X 2 R X displaystyle S_2frac 14pi alpha int d2zsqrt gamma leftgamma abG_mu nu Xpartial _aXmu partial _bXnu alpha 2RPhi Xright where 2 R displaystyle 2R is the Ricci scalar in two dimensions displaystyle Phi the dilaton field and displaystyle alpha the string constant The indices a b displaystyle ab range over 12 and displaystyle mu nu over 1 D displaystyle 1ldots D where D the dimension of the target space A further antisymmetric field could be added This is generally considered when one wants this action generating a potential for inflation Otherwise a generic potential is inserted by hand as well as a cosmological constant The above string action has a conformal invariance This is a property of a two dimensional Riemannian manifold At the quantum level this property is lost due to anomalies and the theory itself is not consistent having no unitarity So it is necessary to require that conformal invariance is kept at any order of perturbation theory Perturbation theory is the only known approach to manage the quantum field theory Indeed the beta functions at two loops are G R 2 O 2 displaystyle beta _mu nu GR_mu nu 2alpha nabla _mu Phi nabla _nu Phi Oalpha 2 and D 26 6 2 2 O 2 displaystyle beta Phi frac D266frac alpha 2nabla 2Phi alpha nabla _kappa Phi nabla kappa Phi Oalpha 2 The assumption that conformal invariance holds implies that G 0 displaystyle beta _mu nu Gbeta Phi 0 producing the corresponding equations of motion of lowenergy physics These conditions can only be satisfied perturbatively but this has to hold at any order of perturbation theory The first term in displaystyle beta Phi is just the anomaly of the bosonic string theory in a flat spacetime But here there are further terms that can grant compensation of the anomaly also when D 26 displaystyle Dneq 26 and from this cosmological models of a prebig bang scenario can be constructed Indeed this low energy equations can be obtained from the following action S 1 2 0 2 d D x G e 2 2 D 26 3 R 4 O displaystyle Sfrac 12kappa _02int dDxsqrt Ge2Phi leftfrac 2D263alpha R4partial _mu Phi partial mu Phi Oalpha right where 0 2 displaystyle kappa _02 is a constant that can always be changed by redefining the dilaton field One can also rewrite this action in a more familiar form by redefining the fields Einstein frame as g e 2 G displaystyle g_mu nu e2omega G_mu nu 2 0 D 2 displaystyle omega frac 2Phi _0Phi D2 and using 0 displaystyle tilde Phi Phi Phi _0 one can write S 1 2 2 d D x g 2 D 26 3 e 4 D 2 R 4 D 2 O displaystyle Sfrac 12kappa 2int dDxsqrt gleftfrac 2D263alpha efrac 4tilde Phi D2tilde Rfrac 4D2partial _mu tilde Phi partial mu tilde Phi Oalpha right where R e 2 R D 1 2 D 2 D 1 displaystyle tilde Re2omega RD1nabla 2omega D2D1partial _mu omega partial mu omega This is the formula for the Einstein action describing a scalar field interacting with a gravitational field in D dimensions Indeed the following identity holds 0 e 2 0 8 G D 1 2 8 M p displaystyle kappa kappa _0e2Phi _08pi G_Dfrac 12frac sqrt 8pi M_p where G D displaystyle G_D is the Newton constant in D dimensions and M p displaystyle M_p the corresponding Planck mass When setting D 4 displaystyle D4 in this action the conditions for inflation are not fulfilled unless a potential or antisymmetric term is added to the string action in which case powerlaw inflation is possible Notes References Polchinski Joseph 1998a String Theory Vol I An Introduction to the Bosonic String Cambridge University Press ISBN 9780521633031 Polchinski Joseph 1998b String Theory Vol II Superstring Theory and Beyond Cambridge University Press ISBN 9780521633048 Lidsey James D Wands David Copeland E J 2000 Superstring Cosmology Physics Reports 337 45 343492 arXivhepth9909061 Bibcode2000PhR337343L doi101016S0370157300000648 S2CID 119349072 Cicoli Michele Conlon Joseph P Maharana Anshuman Parameswaran Susha Quevedo Fernando Zavala Ivonne 2023 String Cosmology from the Early Universe to Today arXiv230304819 cite journal Cite journal requires journal help External links String cosmology on arxivorg Maurizio Gasperinis homepage",
  },
  {
    title: "Computer architecture",
    originalContent:
      "In computer science and computer engineering computer architecture is a description of the structure of a computer system made from component parts It can sometimes be a highlevel description that ignores details of the implementation At a more detailed level the description may include the instruction set architecture design microarchitecture design logic design and implementation History The first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace describing the analytical engine While building the computer Z1 in 1936 Konrad Zuse described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data ie the storedprogram concept Two other early and important examples are John von Neumanns 1945 paper First Draft of a Report on the EDVAC which described an organization of logical elements and Alan Turings more detailed Proposed Electronic Calculator for the Automatic Computing Engine also 1945 and which cited John von Neumanns paper The term architecture in computer literature can be traced to the work of Lyle R Johnson and Frederick P Brooks Jr members of the Machine Organization department in IBMs main research center in 1959 Johnson had the opportunity to write a proprietary research communication about the Stretch an IBMdeveloped supercomputer for Los Alamos National Laboratory at the time known as Los Alamos Scientific Laboratory To describe the level of detail for discussing the luxuriously embellished computer he noted that his description of formats instruction types hardware parameters and speed enhancements were at the level of system architecture a term that seemed more useful than machine organization Subsequently Brooks a Stretch designer opened Chapter 2 of a book called Planning a Computer System Project Stretch by stating Computer architecture like other architecture is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints Brooks went on to help develop the IBM System360 line of computers in which architecture became a noun defining what the user needs to know The System360 line was succeeded by several compatible lines of computers including the current IBM Z line Later computer users came to use the term in many less explicit ways The earliest computer architectures were designed on paper and then directly built into the final hardware form Later computer architecture prototypes were physically built in the form of a transistortransistor logic TTL computersuch as the prototypes of the 6800 and the PARISCtested and tweaked before committing to the final hardware form As of the 1990s new computer architectures are typically built tested and tweakedinside some other computer architecture in a computer architecture simulator or inside a FPGA as a soft microprocessor or bothbefore committing to the final hardware form Subcategories The discipline of computer architecture has three main subcategories Instruction set architecture ISA defines the machine code that a processor reads and acts upon as well as the word size memory address modes processor registers and data type Microarchitecture also known as computer organization this describes how a particular processor will implement the ISA The size of a computers CPU cache for instance is an issue that generally has nothing to do with the ISA Systems design includes all of the other hardware components within a computing system such as data processing other than the CPU eg direct memory access virtualization and multiprocessing There are other technologies in computer architecture The following technologies are used in bigger companies like Intel and were estimated in 2002 to count for 1 of all of computer architecture Macroarchitecture architectural layers more abstract than microarchitecture Assembly instruction set architecture A smart assembler may convert an abstract assembly language common to a group of machines into slightly different machine language for different implementations Programmervisible macroarchitecture higherlevel language tools such as compilers may define a consistent interface or contract to programmers using them abstracting differences between underlying ISAs and microarchitectures For example the C C or Java standards define different programmervisible macroarchitectures Microcode microcode is software that translates instructions to run on a chip It acts like a wrapper around the hardware presenting a preferred version of the hardwares instruction set interface This instruction translation facility gives chip designers flexible options Eg 1 A new improved version of the chip can use microcode to present the exact same instruction set as the old chip version so all software targeting that instruction set will run on the new chip without needing changes Eg 2 Microcode can present a variety of instruction sets for the same underlying chip allowing it to run a wider variety of software Pin architecture The hardware functions that a microprocessor should provide to a hardware platform eg the x86 pins A20M FERRIGNNE or FLUSH Also messages that the processor should emit so that external caches can be invalidated emptied Pin architecture functions are more flexible than ISA functions because external hardware can adapt to new encodings or change from a pin to a message The term architecture fits because the functions must be provided for compatible systems even if the detailed method changes Roles Definition Computer architecture is concerned with balancing the performance efficiency cost and reliability of a computer system The case of instruction set architecture can be used to illustrate the balance of these competing factors More complex instruction sets enable programmers to write more space efficient programs since a single instruction can encode some higherlevel abstraction such as the x86 Loop instruction However longer and more complex instructions take longer for the processor to decode and can be more costly to implement effectively The increased complexity from a large instruction set also creates more room for unreliability when instructions interact in unexpected ways The implementation involves integrated circuit design packaging power and cooling Optimization of the design requires familiarity with topics from compilers and operating systems to logic design and packaging Instruction set architecture An instruction set architecture ISA is the interface between the computers software and hardware and also can be viewed as the programmers view of the machine Computers do not understand highlevel programming languages such as Java C or most programming languages used A processor only understands instructions encoded in some numerical fashion usually as binary numbers Software tools such as compilers translate those high level languages into instructions that the processor can understand Besides instructions the ISA defines items in the computer that are available to a programeg data types registers addressing modes and memory Instructions locate these available items with register indexes or names and memory addressing modes The ISA of a computer is usually described in a small instruction manual which describes how the instructions are encoded Also it may define short vaguely mnemonic names for the instructions The names can be recognized by a software development tool called an assembler An assembler is a computer program that translates a humanreadable form of the ISA into a computerreadable form Disassemblers are also widely available usually in debuggers and software programs to isolate and correct malfunctions in binary computer programs ISAs vary in quality and completeness A good ISA compromises between programmer convenience how easy the code is to understand size of the code how much code is required to do a specific action cost of the computer to interpret the instructions more complexity means more hardware needed to decode and execute the instructions and speed of the computer with more complex decoding hardware comes longer decode time Memory organization defines how instructions interact with the memory and how memory interacts with itself During design emulation emulators can run programs written in a proposed instruction set Modern emulators can measure size cost and speed to determine whether a particular ISA is meeting its goals Computer organization Computer organization helps optimize performancebased products For example software engineers need to know the processing power of processors They may need to optimize software in order to gain the most performance for the lowest price This can require quite a detailed analysis of the computers organization For example in an SD card the designers might need to arrange the card so that the most data can be processed in the fastest possible way Computer organization also helps plan the selection of a processor for a particular project Multimedia projects may need very rapid data access while virtual machines may need fast interrupts Sometimes certain tasks need additional components as well For example a computer capable of running a virtual machine needs virtual memory hardware so that the memory of different virtual computers can be kept separated Computer organization and features also affect power consumption and processor cost Implementation Once an instruction set and microarchitecture have been designed a practical machine must be developed This design process is called the implementation Implementation is usually not considered architectural design but rather hardware design engineering Implementation can be further broken down into several steps Logic implementation designs the circuits required at a logicgate level Circuit implementation does transistorlevel designs of basic elements eg gates multiplexers latches as well as of some larger blocks ALUs caches etc that may be implemented at the logicgate level or even at the physical level if the design calls for it Physical implementation draws physical circuits The different circuit components are placed in a chip floor plan or on a board and the wires connecting them are created Design validation tests the computer as a whole to see if it works in all situations and all timings Once the design validation process starts the design at the logic level are tested using logic emulators However this is usually too slow to run a realistic test So after making corrections based on the first test prototypes are constructed using FieldProgrammable GateArrays FPGAs Most hobby projects stop at this stage The final step is to test prototype integrated circuits which may require several redesigns For CPUs the entire implementation process is organized differently and is often referred to as CPU design Design goals The exact form of a computer system depends on the constraints and goals Computer architectures usually trade off standards power versus performance cost memory capacity latency latency is the amount of time that it takes for information from one node to travel to the source and throughput Sometimes other considerations such as features size weight reliability and expandability are also factors The most common scheme does an indepth power analysis and figures out how to keep power consumption low while maintaining adequate performance Performance Modern computer performance is often described in instructions per cycle IPC which measures the efficiency of the architecture at any clock frequency a faster IPC rate means the computer is faster Older computers had IPC counts as low as 01 while modern processors easily reach nearly 1 Superscalar processors may reach three to five IPC by executing several instructions per clock cycle Counting machinelanguage instructions would be misleading because they can do varying amounts of work in different ISAs The instruction in the standard measurements is not a count of the ISAs machinelanguage instructions but a unit of measurement usually based on the speed of the VAX computer architecture Many people used to measure a computers speed by the clock rate usually in MHz or GHz This refers to the cycles per second of the main clock of the CPU However this metric is somewhat misleading as a machine with a higher clock rate may not necessarily have greater performance As a result manufacturers have moved away from clock speed as a measure of performance Other factors influence speed such as the mix of functional units bus speeds available memory and the type and order of instructions in the programs There are two main types of speed latency and throughput Latency is the time between the start of a process and its completion Throughput is the amount of work done per unit time Interrupt latency is the guaranteed maximum response time of the system to an electronic event like when the disk drive finishes moving some data Performance is affected by a very wide range of design choices for example pipelining a processor usually makes latency worse but makes throughput better Computers that control machinery usually need low interrupt latencies These computers operate in a realtime environment and fail if an operation is not completed in a specified amount of time For example computercontrolled antilock brakes must begin braking within a predictable and limited time period after the brake pedal is sensed or else failure of the brake will occur Benchmarking takes all these factors into account by measuring the time a computer takes to run through a series of test programs Although benchmarking shows strengths it should not be how you choose a computer Often the measured machines split on different measures For example one system might handle scientific applications quickly while another might render video games more smoothly Furthermore designers may target and add special features to their products through hardware or software that permit a specific benchmark to execute quickly but do not offer similar advantages to general tasks Power efficiency Power efficiency is another important measurement in modern computers Higher power efficiency can often be traded for lower speed or higher cost The typical measurement when referring to power consumption in computer architecture is MIPSW millions of instructions per second per watt Modern circuits have less power required per transistor as the number of transistors per chip grows This is because each transistor that is put in a new chip requires its own power supply and requires new pathways to be built to power it However the number of transistors per chip is starting to increase at a slower rate Therefore power efficiency is starting to become as important if not more important than fitting more and more transistors into a single chip Recent processor designs have shown this emphasis as they put more focus on power efficiency rather than cramming as many transistors into a single chip as possible In the world of embedded computers power efficiency has long been an important goal next to throughput and latency Shifts in market demand Increases in clock frequency have grown more slowly over the past few years compared to power reduction improvements This has been driven by the end of Moores Law and demand for longer battery life and reductions in size for mobile technology This change in focus from higher clock rates to power consumption and miniaturization can be shown by the significant reductions in power consumption as much as 50 that were reported by Intel in their release of the Haswell microarchitecture where they dropped their power consumption benchmark from 3040 watts down to 1020 watts Comparing this to the processing speed increase of 3 GHz to 4 GHz 2002 to 2006 it can be seen that the focus in research and development is shifting away from clock frequency and moving towards consuming less power and taking up less space See also References Sources John L Hennessy and David Patterson 2006 Computer Architecture A Quantitative Approach Fourth ed Morgan Kaufmann ISBN 9780123704900 Barton Robert S Functional Design of Computers Communications of the ACM 49 405 1961 Barton Robert S A New Approach to the Functional Design of a Digital Computer Proceedings of the Western Joint Computer Conference May 1961 pp 393396 About the design of the Burroughs B5000 computer Bell C Gordon and Newell Allen 1971 Computer Structures Readings and Examples McGrawHill Blaauw GA and Brooks FP Jr The Structure of System360 Part IOutline of the Logical Structure IBM Systems Journal vol 3 no 2 pp 119135 1964 Tanenbaum Andrew S 1979 Structured Computer Organization Englewood Cliffs New Jersey PrenticeHall ISBN 0131485210 External links ISCA Proceedings of the International Symposium on Computer Architecture Micro IEEEACM International Symposium on Microarchitecture HPCA International Symposium on High Performance Computer Architecture ASPLOS International Conference on Architectural Support for Programming Languages and Operating Systems ACM Transactions on Architecture and Code Optimization IEEE Transactions on Computers The von Neumann Architecture of Computer Systems at the Wayback Machine archived 20171031",
  },
  {
    title: "DevOps",
    originalContent:
      "DevOps is a methodology integrating and automating the work of software development Dev and information technology operations Ops It serves as a means for improving and shortening the systems development life cycle DevOps is complementary to agile software development several DevOps aspects came from the agile approach Automation is an important part of DevOps Software programmers and architects should use fitness functions to keep their software in check According to Neal Ford DevOps particularly through continuous delivery employs the Bring the pain forward principle tackling tough tasks early fostering automation and swift issue detection Definition Other than it being a crossfunctional combination and a portmanteau of the terms and concepts for development and operations academics and practitioners have not developed a universal definition for the term DevOps Most often DevOps is characterized by key principles shared ownership workflow automation and rapid feedback From an academic perspective Len Bass Ingo Weber and Liming Zhuthree computer science researchers from the CSIRO and the Software Engineering Institutesuggested defining DevOps as a set of practices intended to reduce the time between committing a change to a system and the change being placed into normal production while ensuring high quality However the term is used in multiple contexts At its most successful DevOps is a combination of specific practices culture change and tools History Proposals to combine software development methodologies with deployment and operations concepts began to appear in the late 80s and early 90s Around 2007 and 2008 concerns were raised by those within the software development and IT communities that the separation between the two industries where one wrote and created software entirely separate from those that deploy and support the software was creating a fatal level of dysfunction within the industry In 2009 the first conference named DevOps Days was held in Ghent Belgium The conference was founded by Belgian consultant project manager and agile practitioner Patrick Debois The conference has now spread to other countries In 2012 a report called State of DevOps was first published by Alanna Brown at Puppet Labs As of 2014 the annual State of DevOps report was published by Nicole Forsgren Gene Kim Jez Humble and others They stated that the adoption of DevOps was accelerating Also in 2014 Lisa Crispin and Janet Gregory wrote the book More Agile Testing containing a chapter on testing and DevOps In 2016 the DORA metrics for throughput deployment frequency lead time for changes and stability mean time to recover change failure rate were published in the State of DevOps report However the research methodology and metrics were criticized by experts In response to these criticisms the 2023 State of DevOps report published changes that updated the stability metric mean time to recover to failed deployment recovery time acknowledging the confusion the former metric has caused Relevant metrics DORA metrics are a set of key metrics developed by DevOps Research and Assessment DORA which can help to measure software development efficiency and reliability These metrics include Deployment Frequency Time between code deployments Mean Lead Time for Changes Time between code commit and deployment Change Failure Rate Percentage of deployments causing production issues Mean Time To Recovery Time to resolve production issues Reliability added in 2021 Measures operational performance focusing on availability and adherence to user expectations These metrics when applied appropriately and within relevant context facilitate insights into DevOps performance enabling teams to optimize deployment speed reliability and quality thereby informing datadriven decisions to enhance software development processes Relationship to other approaches Many of the ideas fundamental to DevOps practices are inspired by or mirror other well known practices such as Lean and Demings PlanDoCheckAct cycle through to The Toyota Way and the Agile approach of breaking down components and batch sizes Contrary to the topdown prescriptive approach and rigid framework of ITIL in the 1990s DevOps is bottomup and flexible having been created by software engineers for their own needs Agile The motivations for what has become modern DevOps and several standard DevOps practices such as automated build and test continuous integration and continuous delivery originated in the Agile world which dates informally to the 1990s and formally to 2001 Agile development teams using methods such as extreme programming couldnt satisfy the customer through early and continuous delivery of valuable software unless they took responsibility for operations and infrastructure for their applications automating much of that work Because Scrum emerged as the dominant Agile framework in the early 2000s and it omitted the engineering practices that were part of many Agile teams the movement to automate operations and infrastructure functions splintered from Agile and expanded into what has become modern DevOps Today DevOps focuses on the deployment of developed software whether it is developed using Agile oriented methodologies or other methodologies ArchOps ArchOps presents an extension for DevOps practice starting from software architecture artifacts instead of source code for operation deployment ArchOps states that architectural models are firstclass entities in software development deployment and operations Continuous Integration and Delivery CICD Automation is a core principle for achieving DevOps success and CICD is a critical component Plus improved collaboration and communication between and within teams helps achieve faster time to market with reduced risks Mobile DevOps Mobile DevOps is a set of practices that applies the principles of DevOps specifically to the development of mobile applications Traditional DevOps focuses on streamlining the software development process in general but mobile development has its own unique challenges that require a tailored approach Mobile DevOps is not simply as a branch of DevOps specific to mobile app development instead an extension and reinterpretation of the DevOps philosophy due to very specific requirements of the mobile world Sitereliability engineering In 2003 Google developed site reliability engineering SRE an approach for releasing new features continuously into largescale highavailability systems while maintaining highquality enduser experience While SRE predates the development of DevOps they are generally viewed as being related to each other Some of the original authors of the discipline consider SRE as an implementation of DevOps Toyota production system lean thinking kaizen Toyota production system also known under the acronym TPS was the inspiration for lean thinking with its focus on continuous improvement kaizen flow and small batches The andon cord principle to create fast feedback swarm and solve problems stems from TPS DevSecOps shifting security left DevSecOps is an augmentation of DevOps to allow for security practices to be integrated into the DevOps approach Contrary to a traditional centralized security team model each delivery team is empowered to factor in the correct security controls into their software delivery Security practices and testing are performed earlier in the development lifecycle hence the term shift left Security is tested in three main areas static software composition and dynamic Checking software statically via static application security testing SAST is whitebox testing with special focus on security Depending on the programming language different tools are needed to do such static code analysis The software composition is analyzed especially libraries and the version of each component is checked against vulnerability lists published by CERT and other expert groups When giving software to clients library licenses and their match to the license of the software distributed are in focus especially copyleft licenses In dynamic testing also called blackbox testing software is tested without knowing its inner functions In DevSecOps this practice may be referred to as dynamic application security testing DAST or penetration testing The goal is early detection of defects including crosssite scripting and SQL injection vulnerabilities Threat types are published by the open web application security project eg its TOP10 and by other bodies DevSecOps has also been described as a cultural shift involving a holistic approach to producing secure software by integrating security education security by design and security automation Cultural change DevOps initiatives can create cultural changes in companies by transforming the way operations developers and testers collaborate during the development and delivery processes Getting these groups to work cohesively is a critical challenge in enterprise DevOps adoption DevOps is as much about culture as it is about the toolchain Microservices Although in principle it is possible to practice DevOps with any architectural style the microservices architectural style is becoming the standard for building continuously deployed systems Small size service allows the architecture of an individual service to emerge through continuous refactoring DevOps automation It also supports consistency reliability and efficiency within the organization and is usually enabled by a shared code repository or version control As DevOps researcher Ravi Teja Yarlagadda hypothesizes Through DevOps there is an assumption that all functions can be carried out controlled and managed in a central place using a simple code Automation with version control Many organizations use version control to power DevOps automation technologies like virtual machines containerization or OSlevel virtualization and CICD The paper DevOps development of a toolchain in the banking domain notes that with teams of developers working on the same project All developers need to make changes to the same codebase and sometimes edit even the same files For efficient working there has to be a system that helps engineers avoid conflicts and retain the codebase history with the Git version control system and the GitHub platform referenced as examples GitOps GitOps evolved from DevOps The specific state of deployment configuration is versioncontrolled Because the most popular versioncontrol is Git GitOps approach has been named after Git Changes to configuration can be managed using code review practices and can be rolled back using versioncontrolling Essentially all of the changes to a code are tracked bookmarked and making any updates to the history can be made easier As explained by Red Hat visibility to change means the ability to trace and reproduce issues quickly improving overall security Best practices for cloud systems The following practices can enhance productivity of DevOps pipelines especially in systems hosted in the cloud Number of Pipelines Small teams can be more productive by having one repository and one pipeline In contrast larger organizations may have separate repositories and pipelines for each team or even separate repositories and pipelines for each service within a team Permissions In the context of pipelinerelated permissions adhering to the principle of least privilege can be challenging due to the dynamic nature of architecture Administrators may opt for more permissive permissions while implementing compensating security controls to minimize the blast radius See also DataOps DevOps toolchain TwelveFactor App methodology Infrastructure as code Lean software development Site reliability engineering Value stream List of build automation software Notes References Further reading Davis Jennifer Daniels Ryn 20160530 Effective DevOps building a culture of collaboration affinity and tooling at scale Sebastopol CA OReilly ISBN 9781491926437 OCLC 951434424 Kim Gene Debois Patrick Willis John Humble Jez Allspaw John 20151007 The DevOps handbook how to create worldclass agility reliability and security in technology organizations First ed Portland OR ISBN 9781942788003 OCLC 907166314cite book CS1 maint location missing publisher link Forsgren Nicole Humble Jez Kim Gene 27 March 2018 Accelerate The Science of Lean Software and DevOps Building and Scaling High Performing Technology Organizations First ed IT Revolution Press ISBN 9781942788331",
  },
  {
    title: "Continuous integration",
    originalContent:
      "Continuous integration CI is the practice of integrating source code changes frequently and ensuring that the integrated codebase is in a workable state Typically developers merge changes to an integration branch and an automated system builds and tests the software system Often the automated process runs on each commit or runs on a schedule such as once a day Grady Booch first proposed the term CI in 1991 although he did not advocate integrating multiple times a day but later CI came to include that aspect History The earliest known work 1989 on continuous integration was the Infuse environment developed by G E Kaiser D E Perry and W M Schell In 1994 Grady Booch used the phrase continuous integration in ObjectOriented Analysis and Design with Applications 2nd edition to explain how when developing using micro processes internal releases represent a sort of continuous integration of the system and exist to force closure of the micro process In 1997 Kent Beck and Ron Jeffries invented extreme programming XP while on the Chrysler Comprehensive Compensation System project including continuous integration Beck published about continuous integration in 1998 emphasising the importance of facetoface communication over technological support In 1999 Beck elaborated more in his first full book on Extreme Programming CruiseControl one of the first opensource CI tools was released in 2001 In 2010 Timothy Fitz published an article detailing how IMVUs engineering team had built and been using the first practical CD system While his post was originally met with skepticism it quickly caught on and found widespread adoption as part of the lean software development methodology also based on IMVU Practices The core activities of CI are developers colocate code changes in a shared integration area frequently and that the resulting integrated codebase is verified for correctness The first part generally involves merging changes to a common version control branch The second part generally involves automated processes including building testing and many other processes Typically a server builds from the integration area frequently ie after each commit or periodically like once a day The server may perform quality control checks such as running unit tests and collect software quality metrics via processes such as static analysis and performance testing Related practices The template Howto is being considered for merging This section lists best practices from practitioners for other practices that enhance CI Build automation Build automation is a best practice Atomic commits CI requires the version control system to support atomic commits ie all of a developers changes are handled as a single commit Committing changes When making a code change a developer creates a branch that is a copy of the current codebase As other changes are committed to the repository this copy diverges from the latest version The longer development continues on a branch without merging to the integration branch the greater the risk of multiple integration conflicts and failures when the developer branch is eventually merged back When developers submit code to the repository they must first update their code to reflect the changes in the repository since they took their copy The more changes the repository contains the more work developers must do before submitting their own changes Eventually the repository may become so different from the developers baselines that they enter what is sometimes referred to as merge hell or integration hell where the time it takes to integrate exceeds the time it took to make their original changes Testing locally Proponents of CI suggest that developers should use testdriven development and to ensure that all unit tests pass locally before committing to the integration branch so that one developers work does not break another developers copy Incomplete features can be disabled before committing using feature toggles Continuous delivery and continuous deployment Continuous delivery ensures the software checked in on an integration branch is always in a state that can be deployed to users and continuous deployment automates the deployment process Continuous delivery and continuous deployment are often performed in conjunction with CI and together form a CICD pipeline Version control Proponents of CI recommend storing all files and information needed for building in version control for git a repository that the system should be buildable from a fresh checkout and not require additional dependencies Martin Fowler recommends that all developers commit to the same integration branch Automate the build Build automation tools automate building Proponents of CI recommend that a single command should have the capability of building the system Automation often includes automating the integration which often includes deployment into a productionlike environment In many cases the build script not only compiles binaries but also generates documentation website pages statistics and distribution media such as Debian DEB Red Hat RPM or Windows MSI files Commit frequently Developers can reduce the effort of resolving conflicting changes by synchronizing changes with each other frequently at least daily Checking in a weeks worth of work risks conflict both in likelihood of occurrence and complexity to resolve Relatively small conflicts are significantly easier to resolve than larger ones Integrating committing changes at least once a day is considered good practice and more often better Daily build Building daily if not more often is generally recommended Every commit should be built The system should build commits to the current working version to verify that they integrate correctly A common practice is to use Automated Continuous Integration although this may be done manually Automated Continuous Integration employs a continuous integration server or daemon to monitor the revision control system for changes then automatically run the build process Every bugfix commit should come with a test case When fixing a bug it is a good practice to push a test case that reproduces the bug This avoids the fix to be reverted and the bug to reappear which is known as a regression Keep the build fast The build needs to complete rapidly so that if there is a problem with integration it is quickly identified Test in a clone of the production environment Having a test environment can lead to failures in tested systems when they deploy in the production environment because the production environment may differ from the test environment in a significant way However building a replica of a production environment is costprohibitive Instead the test environment or a separate preproduction environment staging should be built to be a scalable version of the production environment to alleviate costs while maintaining technology stack composition and nuances Within these test environments service virtualisation is commonly used to obtain ondemand access to dependencies eg APIs thirdparty applications services mainframes etc that are beyond the teams control still evolving or too complex to configure in a virtual test lab Make it easy to get the latest deliverables Making builds readily available to stakeholders and testers can reduce the amount of rework necessary when rebuilding a feature that doesnt meet requirements Additionally early testing reduces the chances that defects survive until deployment Finding errors earlier can reduce the amount of work necessary to resolve them All programmers should start the day by updating the project from the repository That way they will all stay up to date Everyone can see the results of the latest build It should be easy to find out whether the build breaks and if so who made the relevant change and what that change was Automate deployment Most CI systems allow the running of scripts after a build finishes In most situations it is possible to write a script to deploy the application to a live test server that everyone can look at A further advance in this way of thinking is continuous deployment which calls for the software to be deployed directly into production often with additional automation to prevent defects or regressions Benefits CI benefits include Facilitates detecting bugs earlier Reduces effort to find cause of bugs if a CI test fails then changes since last good build contain causing change if build after each change then exactly one change is the cause Avoids the chaos of integrating many changes When a test fails or a bug is found reverting the codebase to a good state results in fewer lost changes Frequent availability of a knowngood build for testing demo and release Frequent code commit encourages modular less complex code Quick feedback on systemwide impact of code changes Supports collection of software metrics such as code coverage code complexity Risks Risks of CI include Build system setup requires effort Writing and maintaining an automated test suite requires effort Value added depends on the quality of tests High build latency sitting in queue limits value Implies that incomplete code should not be integrated which is counter to some developers preferred practice Safety and missioncritical development assurance eg DO178C ISO 26262 require documentation and review which may be difficult to achieve Best practices for cloud systems The following practices can enhance productivity of pipelines especially in systems hosted in the cloud Number of Pipelines Small teams can be more productive by having one repository and one pipeline In contrast larger organizations may have separate repositories and pipelines for each team or even separate repositories and pipelines for each service within a team Permissions In the context of pipelinerelated permissions adhering to the principle of least privilege can be challenging due to the dynamic nature of architecture Administrators may opt for more permissive permissions while implementing compensating security controls to minimize the blast radius See also Application release automation Process of packaging and deploymentPages displaying short descriptions of redirect targets Build light indicator visual device used in agile software development to inform the team on the build progressPages displaying wikidata descriptions as a fallback Comparison of continuous integration software Continuous design modular design process in which components can be freely substituted to improve the design modify performance or change another feature at a later timePages displaying wikidata descriptions as a fallback Continuous testing process of executing automated tests as part of the software delivery pipeline to obtain immediate feedback on the business risks associated with a release candidatePages displaying wikidata descriptions as a fallback Multistage continuous integration Software development technique Rapid application development Concept of software development References External links Continuous Integration wiki a collegial discussion C2 cite journal Cite journal requires journal help Richardson Jared Continuous Integration The Cornerstone of a Great Shop introduction Flowers Jay A Recipe for Build Maintainability and Reusability Archived from the original on 25 June 2020 Retrieved 28 May 2006 Duvall Paul 4 December 2007 Developer works IBM Version lifecycle MediaWiki June 2024",
  },
  {
    title: "Edge computing",
    originalContent:
      "Edge computing is a distributed computing model that brings computation and data storage closer to the sources of data More broadly it refers to any design that pushes computation physically closer to a user so as to reduce the latency compared to when an application runs on a centralized data centre The term began being used in the 1990s to describe content delivery networksthese were used to deliver website and video content from servers located near users In the early 2000s these systems expanded their scope to hosting other applications leading to early edge computing services These services could do things like find dealers manage shopping carts gather realtime data and place ads The Internet of Things IoT where devices are connected to the internet is often linked with edge computing However its important to understand that edge computing and IoT are not the same thing Definition Edge computing involves running computer programs that deliver quick responses close to where requests are made Karim Arabi during an IEEE DAC 2014 keynote and later at an MIT MTL Seminar in 2015 described edge computing as computing that occurs outside the cloud at the networks edge particularly for applications needing immediate data processing Edge computing is often equated with fog computing particularly in smaller setups However in larger deployments such as smart cities fog computing serves as a distinct layer between edge computing and cloud computing with each layer having its own responsibilities The State of the Edge report explains that edge computing focuses on servers located close to the endusers Alex Reznik Chair of the ETSI MEC ISG standards committee defines edge loosely as anything thats not a traditional data center In cloud gaming edge nodes known as gamelets are typically within one or two network hops from the client ensuring quick response times for realtime games Edge computing might use virtualization technology to simplify deploying and managing various applications on edge servers Concept The worlds data is expected to grow 61 percent to 175 zettabytes by 2025 According to research firm Gartner around 10 percent of enterprisegenerated data is created and processed outside a traditional centralized data center or cloud By 2025 the firm predicts that this figure will reach 75 percent The increase in IoT devices at the edge of the network is producing a massive amount of data storing and using all that data in cloud data centers pushes network bandwidth requirements to the limit Despite the improvements in network technology data centers cannot guarantee acceptable transfer rates and response times which often is a critical requirement for many applications Furthermore devices at the edge constantly consume data coming from the cloud forcing companies to decentralize data storage and service provisioning leveraging physical proximity to the end user In a similar way the aim of edge computing is to move the computation away from data centers towards the edge of the network exploiting smart objects mobile phones or network gateways to perform tasks and provide services on behalf of the cloud By moving services to the edge it is possible to provide content caching service delivery persistent data storage and IoT management resulting in better response times and transfer rates At the same time distributing the logic to different network nodes introduces new issues and challenges Privacy and security The distributed nature of this paradigm introduces a shift in security schemes used in cloud computing In edge computing data may travel between different distributed nodes connected through the Internet and thus requires special encryption mechanisms independent of the cloud Edge nodes may also be resourceconstrained devices limiting the choice in terms of security methods Moreover a shift from centralized topdown infrastructure to a decentralized trust model is required On the other hand by keeping and processing data at the edge it is possible to increase privacy by minimizing the transmission of sensitive information to the cloud Furthermore the ownership of collected data shifts from service providers to endusers Scalability Scalability in a distributed network must face different issues First it must take into account the heterogeneity of the devices having different performance and energy constraints the highly dynamic condition and the reliability of the connections compared to more robust infrastructure of cloud data centers Moreover security requirements may introduce further latency in the communication between nodes which may slow down the scaling process The stateoftheart scheduling technique can increase the effective utilization of edge resources and scales the edge server by assigning minimum edge resources to each offloaded task Reliability Management of failovers is crucial in order to keep a service alive If a single node goes down and is unreachable users should still be able to access a service without interruptions Moreover edge computing systems must provide actions to recover from a failure and alert the user about the incident To this aim each device must maintain the network topology of the entire distributed system so that detection of errors and recovery become easily applicable Other factors that may influence this aspect are the connection technologies in use which may provide different levels of reliability and the accuracy of the data produced at the edge that could be unreliable due to particular environment conditions As an example an edge computing device such as a voice assistant may continue to provide service to local users even during cloud service or internet outages Speed Edge computing brings analytical computational resources close to the end users and therefore can increase the responsiveness and throughput of applications A welldesigned edge platform would significantly outperform a traditional cloudbased system Some applications rely on short response times making edge computing a significantly more feasible option than cloud computing Examples range from IoT to autonomous driving anything health or human public safety relevant or involving human perception such as facial recognition which typically takes a human between 370620 ms to perform Edge computing is more likely to be able to mimic the same perception speed as humans which is useful in applications such as augmented reality where the headset should preferably recognize who a person is at the same time as the wearer does Efficiency Due to the nearness of the analytical resources to the end users sophisticated analytical tools and Artificial Intelligence tools can run on the edge of the system This placement at the edge helps to increase operational efficiency and is responsible for many advantages to the system Additionally the usage of edge computing as an intermediate stage between client devices and the wider internet results in efficiency savings that can be demonstrated in the following example A client device requires computationally intensive processing on video files to be performed on external servers By using servers located on a local edge network to perform those computations the video files only need to be transmitted in the local network Avoiding transmission over the internet results in significant bandwidth savings and therefore increases efficiency Another example is voice recognition If the recognition is performed locally it is possible to send the recognized text to the cloud rather than audio recordings significantly reducing the amount of required bandwidth Applications Edge application services reduce the volumes of data that must be moved the consequent traffic and the distance that data must travel That provides lower latency and reduces transmission costs Computation offloading for realtime applications such as facial recognition algorithms showed considerable improvements in response times as demonstrated in early research Further research showed that using resourcerich machines called cloudlets or micro data centers near mobile users which offer services typically found in the cloud provided improvements in execution time when some of the tasks are offloaded to the edge node On the other hand offloading every task may result in a slowdown due to transfer times between device and nodes so depending on the workload an optimal configuration can be defined IoTbased power grid system enables communication of electricity and data to monitor and control the power grid which makes energy management more efficient Other notable applications include connected cars autonomous cars smart cities Industry 40 home automation and satellite systems The nascent field of edge artificial intelligence edge AI implements the artificial intelligence in an edge computing environment on the device or close to where data is collected See also References",
  },
  {
    title: "Mixed reality",
    originalContent:
      "Mixed reality MR is a term used to describe the merging of a realworld environment and a computergenerated one Physical and virtual objects may coexist in mixed reality environments and interact in real time Mixed reality that incorporates haptics has sometimes been referred to as visuohaptic mixed reality In a physics context the term interreality system refers to a virtual reality system coupled with its realworld counterpart A 2007 paper describes an interreality system comprising a real physical pendulum coupled to a pendulum that only exists in virtual reality This system has two stable states of motion a dual reality state in which the motion of the two pendula are uncorrelated and a mixed reality state in which the pendula exhibit stable phaselocked motion which is highly correlated The use of the terms mixed reality and interreality is clearly defined in the context of physics and may be slightly different in other fields however it is generally seen as bridging the physical and virtual world Applications Mixed reality has been used in applications across fields including design education entertainment military training healthcare product content management and humanintheloop operation of robots Education Simulationbased learning includes VR and AR based training and interactive experiential learning There are many potential use cases for mixed reality in both educational settings and professional training settings In education AR has been used to simulate historical battles providing an unparalleled immersive experience for students and potentially enhanced learning experiences In addition AR has shown effectiveness in university education for health science and medical students within disciplines that benefit from 3D representations of models such as physiology and anatomy Entertainment From television shows to game consoles mixed reality has many applications in the field of entertainment The 2004 British game show Bamzooki called upon child contestants to create virtual Zooks and watch them compete in a variety of challenges The show used mixed reality to bring the Zooks to life The television show ran for four seasons ending in 2010 The 2003 game show FightBox also called upon contestants to create competitive characters and used mixed reality to allow them to interact Unlike Bamzoomis generally nonviolent challenges the goal of FightBox was for new contestants to create the strongest fighter to win the competition In 2009 researchers presented to the International Symposium on Mixed and Augmented Reality ISMAR their social product called BlogWall which consisted of a projected screen on a wall Users could post short text clips or images on the wall and play simple games such as Pong The BlogWall also featured a poetry mode where it would rearrange the messages it received to form a poem and a polling mode where users could ask others to answer their polls Mario Kart Live Home Circuit is a mixed reality racing game for the Nintendo Switch that was released in October 202016aNew The game allows players to use their home as a race track Within the first week of release 73918 copies were sold in Japan making it the countrys best selling game of the week Other research has examined the potential for mixed reality to be applied to theatre film and theme parks Military training The first fully immersive mixed reality system was the Virtual Fixtures platform which was developed in 1992 by Louis Rosenberg at the Armstrong Laboratories of the United States Air Force It enabled human users to control robots in realworld environments that included real physical objects and 3D virtual overlays fixtures that were added enhance human performance of manipulation tasks Published studies showed that by introducing virtual objects into the real world significant performance increases could be achieved by human operators Combat reality can be simulated and represented using complex layered data and visual aides most of which are headmounted displays HMD which encompass any display technology that can be worn on the users head Military training solutions are often built on commercial offtheshelf COTS technologies such as Improbables synthetic environment platform Virtual Battlespace 3 and VirTra with the latter two platforms used by the United States Army As of 2018 VirTra is being used by both civilian and military law enforcement to train personnel in a variety of scenarios including active shooter domestic violence and military traffic stops Mixed reality technologies have been used by the United States Army Research Laboratory to study how this stress affects decisionmaking With mixed reality researchers may safely study military personnel in scenarios where soldiers would not likely survive In 2017 the US Army was developing the Synthetic Training Environment STE a collection of technologies for training purposes that was expected to include mixed reality As of 2018 STE was still in development without a projected completion date Some recorded goals of STE included enhancing realism and increasing simulation training capabilities and STE availability to other systems It was claimed that mixedreality environments like STE could reduce training costs such as reducing the amount of ammunition expended during training In 2018 it was reported that STE would include representation of any part of the worlds terrain for training purposes STE would offer a variety of training opportunities for squad brigade and combat teams including Stryker armory and infantry teams Blended spaces A blended space is a space in which a physical environment and a virtual environment are deliberately integrated in a close knit way The aim of blended space design is to provide people with the experience of feeling a sense of presence in the blended space acting directly on the content of the blended space Examples of blended spaces include augmented reality devices such as the Microsoft HoloLens and games such as Pokmon Go in addition to many smartphone tourism apps smart meeting rooms and applications such as bus tracker systems The idea of blending comes from the ideas of conceptual integration or conceptual blending introduced by Gilles Fauconnier and Mark Turner Manuel Imaz and David Benyon introduced blending theory to look at concepts in software engineering and humancomputer interaction The simplest implementation of a blended space requires two features The first required feature is input The input can range from tactile to changes in the environment The next required feature is notifications received from the digital spaces The correspondences between the physical and digital space have to be abstracted and exploited by the design of the blended space Seamless integration of both the spaces is rare Blended spaces need anchoring points or technologies to link the spaces A well designed blended space advertises and conveys the digital content in a subtle and unobtrusive way Presence can be measured using physiological behavioral and subjective measures derived from the space There are two main components to any space They are Objects The actual distinct objects which make up the mediumspace The objects thus effectively describe the space Agents Correspondentsusers inside the space who interact with it through the objects For presence in a blended space there must be a physical space and a digital space In the context of blended space the higher the communication between the physical and digital spaces the richer the experience This communication happens through the medium of correspondents which relay the state and nature of objectsFor the purpose of looking at blended spaces the nature and characteristics of any space can be represented by these factors Ontology Different types of objects present in the space the total number of objects and the relationships between objects and the space Topology The way objects are placed and positioned Volatility Frequency with which the objects change Agency Medium of communication between the objects and between the objects and users Agency also encompasses the users inside the space Physical space Physical spaces are spaces which afford spatial interaction This kind of spatial interaction greatly impacts the users cognitive model Digital space Digital space also called the information space consists of all the information content This content can be in any form Remote working Mixed reality allows a global workforce of remote teams to work together and tackle an organizations business challenges No matter where they are physically located an employee can wear a headset and noisecanceling headphones and enter a collaborative immersive virtual environment As these applications can accurately translate in real time language barriers become irrelevant This process also increases flexibility While many employers still use inflexible models of fixed working time and location there is evidence that employees are more productive if they have greater autonomy over where when and how they work Some employees prefer loud work environments while others need silence Some work best in the morning others work best at night Employees also benefit from autonomy in how they work because of different ways of processing information The classic model for learning styles differentiates between visual auditory and kinesthetic learners Machine maintenance can also be executed with the help of mixed reality Larger companies with multiple manufacturing locations and a lot of machinery can use mixed reality to educate and instruct their employees The machines need regular checkups and have to be adjusted every now and then These adjustments are mostly done by humans so employees need to be informed about needed adjustments By using mixed reality employees from multiple locations can wear headsets and receive live instructions about the changes Instructors can operate the representation that every employee sees and can glide through the production area zooming in to technical details and explaining every change needed Employees completing a fiveminute training session with such a mixedreality program have been shown to attain the same learning results as reading a 50page training manual An extension to this environment is the incorporation of live data from operating machinery into the virtual collaborative space and then associated with three dimensional virtual models of the equipment This enables training and execution of maintenance operational and safety work processes which would otherwise be difficult in a live setting while making use of expertise no matter their physical location Functional mockup Mixed reality can be used to build mockups that combine physical and digital elements With the use of simultaneous localization and mapping SLAM mockups can interact with the physical world to gain control of more realistic sensory experiences like object permanence which would normally be infeasible or extremely difficult to track and analyze without the use of both digital and physical aides Healthcare Smartglasses can be incorporated into the operating room to aide in surgical procedures possibly displaying patient data conveniently while overlaying precise visual guides for the surgeon Mixed reality headsets like the Microsoft HoloLens have been theorized to allow for efficient sharing of information between doctors in addition to providing a platform for enhanced training This can in some situations ie patient infected with contagious disease improve doctor safety and reduce PPE use While mixed reality has lots of potential for enhancing healthcare it does have some drawbacks too The technology may never fully integrate into scenarios when a patient is present as there are ethical concerns surrounding the doctor not being able to see the patient Mixed reality is also useful for healthcare education For example according to a 2022 report from the World Economic Forum 85 of firstyear medical students at Case Western Reserve University reported that mixed reality for teaching anatomy was equivalent or better than the inperson class Product content management Product content management before the advent of mixed reality consisted largely of brochures and little customerproduct engagement outside of this 2dimensional realm With mixed reality technology improvements new forms of interactive product content management has emerged Most notably 3dimensional digital renderings of normally 2dimensional products have increased reachability and effectiveness of consumerproduct interaction Humanintheloop operation of robots Recent advances in mixedreality technologies have renewed interest in alternative modes of communication for humanrobot interaction Human operators wearing mixed reality glasses such as HoloLens can interact with control and monitor eg robots and lifting machines on site in a digital factory setup This use case typically requires realtime data communication between a mixed reality interface with the machine process system which could be enabled by incorporating digital twin technology Business firms Mixed reality allows sellers to show the customers how a certain commodity will suit their demands A seller may demonstrate how a certain product will fit into the homes of the buyer The buyer with the assistance of the VR can virtually pick the item spin around and place to their desired points This improves the buyers confidence of making a purchase and reduces the number of returns Architectural firms can allow customers to virtually visit their desired homes Display technologies and products While mixed reality refers to the intertwining of the virtual world and the physical world at a high level there are a variety of digital mediums used to accomplish a mixed reality environment They may range from handheld devices to entire rooms each having practical uses in different disciplines Cave automatic virtual environment The cave automatic virtual environment CAVE is an environment typically a small room located in a larger outer room in which a user is surrounded by projected displays around them above them and below them 3D glasses and surround sound complement the projections to provide the user with a sense of perspective that aims to simulate the physical world Since being developed CAVE systems have been adopted by engineers developing and testing prototype products They allow product designers to test their prototypes before expending resources to produce a physical prototype while also opening doors for handson testing on nontangible objects such as microscopic environments or entire factory floors After developing the CAVE the same researchers eventually released the CAVE2 which builds off of the original CAVEs shortcomings The original projections were substituted for 37 megapixel 3D LCD panels network cables integrate the CAVE2 with the internet and a more precise camera system allows the environment to shift as the user moves throughout it Headup display Headup display HUD is a display that projects imagery directly in front of a viewer without heavily obfuscating their environment A standard HUD is composed of three elements a projector which is responsible for overlaying the graphics of the HUD the combiner which is the surface the graphics are projected onto and the computer which integrates the two other components and computes any realtime calculations or adjustments Prototype HUDs were first used in military applications to aid fighter pilots in combat but eventually evolved to aid in all aspects of flight not just combat HUDs were then standardized across commercial aviation as well eventually creeping into the automotive industry One of the first applications of HUD in automotive transport came with Pioneers Headsup system which replaces the driverside sun visor with a display that projects navigation instructions onto the road in front of the driver Major manufacturers such as General Motors Toyota Audi and BMW have since included some form of headup display in certain models Headmounted display A headmounted display HMD worn over the entire head or worn in front of the eyes is a device that uses one or two optics to project an image directly in front of the users eyes Its applications range across medicine entertainment aviation and engineering providing a layer of visual immersion that traditional displays cannot achieve Headmounted displays are most popular with consumers in the entertainment market with major tech companies developing HMDs to complement their existing products However these headmounted displays are virtual reality displays and do not integrate the physical world Popular augmented reality HMDs however are more favorable in enterprise environments Microsofts HoloLens is an augmented reality HMD that has applications in medicine giving doctors more profound realtime insight as well as engineering overlaying important information on top of the physical world Another notable augmented reality HMD has been developed by Magic Leap a startup developing a similar product with applications in both the private sector and the consumer market Mobile devices Mobile devices including smartphones and tablets have continued to increase in computing power and portability Many modern mobile devices come equipped with toolkits for developing augmented reality applications These applications allow developers to overlay computer graphics over videos of the physical world The first augmented reality mobile game with widespread success was Pokmon GO which released in 2016 and accumulated 800 million downloads While entertainment applications utilizing AR have proven successful productivity and utility apps have also begun integrating AR features Google has released updates to their Google Maps application that includes AR navigation directions overlaid onto the streets in front of the user as well as expanding their translate app to overlay translated text onto physical writing in over 20 foreign languages Mobile devices are unique display technologies due to the fact that they are commonly equipped at all times See also References Further reading Signer Beat Curtin Timothy J 2017 Tangible Holograms Towards Mobile Physical Augmentation of Virtual Objects Technical Report WISE Lab WISE201701 March 2017 Fleischmann Monika Strauss Wolfgang eds 2001 Proceedings Archived 3 March 2016 at the Wayback Machine of CAST01Living in Mixed Realities Archived 16 November 2020 at the Wayback Machine Intl Conf On Communication of Art Science and Technology Fraunhofer IMK 2001 401 ISSN 16181379 Print ISSN 16181387 Internet Interactive Multimedia Lab A research lab at the National University of Singapore focuses on Multimodal Mixed Reality interfaces Mixed Reality Geographical Information System MRGIS Costanza E Kunz A and Fjeld M 2009 Mixed Reality A Survey Costanza E Kunz A and Fjeld M 2009 Mixed Reality A Survey In Human Machine interaction Research Results of the MMI Program D Lalanne and J Kohlas Eds LNCS 5440 pp 4768 H Regenbrecht and C Ott and M Wagner and T Lum and P Kohler and W Wilke and E Mueller An Augmented Virtuality Approach to 3D Videoconferencing Proceedings of the 2nd IEEE and ACM International Symposium on Mixed and Augmented Reality pp 290291 2003 Kristian Simsarian and KarlPetter Akesson Windows on the World An example of Augmented Virtuality Interface Sixth International Conference Montpellier Manmachine interaction pp 6871 1997 Mixed Reality Project experimental applications on Mixed Reality Augmented Reality Augmented Virtuality and Virtual Reality Mixed Reality Scale Milgram and Kishinos 1994 Virtuality Continuum paraphrase with examples IEICE Transactions on Information Systems Vol E77D No12 December 1994 A taxonomy of mixed reality visual displays Paul Milgram Fumio Kishino Archived 4 May 2017 at the Wayback Machine External links Media related to Mixed reality at Wikimedia Commons",
  },
  {
    title: "Artificial organs",
    originalContent:
      "An artificial organ is a humanmade organ device or tissue that is implanted or integrated into a human interfacing with living tissue to replace a natural organ to duplicate or augment a specific function or functions so the patient may return to a normal life as soon as possible The replaced function does not have to be related to life support but it often is For example replacement bones and joints such as those found in hip replacements could also be considered artificial organs Implied by definition is that the device must not be continuously tethered to a stationary power supply or other stationary resources such as filters or chemical processing units Periodic rapid recharging of batteries refilling of chemicals andor cleaningreplacing of filters would exclude a device from being called an artificial organ Thus a dialysis machine while a very successful and critically important life support device that almost completely replaces the duties of a kidney is not an artificial organ Purpose Constructing and installing artificial organs an extremely researchintensive and expensive process initially may entail many years of ongoing maintenance services not needed by a natural organ providing life support to prevent imminent death while awaiting a transplant eg artificial heart dramatically improving the patients ability for self care eg artificial limb improving the patients ability to interact socially eg cochlear implant or improving a patients quality of life through cosmetic restoration after cancer surgery or an accident The use of any artificial organ by humans is almost always preceded by extensive experiments with animals Initial testing in humans is frequently limited to those either already facing death or who have exhausted every other treatment possibility Examples Artificial limbs Artificial arms and legs or prosthetics are intended to restore a degree of normal function to amputees Mechanical devices that allow amputees to walk again or continue to use two hands have probably been in use since ancient times the most notable one being the simple peg leg Since then the development of artificial limbs has progressed rapidly New plastics and other materials such as carbon fiber have allowed artificial limbs to become stronger and lighter limiting the amount of extra energy necessary to operate the limb Additional materials have allowed artificial limbs to look much more realistic Prostheses can roughly be categorized as upper and lowerextremity and can take many shapes and sizes New advances in artificial limbs include additional levels of integration with the human body Electrodes can be placed into nervous tissue and the body can be trained to control the prosthesis This technology has been used in both animals and humans The prosthetic can be controlled by the brain using a direct implant or implant into various muscles Bladder The two main methods for replacing bladder function involve either redirecting urine flow or replacing the bladder in situ Standard methods for replacing the bladder involve fashioning a bladderlike pouch from intestinal tissue As of 2017 methods to grow bladders using stem cells had been attempted in clinical research but this procedure was not part of medicine Brain Neural prostheses are a series of devices that can substitute a motor sensory or cognitive modality that might have been damaged as a result of an injury or a disease Neurostimulators including deep brain stimulators send electrical impulses to the brain in order to treat neurological and movement disorders including Parkinsons disease epilepsy treatment resistant depression and other conditions such as urinary incontinence Rather than replacing existing neural networks to restore function these devices often serve by disrupting the output of existing malfunctioning nerve centers to eliminate symptoms Scientists in 2013 created a mini brain that developed key neurological components until the early gestational stages of fetal maturation Corpora cavernosa To treat erectile dysfunction both corpora cavernosa can be irreversibly surgically replaced with manually inflatable penile implants This is a drastic therapeutic surgery meant only for men who have complete impotence resistant to all other treatment approaches An implanted pump in the groin or scrotum can be manipulated by hand to fill these artificial cylinders normally sized to be direct replacements for the natural corpora cavernosa from an implanted reservoir in order to achieve an erection Ear In cases when a person is profoundly deaf or severely hard of hearing in both ears a cochlear implant may be surgically implanted Cochlear implants bypass most of the peripheral auditory system to provide a sense of sound via a microphone and some electronics that reside outside the skin generally behind the ear The external components transmit a signal to an array of electrodes placed in the cochlea which in turn stimulates the cochlear nerve In the case of an outer ear trauma a craniofacial prosthesis may be necessary Thomas Cervantes and his colleagues who are from Massachusetts General Hospital built an artificial ear from sheep cartilage by a 3D printer With a lot of calculations and models they managed to build an ear shaped like a typical human one Modeled by a plastic surgeon they had to adjust several times so the artificial ear can have curves and lines just like a human ear The researchers said The technology is now under development for clinical trials and thus we have scaled up and redesigned the prominent features of the scaffold to match the size of an adult human ear and to preserve the aesthetic appearance after implantation Their artificial ears have not been announced as successful but they are still currently developing the project Each year thousands of children were born with a congenital deformity called microtia where the external ear does not fully develop This could be a major step forward in medical and surgical microtia treatment Eye The most successful functionreplacing artificial eye so far is actually an external miniature digital camera with a remote unidirectional electronic interface implanted on the retina optic nerve or other related locations inside the brain The present state of the art yields only partial functionality such as recognizing levels of brightness swatches of color andor basic geometric shapes proving the concepts potential Various researchers have demonstrated that the retina performs strategic image preprocessing for the brain The problem of creating a completely functional artificial electronic eye is even more complex Advances towards tackling the complexity of the artificial connection to the retina optic nerve or related brain areas combined with ongoing advances in computer science are expected to dramatically improve the performance of this technology Heart Cardiovascularrelated artificial organs are implanted in cases where the heart its valves or another part of the circulatory system is in disorder The artificial heart is typically used to bridge the time to heart transplantation or to permanently replace the heart in case heart transplantation is impossible Artificial pacemakers represent another cardiovascular device that can be implanted to either intermittently augment defibrillator mode continuously augment or completely bypass the natural living cardiac pacemaker as needed Ventricular assist devices are another alternative acting as mechanical circulatory devices that partially or completely replace the function of a failing heart without the removal of the heart itself Besides these labgrown hearts and 3D bioprinted hearts are also being researched Currently scientists are limited in their ability to grow and print hearts due to difficulties in getting blood vessels and labmade tissues to function cohesively Liver HepaLife is developing a bioartificial liver device intended for the treatment of liver failure using stem cells The artificial liver is designed to serve as a supportive device either allowing the liver to regenerate upon failure or to bridge the patients liver functions until transplant is available It is only made possible by the fact that it uses real liver cells hepatocytes and even then it is not a permanent substitute Lungs With some almost fully functional artificial lungs promise to be a great success in the near future An Ann Arbor company MC3 is currently working on this type of medical device Extracorporeal membrane oxygenation ECMO can be used to take significant load off of the native lung tissue and heart In ECMO one or more catheters are placed into the patient and a pump is used to flow blood over hollow membrane fibers which exchange oxygen and carbon dioxide with the blood Similar to ECMO Extracorporeal CO2 Removal ECCO2R has a similar setup but mainly benefits the patient through carbon dioxide removal rather than oxygenation with the goal of allowing the lungs to relax and heal Ovaries The ground work for the development of the artificial ovary was laid in the early 1990s Reproductive age patients who develop cancer often receive chemotherapy or radiation therapy which damages oocytes and leads to early menopause An artificial human ovary has been developed at Brown University with selfassembled microtissues created using novel 3D petri dish technology In a study funded and conducted by the NIH in 2017 scientists were successful in printing 3D ovaries and implanting them in sterile mice In the future scientists hope to replicate this in larger animals as well as humans The artificial ovary will be used for the purpose of in vitro maturation of immature oocytes and the development of a system to study the effect of environmental toxins on folliculogenesis Pancreas An artificial pancreas is used to substitute endocrine functionality of a healthy pancreas for diabetic and other patients who require it It can be used to improve insulin replacement therapy until glycemic control is practically normal as evident by the avoidance of the complications of hyperglycemia and it can also ease the burden of therapy for the insulindependent Approaches include using an insulin pump under closed loop control developing a bioartificial pancreas consisting of a biocompatible sheet of encapsulated beta cells or using gene therapy Red blood cells Artificial red blood cells RBC have already been in projects for about 60 years but they started getting interest when the HIVcontaminateddonor blood crisis Artificial RBCs will be dependent 100 on nanotechnology A successful artificial RBC should be able to totally replace human RBC which means it can carry on all the functions that a human RBC does The first artificial RBC made by Chang and Poznanski in 1968 was made to transport Oxygen and Carbon Dioxide also fulfilled antioxidant functions Scientists are working on a new kind of artificial RBC which is onefiftieth the size of a human RBC They are made from purified human hemoglobin proteins that have been coated with a synthetic polymer Thanks to the special materials of the artificial RBC they can capture oxygen when blood pH is high and release oxygen when blood pH is low The polymer coating also keeps the hemoglobin from reacting with nitric oxide in the bloodstream thus preventing dangerous constriction of the blood vessels Allan Doctor MD stated that the artificial RBC can be used by anyone with any blood type because the coating is immune silent Testes Men whom have sustained testicular abnormalities through birth defects or injury have been able to replace the damaged testicle with a testicular prosthesis Although the prosthetic does not restore biological reproductive function the device has been shown to improve mental health for these patients Thymus An implantable machine that performs the function of a thymus does not exist However researchers have been able to grow a thymus from reprogrammed fibroblasts They expressed hope that the approach could one day replace or supplement neonatal thymus transplantation As of 2017 researchers at UCLA developed an artificial thymus that although not yet implantable is capable of performing all functions of a true thymus The artificial thymus would play an important role in the immune system and it would use blood stem cells to produce more T cells which in turn help the body fight infections It would ultimately give the body a better ability to fight cancer cells As people age if their thymus stops working well an artificial thymus could also be a potentially viable option The idea of using T cells to fight against infections has been around for a time but until recently the idea of using a T cell source an artificial thymus is proposed We know that the key to creating a consistent and safe supply of cancerfighting T cells would be to control the process in a way that deactivates all T cell receptors in the transplanted cells except for the cancerfighting receptors said Dr Gay Crooks of UCLA The scientist also found that the T cells produced by the artificial thymus carried a diverse range of T cell receptors and worked similarly to the T cells produced by a normal thymus Since they can work like human thymus artificial thymus can supply a consistent amount of T cells to the body for the patients who are in need of treatments Trachea The field of artificial tracheas went through a period of high interest and excitement with the work of Paolo Macchiarini at the Karolinska Institute and elsewhere from 2008 to around 2014 with frontpage coverage in newspapers and on television Concerns were raised about his work in 2014 and by 2016 he had been fired and high level management at Karolinska had been dismissed including people involved in the Nobel Prize As of 2017 engineering a trachea a hollow tube lined with cells had proved more challenging than originally thought challenges include the difficult clinical situation of people who present as clinical candidates who generally have been through multiple procedures already creating an implant that can become fully developed and integrate with host while withstanding respiratory forces as well as the rotational and longitudinal movement the trachea undergoes Enhancement It is also possible to construct and install an artificial organ to give its possessor abilities that are not naturally occurring Research is proceeding in areas of vision memory and information processing Some current research focuses on restoring shortterm memory in accident victims and longterm memory in dementia patients One area of success was achieved when Kevin Warwick carried out a series of experiments extending his nervous system over the internet to control a robotic hand and the first direct electronic communication between the nervous systems of two humans This might also include the existing practice of implanting subcutaneous chips for identification and location purposes ex RFID tags Microchips Organ chips are devices containing hollow microvessels filled with cells simulating tissue andor organs as a microfluidic system that can provide key chemical and electrical signal information This is distinct from an alternative use of the term microchip which refers to small electronic chips that are commonly used as an identifier and can also contain a transponder This information can create various applications such as creating human in vitro models for both healthy and diseased organs drug advancements in toxicity screening as well as replacing animal testing Using 3D cell culture techniques enables scientists to recreate the complex extracellular matrix ECM found in in vivo to mimic human response to drugs and human diseases Organs on chips are used to reduce the failure rate in new drug development microengineering these allows for a microenvironment to be modeled as an organ See also References Further reading External links Artificial Organs ISSN 15251594 American Society for Artificial Internal Organs ASAIO Elon Musk wants to hook your brain up directly to computers starting next year at NBC News",
  },
  {
    title: "Gene editing",
    originalContent:
      "Gene editing may refer to Genetic engineering of any organism by genome editing Gene editing is the emerging molecular biology technique which makes very specific targeted changes by insertion deletion or substitution of genetic material in an organisms DNA to obtain desired results Examples of gene editing are CRISPR zinc finger nuclease transcription activatorlike effector nuclease TALEN oligonucleotide directed mutagenesis meganucleases Genome editing a type of genetic engineering Gene therapy the therapeutic delivery of nucleic acid polymers into a patients cells as a drug to treat disease CRISPR gene editing a genetic engineering techniqueCRISPR are termed as site directed nucleases SDN since they target specific part of genome there are 3 different categories of SDN SDN1 makes random mutations at target site to repair the damaged host DNA without involving any foreign DNA SDN2 uses small non coding homologous repair DNA to achieve specific nucleotide sequence to repair the host DNA by homology directed repair HDR which is a natural nucleic acid repair system SDN3 uses a large stretch of protein coding donor DNA which is targeted for insertion through HDR at a predefined genomic locus TALEN editing using transcription activatorlike effector nucleases TALENs are another type of genome editing tool They work by using engineered proteins that can recognize and bind to specific DNA sequences which then triggers a cut in the DNA TALENs are less efficient than CRISPRCas9 but they are still a useful tool for genome editing Zinc finger editing using zinc finger nucleases Natural genetic engineering NGE has been proposed by molecular biologist James A Shapiro to account for novelty created in the course of biological evolution See also Genetic editing an approach to scholarly editing of literary texts",
  },
  {
    title: "Immunotherapy",
    originalContent:
      "Immunotherapy or biological therapy is the treatment of disease by activating or suppressing the immune system Immunotherapies designed to elicit or amplify an immune response are classified as activation immunotherapies while immunotherapies that reduce or suppress are classified as suppression immunotherapies Immunotherapy is under preliminary research for its potential to treat various forms of cancer Cellbased immunotherapies are effective for some cancers Immune effector cells such as lymphocytes macrophages dendritic cells natural killer cells and cytotoxic T lymphocytes work together to defend the body against cancer by targeting abnormal antigens expressed on the surface of tumor cells Vaccineinduced immunity to COVID19 relies mostly on an immunomodulatory Tcell response Therapies such as granulocyte colonystimulating factor GCSF interferons imiquimod and cellular membrane fractions from bacteria are licensed for medical use Others including IL2 IL7 IL12 various chemokines synthetic cytosine phosphateguanosine CpG oligodeoxynucleotides and glucans are involved in clinical and preclinical studies Immunomodulators Immunomodulators are the active agents of immunotherapy They are a diverse array of recombinant synthetic and natural preparations Activation immunotherapies Cancer Cancer treatment used to be focused on killing or removing cancer cells and tumours with chemotherapy or surgery or radiation In 2018 the Nobel Prize in Physiology or Medicine was awarded to James P Allison and Tasuku Honjo for their discovery of cancer therapy by inhibition of negative immune regulation Cancer immunotherapy attempts to stimulate the immune system to destroy tumours A variety of strategies are in use or are undergoing research and testing Randomized controlled studies in different cancers resulting in significant increase in survival and disease free period have been reported and its efficacy is enhanced by 2030 when cellbased immunotherapy is combined with conventional treatment methods One of the oldest forms of cancer immunotherapy is the use of BCG vaccine which was originally to vaccinate against tuberculosis and later was found to be useful in the treatment of bladder cancer BCG immunotherapy induces both local and systemic immune responses The mechanisms by which BCG immunotherapy mediates tumor immunity have been widely studied but they are still not completely understood The use of monoclonal antibodies in cancer therapy was first introduced in 1997 with rituximab an antiCD20 antibody for treatment of B cell lymphoma Since then several monoclonal antibodies have been approved for treatment of various haematological malignancies as well as for solid tumours The extraction of GCSF lymphocytes from the blood and expanding in vitro against a tumour antigen before reinjecting the cells with appropriate stimulatory cytokines The cells then destroy the tumour cells that express the antigen Topical immunotherapy utilizes an immune enhancement cream imiquimod which produces interferon causing the recipients killer T cells to destroy warts actinic keratoses basal cell cancer vaginal intraepithelial neoplasia squamous cell cancer cutaneous lymphoma and superficial malignant melanoma Injection immunotherapy intralesional or intratumoural uses mumps candida the HPV vaccine or trichophytin antigen injections to treat warts HPV induced tumours Adoptive cell transfer has been tested on lung and other cancers with greatest success achieved in melanoma Dendritic cellbased pumppriming or vaccination Dendritic cells DC can be stimulated to activate a cytotoxic response towards an antigen Dendritic cells a type of antigenpresenting cell are harvested from the person needing the immunotherapy These cells are then either pulsed with an antigen or tumour lysate or transfected with a viral vector causing them to display the antigen Upon transfusion into the person these activated cells present the antigen to the effector lymphocytes CD4 helper T cells cytotoxic CD8 T cells and B cells This initiates a cytotoxic response against tumour cells expressing the antigen against which the adaptive response has now been primed The first FDAapproved cellbased immunotherapy the cancer vaccine SipuleucelT is one example of this approach The Immune Response Corporation IRC developed this immunotherapy and licensed the technology to Dendreon which obtained FDA clearance The current approaches for DCbased vaccination are mainly based on antigen loading on in vitrogenerated DCs from monocytes or CD34 cells activating them with different TLR ligands cytokine combinations and injecting them back to the patients The in vivo targeting approaches comprise administering specific cytokines eg Flt3L GMCSF and targeting the DCs with antibodies to Ctype lectin receptors or agonistic antibodies eg antiCD40 that are conjugated with antigen of interest Multiple nextgeneration antiCD40 platforms are being actively developed Future approach may target DC subsets based on their specifically expressed Ctype lectin receptors or chemokine receptors Another potential approach is the generation of genetically engineered DCs from induced pluripotent stem cells and use of neoantigenloaded DCs for inducing better clinical outcome Tcell adoptive transfer Adoptive cell transfer in vitro cultivates autologous extracted T cells for later transfusion Alternatively Genetically engineered T cells are created by harvesting T cells and then infecting the T cells with a retrovirus that contains a copy of a T cell receptor TCR gene that is specialised to recognise tumour antigens The virus integrates the receptor into the T cells genome The cells are expanded nonspecifically andor stimulated The cells are then reinfused and produce an immune response against the tumour cells The technique has been tested on refractory stage IV metastatic melanomas and advanced skin cancer The first FDAapproved CART drug Kymriah used this approach To obtain the clinical and commercial supply of this CART Novartis purchased the manufacturing plant the distribution system and hired the production team that produced SipuleucelT developed by Dendreon and the Immune Response Corporation Whether T cells are genetically engineered or not before reinfusion lymphodepletion of the recipient is required to eliminate regulatory T cells as well as unmodified endogenous lymphocytes that compete with the transferred cells for homeostatic cytokines Lymphodepletion may be achieved by myeloablative chemotherapy to which total body irradiation may be added for greater effect Transferred cells multiplied in vivo and persisted in peripheral blood in many people sometimes representing levels of 75 of all CD8 T cells at 612 months after infusion As of 2012 clinical trials for metastatic melanoma were ongoing at multiple sites Clinical responses to adoptive transfer of T cells were observed in patients with metastatic melanoma resistant to multiple immunotherapies Checkpoint inhibitors AntiPD1PDL1 and antiCTLA4 antibodies are the two types of checkpoint inhibitors currently available to patients The approval of anticytotoxic Tlymphocyteassociated protein 4 CTLA4 and antiprogrammed cell death protein 1 PD1 antibodies for human use has already resulted in significant improvements in disease outcomes for various cancers Although these molecules were originally discovered as molecules playing a role in T cell activation or apoptosis subsequent preclinical research showed their important role in the maintenance of peripheral immune tolerance Immune checkpoint inhibitors are approved to treat some patients with a variety of cancer types including melanoma breast cancer bladder cancer cervical cancer colon cancer lung cancer head and neck cancer or Hodgkin lymphoma These therapies have revolutionized cancer immunotherapy as they showed for the first time in many years of research in metastatic melanoma which is considered one of the most immunogenic human cancers an improvement in overall survival with an increasing group of patients benefiting longterm from these treatments although caution remains needed for specific subgroups The next generation of checkpoint inhibitors targets other receptors such as lymphocyteactivation gene 3 LAG3 Tcell immunoglobulin and mucindomain containing3 TIM3 and T cell immunoreceptor with Ig and ITIM domains TIGIT Antibodies against these receptors have been evaluated in clinical studies but have not yet been approved for widespread use Immune enhancement therapy Autologous immune enhancement therapy use a persons own peripheral bloodderived natural killer cells cytotoxic T lymphocytes epithelial cells and other relevant immune cells are expanded in vitro and then reinfused The therapy has been tested against hepatitis C chronic fatigue syndrome and HHV6 infection Suppression immunotherapies Immune suppression dampens an abnormal immune response in autoimmune diseases or reduces a normal immune response to prevent rejection of transplanted organs or cells Immunosuppressive drugs Immunosuppressive drugs can be used to control the immune system with organ transplantation and with autoimmune disease Immune responses depend on lymphocyte proliferation Lymphocyte proliferation is the multiplication of lymphocyte cells used to fight and remember foreign invaders Cytostatic drugs are a type of immunosuppressive drug that aids in slowing down the growth of rapidly dividing cells Another example of an immunosuppressive drug is Glucocorticoids which are more specific inhibitors of lymphocyte activation Glucocorticoids work by emulating actions of natural actions of the bodys adrenal glands to help suppress the immune system which is helpful with autoimmune diseases Alternatively inhibitors of immunophilins more specifically target T lymphocyte activation the process by which Tlymphocytes stimulate and begin to respond to a specific antigen There is also Immunosuppressive antibodies which target steps in the immune response to prevent the body from attacking its tissues which is a problem with autoimmune diseases There are various other drugs that modulate immune responses and can be used to induce immune regulation It was observed in a preclinical trial that regulation of the immune system by small immunosuppressive molecules such as vitamin D dexamethasone and curcumin could be helpful in preventing or treating chronic inflation Given that the molecules are administered under a lowdose regimen and subcutaneously A study provides a promising preclinical demonstration of the effectiveness and ease of preparation of Valrubicinloaded immunoliposomes ValILs as a novel nanoparticle technology to target immunosuppressive cells ValILs have the potential to be used as a precise and effective therapy based on targeted vesiclemediated cell death of immunosuppressive cells Immune tolerance The body naturally does not launch an immune system attack on its own tissues Models generally identify CD4 Tcells at the centre of the autoimmune response Loss of Tcell tolerance then unleashes Bcells and other immune effector cells on to the target tissue The ideal tolerogenic therapy would target the specific Tcell clones coordinating the autoimmune attack Immune tolerance therapies seek to reset the immune system so that the body stops mistakenly attacking its own organs or cells in autoimmune disease or accepts foreign tissue in organ transplantation A recent therapeutic approach is the infusion of regulatory immune cells into transplant recipients The transfer of regulatory immune cells has the potential to inhibit the activity of effector Creating immune tolerance reduces or eliminates the need for lifelong immunosuppression and attendant side effects It has been tested on transplantations rheumatoid arthritis type 1 diabetes and other autoimmune disorders Allergen immunotherapy Immunotherapy can also be used to treat allergies While allergy treatments such as antihistamines or corticosteroids treat allergic symptoms immunotherapy can reduce sensitivity to allergens lessening its severity Allergen immunotherapy can also be referred to as allergen desensitization or hyposensitization Immunotherapy may produce longterm benefits Immunotherapy is partly effective in some people and ineffective in others but it offers people with allergies a chance to reduce or stop their symptoms Subcutaneous allergen immunotherapy was first introduced in 1911 through the hypothesis that people with hay fever were sensitive to pollen from grass A process was developed to create an extract by drawing out timothy pollen in distilled water and then boiling it This was injected into patients in increasing doses to help alleviate symptoms Allergen Immunotherapy is indicated for people who are extremely allergic or who cannot avoid specific allergens and when there is evidence of an IgEmediated reaction that correlates with allergen symptoms These IgEmediated reactions can be identified via a blood IgE test or skin testing If a specific IgE antibody is negative there is no evidence that allergen immunotherapy will be effective for that patient However there are risks associated with allergen immunotherapy as it is the administration of an agent the patient is known to be highly allergic to Patients are at increased risk of fatal anaphylaxis local reaction at the site of injection or lifethreatening systemic allergic reactions A promising approach to treat food allergies is the use of oral immunotherapy OIT OIT consists in a gradual exposure to increasing amounts of allergen can lead to the majority of subjects tolerating doses of food sufficient to prevent reaction on accidental exposure Dosages increase over time as the person becomes desensitized This technique has been tested on infants to prevent peanut allergies Helminthic therapies Whipworm ova Trichuris suis and hookworm Necator americanus have been tested for immunological diseases and allergies and have proved beneficial on multiple fronts yet it is not entirely understood Scientists have found that the immune response triggered by the burrowing of hookworm larvae to pass through the lungs and blood so the production of mast cells and specific antibodies are now present They also reduce inflammation or responses ties to autoimmune diseases but despite this the hookworms effects are considered to be negative typically Helminthic therapy has been investigated as a treatment for relapsing remitting multiple sclerosis Crohns allergies and asthma While there is much to be learned about this many researchers think that the change in the immune response is thanks to the parasites shifting to a more antiinflammatory or regulatory system which would in turn decrease inflammation and self inflicted immune damage as seen in Crohns and multiple sclerosis Specifically MS patients saw lower relapse rates and calmer symptoms in some cases when experimenting with helminthic therapy Hypothesized mechanisms include repolarisation of the Th1 Th2 response and modulation of dendritic cell function The helminths downregulate the proinflammatory Th1 cytokines interleukin12 IL12 interferongamma IFN and tumor necrosis factoralpha TNF while promoting the production of regulatory Th2 cytokines such as IL10 IL4 IL5 and IL13 Coevolution with helminths has shaped some of the genes associated with interleukin expression and immunological disorders such Crohns ulcerative colitis and celiac disease Helminths relationship to humans as hosts should be classified as mutualistic or symbiotic In some ways the relationship is symbiotic because the worms themselves need the host humans for survival because this body supplies them with nutrients and a home From another perspective it could be reasoned that it is mutualistic being that the above information about benefits in autoimmune disorders continues to remain true and supported Also some say that the worms can regulate gut bacteria Another possibility is one of this being a parasitic relationship arguing that the possibile rosks of anemia and other disorders outweighs the benefits yet this is significantly less supported with the research alluding to the mutualitic and symbiotic approach being much more likely See also Biological response modifier Sepsivac Checkpoint inhibitor Interleukin2 immunotherapy Immunostimulant Microtransplantation Photoimmunotherapy in vitro or in vivo References External links Langreth R 12 February 2009 Cancer Miracles Forbes International Society for Biological Therapy of Cancer Cancer Research Institute Annual International Cancer Immunotherapy Symposia Series The story behind immunotherapys innovative cellular voyage",
  },
  {
    title: "Cancer research",
    originalContent:
      "Cancer research is research into cancer to identify causes and develop strategies for prevention diagnosis treatment and cure Cancer research ranges from epidemiology molecular bioscience to the performance of clinical trials to evaluate and compare applications of the various cancer treatments These applications include surgery radiation therapy chemotherapy hormone therapy immunotherapy and combined treatment modalities such as chemoradiotherapy Starting in the mid1990s the emphasis in clinical cancer research shifted towards therapies derived from biotechnology research such as cancer immunotherapy and gene therapy Cancer research is done in academia research institutes and corporate environments and is largely government funded History Cancer research has been ongoing for centuries Early research focused on the causes of cancer Percivall Pott identified the first environmental trigger chimney soot for cancer in 1775 and cigarette smoking was identified as a cause of lung cancer in 1950 Early cancer treatment focused on improving surgical techniques for removing tumors Radiation therapy took hold in the 1900s Chemotherapeutics were developed and refined throughout the 20th century The US declared a War on Cancer in the 1970s and increased the funding and support for cancer research Seminal papers Some of the most highly cited and most influential research reports include The Hallmarks of Cancer published in 2000 and Hallmarks of Cancer The Next Generation published in 2011 by Douglas Hanahan and Robert Weinberg Together these articles have been cited in over 30000 published papers Types of research Cancer research encompasses a variety of types and interdisciplinary areas of research Scientists involved in cancer research may be trained in areas such as chemistry biochemistry molecular biology physiology medical physics epidemiology and biomedical engineering Research performed on a foundational level is referred to as basic research and is intended to clarify scientific principles and mechanisms Translational research aims to elucidate mechanisms of cancer development and progression and transform basic scientific findings into concepts that can be applicable to the treatment and prevention of cancer Clinical research is devoted to the development of pharmaceuticals surgical procedures and medical technologies for the eventual treatment of patients Prevention and epidemiology Epidemiologic analysis indicates that at least 35 of all cancer deaths in the world could now be avoided by primary prevention According to a newer GBD systematic analysis in 2019 44 of all cancer deaths or 45 million deaths or 105 million lost disabilityadjusted life years were due to known clearly preventable risk factors led by smoking alcohol use and high BMI However one 2015 study suggested that between 70 and 90 of cancers are due to environmental factors and therefore potentially preventable Furthermore it is estimated that with further research cancer death rates could be reduced by 70 around the world even without the development of any new therapies Cancer prevention research receives only 29 of global cancer research funding albeit many of the options for prevention are already wellknown without further cancerspecific research but are not reflected in economics and policy Mutational signatures of various cancers for example could reveal further causes of cancer and support causal attribution Detection Prompt detection of cancer is important since it is usually more difficult to treat in later stages Accurate detection of cancer is also important because false positives can cause harm from unnecessary medical procedures Some screening protocols are currently not accurate such as prostatespecific antigen testing Others such as a colonoscopy or mammogram are unpleasant and as a result some patients may opt out Active research is underway to address all these problems to develop novel ways of cancer screening and to increase detection rates For example Multimodal learning AI systems are being developed to help detect many cancer types via integrating different types of data Scientists work on identifying and measurability of novel biomarkers or sets of such to detect cancer early such as tumorassociated mycobiomes and bacterial microbiomes Researchers investigate whether ants could be used as biosensors to detect cancer via urine Treatment Emerging topics of cancer treatment research include Anticancer vaccines Oncophage SipuleucelT Provenge is a prostate cancer vaccine Inactivated tumor cells are investigated as potential bifunctional cancer vaccines Newer forms of chemotherapy Gene therapy Photodynamic therapy Radiation therapy Reoviridae Reolysin drug therapy Targeted therapy Medical microbots including bacterial nanobots and bacterial cyborg cells Virotherapy Antibodies Photoimmunotherapy for brain cancer Natural killer cells can induce immunological memory Research is being developed to modify their action against cancer How treatments can best be combined in combination therapies Cause and development of cancer Research into the cause of cancer involves many different disciplines including genetics diet environmental factors ie chemical carcinogens In regard to investigation of causes and potential targets for therapy the route used starts with data obtained from clinical observations enters basic research and once convincing and independently confirmed results are obtained proceeds with clinical research involving appropriately designed trials on consenting human subjects with the aim to test safety and efficiency of the therapeutic intervention method An important part of basic research is characterization of the potential mechanisms of carcinogenesis in regard to the types of genetic and epigenetic changes that are associated with cancer development The mouse is often used as a mammalian model for manipulation of the function of genes that play a role in tumor formation while basic aspects of tumor initiation such as mutagenesis are assayed on cultures of bacteria and mammalian cells Genes involved in cancer The goal of oncogenomics is to identify new oncogenes or tumor suppressor genes that may provide new insights into cancer diagnosis predicting clinical outcome of cancers and new targets for cancer therapies As the Cancer Genome Project stated in a 2004 review article a central aim of cancer research has been to identify the mutated genes that are causally implicated in oncogenesis cancer genes The Cancer Genome Atlas project is a related effort investigating the genomic changes associated with cancer while the COSMIC cancer database documents acquired genetic mutations from hundreds of thousands of human cancer samples These large scale projects involving about 350 different types of cancer have identified 130000 mutations in 3000 genes that have been mutated in the tumors The majority occurred in 319 genes of which 286 were tumor suppressor genes and 33 oncogenes Several hereditary factors can increase the chance of cancercausing mutations including the activation of oncogenes or the inhibition of tumor suppressor genes The functions of various onco and tumor suppressor genes can be disrupted at different stages of tumor progression Mutations in such genes can be used to classify the malignancy of a tumor In later stages tumors can develop a resistance to cancer treatment The identification of oncogenes and tumor suppressor genes is important to understand tumor progression and treatment success The role of a given gene in cancer progression may vary tremendously depending on the stage and type of cancer involved Cancer epigenetics Diet and cancer Periods of intermittent fasting timerestricted feeding which may not include caloric restriction is investigated for potential usefulness in cancer prevention and treatment and as of 2021 additional trials are needed to elucidate the risks and benefits In some cases caloric restrictions could hinder both cancer growth and progression besides enhancing the efficacy of chemotherapy and radiation therapy Caloric restriction mimetics including some present in foods like spermidine are also investigated for these or similar reasons Such and similar dietary supplements may contribute to prevention or treatment with candidate substances including apigenin berberine jiaogulan and rhodiola rosea Research funding Cancer research is funded by government grants charitable foundations and pharmaceutical and biotechnology companies In the early 2000s most funding for cancer research came from taxpayers and charities rather than from corporations In the US less than 30 of all cancer research was funded by commercial researchers such as pharmaceutical companies Per capita public spending on cancer research by taxpayers and charities in the US was five times as much in 200203 as public spending by taxpayers and charities in the 15 countries that were full members of the European Union As a percentage of GDP the noncommercial funding of cancer research in the US was four times the amount dedicated to cancer research in Europe Half of Europes noncommercial cancer research is funded by charitable organizations The National Cancer Institute is the major funding institution in the United States In the 2023 fiscal year the NCI funded 71 billion in cancer research Difficulties Difficulties inherent to cancer research are shared with many types of biomedical research Cancer research processes have been criticised These include especially in the US for the financial resources and positions required to conduct research Other consequences of competition for research resources appear to be a substantial number of research publications whose results cannot be replicated Replicability Public participation Distributed computing One can share computer time for distributed cancer research projects like Help Conquer Cancer World Community Grid also had a project called Help Defeat Cancer Other related projects include the Foldinghome and Rosettahome projects which focus on groundbreaking protein folding and protein structure prediction research Vodafone has also partnered with the Garvan Institute to create the DreamLab Project which uses distributed computing via an app on cellphones to perform cancer research Clinical trials Members of the public can also join clinical trials as healthy control subjects or for methods of cancer detection There could be software and datarelated procedures that increase participation in trials and make them faster and less expensive One open source platform matches genomically profiled cancer patients to precision medicine drug trials Organizations Organizations exist as associations for scientists participating in cancer research such as the American Association for Cancer Research and American Society of Clinical Oncology and as foundations for public awareness or raising funds for cancer research such as Relay For Life and the American Cancer Society Awareness campaigns Supporters of different types of cancer have adopted different colored awareness ribbons and promote months of the year as being dedicated to the support of specific types of cancer The American Cancer Society began promoting October as Breast Cancer Awareness Month in the United States in the 1980s Pink products are sold to both generate awareness and raise money to be donated for research purposes This has led to pinkwashing or the selling of ordinary products turned pink as a promotion for the company See also cancerresearch Exposome References External links Cancer Genome Anatomy Project The NIH The Integrative Cancer Biology Program National Cancer Institute",
  },
  {
    title: "Zika virus",
    originalContent:
      "Zika virus ZIKV pronounced or is a member of the virus family Flaviviridae It is spread by daytimeactive Aedes mosquitoes such as A aegypti and A albopictus Its name comes from the Ziika Forest of Uganda where the virus was first isolated in 1947 Zika virus shares a genus with the dengue yellow fever Japanese encephalitis and West Nile viruses Since the 1950s it has been known to occur within a narrow equatorial belt from Africa to Asia From 2007 to 2016 the virus spread eastward across the Pacific Ocean to the Americas leading to the 20152016 Zika virus epidemic The infection known as Zika fever or Zika virus disease often causes no or only mild symptoms similar to a very mild form of dengue fever While there is no specific treatment paracetamol acetaminophen and rest may help with the symptoms As of April 2019 no vaccines have been approved for clinical use however a number of vaccines are currently in clinical trials Zika can spread from a pregnant woman to her baby This can result in microcephaly severe brain malformations and other birth defects Zika infections in adults may result rarely in GuillainBarr syndrome In January 2016 the United States Centers for Disease Control and Prevention CDC issued travel guidance on affected countries including the use of enhanced precautions and guidelines for pregnant women including considering postponing travel Other governments or health agencies also issued similar travel warnings while Colombia the Dominican Republic Puerto Rico Ecuador El Salvador and Jamaica advised women to postpone getting pregnant until more is known about the risks Virology Zika virus belongs to the family Flaviviridae and the genus Flavivirus thus is related to the dengue yellow fever Japanese encephalitis and West Nile viruses Like other flaviviruses Zika virus is enveloped and icosahedral and has a nonsegmented singlestranded 10 kilobase positivesense RNA genome It is most closely related to the Spondweni virus and is one of the two known viruses in the Spondweni virus clade A positivesense RNA genome can be directly translated into viral proteins As in other flaviviruses such as the similarly sized West Nile virus the RNA genome encodes seven nonstructural proteins and three structural proteins in the form of a single polyprotein Q32ZE1 One of the structural proteins encapsulates the virus This protein is the flavivirus envelope glycoprotein that binds to the endosomal membrane of the host cell to initiate endocytosis The RNA genome forms a nucleocapsid along with copies of the 12kDa capsid protein The nucleocapsid in turn is enveloped within a hostderived membrane modified with two viral glycoproteins Viral genome replication depends on the making of doublestranded RNA from the singlestranded positivesense RNA ssRNA genome followed by transcription and replication to provide viral mRNAs and new ssRNA genomes A longitudinal study shows that 6 hours after cells are infected with Zika virus the vacuoles and mitochondria in the cells begin to swell This swelling becomes so severe it results in cell death also known as paraptosis This form of programmed cell death requires gene expression IFITM3 is a transmembrane protein in a cell that is able to protect it from viral infection by blocking virus attachment Cells are most susceptible to Zika infection when levels of IFITM3 are low Once the cell has been infected the virus restructures the endoplasmic reticulum forming the large vacuoles resulting in cell death There are two Zika lineages the African lineage and the Asian lineage Phylogenetic studies indicate that the virus spreading in the Americas is 89 identical to African genotypes but is most closely related to the Asian strain that circulated in French Polynesia during the 20132014 outbreak The Asian strain appears to have first evolved around 1928 Transmission The vertebrate hosts of the virus were primarily monkeys in a socalled enzootic mosquitomonkeymosquito cycle with only occasional transmission to humans Before 2007 Zika rarely caused recognized spillover infections in humans even in highly enzootic areas Infrequently however other arboviruses have become established as a human disease and spread in a mosquitohumanmosquito cycle like the yellow fever virus and the dengue fever virus both flaviviruses and the chikungunya virus a togavirus Though the reason for the pandemic is unknown dengue a related arbovirus that infects the same species of mosquito vectors is known in particular to be intensified by urbanization and globalization Zika is primarily spread by Aedes aegypti mosquitoes and can also be transmitted through sexual contact or blood transfusions The basic reproduction number R0 a measure of transmissibility of Zika virus has been estimated to be between 14 and 66 In 2015 news reports drew attention to the rapid spread of Zika in Latin America and the Caribbean At that time the Pan American Health Organization published a list of countries and territories that experienced local Zika virus transmission comprising Barbados Bolivia Brazil Colombia the Dominican Republic Ecuador El Salvador French Guiana Guadeloupe Guatemala Guyana Haiti Honduras Martinique Mexico Panama Paraguay Puerto Rico Saint Martin Suriname and Venezuela By August 2016 more than 50 countries had experienced active local transmission of Zika virus Mosquito Zika is primarily spread by the female Aedes aegypti mosquito which is active mostly in the daytime The mosquitos must feed on blood to lay eggs 2 The virus has also been isolated from a number of arboreal mosquito species in the genus Aedes such as A africanus A apicoargenteus A furcifer A hensilli A luteocephalus and A vittatus with an extrinsic incubation period in mosquitoes around 10 days The true extent of the vectors is still unknown Zika has been detected in many more species of Aedes along with Anopheles coustani Mansonia uniformis and Culex perfuscus although this alone does not incriminate them as vectors To detect the presence of the virus usually requires genetic material to be analysed in a lab using the technique RTPCR A much cheaper and faster method involves shining a light at the head and thorax of the mosquito and detecting chemical compounds characteristic of the virus using nearinfrared spectroscopy Transmission by A albopictus the tiger mosquito was reported from a 2007 urban outbreak in Gabon where it had newly invaded the country and become the primary vector for the concomitant chikungunya and dengue virus outbreaks New outbreaks can occur if a person carrying the virus travels to another region where A albopictus is common The potential societal risk of Zika can be delimited by the distribution of the mosquito species that transmit it The global distribution of the most cited carrier of Zika A aegypti is expanding due to global trade and travel A aegypti distribution is now the most extensive ever recorded on parts of all continents except Antarctica including North America and even the European periphery Madeira the Netherlands and the northeastern Black Sea coast A mosquito population capable of carrying Zika has been found in a Capitol Hill neighborhood of Washington DC and genetic evidence suggests they survived at least four consecutive winters in the region The study authors conclude that mosquitos are adapting for persistence in a northern climate Zika virus appears to be contagious via mosquitoes for around a week after infection The virus is thought to be infectious for a longer period of time after infection at least 2 weeks when transmitted via semen Research into its ecological niche suggests that Zika may be influenced to a greater degree by changes in precipitation and temperature than dengue making it more likely to be confined to tropical areas However rising global temperatures would allow for the disease vector to expand its range further north allowing Zika to follow Sexual Zika can be transmitted from men and women to their sexual partners most known cases involve transmission from symptomatic men to women As of April 2016 sexual transmission of Zika has been documented in six countries Argentina Australia France Italy New Zealand and the United States during the 2015 outbreak ZIKV can persist in semen for several months with viral RNA detected up to one year The virus replicates in the human testis where it infects several cell types including testicular macrophages peritubular cells and germ cells the spermatozoa precursors Semen parameters can be altered in patients for several weeks postsymptoms onset and spermatozoa can be infectious Since October 2016 the CDC has advised men who have traveled to an area with Zika should use condoms or not have sex for at least six months after their return as the virus is still transmissible even if symptoms never develop Pregnancy Zika virus can spread by vertical or mothertochild transmission during pregnancy or at delivery An infection during pregnancy has been linked to changes in neuronal development of the unborn child Severe progressions of infection have been linked to the development of microcephaly in the unborn child while mild infections potentially can lead to neurocognitive disorders later in life Congenital brain abnormalities other than microcephaly have also been reported after a Zika outbreak Studies in mice have suggested that maternal immunity to dengue virus may enhance fetal infection with Zika worsen the microcephaly phenotype andor enhance damage during pregnancy but it is unknown whether this occurs in humans Blood transfusion As of April 2016 two cases of Zika transmission through blood transfusions have been reported globally both from Brazil after which the US Food and Drug Administration FDA recommended screening blood donors and deferring highrisk donors for 4 weeks A potential risk had been suspected based on a blooddonor screening study during the French Polynesian Zika outbreak in which 28 42 of donors from November 2013 and February 2014 tested positive for Zika RNA and were all asymptomatic at the time of blood donation Eleven of the positive donors reported symptoms of Zika fever after their donation but only three of 34 samples grew in culture Pathogenesis Zika virus replicates in the mosquitos midgut epithelial cells and then its salivary gland cells After 510 days the virus can be found in the mosquitos saliva If the mosquitos saliva is inoculated into human skin the virus can infect epidermal keratinocytes skin fibroblasts in the skin and the Langerhans cells The pathogenesis of the virus is hypothesized to continue with a spread to lymph nodes and the bloodstream Flaviviruses replicate in the cytoplasm but Zika antigens have been found in infected cell nuclei The viral protein numbered NS4A can lead to small head size microcephaly because it disrupts brain growth by hijacking a pathway which regulates growth of new neurons In fruit flies both NS4A and the neighboring NS4B restrict eye growth Zika fever Zika fever also known as Zika virus disease is an illness caused by Zika virus Around 80 of cases are estimated to be asymptomatic though the accuracy of this figure is hindered by the wide variance in data quality and figures from different outbreaks can vary significantly Symptomatic cases are usually mild and can resemble dengue fever Symptoms may include fever red eyes joint pain headache and a maculopapular rash Symptoms generally last less than seven days It has not caused any reported deaths during the initial infection Infection during pregnancy causes microcephaly and other brain malformations in some babies Infection in adults has been linked to GuillainBarr syndrome GBS and Zika virus has been shown to infect human Schwann cells Diagnosis is by testing the blood urine or saliva for the presence of Zika virus RNA when the person is sick In 2019 an improved diagnostic test based on research from Washington University in St Louis that detects Zika infection in serum was granted market authorization by the FDA Prevention involves decreasing mosquito bites in areas where the disease occurs and proper use of condoms Efforts to prevent bites include the use of DEET or picaridin based insect repellent covering much of the body with clothing mosquito nets and getting rid of standing water where mosquitoes reproduce There is no vaccine Health officials recommended that women in areas affected by the 20152016 Zika outbreak consider putting off pregnancy and that pregnant women not travel to these areas While no specific treatment exists paracetamol acetaminophen and rest may help with the symptoms Admission to a hospital is rarely necessary Treatment It is advised for an affected person with the zika virus to drink a lot of water to stay hydrated to lie down and to treat the fever and agony with liquid solutions taking drugs like acetaminophen or paracetamol helps to relieve fever and pain Referring to the US CDC it is not recommended to take antiinflammatory and nonsteroid drugs like aspirin for example If the patient affected is already taking treatment for another medical condition it is advisable to inform your attending physician before taking any other drug or additional treatment Vaccine development The World Health Organization has suggested that priority should be to develop inactivated vaccines and other nonlive vaccines which are safe to use in pregnant women As of March 2016 18 companies and institutions were developing vaccines against Zika but they state a vaccine is unlikely to be widely available for about 10 years In June 2016 the FDA granted the first approval for a human clinical trial for a Zika vaccine In March 2017 a DNA vaccine was approved for phase2 clinical trials This vaccine consists of a small circular piece of DNA known as a plasmid that expresses the genes for the Zika virus envelope proteins As the vaccine does not contain the full sequence of the virus it cannot cause infection As of April 2017 both subunit and inactivated vaccines have entered clinical trials History Virus isolation in monkeys and mosquitoes 1947 The virus was first isolated in April 1947 from a rhesus macaque monkey placed in a cage in the Ziika Forest of Uganda near Lake Victoria by the scientists of the Yellow Fever Research Institute A second isolation from the mosquito A africanus followed at the same site in January 1948 When the monkey developed a fever researchers isolated from its serum a filterable transmissible agent which was named Zika in 1948 First evidence of human infection 1952 Zika was first known to infect humans from the results of a serological survey in Uganda published in 1952 Of 99 human blood samples tested 61 had neutralizing antibodies As part of a 1954 outbreak investigation of jaundice suspected to be yellow fever researchers reported isolation of the virus from a patient but the pathogen was later shown to be the closely related Spondweni virus Spondweni was also determined to be the cause of a selfinflicted infection in a researcher reported in 1956 Spread in equatorial Africa and to Asia 1951present Subsequent serological studies in several African and Asian countries indicated the virus had been widespread within human populations in these regions The first true case of human infection was identified by Simpson in 1964 who was himself infected while isolating the virus from mosquitoes From then until 2007 there were only 13 further confirmed human cases of Zika infection from Africa and Southeast Asia A study published in 2017 showed that the Zika virus despite only a few cases were reported has been silently circulated in West Africa for the last two decades when blood samples collected between 1992 and 2016 were tested for the ZIKV IgM antibodies In 2017 Angola reported two cases of Zika fever Zika was also occurring in Tanzania as of 2016 Micronesia 2007 In April 2007 the first outbreak outside of Africa and Asia occurred on the island of Yap in the Federated States of Micronesia characterized by rash conjunctivitis and arthralgia which was initially thought to be dengue chikungunya or Ross River disease Serum samples from patients in the acute phase of illness contained RNA of Zika There were 49 confirmed cases 59 unconfirmed cases no hospitalizations and no deaths 20132014 After October 2013 Oceanias first outbreak showed an estimated 11 population infected for French Polynesia that also presented with GuillainBarre syndrome GBS The spread of ZIKV continued to New Caledonia Easter Island and the Cook Islands and where 1385 cases were confirmed by January 2014 During the same year Easter Island acknowledged 51 cases Australia began seeing cases in 2012 Research showed it was brought by travelers returning from Indonesia and other infected countries New Zealand also experienced infections rate increases through returning foreign travelers Oceania countries experiencing Zika today are New Caledonia Vanuatu Solomon Islands Marshall Islands American Samoa Samoa and Tonga Between 2013 and 2014 further epidemics occurred in French Polynesia Easter Island the Cook Islands and New Caledonia Americas 2015present There was an epidemic in 2015 and 2016 in the Americas The outbreak began in April 2015 in Brazil and spread to other countries in South America Central America North America and the Caribbean In January 2016 the WHO said the virus was likely to spread throughout most of the Americas by the end of the year and in February 2016 the WHO declared the cluster of microcephaly and GuillainBarr syndrome cases reported in Brazil strongly suspected to be associated with the Zika outbreak a Public Health Emergency of International Concern It was estimated that 15 million people were infected by Zika in Brazil with over 3500 cases of microcephaly reported between October 2015 and January 2016 A number of countries issued travel warnings and the outbreak was expected to significantly impact the tourism industry Several countries took the unusual step of advising their citizens to delay pregnancy until more was known about the virus and its impact on fetal development With the 2016 Summer Olympics hosted in Rio de Janeiro health officials worldwide voiced concerns over a potential crisis both in Brazil and when international athletes and tourists returned home and possibly would spread the virus Some researchers speculated that only one or two tourists might be infected during the threeweek period or approximately 32 infections per 100000 tourists In November 2016 the World Health Organization declared that Zika virus was no longer a global emergency while noting that the virus still represents a highly significant and a longterm problem As of August 2017 the number of new Zika virus cases in the Americas had fallen dramatically India Bangladesh On May 15 2017 three cases of Zika virus infection in India were reported in the state of Gujarat By late 2018 there had been at least 159 cases in Rajasthan and 127 in Madhya Pradesh In July 2021 the first case of Zika virus infection in the Indian state of Kerala was reported After the first confirmed case 19 other people who had previously presented symptoms were tested and 13 of those had positive results showing that Zika had been circulating in Kerala since at least May 2021 By August 6th 2021 there had been 65 reported cases in Kerala On October 22 2021 an officer in the Indian Air Force in Kanpur tested positive for Zika virus making it the first reported case in the Indian state of Uttar Pradesh On 22 March 2016 Reuters reported that Zika was isolated from a 2014 blood sample of an elderly man in Chittagong in Bangladesh as part of a retrospective study East Asia Between August and November 2016 455 cases of Zika virus infection were confirmed in Singapore In 2023 722 Zika virus cases were reported in Thailand From 20192022 the Robert KochInstitut reported 29 imported Zikavirus cases imported into Germany Of the altogether 16 imported Zika virus cases in 2023 10 were diagnosed after a trip to Thailand with 62 of all Zika virus cases a significant relative and absolute increase See also Wolbachia World mosquito program References This article incorporates public domain material from websites or documents of the Centers for Disease Control and Prevention External links",
  },
  {
    title: "Geothermal energy",
    originalContent:
      "Geothermal energy is thermal energy extracted from the Earths crust It combines energy from the formation of the planet and from radioactive decay Geothermal energy has been exploited as a source of heat andor electric power for millennia Geothermal heating using water from hot springs for example has been used for bathing since Paleolithic times and for space heating since Roman times Geothermal power generation of electricity from geothermal energy has been used since the 20th century Unlike wind and solar energy geothermal plants produce power at a constant rate without regard to weather conditions Geothermal resources are theoretically more than adequate to supply humanitys energy needs Most extraction occurs in areas near tectonic plate boundaries The cost of generating geothermal power decreased by 25 during the 1980s and 1990s Technological advances continued to reduce costs and thereby expand the amount of viable resources In 2021 the US Department of Energy estimated that power from a plant built today costs about 005kWh In 2019 13900 megawatts MW of geothermal power was available worldwide An additional 28 gigawatts provided heat for district heating space heating spas industrial processes desalination and agricultural applications as of 2010 As of 2019 the industry employed about one hundred thousand people The adjective geothermal originates from the Greek roots g meaning Earth and therms meaning hot History Hot springs have been used for bathing since at least Paleolithic times The oldest known spa is at the site of the Huaqing Chi palace In the first century CE Romans conquered Aquae Sulis now Bath Somerset England and used the hot springs there to supply public baths and underfloor heating The admission fees for these baths probably represent the first commercial use of geothermal energy The worlds oldest geothermal district heating system in ChaudesAigues France has been operating since the 15th century The earliest industrial exploitation began in 1827 with the use of geyser steam to extract boric acid from volcanic mud in Larderello Italy In 1892 the USs first district heating system in Boise Idaho was powered by geothermal energy It was copied in Klamath Falls Oregon in 1900 The worlds first known building to utilize geothermal energy as its primary heat source was the Hot Lake Hotel in Union County Oregon beginning in 1907 A geothermal well was used to heat greenhouses in Boise in 1926 and geysers were used to heat greenhouses in Iceland and Tuscany at about the same time Charles Lieb developed the first downhole heat exchanger in 1930 to heat his house Geyser steam and water began heating homes in Iceland in 1943 In the 20th century geothermal energy came into use as a generating source Prince Piero Ginori Conti tested the first geothermal power generator on 4 July 1904 at the Larderello steam field It successfully lit four light bulbs In 1911 the worlds first commercial geothermal power plant was built there It was the only industrial producer of geothermal power until New Zealand built a plant in 1958 In 2012 it produced some 594 megawatts In 1960 Pacific Gas and Electric began operation of the first US geothermal power plant at The Geysers in California The original turbine lasted for more than 30 years and produced 11 MW net power An organic fluid based binary cycle power station was first demonstrated in 1967 in the USSR and later introduced to the US in 1981 This technology allows the use of temperature resources as low as 81 C In 2006 a binary cycle plant in Chena Hot Springs Alaska came online producing electricity from a record low temperature of 57 C 135 F Resources The Earth has an internal heat content of 1031 joules 31015 TWh About 20 of this is residual heat from planetary accretion the remainder is attributed to past and current radioactive decay of naturally occurring isotopes For example a 5275 m deep borehole in United Downs Deep Geothermal Power Project in Cornwall England found granite with very high thorium content whose radioactive decay is believed to power the high temperature of the rock Earths interior temperature and pressure are high enough to cause some rock to melt and the solid mantle to behave plastically Parts of the mantle convect upward since it is lighter than the surrounding rock Temperatures at the coremantle boundary can reach over 4000 C 7230 F The Earths internal thermal energy flows to the surface by conduction at a rate of 442 terawatts TW and is replenished by radioactive decay of minerals at a rate of 30 TW These power rates are more than double humanitys current energy consumption from all primary sources but most of this energy flux is not recoverable In addition to the internal heat flows the top layer of the surface to a depth of 10 m 33 ft is heated by solar energy during the summer and cools during the winter Outside of the seasonal variations the geothermal gradient of temperatures through the crust is 2530 C 7786 F per km of depth in most of the world The conductive heat flux averages 01 MWkm2 These values are much higher near tectonic plate boundaries where the crust is thinner They may be further augmented by combinations of fluid circulation either through magma conduits hot springs hydrothermal circulation The thermal efficiency and profitability of electricity generation is particularly sensitive to temperature Applications receive the greatest benefit from a high natural heat flux most easily from a hot spring The next best option is to drill a well into a hot aquifer An artificial hot water reservoir may be built by injecting water to hydraulically fracture bedrock The systems in this last approach are called enhanced geothermal systems 2010 estimates of the potential for electricity generation from geothermal energy vary sixfold from 0035to2TW depending on the scale of investments Upper estimates of geothermal resources assume wells as deep as 10 kilometres 6 mi although 20th century wells rarely reached more than 3 kilometres 2 mi deep Wells of this depth are common in the petroleum industry Geothermal power Geothermal power is electrical power generated from geothermal energy Dry steam flash steam and binary cycle power stations have been used for this purpose As of 2010 geothermal electricity was generated in 26 countries As of 2019 worldwide geothermal power capacity amounted to 154 gigawatts GW of which 2386 percent or 368 GW were in the United States Geothermal energy supplies a significant share of the electrical power in Iceland El Salvador Kenya the Philippines and New Zealand Geothermal power is considered to be a renewable energy because heat extraction rates are insignificant compared to the Earths heat content The greenhouse gas emissions of geothermal electric stations are on average 45 grams of carbon dioxide per kilowatthour of electricity or less than 5 percent of that of coalfired plants Geothermal electric plants were traditionally built on the edges of tectonic plates where hightemperature geothermal resources approach the surface The development of binary cycle power plants and improvements in drilling and extraction technology enable enhanced geothermal systems over a greater geographical range Demonstration projects are operational in LandauPfalz Germany and SoultzsousForts France while an earlier effort in Basel Switzerland was shut down after it triggered earthquakes Other demonstration projects are under construction in Australia the United Kingdom and the US In Myanmar over 39 locations are capable of geothermal power production some of which are near Yangon Geothermal heating Geothermal heating is the use of geothermal energy to heat buildings and water for human use Humans have done this since the Paleolithic era Approximately seventy countries made direct use of a total of 270 PJ of geothermal heating in 2004 As of 2007 28 GW of geothermal heating satisfied 007 of global primary energy consumption Thermal efficiency is high since no energy conversion is needed but capacity factors tend to be low around 20 since the heat is mostly needed in the winter Even cold ground contains heat below 6 metres 20 ft the undisturbed ground temperature is consistently at the Mean Annual Air Temperature that may be extracted with a ground source heat pump Types Hydrothermal systems Hydrothermal systems produce geothermal energy by accessing naturallyoccurring hydrothermal reservoirs Hydrothermal systems come in either vapordominated or liquiddominated forms Vapordominated plants Larderello and The Geysers are vapordominated Vapordominated sites offer temperatures from 240 to 300 C that produce superheated steam Liquiddominated plants Liquiddominated reservoirs LDRs are more common with temperatures greater than 200 C 392 F and are found near volcanoes inaround the Pacific Ocean and in rift zones and hot spots Flash plants are the common way to generate electricity from these sources Steam from the well is sufficient to power the plant Most wells generate 210 MW of electricity Steam is separated from liquid via cyclone separators and drives electric generators Condensed liquid returns down the well for reheatingreuse As of 2013 the largest liquid system was Cerro Prieto in Mexico which generates 750 MW of electricity from temperatures reaching 350 C 662 F Lowertemperature LDRs 120200 C require pumping They are common in extensional terrains where heating takes place via deep circulation along faults such as in the Western US and Turkey Water passes through a heat exchanger in a Rankine cycle binary plant The water vaporizes an organic working fluid that drives a turbine These binary plants originated in the Soviet Union in the late 1960s and predominate in new plants Binary plants have no emissions Engineered geothermal systems An engineered geothermal system is a geothermal system that engineers have artificially created or improved Engineered geothermal systems are used in a variety of geothermal reservoirs that have hot rocks but insufficient natural reservoir quality for example insufficient geofluid quantity or insufficient rock permeability or porosity to operate as natural hydrothermal systems Types of engineered geothermal systems include enhanced geothermal systems closedloop or advanced geothermal systems and some superhot rock geothermal systems Enhanced geothermal systems Enhanced geothermal systems EGS actively inject water into wells to be heated and pumped back out The water is injected under high pressure to expand existing rock fissures to enable the water to flow freely The technique was adapted from oil and gas fracking techniques The geologic formations are deeper and no toxic chemicals are used reducing the possibility of environmental damage Instead proppants such as sand or ceramic particles are used to keep the cracks open and producing optimal flow rates Drillers can employ directional drilling to expand the reservoir size Smallscale EGS have been installed in the Rhine Graben at SoultzsousForts in France and at Landau and Insheim in Germany Closedloop geothermal systems Closedloop geothermal systems sometimes colloquially referred to as Advanced Geothermal Systems AGS are engineered geothermal systems containing subsurface working fluid that is heated in the hot rock reservoir without direct contact with rock pores and fractures Instead the subsurface working fluid stays inside a closed loop of deeply buried pipes that conduct Earths heat The advantages of a deep closedloop geothermal circuit include 1 no need for a geofluid 2 no need for the hot rock to be permeable or porous and 3 all the introduced working fluid can be recirculated with zero loss Eavortm a Canadianbased geothermal startup piloted their closedloop system in shallow soft rock formations in Alberta Canada Situated within a sedimentary basin the geothermal gradient proved to be insufficient for electrical power generation However the system successfully produced approximately 11000 MWh of thermal energy during its initial two years of operation Economics As with wind and solar energy geothermal power has minimal operating costs capital costs dominate Drilling accounts for over half the costs and not all wells produce exploitable resources For example a typical well pair one for extraction and one for injection in Nevada can produce 45 megawatts MW and costs about 10 million to drill with a 20 failure rate making the average cost of a successful well 50 million Drilling geothermal wells is more expensive than drilling oil and gas wells of comparable depth for several reasons Geothermal reservoirs are usually in igneous or metamorphic rock which is harder to penetrate than the sedimentary rock of typical hydrocarbon reservoirs The rock is often fractured which causes vibrations that damage bits and other drilling tools The rock is often abrasive with high quartz content and sometimes contains highly corrosive fluids The rock is hot which limits use of downhole electronics Well casing must be cemented from top to bottom to resist the casings tendency to expand and contract with temperature changes Oil and gas wells are usually cemented only at the bottom Well diameters are considerably larger than typical oil and gas wells As of 2007 plant construction and well drilling cost about 25 million per MW of electrical capacity while the breakeven price was 004010 per kWh Enhanced geothermal systems tend to be on the high side of these ranges with capital costs above 4 million per MW and breakeven above 0054 per kWh Between 2013 and 2020 private investments were the main source of funding for renewable energy comprising approximately 75 of total financing The mix between private and public funding varies among different renewable energy technologies influenced by their market appeal and readiness In 2020 geothermal energy received just 32 of its investment from private sources Socioeconomic benefits In January 2024 the Energy Sector Management Assistance Program ESMAP report Socioeconomic Impacts of Geothermal Energy Development was published highlighting the substantial socioeconomic benefits of geothermal energy development which notably exceeds those of wind and solar by generating an estimated 34 jobs per megawatt across various sectors The report details how geothermal projects contribute to skill development through practical onthejob training and formal education thereby strengthening the local workforce and expanding employment opportunities It also underscores the collaborative nature of geothermal development with local communities which leads to improved infrastructure skillbuilding programs and revenuesharing models thereby enhancing access to reliable electricity and heat These improvements have the potential to boost agricultural productivity and food security The report further addresses the commitment to advancing gender equality and social inclusion by offering job opportunities education and training to underrepresented groups ensuring fair access to the benefits of geothermal development Collectively these efforts are instrumental in driving domestic economic growth increasing fiscal revenues and contributing to more stable and diverse national economies while also offering significant social benefits such as better health education and community cohesion Development Geothermal projects have several stages of development Each phase has associated risks Many projects are canceled during the stages of reconnaissance and geophysical surveys which are unsuitable for traditional lending At later stages can often be equityfinanced Precipitate scaling A common issue encountered in geothermal systems arises when the system is situated in carbonaterich formations In such cases the fluids extracting heat from the subsurface often dissolve fragments of the rock during their ascent towards the surface where they subsequently cool As the fluids cool dissolved cations precipitate out of solution leading to the formation of calcium scale a phenomenon known as calcite scaling This calcite scaling has the potential to decrease flow rates and necessitate system downtime for maintenance purposes Sustainability Geothermal energy is considered to be sustainable because the heat extracted is so small compared to the Earths heat content which is approximately 100 billion times 2010 worldwide annual energy consumption Earths heat flows are not in equilibrium the planet is cooling on geologic timescales Anthropic heat extraction typically does not accelerate the cooling process Wells can further be considered renewable because they return the extracted water to the borehole for reheating and reextraction albeit at a lower temperature Replacing material use with energy has reduced the human environmental footprint in many applications Geothermal has the potential to allow further reductions For example Iceland has sufficient geothermal energy to eliminate fossil fuels for electricity production and to heat Reykjavik sidewalks and eliminate the need for gritting However local effects of heat extraction must be considered Over the course of decades individual wells draw down local temperatures and water levels The three oldest sites at Larderello Wairakei and the Geysers experienced reduced output because of local depletion Heat and water in uncertain proportions were extracted faster than they were replenished Reducing production and injecting additional water could allow these wells to recover their original capacity Such strategies have been implemented at some sites These sites continue to provide significant energy The Wairakei power station was commissioned in November 1958 and it attained its peak generation of 173 MW in 1965 but already the supply of highpressure steam was faltering In 1982 it was downrated to intermediate pressure and the output to 157 MW In 2005 two 8 MW isopentane systems were added boosting output by about 14 MW Detailed data were lost due to reorganisations Environmental effects Fluids drawn from underground carry a mixture of gasses notably carbon dioxide CO2 hydrogen sulfide H2S methane CH4 and ammonia NH3 These pollutants contribute to global warming acid rain and noxious smells if released Existing geothermal electric plants emit an average of 122 kilograms 269 lb of CO2 per megawatthour MWh of electricity a small fraction of the emission intensity of fossil fuel plants A few plants emit more pollutants than gasfired power at least in the first few years such as some geothermal power in Turkey Plants that experience high levels of acids and volatile chemicals are typically equipped with emissioncontrol systems to reduce the exhaust New emerging closed looped technologies developed by Eavor have the potential to reduce these emissions to zero Water from geothermal sources may hold in solution trace amounts of toxic elements such as mercury arsenic boron and antimony These chemicals precipitate as the water cools and can damage surroundings if released The modern practice of returning geothermal fluids into the Earth to stimulate production has the side benefit of reducing this environmental impact Construction can adversely affect land stability Subsidence occurred in the Wairakei field In Staufen im Breisgau Germany tectonic uplift occurred instead A previously isolated anhydrite layer came in contact with water and turned it into gypsum doubling its volume Enhanced geothermal systems can trigger earthquakes as part of hydraulic fracturing A project in Basel Switzerland was suspended because more than 10000 seismic events measuring up to 34 on the Richter Scale occurred over the first 6 days of water injection Geothermal power production has minimal land and freshwater requirements Geothermal plants use 35 square kilometres 14 sq mi per gigawatt of electrical production not capacity versus 32 square kilometres 12 sq mi and 12 square kilometres 46 sq mi for coal facilities and wind farms respectively They use 20 litres 53 US gal of freshwater per MWh versus over 1000 litres 260 US gal per MWh for nuclear coal or oil Production Philippines The Philippines began geothermal research in 1962 when the Philippine Institute of Volcanology and Seismology inspected the geothermal region in Tiwi Albay The first geothermal power plant in the Philippines was built in 1977 located in Tongonan Leyte The New Zealand government contracted with the Philippines to build the plant in 1972 The Tongonan Geothermal Field TGF added the Upper Mahiao Matlibog and South Sambaloran plants which resulted in a 508 MV capacity The first geothermal power plant in the Tiwi region opened in 1979 while two other plants followed in 1980 and 1982 The Tiwi geothermal field is located about 450 km from Manila The three geothermal power plants in the Tiwi region produce 330 MWe putting the Philippines behind the United States and Mexico in geothermal growth The Philippines has 7 geothermal fields and continues to exploit geothermal energy by creating the Philippine Energy Plan 20122030 that aims to produce 70 of the countrys energy by 2030 United States According to the Geothermal Energy Association GEA installed geothermal capacity in the United States grew by 5 or 14705 MW in 2013 This increase came from seven geothermal projects that began production in 2012 GEA revised its 2011 estimate of installed capacity upward by 128 MW bringing installed US geothermal capacity to 3386 MW Hungary The municipal government of Szeged is trying to cut down its gas consumption by 50 percent by utilizing geothermal energy for its district heating system The Szeged geothermal power station has 27 wells 16 heating plants and 250 kilometres of distribution pipes See also 2010 World Geothermal Congress Deep water source cooling Earths internal heat budget Geothermal activity Hydrothermal vent International Geothermal Association Ocean thermal energy conversion Relative cost of electricity generated by different sources List of renewable energy topics by country and territory Thermal battery References External links The Future of Geothermal Energy PDF International Energy Agency IEA December 2024 Archived PDF from the original on 14 December 2024 Hawaii Groundwater Geothermal Resources Center University of Hawaii at Manoa 20230716 Retrieved 20230807 Geothermal Rising Using the Earth to Save the Earth wwwgeothermalorg Retrieved 20230807 Energy Efficiency and Renewable Energy Geothermal Technologies Office International Energy Agency Geothermal Energy Homepage NREL Geothermal Research 2022 discussion of geothermal energy advantages and challenges",
  },
  {
    title: "Bioenergy",
    originalContent:
      "Bioenergy is a type of renewable energy that is derived from plants and animal waste The biomass that is used as input materials consists of recently living but now dead organisms mainly plants Thus fossil fuels are not regarded as biomass under this definition Types of biomass commonly used for bioenergy include wood food crops such as corn energy crops and waste from forests yards or farms Bioenergy can help with climate change mitigation but in some cases the required biomass production can increase greenhouse gas emissions or lead to local biodiversity loss The environmental impacts of biomass production can be problematic depending on how the biomass is produced and harvested The IEAs Net Zero by 2050 scenario calls for traditional bioenergy to be phased out by 2030 with modern bioenergys share increasing from 66 in 2020 to 131 in 2030 and 187 in 2050 Bioenergy has a significant climate change mitigation potential if implemented correctly 637 Most of the recommended pathways to limit global warming include substantial contributions from bioenergy in 2050 average at 200 EJ B 74 Definition and terminology The IPCC Sixth Assessment Report defines bioenergy as energy derived from any form of biomass or its metabolic byproducts 1795 It goes on to define biomass in this context as organic material excluding the material that is fossilised or embedded in geological formations 1795 This means that coal or other fossil fuels is not a form of biomass in this context The term traditional biomass for bioenergy means the combustion of wood charcoal agricultural residues andor animal dung for cooking or heating in open fires or in inefficient stoves as is common in lowincome countries 1796 Since biomass can also be used as a fuel directly eg wood logs the terms biomass and biofuel have sometimes been used interchangeably However the term biomass usually denotes the biological raw material the fuel is made of The terms biofuel or biogas are generally reserved for liquid or gaseous fuels respectively Input materials Wood and wood residues is the largest biomass energy source today Wood can be used as a fuel directly or processed into pellet fuel or other forms of fuels Other plants can also be used as fuel for instance maize switchgrass miscanthus and bamboo The main waste feedstocks are wood waste agricultural waste municipal solid waste and manufacturing waste Upgrading raw biomass to higher grade fuels can be achieved by different methods broadly classified as thermal chemical or biochemical Thermal conversion processes use heat as the dominant mechanism to upgrade biomass into a better and more practical fuel The basic alternatives are torrefaction pyrolysis and gasification these are separated mainly by the extent to which the chemical reactions involved are allowed to proceed mainly controlled by the availability of oxygen and conversion temperature Many chemical conversions are based on established coalbased processes such as the FischerTropsch synthesis Like coal biomass can be converted into multiple commodity chemicals Biochemical processes have developed in nature to break down the molecules of which biomass is composed and many of these can be harnessed In most cases microorganisms are used to perform the conversion The processes are called anaerobic digestion fermentation and composting Applications Biomass for heating Biofuel for transportation Based on the source of biomass biofuels are classified broadly into two major categories depending if food crops are used or not Firstgeneration or conventional biofuels are made from food sources grown on arable lands such as sugarcane and maize Sugars present in this biomass are fermented to produce bioethanol an alcohol fuel which serves as an additive to gasoline or in a fuel cell to produce electricity Bioethanol is made by fermentation mostly from carbohydrates produced in sugar or starch crops such as corn sugarcane or sweet sorghum Bioethanol is widely used in the United States and in Brazil Biodiesel is produced from the oils in for instance rapeseed or sugar beets and is the most common biofuel in Europe Secondgeneration biofuels also called advanced biofuels utilize nonfoodbased biomass sources such as perennial energy crops and agricultural residueswaste The feedstock used to make the fuels either grow on arable land but are byproducts of the main crop or they are grown on marginal land Waste from industry agriculture forestry and households can also be used for secondgeneration biofuels using eg anaerobic digestion to produce biogas gasification to produce syngas or by direct combustion Cellulosic biomass derived from nonfood sources such as trees and grasses is being developed as a feedstock for ethanol production and biodiesel can be produced from leftover food products like vegetable oils and animal fats Production of liquid fuels Biomass to liquid Bioconversion of biomass to mixed alcohol fuels Comparison with other renewable energy types Land requirement The surface power production densities of a crop will determine how much land is required for production The average lifecycle surface power densities for biomass wind hydro and solar power production are 030 Wm2 1 Wm2 3 Wm2 and 5 Wm2 respectively power in the form of heat for biomass and electricity for wind hydro and solar Lifecycle surface power density includes land used by all supporting infrastructure manufacturing miningharvesting and decommissioning Another estimate puts the values at 008 Wm2 for biomass 014 Wm2 for hydro 184 Wm2 for wind and 663 Wm2 for solar median values with none of the renewable sources exceeding 10 Wm2 Related technologies Bioenergy with carbon capture and storage BECCS Carbon capture and storage technology can be used to capture emissions from bioenergy power plants This process is known as bioenergy with carbon capture and storage BECCS and can result in net carbon dioxide removal from the atmosphere However BECCS can also result in net positive emissions depending on how the biomass material is grown harvested and transported Deployment of BECCS at scales described in some climate change mitigation pathways would require converting large amounts of cropland Climate and sustainability aspects Environmental impacts Bioenergy can either mitigate ie reduce or increase greenhouse gas emissions There is also agreement that local environmental impacts can be problematic For example increased biomass demand can create significant social and environmental pressure in the locations where the biomass is produced The impact is primarily related to the low surface power density of biomass The low surface power density has the effect that much larger land areas are needed in order to produce the same amount of energy compared to for instance fossil fuels Longdistance transport of biomass have been criticised as wasteful and unsustainable and there have been protests against forest biomass export in Sweden and Canada Scale and future trends In 2020 bioenergy produced 58 EJ exajoules of energy compared to 172 EJ from crude oil 157 EJ from coal 138 EJ from natural gas 29 EJ from nuclear 16 EJ from hydro and 15 EJ from wind solar and geothermal combined Most of the global bioenergy is produced from forest resources 3 1 Generally bioenergy expansion fell by 50 in 2020 China and Europe are the only two regions that reported significant expansion in 2020 adding 2 GW and 12 GW of bioenergy capacity respectively Almost all available sawmill residue is already being utilized for pellet production so there is no room for expansion For the bioenergy sector to significantly expand in the future more of the harvested pulpwood must go to pellet mills However the harvest of pulpwood tree thinnings removes the possibility for these trees to grow old and therefore maximize their carbon holding capacity 19 Compared to pulpwood sawmill residues have lower net emissions Some types of biomass feedstock can be carbonneutral at least over a period of a few years including in particular sawmill residues These are wastes from other forest operations that imply no additional harvesting and if otherwise burnt as waste or left to rot would release carbon to the atmosphere in any case 68 By country See also References Sources",
  },
  {
    title: "Nuclear energy",
    originalContent:
      "Nuclear energy may refer to Nuclear power the use of sustained nuclear fission or nuclear fusion to generate heat and electricity Nuclear binding energy the energy needed to fuse or split a nucleus of an atom Nuclear potential energy the potential energy of the particles inside an atomic nucleus Nuclear Energy sculpture a bronze sculpture by Henry Moore in the University of Chicago",
  },
  {
    title: "Electric power distribution",
    originalContent:
      "Electric power distribution is the final stage in the delivery of electricity Electricity is carried from the transmission system to individual consumers Distribution substations connect to the transmission system and lower the transmission voltage to medium voltage ranging between 2 kV and 33 kV with the use of transformers Primary distribution lines carry this medium voltage power to distribution transformers located near the customers premises Distribution transformers again lower the voltage to the utilization voltage used by lighting industrial equipment and household appliances Often several customers are supplied from one transformer through secondary distribution lines Commercial and residential customers are connected to the secondary distribution lines through service drops Customers demanding a much larger amount of power may be connected directly to the primary distribution level or the subtransmission level The transition from transmission to distribution happens in a power substation which has the following functions Circuit breakers and switches enable the substation to be disconnected from the transmission grid or for distribution lines to be disconnected Transformers step down transmission voltages 35 kV or more down to primary distribution voltages These are medium voltage circuits usually 60035000 V From the transformer power goes to the busbar that can split the distribution power off in multiple directions The bus distributes power to distribution lines which fan out to customers Urban distribution is mainly underground sometimes in common utility ducts Rural distribution is mostly above ground with utility poles and suburban distribution is a mix Closer to the customer a distribution transformer steps the primary distribution power down to a lowvoltage secondary circuit usually 120240 V in the US for residential customers The power comes to the customer via a service drop and an electricity meter The final circuit in an urban system may be less than 15 metres 50 ft but may be over 91 metres 300 ft for a rural customer History Electric power distribution become necessary only in the 1880s when electricity started being generated at power stations Until then electricity was usually generated where it was used The first powerdistribution systems installed in European and US cities were used to supply lighting arc lighting running on veryhighvoltage around 3000 V alternating current AC or direct current DC and incandescent lighting running on lowvoltage 100 V direct current Both were supplanting gas lighting systems with arc lighting taking over largearea and street lighting and incandescent lighting replacing gas lights for business and residential users The high voltages used in arc lighting allowed a single generating station to supply a string of lights up to 7 miles 11 km long And each doubling of voltage would allow a given cable to transmit the same amount of power four times the distance than at the lower voltage with the same power loss By contrast directcurrent indoor incandescent lighting systems such as Edisons first power station installed in 1882 had difficulty supplying customers more than a mile away because they used a low voltage 110 V from generation to end use The low voltage translated to higher current and required thick copper cables for transmission In practice Edisons DC generating plants needed to be within about 15 miles 24 km of the farthest customer to avoid even thicker and more expensive conductors Introduction of the transformer The problem of transmitting electricity over longer distances became a recognized engineering roadblock to electric power distribution with many lessthansatisfactory solutions tested by lighting companies But the mid1880s saw a breakthrough with the development of functional transformers that allowed AC power to be stepped up to a much higher voltage for transmission then dropped down to a lower voltage near the end user Compared to direct current AC had much cheaper transmission costs and greater economies of scale with large AC generating plants capable of supplying whole cities and regions which led to the use of AC spreading rapidly In the US the competition between direct current and alternating current took a personal turn in the late 1880s in the form of a war of currents when Thomas Edison started attacking George Westinghouse and his development of the first US AC transformer systems highlighting the deaths caused by highvoltage AC systems over the years and claiming any AC system was inherently dangerous Edisons propaganda campaign was shortlived with his company switching over to AC in 1892 AC became the dominant form of transmission of power with innovations in Europe and the US in electric motor designs and the development of engineered universal systems allowing the large number of legacy systems to be connected to large AC grids In the first half of the 20th century in many places the electric power industry was vertically integrated meaning that one company did generation transmission distribution metering and billing Starting in the 1970s and 1980s nations began the process of deregulation and privatization leading to electricity markets The distribution system would remain regulated but generation retail and sometimes transmission systems were transformed into competitive markets Generation and transmission Electric power begins at a generating station where the potential difference can be as high as 33000 volts AC is usually used Users of large amounts of DC power such as some railway electrification systems telephone exchanges and industrial processes such as aluminium smelting use rectifiers to derive DC from the public AC supply or may have their own generation systems Highvoltage DC can be advantageous for isolating alternatingcurrent systems or controlling the quantity of electricity transmitted For example HydroQubec has a directcurrent line which goes from the James Bay region to Boston From the generating station it goes to the generating stations switchyard where a stepup transformer increases the voltage to a level suitable for transmission from 44 kV to 765 kV Once in the transmission system electricity from each generating station is combined with electricity produced elsewhere For alternatingcurrent generators all generating units connected to a common network must be synchronized operating at the same frequency within a small tolerance Alternatively disparate sources can be combined to serve a common load if some external power converter such as a rotating machine or a direct current converter system is interposed Electricity is consumed as soon as it is produced It is transmitted at a very high speed close to the speed of light Primary distribution Primary distribution voltages range from 4 kV to 35 kV phasetophase 24 kV to 20 kV phasetoneutral Only large consumers are fed directly from distribution voltages most utility customers are connected to a transformer which reduces the distribution voltage to the low voltage utilization voltage supply voltage or mains voltage used by lighting and interior wiring systems Network configurations Distribution networks are divided into two types radial or network A radial system is arranged like a tree where each customer has one source of supply A network system has multiple sources of supply operating in parallel Spot networks are used for concentrated loads Radial systems are commonly used in rural or suburban areas Radial systems usually include emergency connections where the system can be reconfigured in case of problems such as a fault or planned maintenance This can be done by opening and closing switches to isolate a certain section from the grid Long feeders experience voltage drop power factor distortion requiring capacitors or voltage regulators to be installed Reconfiguration by exchanging the functional links between the elements of the system represents one of the most important measures which can improve the operational performance of a distribution system The problem of optimization through the reconfiguration of a power distribution system in terms of its definition is a historical single objective problem with constraints Since 1975 when Merlin and Back introduced the idea of distribution system reconfiguration for active power loss reduction until nowadays a lot of researchers have proposed diverse methods and algorithms to solve the reconfiguration problem as a single objective problem Some authors have proposed Pareto optimality based approaches including active power losses and reliability indices as objectives For this purpose different artificial intelligence based methods have been used microgenetic branch exchange particle swarm optimization and nondominated sorting genetic algorithm Rural services Rural electrification systems tend to use higher distribution voltages because of the longer distances covered by distribution lines see Rural Electrification Administration 72 1247 25 and 345 kV distribution is common in the United States 11 kV and 33 kV are common in the UK Australia and New Zealand 11 kV and 22 kV are common in South Africa 10 20 and 35 kV are common in China Other voltages are occasionally used Rural services normally try to minimize the number of poles and wires It uses higher voltages than urban distribution which in turn permits use of galvanized steel wire The strong steel wire allows for less expensive wide pole spacing In rural areas a polemount transformer may serve only one customer In New Zealand Australia Saskatchewan Canada and South Africa Singlewire earth return systems SWER are used to electrify remote rural areas Three phase service provides power for large agricultural facilities petroleum pumping facilities water plants or other customers that have large loads threephase equipment In North America overhead distribution systems may be three phase four wire with a neutral conductor Rural distribution system may have long runs of one phase conductor and a neutral In other countries or in extreme rural areas the neutral wire is connected to the ground to use that as a return singlewire earth return Secondary distribution Electricity is delivered at a frequency of either 50 or 60 Hz depending on the region It is delivered to domestic customers as singlephase electric power In some countries as in Europe a three phase supply may be made available for larger properties Seen with an oscilloscope the domestic power supply in North America would look like a sine wave oscillating between 170 volts and 170 volts giving an effective voltage of 120 volts RMS Threephase electric power is more efficient in terms of power delivered per cable used and is more suited to running large electric motors Some large European appliances may be powered by threephase power such as electric stoves and clothes dryers A ground connection is normally provided for the customers system as well as for the equipment owned by the utility The purpose of connecting the customers system to ground is to limit the voltage that may develop if high voltage conductors fall down onto lowervoltage conductors which are usually mounted lower to the ground or if a failure occurs within a distribution transformer Earthing systems can be TT TNS TNCS or TNC Regional variations 220240 volt systems Most of the world uses 50 Hz 220 or 230 V single phase or 400 V threephase for residential and light industrial services In this system the primary distribution network supplies a few substations per area and the 230 V 400 V power from each substation is directly distributed to end users over a region of normally less than 1 km radius Three live hot wires and the neutral are connected to the building for a three phase service Singlephase distribution with one live wire and the neutral is used domestically where total loads are light In Europe electricity is normally distributed for industry and domestic use by the threephase four wire system This gives a phasetophase voltage of 400 volts wye service and a singlephase voltage of 230 volts between any one phase and neutral In the UK a typical urban or suburban lowvoltage substation would normally be rated between 150 kVA and 1 MVA and supply a whole neighbourhood of a few hundred houses Transformers are typically sized on an average load of 1 to 2 kW per household and the service fuses and cable is sized to allow any one property to draw a peak load of perhaps ten times this For industrial customers 3phase 690 400 volt is also available or may be generated locally Large industrial customers have their own transformers with an input from 11 kV to 220 kV 100120 volt systems Most of the Americas use 60 Hz AC the 120240 volt splitphase system domestically and three phase for larger installations North American transformers usually power homes at 240 volts similar to Europes 230 volts It is the splitphase that allows use of 120 volts in the home In the electricity sector in Japan the standard voltage is 100 V with both 50 and 60 Hz AC frequencies being used Parts of the country use 50 Hz while other parts use 60 Hz This is a relic from the 1890s Some local providers in Tokyo imported 50 Hz German equipment while the local power providers in Osaka brought in 60 Hz generators from the United States The grids grew until eventually the entire country was wired Today the frequency is 50 Hz in Eastern Japan including Tokyo Yokohama Tohoku and Hokkaido and 60 Hz in Western Japan including Nagoya Osaka Kyoto Hiroshima Shikoku and Kyushu Most household appliances are made to work on either frequency The problem of incompatibility came into the public eye when the 2011 Thoku earthquake and tsunami knocked out about a third of the easts capacity and power in the west could not be fully shared with the east since the country does not have a common frequency There are four highvoltage direct current HVDC converter stations that move power across Japans AC frequency border Shin Shinano is a backtoback HVDC facility in Japan which forms one of four frequency changer stations that link Japans western and eastern power grids The other three are at HigashiShimizu MinamiFukumitsu and Sakuma Dam Together they can move up to 12 GW of power east or west 240 volt systems and 120 volt outlets Most modern North American homes are wired to receive 240 volts from the transformer and through the use of splitphase electrical power can have both 120 volt receptacles and 240 volt receptacles The 120 volts is typically used for lighting and most wall outlets The 240 volt circuits are typically used for appliances requiring high watt heat output such as ovens and heaters They may also be used to supply an electric car charger Modern distribution systems Traditionally the distribution systems would only operate as simple distribution lines where the electricity from the transmission networks would be shared among the customers Todays distribution systems are heavily integrated with renewable energy generations at the distribution level of the power systems by the means of distributed generation resources such as solar energy and wind energy As a result distribution systems are becoming more independent from the transmission networks daybyday Balancing the supplydemand relationship at these modern distribution networks sometimes referred to as microgrids is extremely challenging and it requires the use of various technological and operational means to operate Such tools include battery storage power station data analytics optimization tools etc See also References External links IEEE Power Engineering Society IEEE Power Engineering Society Distribution Subcommittee US Department of Energy Electric Distribution website",
  },
  {
    title: "Energy efficiency",
    originalContent:
      "Energy efficiency may refer to Energy efficiency physics the ratio between the useful output and input of an energy conversion process Electrical efficiency useful power output per electrical power consumed Mechanical efficiency a ratio of the measured performance to the performance of an ideal machine Thermal efficiency the extent to which the energy added by heat is converted to net work output or vice versa Luminous efficiency a measure of how well a light source produces visible light Fuel efficiency the efficiency of converting potential energy in a fuel into kinetic energy Energy efficiency in transportation the fuel economy of various modes of transportation Energyefficient landscaping a type of landscaping designed for the purpose of conserving energy Efficient energy use minimizing the amount of energy used for a given constant energy service Energy conservation reducing energy consumption by using less of an energy service See also Energy disambiguation Efficiency disambiguation Energy rating disambiguation All pages with titles containing Energy efficiency All pages with titles containing Energy efficient",
  },
  {
    title: "Sustainable development",
    originalContent:
      "Sustainable development is an approach to growth and human development that aims to meet the needs of the present without compromising the ability of future generations to meet their own needs The aim is to have a society where living conditions and resources meet human needs without undermining planetary integrity Sustainable development aims to balance the needs of the economy environment and social wellbeing The Brundtland Report in 1987 helped to make the concept of sustainable development better known Sustainable development overlaps with the idea of sustainability which is a normative concept UNESCO formulated a distinction between the two concepts as follows Sustainability is often thought of as a longterm goal ie a more sustainable world while sustainable development refers to the many processes and pathways to achieve it The Rio Process that began at the 1992 Earth Summit in Rio de Janeiro has placed the concept of sustainable development on the international agenda Sustainable development is the foundational concept of the Sustainable Development Goals SDGs These global goals for the year 2030 were adopted in 2015 by the United Nations General Assembly UNGA They address the global challenges including for example poverty climate change biodiversity loss and peace There are some problems with the concept of sustainable development Some scholars say it is an oxymoron because according to them development is inherently unsustainable Other commentators are disappointed in the lack of progress that has been achieved so far Scholars have stated that sustainable development is openended much critiqued as ambiguous incoherent and therefore easily appropriated Definition of sustainable development In 1987 the United Nations World Commission on Environment and Development released the report Our Common Future commonly called the Brundtland Report The report included a definition of sustainable development which is now widely used Sustainable development is a development that meets the needs of the present without compromising the ability of future generations to meet their own needs It contains two key concepts within it The concept of needs in particular the essential needs of the worlds poor to which overriding priority should be given and The idea of limitations imposed by the state of technology and social organization on the environments ability to meet present and future needsSustainable development thus tries to find a balance between economic development environmental protection and social wellbeing However scholars have pointed out that there are manifold understandings of sustainable development Also there are incoherencies in the dominant marketbased socioeconomicpolitical organisation Attempts towards universal sustainable development need to account for the extremely varied challenges circumstances and choices that shape prospects and prosperity for all everywhere The discourse of sustainable development is highly influential in global and national governance frameworks though its meaning and operationalization are contextdependent and have evolved over time The evolution of this discourse can for example be seen in the transition from the Millennium Development Goals MDGs years 2000 to 2015 to the Sustainable Development Goals SDGs years 2015 to 2030 Development of the concept Sustainable development has its roots in ideas regarding sustainable forest management which were developed in Europe during the 17th and 18th centuries In response to a growing awareness of the depletion of timber resources in England John Evelyn argued in his 1662 essay Sylva that sowing and planting of trees had to be regarded as a national duty of every landowner in order to stop the destructive over exploitation of natural resources In 1713 Hans Carl von Carlowitz a senior mining administrator in the service of Elector Frederick Augustus I of Saxony published Sylvicultura economics a 400page work on forestry Building upon the ideas of Evelyn and French minister JeanBaptiste Colbert von Carlowitz developed the concept of managing forests for sustained yield His work influenced others including Alexander von Humboldt and Georg Ludwig Hartig eventually leading to the development of the science of forestry This in turn influenced people like Gifford Pinchot the first head of the US Forest Service whose approach to forest management was driven by the idea of wise use of resources and Aldo Leopold whose land ethic was influential in the development of the environmental movement in the 1960s Following the publication of Rachel Carsons Silent Spring in 1962 the developing environmental movement drew attention to the relationship between economic growth and environmental degradation Kenneth E Boulding in his influential 1966 essay The Economics of the Coming Spaceship Earth identified the need for the economic system to fit itself to the ecological system with its limited pools of resources Another milestone was the 1968 article by Garrett Hardin that popularized the term tragedy of the commons The direct linking of sustainability and development in a contemporary sense can be traced to the early 1970s Strategy of Progress a 1972 book in German by Ernst Basler explained how the longacknowledged sustainability concept of preserving forests for future wood production can be directly transferred to the broader importance of preserving environmental resources to sustain the world for future generations That same year the interrelationship of environment and development was formally demonstrated in a systems dynamic simulation model reported in the classic report on Limits to Growth This was commissioned by the Club of Rome and written by a group of scientists led by Dennis and Donella Meadows of the Massachusetts Institute of Technology Describing the desirable state of global equilibrium the authors wrote We are searching for a model output that represents a world system that is sustainable without sudden and uncontrolled collapse and capable of satisfying the basic material requirements of all of its people The year 1972 also saw the publication of the influential book A Blueprint for Survival In 1975 an MIT research group prepared ten days of hearings on Growth and Its Implication for the Future for the US Congress the first hearings ever held on sustainable development In 1980 the International Union for Conservation of Nature published a world conservation strategy that included one of the first references to sustainable development as a global priority and introduced the term sustainable development 4 Two years later the United Nations World Charter for Nature raised five principles of conservation by which human conduct affecting nature is to be guided and judged Since the Brundtland Report the concept of sustainable development has developed beyond the initial intergenerational framework to focus more on the goal of socially inclusive and environmentally sustainable economic growth 5 In 1992 the UN Conference on Environment and Development published the Earth Charter which outlines the building of a just sustainable and peaceful global society in the 21st century The action plan Agenda 21 for sustainable development identified information integration and participation as key building blocks to help countries achieve development that recognizes these interdependent pillars Furthermore Agenda 21 emphasizes that broad public participation in decisionmaking is a fundamental prerequisite for achieving sustainable development The Rio Protocol was a huge leap forward for the first time the world agreed on a sustainability agenda In fact a global consensus was facilitated by neglecting concrete goals and operational details Global governance framework The most comprehensive global governance framework for sustainable development is the 2030 Agenda for Sustainable Development with its 17 Sustainable Development Goals SDGs This agenda was a followup to the Millennium Declaration from the year 2000 with its eight Millennium Development Goals MDGs the first comprehensive global governance framework for the achievement of sustainable development The SDGs have concrete targets unlike the results from the Rio Process but no methods for sanctions 137 They contain goals targets and indicators for example in the areas of poverty reduction environmental protection human prosperity and peace Sustainability means different things to different people and the concept of sustainable development has led to a diversity of discourses that legitimize competing sociopolitical projects Global environmental governance scholars have identified a comprehensive set of discourses within the public space that mostly convey four sustainability frames mainstream sustainability progressive sustainability a limits discourse and radical sustainability First mainstream sustainability is a conservative approach on both economic and political terms Second progressive sustainability is an economically conservative yet politically reformist approach Under this framing sustainable development is still centered on economic growth which is deemed compatible with environmental sustainability However human wellbeing and development can only be achieved through a redistribution of power to even out inequalities between developed and developing countries Third a limits discourse is an economically reformist yet politically conservative approach to sustainability Fourth radical sustainability is a transformative approach seeking to break with existing global economic and political structures Related concepts Sustainability Dimensions Sustainable development like sustainability is regarded to have three dimensions the environment economy and society The idea is that a good balance between the three dimensions should be achieved Instead of calling them dimensions other terms commonly used are pillars domains aspects spheres Pathways Six interdependent capacities are deemed to be necessary for the successful pursuit of sustainable development These are the capacities to measure progress towards sustainable development promote equity within and between generations adapt to shocks and surprises transform the system onto more sustainable development pathways link knowledge with action for sustainability and to devise governance arrangements that allow people to work together During the MDG era year 2000 to 2015 the key objective of sustainable development was poverty reduction to be reached through economic growth and participation in the global trade system The SDGs take a much more comprehensive approach to sustainable development than the MDGs did They offer a more peoplecentred development agenda Out of the 17 SDGs for example 11 goals contain targets related to equity equality or inclusion and SDG 10 is solely devoted to addressing inequality within and among countries Improving on environmental sustainability An unsustainable situation occurs when natural capital the total of natures resources is used up faster than it can be replenished 58 Sustainability requires that human activity only uses natures resources at a rate at which they can be replenished naturally The concept of sustainable development is intertwined with the concept of carrying capacity Theoretically the longterm result of environmental degradation is the inability to sustain human life Important operational principles of sustainable development were published by Herman Daly in 1990 renewable resources should provide a sustainable yield the rate of harvest should not exceed the rate of regeneration for nonrenewable resources there should be equivalent development of renewable substitutes waste generation should not exceed the assimilative capacity of the environment In 2019 a summary for policymakers of the largest most comprehensive study to date of biodiversity and ecosystem services was published by the Intergovernmental SciencePolicy Platform on Biodiversity and Ecosystem Services It recommended that human civilization will need a transformative change including sustainable agriculture reductions in consumption and waste fishing quotas and collaborative water management Environmental problems associated with industrial agriculture and agribusiness are now being addressed through approaches such as sustainable agriculture organic farming and more sustainable business practices At the local level there are various movements working towards sustainable food systems which may include less meat consumption local food production slow food sustainable gardening and organic gardening The environmental effects of different dietary patterns depend on many factors including the proportion of animal and plant foods consumed and the method of food production As global population and affluence have increased so has the use of various materials increased in volume diversity and distance transported By 2050 humanity could consume an estimated 140 billion tons of minerals ores fossil fuels and biomass per year three times its current amount unless the economic growth rate is decoupled from the rate of natural resource consumption Sustainable use of materials has targeted the idea of dematerialization converting the linear path of materials extraction use disposal in landfill to a circular material flow that reuses materials as much as possible much like the cycling and reuse of waste in nature This way of thinking is expressed in the concept of circular economy which employs reuse sharing repair refurbishment remanufacturing and recycling to create a closedloop system minimizing the use of resource inputs and the creation of waste pollution and carbon emissions The European Commission has adopted an ambitious Circular Economy Action Plan in 2020 which aims at making sustainable products the norm in the EU Improving on economic and social aspects It has been suggested that because of the rural poverty and overexploitation environmental resources should be treated as important economic assets called natural capital Economic development has traditionally required a growth in the gross domestic product This model of unlimited personal and GDP growth may be over Sustainable development may involve improvements in the quality of life for many but may necessitate a decrease in resource consumption Growth generally ignores the direct effect that the environment may have on social welfare whereas development takes it into account As early as the 1970s the concept of sustainability was used to describe an economy in equilibrium with basic ecological support systems Scientists in many fields have highlighted The Limits to Growth and economists have presented alternatives for example a steadystate economy to address concerns over the impacts of expanding human development on the planet In 1987 the economist Edward Barbier published the study The Concept of Sustainable Economic Development where he recognized that goals of environmental conservation and economic development are not conflicting and can be reinforcing each other A World Bank study from 1999 concluded that based on the theory of genuine savings defined as traditional net savings less the value of resource depletion and environmental degradation plus the value of investment in human capital policymakers have many possible interventions to increase sustainability in macroeconomics or purely environmental Several studies have noted that efficient policies for renewable energy and pollution are compatible with increasing human welfare eventually reaching a goldenrule steady state A meta review in 2002 looked at environmental and economic valuations and found a lack of concrete understanding of what sustainability policies might entail in practice A study concluded in 2007 that knowledge manufactured and human capital health and education has not compensated for the degradation of natural capital in many parts of the world It has been suggested that intergenerational equity can be incorporated into a sustainable development and decision making as has become common in economic valuations of climate economics The World Business Council for Sustainable Development published a Vision 2050 document in 2021 to show How business can lead the transformations the world needs The vision states that we envision a world in which 9billion people can live well within planetary boundaries by 2050 This report was highlighted by The Guardian as the largest concerted corporate sustainability action plan to date include reversing the damage done to ecosystems addressing rising greenhouse gas emissions and ensuring societies move to sustainable agriculture Barriers Assessments and reactions The concept of sustainable development has been and still is subject to criticism including the question of what is to be sustained in sustainable development It has been argued that there is no such thing as sustainable use of a nonrenewable resource since any positive rate of exploitation will eventually lead to the exhaustion of earths finite stock 13 this perspective renders the Industrial Revolution as a whole unsustainable 20f 6167 22f The sustainable development debate is based on the assumption that societies need to manage three types of capital economic social and natural which may be nonsubstitutable and whose consumption might be irreversible Natural capital can not necessarily be substituted by economic capital While it is possible that we can find ways to replace some natural resources it is much less likely that they will ever be able to replace ecosystem services such as the protection provided by the ozone layer or the climate stabilizing function of the Amazonian forest The concept of sustainable development has been criticized from different angles While some see it as paradoxical or an oxymoron and regard development as inherently unsustainable others are disappointed in the lack of progress that has been achieved so far Part of the problem is that development itself is not consistently defined 16 The vagueness of the Brundtland definition of sustainable development has been criticized as follows 17 The definition has opened up the possibility of downplaying sustainability Hence governments spread the message that we can have it all at the same time ie economic growth prospering societies and a healthy environment No new ethic is required This socalled weak version of sustainability is popular among governments and businesses but profoundly wrong and not even weak as there is no alternative to preserving the earths ecological integrity 2 Scholars have stated that sustainable development is openended much critiqued as ambiguous incoherent and therefore easily appropriated Society and culture Sustainable development goals Sustainable development is the foundational concept of the Sustainable Development Goals SDGs Policies to achieve the SDGs are meant to cohere around this concept Education for sustainable development Education for sustainable development ESD is a term officially used by the United Nations It is defined as education practices that encourage changes in knowledge skills values and attitudes to enable a more sustainable and just society for humanity ESD aims to empower and equip current and future generations to meet their needs using a balanced and integrated approach to sustainable developments economic social and environmental dimensions Agenda 21 was the first international document that identified education as an essential tool for achieving sustainable development and highlighted areas of action for education ESD is a component of measurement in an indicator for Sustainable Development Goal 12 SDG for responsible consumption and production SDG 12 has 11 targets and target 128 is By 2030 ensure that people everywhere have the relevant information and awareness for sustainable development and lifestyles in harmony with nature 20 years after the Agenda 21 document was declared the Future we want document was proclaimed in the Rio20 UN Conference on Sustainable Development stating that We resolve to promote education for sustainable development and to integrate sustainable development more actively into education beyond the Decade of Education for Sustainable Development One version of education for Sustainable Development recognizes modernday environmental challenges It seeks to define new ways to adjust to a changing biosphere as well as engage individuals to address societal issues that come with them In the International Encyclopedia of Education this approach to education is seen as an attempt to shift consciousness toward an ethics of lifegiving relationships that respects the interconnectedness of man to his natural world to equip future members of society with environmental awareness and a sense of responsibility to sustainability For UNESCO education for sustainable development involves integrating key sustainable development issues into teaching and learning This may include for example instruction about climate change disaster risk reduction biodiversity and poverty reduction and sustainable consumption It also requires participatory teaching and learning methods that motivate and empower learners to change their behaviours and take action for sustainable development ESD consequently promotes competencies like critical thinking imagining future scenarios and making decisions in a collaborative way The Thessaloniki Declaration presented at the International Conference on Environment and Society Education and Public Awareness for Sustainability by UNESCO and the Government of Greece December 1997 highlights the importance of sustainability not only with regards to the natural environment but also with poverty health food security democracy human rights and peace See also List of sustainability topics Outline of sustainability Overview of and topical guide to sustainability Policy coherence for development Approach in development assistance Sustainability measurement Quantitbasis for the informed management of sustainability Sustainable remediation United Nations Decade of Education for Sustainable Development References External links Sustainable Development Knowledge Platform of the UN Sustainable Development Solutions Network",
  },
  {
    title: "Greenhouse gases",
    originalContent:
      "Greenhouse gases GHGs are the gases in the atmosphere that raise the surface temperature of planets such as the Earth What distinguishes them from other gases is that they absorb the wavelengths of radiation that a planet emits resulting in the greenhouse effect The Earth is warmed by sunlight causing its surface to radiate heat which is then mostly absorbed by greenhouse gases Without greenhouse gases in the atmosphere the average temperature of Earths surface would be about 18 C 0 F rather than the present average of 15 C 59 F The five most abundant greenhouse gases in Earths atmosphere listed in decreasing order of average global mole fraction are water vapor carbon dioxide methane nitrous oxide ozone Other greenhouse gases of concern include chlorofluorocarbons CFCs and HCFCs hydrofluorocarbons HFCs perfluorocarbons SF6 and NF3 Water vapor causes about half of the greenhouse effect acting in response to other gases as a climate change feedback Human activities since the beginning of the Industrial Revolution around 1750 have increased carbon dioxide by over 50 and methane levels by 150 Carbon dioxide emissions are causing about threequarters of global warming while methane emissions cause most of the rest The vast majority of carbon dioxide emissions by humans come from the burning of fossil fuels with remaining contributions from agriculture and industry 687 Methane emissions originate from agriculture fossil fuel production waste and other sources The carbon cycle takes thousands of years to fully absorb CO2 from the atmosphere while methane lasts in the atmosphere for an average of only 12 years Natural flows of carbon happen between the atmosphere terrestrial ecosystems the ocean and sediments These flows have been fairly balanced over the past 1 million years although greenhouse gas levels have varied widely in the more distant past Carbon dioxide levels are now higher than they have been for 3 million years If current emission rates continue then global warming will surpass 20 C 36 F sometime between 2040 and 2070 This is a level which the Intergovernmental Panel on Climate Change IPCC says is dangerous Properties and mechanisms Greenhouse gases are infrared active meaning that they absorb and emit infrared radiation in the same long wavelength range as what is emitted by the Earths surface clouds and atmosphere 2233 99 of the Earths dry atmosphere excluding water vapor is made up of nitrogen N2 78 and oxygen O2 21 Because their molecules contain two atoms of the same element they have no asymmetry in the distribution of their electrical charges and so are almost totally unaffected by infrared thermal radiation with only an extremely minor effect from collisioninduced absorption A further 09 of the atmosphere is made up by argon Ar which is monatomic and so completely transparent to thermal radiation On the other hand carbon dioxide 004 methane nitrous oxide and even less abundant trace gases account for less than 01 of Earths atmosphere but because their molecules contain atoms of different elements there is an asymmetry in electric charge distribution which allows molecular vibrations to interact with electromagnetic radiation This makes them infrared active and so their presence causes greenhouse effect Radiative forcing Earth absorbs some of the radiant energy received from the sun reflects some of it as light and reflects or radiates the rest back to space as heat A planets surface temperature depends on this balance between incoming and outgoing energy When Earths energy balance is shifted its surface becomes warmer or cooler leading to a variety of changes in global climate Radiative forcing is a metric calculated in watts per square meter which characterizes the impact of an external change in a factor that influences climate It is calculated as the difference in topofatmosphere TOA energy balance immediately caused by such an external change A positive forcing such as from increased concentrations of greenhouse gases means more energy arriving than leaving at the topofatmosphere which causes additional warming while negative forcing like from sulfates forming in the atmosphere from sulfur dioxide leads to cooling 2245 Within the lower atmosphere greenhouse gases exchange thermal radiation with the surface and limit radiative heat flow away from it which reduces the overall rate of upward radiative heat transfer 139 The increased concentration of greenhouse gases is also cooling the upper atmosphere as it is much thinner than the lower layers and any heat reemitted from greenhouse gases is more likely to travel further to space than to interact with the fewer gas molecules in the upper layers The upper atmosphere is also shrinking as the result Contributions of specific gases to the greenhouse effect Anthropogenic changes to the natural greenhouse effect are sometimes referred to as the enhanced greenhouse effect 2223 This table shows the most important contributions to the overall greenhouse effect without which the average temperature of Earths surface would be about 18 C 0 F instead of around 15 C 59 F This table also specifies tropospheric ozone because this gas has a cooling effect in the stratosphere but a warming influence comparable to nitrous oxide and CFCs in the troposphere Special role of water vapor Water vapor is the most important greenhouse gas overall being responsible for 4167 of the greenhouse effect but its global concentrations are not directly affected by human activity While local water vapor concentrations can be affected by developments such as irrigation it has little impact on the global scale due to its short residence time of about nine days Indirectly an increase in global temperatures cause will also increase water vapor concentrations and thus their warming effect in a process known as water vapor feedback It occurs because ClausiusClapeyron relation establishes that more water vapor will be present per unit volume at elevated temperatures Thus local atmospheric concentration of water vapor varies from less than 001 in extremely cold regions and up to 3 by mass in saturated air at about 32 C Global warming potential GWP and CO2 equivalents List of all greenhouse gases The contribution of each gas to the enhanced greenhouse effect is determined by the characteristics of that gas its abundance and any indirect effects it may cause For example the direct radiative effect of a mass of methane is about 84 times stronger than the same mass of carbon dioxide over a 20year time frame Since the 1980s greenhouse gas forcing contributions relative to year 1750 are also estimated with high accuracy using IPCCrecommended expressions derived from radiative transfer models The concentration of a greenhouse gas is typically measured in parts per million ppm or parts per billion ppb by volume A CO2 concentration of 420 ppm means that 420 out of every million air molecules is a CO2 molecule The first 30 ppm increase in CO2 concentrations took place in about 200 years from the start of the Industrial Revolution to 1958 however the next 90 ppm increase took place within 56 years from 1958 to 2014 Similarly the average annual increase in the 1960s was only 37 of what it was in 2000 through 2007 Many observations are available online in a variety of Atmospheric Chemistry Observational Databases The table below shows the most influential longlived wellmixed greenhouse gases along with their tropospheric concentrations and direct radiative forcings as identified by the Intergovernmental Panel on Climate Change IPCC Abundances of these trace gases are regularly measured by atmospheric scientists from samples collected throughout the world It excludes water vapor because changes in its concentrations are calculated as a climate change feedback indirectly caused by changes in other greenhouse gases as well as ozone whose concentrations are only modified indirectly by various refrigerants that cause ozone depletion Some shortlived gases eg carbon monoxide NOx and aerosols eg mineral dust or black carbon are also excluded because of limited role and strong variation along with minor refrigerants and other halogenated gases which have been massproduced in smaller quantities than those in the table 731738 and Annex III of the 2021 IPCC WG1 Report 49 a Mole fractions molmol ppm parts per million 106 nmolmol ppb parts per billion 109 pmolmol ppt parts per trillion 1012 A The IPCC states that no single atmospheric lifetime can be given for CO2 731 This is mostly due to the rapid growth and cumulative magnitude of the disturbances to Earths carbon cycle by the geologic extraction and burning of fossil carbon As of year 2014 fossil CO2 emitted as a theoretical 10 to 100 GtC pulse on top of the existing atmospheric concentration was expected to be 50 removed by land vegetation and ocean sinks in less than about a century as based on the projections of coupled models referenced in the AR5 assessment A substantial fraction 2035 was also projected to remain in the atmosphere for centuries to millennia where fractional persistence increases with pulse size B Values are relative to year 1750 AR6 reports the effective radiative forcing which includes effects of rapid adjustments in the atmosphere and at the surface Factors affecting concentrations Atmospheric concentrations are determined by the balance between sources emissions of the gas from human activities and natural systems and sinks the removal of the gas from the atmosphere by conversion to a different chemical compound or absorption by bodies of water 512 Airborne fraction The proportion of an emission remaining in the atmosphere after a specified time is the airborne fraction AF The annual airborne fraction is the ratio of the atmospheric increase in a given year to that years total emissions The annual airborne fraction for CO2 had been stable at 045 for the past six decades even as the emissions have been increasing This means that the other 055 of emitted CO2 is absorbed by the land and atmosphere carbon sinks within the first year of an emission In the highemission scenarios the effectiveness of carbon sinks will be lower increasing the atmospheric fraction of CO2 even though the raw amount of emissions absorbed will be higher than in the present 746 Atmospheric lifetime Major greenhouse gases are well mixed and take many years to leave the atmosphere The atmospheric lifetime of a greenhouse gas refers to the time required to restore equilibrium following a sudden increase or decrease in its concentration in the atmosphere Individual atoms or molecules may be lost or deposited to sinks such as the soil the oceans and other waters or vegetation and other biological systems reducing the excess to background concentrations The average time taken to achieve this is the mean lifetime This can be represented through the following formula where the lifetime displaystyle tau of an atmospheric species X in a onebox model is the average time that a molecule of X remains in the box displaystyle tau can also be defined as the ratio of the mass m displaystyle m in kg of X in the box to its removal rate which is the sum of the flow of X out of the box F out displaystyle F_textout chemical loss of X L displaystyle L and deposition of X D displaystyle D all in kgs m F out L D displaystyle tau frac mF_textoutLD If input of this gas into the box ceased then after time displaystyle tau its concentration would decrease by about 63 Changes to any of these variables can alter the atmospheric lifetime of a greenhouse gas For instance methanes atmospheric lifetime is estimated to have been lower in the 19th century than now but to have been higher in the second half of the 20th century than after 2000 Carbon dioxide has an even more variable lifetime which cannot be specified down to a single number 2237 Scientists instead say that while the first 10 of carbon dioxides airborne fraction not counting the 50 absorbed by land and ocean sinks within the emissions first year is removed quickly the vast majority of the airborne fraction 80 lasts for centuries to millennia The remaining 10 stays for tens of thousands of years In some models this longestlasting fraction is as large as 30 During geologic time scales Monitoring Greenhouse gas monitoring involves the direct measurement of atmospheric concentrations and direct and indirect measurement of greenhouse gas emissions Indirect methods calculate emissions of greenhouse gases based on related metrics such as fossil fuel extraction There are several different methods of measuring carbon dioxide concentrations in the atmosphere including infrared analyzing and manometry Methane and nitrous oxide are measured by other instruments such as the rangeresolved infrared differential absorption lidar DIAL Greenhouse gases are measured from space such as by the Orbiting Carbon Observatory and through networks of ground stations such as the Integrated Carbon Observation System The Annual Greenhouse Gas Index AGGI is defined by atmospheric scientists at NOAA as the ratio of total direct radiative forcing due to longlived and wellmixed greenhouse gases for any year for which adequate global measurements exist to that present in year 1990 These radiative forcing levels are relative to those present in year 1750 ie prior to the start of the industrial era 1990 is chosen because it is the baseline year for the Kyoto Protocol and is the publication year of the first IPCC Scientific Assessment of Climate Change As such NOAA states that the AGGI measures the commitment that global society has already made to living in a changing climate It is based on the highest quality atmospheric observations from sites around the world Its uncertainty is very low Data networks Types of sources Natural sources The natural flows of carbon between the atmosphere ocean terrestrial ecosystems and sediments are fairly balanced so carbon levels would be roughly stable without human influence Carbon dioxide is removed from the atmosphere primarily through photosynthesis and enters the terrestrial and oceanic biospheres Carbon dioxide also dissolves directly from the atmosphere into bodies of water ocean lakes etc as well as dissolving in precipitation as raindrops fall through the atmosphere When dissolved in water carbon dioxide reacts with water molecules and forms carbonic acid which contributes to ocean acidity It can then be absorbed by rocks through weathering It also can acidify other surfaces it touches or be washed into the ocean Humanmade sources The vast majority of carbon dioxide emissions by humans come from the burning of fossil fuels Additional contributions come from cement manufacturing fertilizer production and changes in land use like deforestation 687 Methane emissions originate from agriculture fossil fuel production waste and other sources If current emission rates continue then temperature rises will surpass 20 C 36 F sometime between 2040 and 2070 which is the level the United Nations Intergovernmental Panel on Climate Change IPCC says is dangerous Most greenhouse gases have both natural and humancaused sources An exception are purely humanproduced synthetic halocarbons which have no natural sources During the preindustrial Holocene concentrations of existing gases were roughly constant because the large natural sources and sinks roughly balanced In the industrial era human activities have added greenhouse gases to the atmosphere mainly through the burning of fossil fuels and clearing of forests 115 Reducing humancaused greenhouse gases Needed emissions cuts Removal from the atmosphere through negative emissions Several technologies remove greenhouse gas emissions from the atmosphere Most widely analyzed are those that remove carbon dioxide from the atmosphere either to geologic formations such as bioenergy with carbon capture and storage and carbon dioxide air capture or to the soil as in the case with biochar Many longterm climate scenario models require largescale humanmade negative emissions to avoid serious climate change Negative emissions approaches are also being studied for atmospheric methane called atmospheric methane removal History of discovery In the late 19th century scientists experimentally discovered that N2 and O2 do not absorb infrared radiation called at that time dark radiation while water both as true vapor and condensed in the form of microscopic droplets suspended in clouds and CO2 and other polyatomic gaseous molecules do absorb infrared radiation In the early 20th century researchers realized that greenhouse gases in the atmosphere made Earths overall temperature higher than it would be without them The term greenhouse was first applied to this phenomenon by Nils Gustaf Ekholm in 1901 During the late 20th century a scientific consensus evolved that increasing concentrations of greenhouse gases in the atmosphere cause a substantial rise in global temperatures and changes to other parts of the climate system with consequences for the environment and for human health Other planets Greenhouse gases exist in many atmospheres creating greenhouse effects on Mars Titan and particularly in the thick atmosphere of Venus While Venus has been described as the ultimate end state of runaway greenhouse effect such a process would have virtually no chance of occurring from any increases in greenhouse gas concentrations caused by humans as the Suns brightness is too low and it would likely need to increase by some tens of percents which will take a few billion years See also Carbon accounting Processes used to measure emissions of carbon dioxide equivalents Carbon budget Limit on carbon dioxide emission for a given climate impact Carbon sequestration Storing carbon in a carbon pool Climate change feedback Feedback related to climate changePages displaying short descriptions of redirect targets Greenhouse gas monitoring Measurement of greenhouse gas emissions and levels Greenhouse gas inventory Inventory for emissions of greenhouse gases References External links Media related to Greenhouse gases at Wikimedia Commons Carbon Dioxide Information Analysis Center CDIAC US Department of Energy retrieved 26 July 2020 Annual Greenhouse Gas Index AGGI from NOAA Atmospheric spectra of GHGs and other trace gases Archived 25 March 2013 at the Wayback Machine",
  },
  {
    title: "Arctic",
    originalContent:
      "The Arctic or from Greek bear is a polar region located at the northernmost part of Earth The Arctic region from the IERS Reference Meridian travelling east consists of parts of northern Norway Nordland Troms Finnmark Svalbard and Jan Mayen northernmost Sweden Vsterbotten Norrbotten and Lappland northern Finland North Ostrobothnia Kainuu and Lappi Russia Murmansk Siberia Nenets Okrug Novaya Zemlya the United States Alaska Canada Yukon Northwest Territories Nunavut Danish Realm Greenland and northern Iceland Grmsey and Kolbeinsey along with the Arctic Ocean and adjacent seas Land within the Arctic region has seasonally varying snow and ice cover with predominantly treeless permafrost under the tundra Arctic seas contain seasonal sea ice in many places The Arctic region is a unique area among Earths ecosystems The cultures in the region and the Arctic indigenous peoples have adapted to its cold and extreme conditions Life in the Arctic includes zooplankton and phytoplankton fish and marine mammals birds land animals plants and human societies Arctic land is bordered by the subarctic Definition and etymology The word Arctic comes from the Greek word arktikos near the Bear northern and from the word arktos meaning bear The name refers either to the constellation known as Ursa Major the Great Bear which is prominent in the northern portion of the celestial sphere or to the constellation Ursa Minor the Little Bear which contains the celestial north pole currently very near Polaris the current north Pole Star or North Star There are a number of definitions of what area is contained within the Arctic The area can be defined as north of the Arctic Circle about 66 34N the approximate southern limit of the midnight sun and the polar night Another definition of the Arctic which is popular with ecologists is the region in the Northern Hemisphere where the average temperature for the warmest month July is below 10 C 50 F the northernmost tree line roughly follows the isotherm at the boundary of this region Climate The climate of the Arctic region is characterized by cold winters and cool summers Its precipitation mostly comes in the form of snow and is low with most of the area receiving less than 50 cm 20 in High winds often stir up snow creating the illusion of continuous snowfall Average winter temperatures can go as low as 40 C 40 F and the coldest recorded temperature is approximately 68 C 90 F Coastal Arctic climates are moderated by oceanic influences having generally warmer temperatures and heavier snowfalls than the colder and drier interior areas The Arctic is affected by current global warming leading to climate change in the Arctic including Arctic sea ice decline diminished ice in the Greenland ice sheet and Arctic methane emissions as the permafrost thaws The melting of Greenlands ice sheet is linked to polar amplification Due to the poleward migration of the planets isotherms about 56 km 35 mi per decade during the past 30 years as a consequence of global warming the Arctic region as defined by tree line and temperature is currently shrinking Perhaps the most alarming result of this is Arctic sea ice shrinkage There is a large variance in predictions of Arctic sea ice loss with models showing nearcomplete to complete loss in September from 2035 to some time around 2067 Flora and fauna Arctic life is characterized by adaptation to short growing seasons with long periods of sunlight and cold dark snowcovered winter conditions Plants Arctic vegetation is composed of plants such as dwarf shrubs graminoids herbs lichens and mosses which all grow relatively close to the ground forming tundra An example of a dwarf shrub is the bearberry As one moves northward the amount of warmth available for plant growth decreases considerably In the northernmost areas plants are at their metabolic limits and small differences in the total amount of summer warmth make large differences in the amount of energy available for maintenance growth and reproduction Colder summer temperatures cause the size abundance productivity and variety of plants to decrease Trees cannot grow in the Arctic but in its warmest parts shrubs are common and can reach 2 m 6 ft 7 in in height sedges mosses and lichens can form thick layers In the coldest parts of the Arctic much of the ground is bare nonvascular plants such as lichens and mosses predominate along with a few scattered grasses and forbs like the Arctic poppy Animals Herbivores on the tundra include the Arctic hare lemming muskox and reindeer caribou They are preyed on by the snowy owl Arctic fox grizzly bear and Arctic wolf The polar bear is also a predator though it prefers to hunt for marine life from the ice There are also many birds and marine species endemic to the colder regions Other terrestrial animals include wolverines moose Dall sheep ermines and Arctic ground squirrels Marine mammals include seals walruses and several species of cetaceanbaleen whales and also narwhals orcas and belugas An excellent and famous example of a ring species exists and has been described around the Arctic Circle in the form of the Larus gulls Natural resources There are copious natural resources in the Arctic oil gas minerals fresh water fish and if the subarctic is included forest to which modern technology and the economic opening up of Russia have given significant new opportunities The interest of the tourism industry is also on the increase The Arctic contains some of the last and most extensive continuous wilderness areas in the world and its significance in preserving biodiversity and genotypes is considerable The increasing presence of humans fragments vital habitats The Arctic is particularly susceptible to the abrasion of groundcover and to the disturbance of the rare breeding grounds of the animals that are characteristic to the region The Arctic also holds 15 of the Earths water supply Paleontology During the Cretaceous time period the Arctic still had seasonal snows though only a light dusting and not enough to permanently hinder plant growth Animals such as the Chasmosaurus Hypacrosaurus Troodon and Edmontosaurus may have all migrated north to take advantage of the summer growing season and migrated south to warmer climes when winter came A similar situation may also have been found amongst dinosaurs that lived in Antarctic regions such as the Muttaburrasaurus of Australia However others claim that dinosaurs lived yearround at very high latitudes such as near the Colville River which is now at about 70 N but at the time 70 million years ago was 10 further north Indigenous population The earliest inhabitants of North Americas central and eastern Arctic are referred to as the Arctic small tool tradition AST and existed c 2500 BCE AST consisted of several PaleoEskimo cultures including the Independence cultures and PreDorset culture The Dorset culture Inuktitut Tuniit or Tunit refers to the next inhabitants of central and eastern Arctic The Dorset culture evolved because of technological and economic changes during the period of 1050550 BCE With the exception of the Quebec Labrador peninsula the Dorset culture vanished around 1500 CE Supported by genetic testing evidence shows that descendants of the Dorset culture known as the Sadlermiut survived in Aivilik Southampton and Coats Islands until the beginning of the 20th century The Dorset Thule culture transition dates around the ninth10th centuries CE Scientists theorize that there may have been crosscontact of the two cultures with sharing of technology such as fashioning harpoon heads or the Thule may have found Dorset remnants and adapted their ways with the predecessor culture The evidence suggested that Inuit descend from the Birnirk of Siberia who through the Thule culture expanded into northern Canada and Greenland where they genetically and culturally completely replaced the Indigenous Dorset people some time after 1300 CE The question of why the Dorset disappeared so completely has led some to suggest that Thule invaders wiped out the Dorset people in an example of prehistoric genocide By 1300 CE the Inuit presentday Arctic inhabitants and descendants of Thule culture had settled in west Greenland and moved into east Greenland over the following century Inughuit Kalaallit and Tunumiit are modern Greenlandic Inuit groups descended from Thule Over time the Inuit have migrated throughout the Arctic regions of Eastern Russia the United States Canada and Greenland Other Circumpolar North indigenous peoples include the Chukchi Evenks Iupiat Khanty Koryaks Nenets Smi Yukaghir Gwichin and Yupik International cooperation and politics The eight Arctic nations Canada Kingdom of Denmark Greenland The Faroe Islands Finland Iceland Norway Sweden Russia and US are all members of the Arctic Council as are organizations representing six indigenous populations The Aleut International Association Arctic Athabaskan Council Gwichin Council International Inuit Circumpolar Council Russian Association of Indigenous Peoples of the North and Saami Council The council operates on consensus basis mostly dealing with environmental treaties and not addressing boundary or resource disputes Though Arctic policy priorities differ every Arctic nation is concerned about sovereigntydefense resource development shipping routes and environmental protection Much work remains on regulatory agreements regarding shipping tourism and resource development in Arctic waters Arctic shipping is subject to some regulatory control through the International Code for Ships Operating in Polar Waters adopted by the International Maritime Organization on 1 January 2017 and applies to all ships in Arctic waters over 500 tonnes Research in the Arctic has long been a collaborative international effort evidenced by the International Polar Year The International Arctic Science Committee hundreds of scientists and specialists of the Arctic Council and the Barents EuroArctic Council are more examples of collaborative international Arctic research Territorial claims While there are several ongoing territorial claims in the Arctic no country owns the geographic North Pole or the region of the Arctic Ocean surrounding it The surrounding six Arctic states that border the Arctic OceanCanada Kingdom of Denmark with Greenland Iceland Norway Russia and the United Statesare limited to a 200 nautical miles 370 km 230 mi exclusive economic zone EEZ off their coasts Two Arctic states Finland and Sweden do not have direct access to the Arctic Ocean Upon ratification of the United Nations Convention on the Law of the Sea a country has ten years to make claims to an extended continental shelf beyond its 200 nautical mile zone Due to this Norway which ratified the convention in 1996 Russia ratified in 1997 Canada ratified in 2003 and the Kingdom of Denmark ratified in 2004 launched projects to establish claims that certain sectors of the Arctic seabed should belong to their territories On 2 August 2007 two Russian bathyscaphes MIR1 and MIR2 for the first time in history descended to the Arctic seabed beneath the North Pole and placed there a Russian flag made of rustproof titanium alloy The flagplacing during Arktika 2007 generated commentary on and concern for a race for control of the Arctics vast hydrocarbon resources Foreign ministers and other officials representing Canada the Kingdom of Denmark Norway Russia and the United States met in Ilulissat Greenland on 28 May 2008 at the Arctic Ocean Conference and announced the Ilulissat Declaration blocking any new comprehensive international legal regime to govern the Arctic Ocean and pledging the orderly settlement of any possible overlapping claims As of 2012 the Kingdom of Denmark is claiming the continental shelf based on the Lomonosov Ridge between Greenland and over the North Pole to the northern limit of the exclusive economic zone of Russia The Russian Federation is also claiming a large swath of seabed along the Lomonosov Ridge but unlike Denmark confined its claim to its side of the Arctic region In August 2015 Russia made a supplementary submission for the expansion of the external borders of its continental shelf in the Arctic Ocean asserting that the eastern part of the Lomonosov Ridge and the Mendeleyev Ridge are an extension of the Eurasian continent In August 2016 the UN Commission on the Limits of the Continental Shelf began to consider Russias submission Canada claims the Northwest Passage as part of its internal waters belonging to Canada while the United States and most maritime nations regards it as an international strait which means that foreign vessels have right of transit passage Exploration Since 1937 the larger portion of the Asianside Arctic region has been extensively explored by Soviet and Russian crewed drifting ice stations Between 1937 and 1991 88 international polar crews established and occupied scientific settlements on the drift ice and were carried thousands of kilometres by the ice flow Pollution The Arctic is comparatively clean although there are certain ecologically difficult localized pollution problems that present a serious threat to peoples health living around these pollution sources Due to the prevailing worldwide sea and air currents the Arctic area is the fallout region for longrange transport pollutants and in some places the concentrations exceed the levels of densely populated urban areas An example of this is the phenomenon of Arctic haze which is commonly blamed on longrange pollutants Another example is with the bioaccumulation of PCBs polychlorinated biphenyls in Arctic wildlife and people Preservation There have been many proposals to preserve the Arctic over the years Most recently a group of stars at the United Nations Conference on Sustainable Development on 21 June 2012 proposed protecting the Arctic similar to the Antarctic Treaty System The initial focus of the campaign will be a UN resolution creating a global sanctuary around the pole and a ban on oil drilling and unsustainable fishing in the Arctic The Arctic has climate change rates that are amongst the highest in the world Due to the major impacts to the region from climate change the near climate future of the region will be extremely different under all scenarios of warming Climate change The effects of climate change in the Arctic include rising temperatures loss of sea ice and melting of the Greenland ice sheet Potential methane release from the region especially through the thawing of permafrost and methane clathrates is also a concern Because of the amplified response of the Arctic to global warming it is often seen as a leading indicator of global warming The melting of Greenlands ice sheet is linked to polar amplification The Arctic region is especially vulnerable to the effects of any climate change as has become apparent with the reduction of sea ice in recent years Climate models predict much greater climate change in the Arctic than the global average resulting in significant international attention to the region In particular there are concerns that Arctic shrinkage a consequence of melting glaciers and other ice in Greenland could soon contribute to a substantial rise in sea levels worldwide The current Arctic warming is leading to ancient carbon being released from thawing permafrost leading to methane and carbon dioxide production by microorganisms Release of methane and carbon dioxide stored in permafrost could cause abrupt and severe global warming as they are potent greenhouse gases Climate change is also predicted to have a large impact on tundra vegetation causing an increase of shrubs and having a negative impact on bryophytes and lichens Apart from concerns regarding the detrimental effects of warming in the Arctic some potential opportunities have gained attention The melting of the ice is making the Northwest Passage shipping routes through the northernmost latitudes more navigable raising the possibility that the Arctic region will become a prime trade route One harbinger of the opening navigability of the Arctic took place in the summer of 2016 when the Crystal Serenity successfully navigated the Northwest Passage a first for a large cruise ship In addition it is believed that the Arctic seabed may contain substantial oil fields which may become accessible if the ice covering them melts These factors have led to recent international debates as to which nations can claim sovereignty or ownership over the waters of the Arctic Arctic waters Arctic lands See also Arctic ecology Arctic Search and Rescue Agreement List of countries by northernmost point Arctic sanctuary Poverty in the Arctic Arctic Winter Games Winter City Global North Notes References Bibliography Gibbon Guy E Kenneth M Ames 1998 Archaeology of prehistoric native America an encyclopedia Vol 1537 of Garland reference library of the humanities Taylor Francis ISBN 9780815307259 Further reading Brian W Coad James D Reist 2017 Marine Fishes of Arctic Canada University of Toronto Press ISBN 9781442647107 Global Security Climate Change and the Arctic Archived 29 December 2017 at the Wayback Machine 24page special journal issue Fall 2009 Swords and Ploughshares Program in Arms Control Disarmament and International Security ACDIS University of Illinois GLOBIO Human Impact maps Report on human impacts on the Arctic Krupnik Igor Michael A Lang and Scott E Miller eds Smithsonian at the Poles Contributions to International Polar Year Science Washington DC Smithsonian Institution Scholarly Press 2009 Konyshev Valery Sergunin Alexander The Arctic at the Crossroads of Geopolitical Interests Russian Politics and Law 2012 Vol50 No2 pp 3454 Kpyl Juha Mikkola Harri The Global Arctic The Growing Arctic Interests of Russia China the United States and the European Union Archived 15 September 2013 at the Wayback Machine FIIA Briefing Paper 133 August 2013 The Finnish Institute of International Affairs Konyshev Valery Sergunin Alexander The Arctic at the crossroads of geopolitical interests Russian Politics and Law 2012 Vol 50 No 2 p 3454 Konyshev Valery Sergunin Alexander Is Russia a revisionist military power in the Arctic Defense Security Analysis September 2014 Konyshev Valery Sergunin Alexander Russia in search of its Arctic strategy between hard and soft power Polar Journal April 2014 McCannon John A History of the Arctic Nature Exploration and Exploitation Reaktion Books and University of Chicago Press 2012 ISBN 9781780230184 ORourke Ronald 14 October 2016 Changes in the Arctic Background and Issues for Congress PDF Washington DC Congressional Research Service Archived PDF from the original on 9 October 2022 Retrieved 20 October 2016 Sperry Armstrong 1957 All About the Arctic and Antarctic Random House LCCN 57007518 External links Arctic Report Card Blossoming Arctic International Arctic Research Center",
  },
  {
    title: "Population growth",
    originalContent:
      "Population growth is the increase in the number of people in a population or dispersed group Actual global human population growth amounts to around 83 million annually or 11 per year The global population has grown from 1 billion in 1800 to 81 billion in 2024 The UN projected population to keep growing and estimates have put the total population at 86 billion by mid2030 98 billion by mid2050 and 112 billion by 2100 However some academics outside the UN have increasingly developed human population models that account for additional downward pressures on population growth in such a scenario population would peak before 2100 Others have challenged many recent population projections as having underestimated population growth The world human population has been growing since the end of the Black Death around the year 1350 A mix of technological advancement that improved agricultural productivity and sanitation and medical advancement that reduced mortality increased population growth In some geographies this has slowed through the process called the demographic transition where many nations with high standards of living have seen a significant slowing of population growth This is in direct contrast with less developed contexts where population growth is still happening Globally the rate of population growth has declined from a peak of 22 per year in 1963 Population growth alongside increased consumption is a driver of environmental concerns such as biodiversity loss and climate change due to overexploitation of natural resources for human development International policy focused on mitigating the impact of human population growth is concentrated in the Sustainable Development Goals which seeks to improve the standard of living globally while reducing the impact of society on the environment while advancing human wellbeing History World population has been rising continuously since the end of the Black Death around the year 1350 Population began growing rapidly in the Western world during the industrial revolution The most significant increase in the worlds population has been since the 1950s mainly due to medical advancements and increases in agricultural productivity Haber process Due to its dramatic impact on the human ability to grow food the Haber process named after one of its inventors the German chemist Fritz Haber served as the detonator of the population explosion enabling the global population to increase from 16 billion in 1900 to 77 billion by November 2019 Thomas McKeown hypotheses Some of the reasons for the Modern Rise of Population were particularly investigated by the British health scientist Thomas McKeown 19121988 In his publications McKeown challenged four theories about the population growth McKeown stated that the growth in Western population particularly surging in the 19th century was not so much caused by an increase in fertility but largely by a decline of mortality particularly of childhood mortality followed by infant mortality The decline of mortality could largely be attributed to rising standards of living whereby McKeown put most emphasis on improved nutritional status McKeown questioned the effectiveness of public health measures including sanitary reforms vaccination and quarantine The McKeown thesis states that curative medicine measures played little role in mortality decline not only prior to the mid20th century but also until well into the 20th century Although the McKeown thesis has been heavily disputed recent studies have confirmed the value of his ideas His work is pivotal for present day thinking about population growth birth control public health and medical care McKeown had a major influence on many population researchers such as health economists and Nobel prize winners Robert W Fogel 1993 and Angus Deaton 2015 The latter considered McKeown as the founder of social medicine Growth rate models The population growth rate is the rate at which the number of individuals in a population increases in a given time period expressed as a fraction of the initial population Specifically population growth rate refers to the change in population over a unit time period often expressed as a percentage of the number of individuals in the population at the beginning of that period This can be written as the formula valid for a sufficiently small time interval P o p u l a t i o n g r o w t h r a t e P t 2 P t 1 P t 1 t 2 t 1 displaystyle Population growth ratefrac Pt_2Pt_1Pt_1t_2t_1 A positive growth rate indicates that the population is increasing while a negative growth rate indicates that the population is decreasing A growth ratio of zero indicates that there were the same number of individuals at the beginning and end of the perioda growth rate may be zero even when there are significant changes in the birth rates death rates immigration rates and age distribution between the two times A related measure is the net reproduction rate In the absence of migration a net reproduction rate of more than 1 indicates that the population of females is increasing while a net reproduction rate less than one subreplacement fertility indicates that the population of females is decreasing Most populations do not grow exponentially rather they follow a logistic model Once the population has reached its carrying capacity it will stabilize and the exponential curve will level off towards the carrying capacity which is usually when a population has depleted most its natural resources In the world human population growth may be said to have been following a linear trend throughout the last few decades Logistic equation The growth of a population can often be modelled by the logistic equation d P d t r P 1 P K displaystyle frac dPdtrPleft1frac PKright where P t displaystyle Pt the population after time t t displaystyle t time a population grows r displaystyle r the relative growth rate coefficient K displaystyle K the carrying capacity of the population defined by ecologists as the maximum population size that a particular environment can sustain As it is a separable differential equation the population may be solved explicitly producing a logistic function P t K 1 A e r t displaystyle Ptfrac K1Aert where A K P 0 P 0 displaystyle Afrac KP_0P_0 and P 0 displaystyle P_0 is the initial population at time 0 Global population growth rate The world population growth rate peaked in 1963 at 22 per year and subsequently declined In 2017 the estimated annual growth rate was 11 The CIA World Factbook gives the world annual birthrate mortality rate and growth rate as 186 078 and 108 respectively The last 100 years have seen a massive fourfold increase in the population due to medical advances lower mortality rates and an increase in agricultural productivity made possible by the Green Revolution The annual increase in the number of living humans peaked at 880 million in 1989 then slowly declined to 739 million in 2003 after which it rose again to 752 million in 2006 In 2017 the human population increased by 83 million Generally developed nations have seen a decline in their growth rates in recent decades though annual growth rates remain above 2 in some countries of the Middle East and SubSaharan Africa and also in South Asia Southeast Asia and Latin America In some countries the population is declining especially in Eastern Europe mainly due to low fertility rates high death rates and emigration In Southern Africa growth is slowing due to the high number of AIDSrelated deaths Some Western Europe countries might also experience population decline Japans population began decreasing in 2005 The United Nations Population Division projects world population to reach 112 billion by the end of the 21st century The Institute for Health Metrics and Evaluation projects that the global population will peak in 2064 at 973 billion and decline to 889 billion in 2100 A 2014 study in Science concludes that the global population will reach 11 billion by 2100 with a 70 chance of continued growth into the 22nd century The German Foundation for World Population reported in December 2019 that the global human population grows by 26 people every second and could reach 8 billion by 2023 Growth by country According to United Nations population statistics the world population grew by 30 or 16 billion humans between 1990 and 2010 In number of people the increase was highest in India 350 million and China 196 million Population growth rate was among highest in the United Arab Emirates 315 and Qatar 271 Many of the worlds countries including many in SubSaharan Africa the Middle East South Asia and South East Asia have seen a sharp rise in population since the end of the Cold War The fear is that high population numbers are putting further strain on natural resources food supplies fuel supplies employment housing etc in some of the less fortunate countries For example the population of Chad has ultimately grown from 6279921 in 1993 to 10329208 in 2009 further straining its resources Vietnam Mexico Nigeria Egypt Ethiopia and the DRC are witnessing a similar growth in population The following table gives some example countries or territories Notes Eritrea left Ethiopia in 1991 Split into the nations of Sudan and South Sudan during 2011 Japan and the Ryukyu Islands merged in 1972 India and Sikkim merged in 1975 Future population See also List of countries by population growth rate Demographic history Demographic transition Density dependence Ecological overshoot Epidemiological transition Human population planning Irruptive growth Overshoot population Population decline Population density World population Estimates of historical world population Zero population growth References External links World Population Prospects Website of the United Nations Population Division Archived from the original on 11 July 2017 Food Production and Population Growth Daniel Quinn Alan D Thornhill PhD Ecofuture Population and Sustainability Media Nonfiction Probabilistic Population Projections 2nd Revision Website of the United Nations Population Division Archived from the original on 4 October 2013 Population Growth and the Food Supply Population Institute of Canada World population growth and trends 19502050 US Census Archived from the original on 7 July 2010 Feeding the Ten BillionPlants and Population Growth PGR Newsletter FAOBioversity LT Evans 2000 Cambridge University Press ISBN 0521646855 Published in Issue No 125 page 39 to 40 5802 characters",
  },
  {
    title: "Food production",
    originalContent:
      "The food industry is a complex global network of diverse businesses that supplies most of the food consumed by the worlds population The food industry today has become highly diversified with manufacturing ranging from small traditional familyrun activities that are highly labourintensive to large capitalintensive and highly mechanized industrial processes Many food industries depend almost entirely on local agriculture animal farms produce andor fishing It is challenging to find an inclusive way to cover all aspects of food production and sale The UK Food Standards Agency describes it as the whole food industry from farming and food production packaging and distribution to retail and catering The Economic Research Service of the USDA uses the term food system to describe the same thing stating The US food system is a complex network of farmers and the industries that link to them Those links include makers of farm equipment and chemicals as well as firms that provide services to agribusinesses such as providers of transportation and financial services The system also includes the food marketing industries that link farms to consumers and which include food and fiber processors wholesalers retailers and foodservice establishments The food industry includes Agriculture raising crops livestock and seafood Agricultural economics Manufacturing agrichemicals agricultural construction farm machinery and supplies seed etc Food processing preparation of fresh products for market and manufacture of prepared food products Marketing promotion of generic products eg milk board new products advertising marketing campaigns packaging public relations etc Wholesale and food distribution logistics transportation warehousing Foodservice which includes catering Grocery farmers markets public markets and other retailing Regulation local regional national and international rules and regulations for food production and sale including food quality food security food safety marketingadvertising and industry lobbying activities Education academic consultancy vocational Research and development food science food microbiology food technology food chemistry and food engineering Financial services credit insurance Areas of research such as food grading food preservation food rheology food storage directly deal with the quality and maintenance of quality overlapping many of the above processes Only subsistence farmers those who survive on what they grow and huntergatherers can be considered outside the scope of the modern food industry The dominant companies in the food industry have sometimes been referred to as Big Food a term coined by the writer Neil Hamilton Food production Most food produced for the food industry comes from commodity crops using conventional agricultural practices Agriculture is the process of producing food feeding products fiber and other desired products by the cultivation of certain plants and the raising of domesticated animals livestock On average 83 of the food consumed by humans is produced using terrestrial agricultureIn addition to terrestrial agriculture aquaculture and fishing play vital roles in global food production Aquaculture involves the cultivation of aquatic organisms such as fish shrimp and mollusks in controlled environments like ponds tanks or cages It contributes significantly to the worlds seafood supply and provides an important source of protein for human consumption Fishing on the other hand relies on harvesting wild aquatic species from oceans rivers and lakes further diversifying the sources of food for human populations and supporting livelihoods in coastal communities worldwide Together terrestrial agriculture aquaculture and fishing collectively ensure a diverse and ample supply of food to meet the dietary needs of people across the globe Scientists inventors and others devoted to improving farming methods and implements are also said to be engaged in agriculture One in three people worldwide are employed in agriculture yet it only contributes 3 to global GDP In 2017 on average agriculture contributes 4 of national GDPs Global agricultural production is responsible for between 14 and 28 of global greenhouse gas emissions making it one of the largest contributors to global warming in large part due to conventional agricultural practices including nitrogen fertilizers and poor land management Agronomy is the science and technology of producing and using plants for food fuel fibre and land reclamation Agronomy encompasses work in the areas of plant genetics plant physiology meteorology and soil science Agronomy is the application of a combination of sciences Agronomists today are involved with many issues including producing food creating healthier food managing the environmental impact of agriculture and extracting energy from plants Food processing Food processing includes the methods and techniques used to transform raw ingredients into food for human consumption Food processing takes clean harvested or slaughtered and butchered components and uses them to produce marketable food products There are several different ways in which food can be produced Oneoff production This method is used when customers make an order for something to be made to their own specifications for example a wedding cake The making of oneoff products could take days depending on how intricate the design is Batch production This method is used when the size of the market for a product is not clear and where there is a range within a product line A certain number of the same goods will be produced to make up a batch or run for example a bakery may bake a limited number of cupcakes This method involves estimating consumer demand Mass production This method is used when there is a mass market for a large number of identical products for example chocolate bars ready meals and canned food The product passes from one stage of production to another along a production line Justintime JIT production This method of production is mainly used in restaurants All components of the product are available inhouse and the customer chooses what they want in the product It is then prepared in a kitchen or in front of the buyer as in sandwich delicatessens pizzerias and sushi bars Industry influence The food industry has a large influence on consumerism Organizations such as The American Academy of Family Physicians AAFP have been criticized for accepting monetary donations from companies within the food industry such as CocaCola These donations have been criticized for creating a conflict of interest and favoring an interest such as financial gains Criticism Media There are a number of books film TV and webrelated exposs and critiques of the food industry including Eat This Not That nonfiction series published in Mens Health magazine Fast Food Nation 2001 nonfiction book Chew On This 2005 book adaptation of Fast Food Nation for younger readers Fast Food Nation 2006 documentary film Food Inc 2008 documentary film Panic Nation 2006 nonfiction book Super Size Me 2004 documentary film Forks over Knives 2011 documentary film The Jungle 1906 novel by Upton Sinclair that exposed health violations and unsanitary practices in the American meat packing industry during the early 20th century based on his investigation for a socialist newspaper Corporate Influence The Bretton Woods Institutions The World Bank and International Monetary Fund play a large role in how the food industry functions today These global funds were born after World War II to help rebuild Europe and prevent another Great Depression Overall their main purpose was to stabilize economies The IMF provided short term loans while the World Bank was focused on larger projects that would bring electricity back to cities roads and other essential needs The World Banks mission and purpose however transformed as its President Robert McNamara issued a system of loans known as Structural Adjustment In accepting loans from the World Bank countries especially the Global South became economically politically and socially tied to the West Many countries struggled to pay back their loans beginning the process of global debt privatization and the downfall of local economies As a result of Western intervention many small scale farmers have been displaced as US corporations have bought out land in other countries and continued to monopolize on food Today several multinational corporations have pushed agricultural technologies on developing countries including improved seeds chemical fertilizers and pesticides crop production Policy In 2020 scientists reported that reducing emissions from the global food system is essential to achieving the Paris Agreements climate goals In 2020 an evidence review for the European Unions Scientific Advice Mechanism found that without significant change emissions would increase by 3040 by 2050 due to population growth and changing consumption patterns and concluded that the combined environmental cost of food production is estimated to amount to some 12 trillion per year increasing to 16 trillion by 2050 The IPCCs and the EUs reports concluded that adapting the food system to reduce greenhouse gas emissions impacts and food security concerns while shifting towards a sustainable diet is feasible Regulation Since World War II agriculture in the United States and the entire national food system in its entirety has been characterized by models that focus on monetary profitability at the expense of social and environmental integrity Regulations exist to protect consumers and somewhat balance this economic orientation with public interests for food quality food security food safety animal wellbeing environmental protection and health Proactive guidance In 2020 researchers published projections and models of potential impacts of policydependent mechanisms of modulation or lack thereof of how where and what food is produced They analyzed policyeffects for specific regions or nations such as reduction of meat production and consumption reductions in food waste and loss increases in crop yields and international landuse planning Their conclusions include that raising agricultural yields is highly beneficial for biodiversityconservation in subSaharan Africa while measures leading to shifts of diets are highly beneficial in North America and that global coordination and rapid action are necessary Wholesale and distribution A vast global cargo network connects the numerous parts of the industry These include suppliers manufacturers warehousers retailers and the end consumers Wholesale markets for fresh food products have tended to decline in importance in urbanizing countries including Latin America and some Asian countries as a result of the growth of supermarkets which procure directly from farmers or through preferred suppliers rather than going through markets The constant and uninterrupted flow of product from distribution centers to store locations is a critical link in food industry operations Distribution centers run more efficiently throughput can be increased costs can be lowered and manpower better utilized if the proper steps are taken when setting up a material handling system in a warehouse Retail With worldwide urbanization food buying is increasingly removed from food production During the 20th century the supermarket became the defining retail element of the food industry There tens of thousands of products are gathered in one location in continuous yearround supply Food preparation is another area where the change in recent decades has been dramatic Today two food industry sectors are in apparent competition for the retail food dollar The grocery industry sells fresh and largely raw products for consumers to use as ingredients in home cooking The food service industry by contrast offers prepared food either as finished products or as partially prepared components for final assembly Restaurants cafes bakeries and mobile food trucks provide opportunities for consumers to purchase food In the 21st century online grocery stores emerged and digital technologies for communitysupported agriculture have enabled farmers to directly sell produce Some online grocery stores have voluntarily set social goals or values beyond meeting consumer demand and the accumulation of profit Food industry technologies Modern food production is defined by sophisticated technologies These include many areas Agricultural machinery originally led by the tractor has practically eliminated human labor in many areas of production Biotechnology is driving much change in areas as diverse as agrochemicals plant breeding and food processing Many other types of technology are also involved to the point where it is hard to find an area that does not have a direct impact on the food industry As in other fields computer technology is also a central force Other than that there few more modern technologies that can help to improve the industry as well which are robotics and automation blockchain nanotech 3D printing artificial intelligence smart farming and others These new technologies can improve the industry in the following ways Robotics and automation Robotics and automation are being used to automate processes such as packaging sorting and quality control which reduces labor costs and increases efficiency These technologies also reduce the likelihood of contamination by reducing human contact with food Blockchain Blockchain technology is being used to improve food safety by providing transparency in the supply chain This technology allows for realtime tracking of food products from farm to table which helps to identify any potential safety hazards and enables quick response to any issues Nanotechnology Nanotechnology is being used to develop new packaging materials that can extend the shelf life of food and reduce food waste These materials can also be designed to be biodegradable reducing the environmental impact of packaging 3D printing 3D printing is being used to create custom food products and to make food production more efficient With 3D printing it is possible to create complex shapes and designs that would be difficult to achieve with traditional manufacturing techniques Artificial intelligence AI is being used to analyze large amounts of data in the food industry which can help to identify trends and patterns This technology can be used to optimize processes and to improve the quality and safety of food products Smart farming Smart farming involves the use of sensors and data analytics to optimize crop yields and reduce waste This technology can help farmers to make more informed decisions about when to plant water and harvest crops which can improve the efficiency and sustainability of agriculture Marketing As consumers grow increasingly removed from food production the role of product creation advertising and publicity become the primary vehicles for information about food With processed food as the dominant category marketers have almost infinite possibilities in product creation Of the food advertised to children on television 73 is fast or convenience foods One of the main challenges in food industry marketing is the high level of competition in the market Companies must differentiate themselves from their competitors by offering unique products or using innovative marketing techniques For example many food companies are now using social media platforms to promote their products and engage with customers Another important aspect of food industry marketing is understanding consumer behavior and preferences This includes factors such as age gender income and cultural background Companies must also be aware of changing consumer trends and adapt their marketing strategies accordingly Labor and education Until the last 100 years agriculture was laborintensive Farming was a common occupation and millions of people were involved in food production Farmers largely trained from generation to generation carried on the family business That situation has changed dramatically today In America in 1870 7080 of the US population was employed in agriculture As of 2021 less than 2 of the population is directly employed in agriculture and about 83 of the population lives in cities See also Agroindustry Agricultural expansion Dietary supplement Factory farming Food fortification also called Nutrification Geography of food Local food Ultraprocessed food References Works cited IPCC 2019 Shukla P R Skea J Calvo Buendia E MassonDelmotte V et al eds IPCC Special Report on Climate Change Desertification Land Degradation Sustainable Land Management Food Security and Greenhouse gas fluxes in Terrestrial Ecosystems PDF In press Further reading Nelson Scott Reynolds Oceans of Grain How American Wheat Remade the World 2022 excerpt Nestle M 2013 Food Politics How the Food Industry Influences Nutrition and Health California Studies in Food and Culture University of California Press ISBN 9780520955066 534 pages Vasconcellos JA 2003 Quality Assurance for the Food Industry A Practical Approach CRC Press ISBN 9780203498101 448 pages KressRogers E Brimelow CJB 2001 Instrumentation and Sensors for the Food Industry Woodhead Publishing Series in Food Science Technology and Nutrition Woodhead ISBN 9781855735606 836 pages Traill B Pitts E 1998 Competitiveness in the Food Industry Springer ISBN 9780751404319 301 pages Food Fight The Inside Story of the Food Industry External links The Food Industry Center University of Minnesota",
  },
  {
    title: "Cultural heritage",
    originalContent:
      "Cultural heritage is the heritage of tangible and intangible heritage assets of a group or society that is inherited from past generations Not all heritages of past generations are heritage rather heritage is a product of selection by society Cultural heritage includes tangible culture such as buildings monuments landscapes archive materials books works of art and artifacts intangible culture such as folklore traditions language and knowledge and natural heritage including culturally significant landscapes and biodiversity The term is often used in connection with issues relating to the protection of Indigenous intellectual property The deliberate action of keeping cultural heritage from the present for the future is known as preservation American English or conservation British English which cultural and historical ethnic museums and cultural centers promote though these terms may have more specific or technical meanings in the same contexts in the other dialect Preserved heritage has become an anchor of the global tourism industry a major contributor of economic value to local communities Legal protection of cultural property comprises a number of international agreements and national laws United Nations UNESCO and Blue Shield International deal with the protection of cultural heritage This also applies to the integration of United Nations peacekeeping Types of heritage Cultural property Cultural property includes the physical or tangible cultural heritage such as artworks These are generally split into two groups of movable and immovable heritage Immovable heritage includes buildings which themselves may include installed art such as organs stained glass windows and frescos large industrial installations residential projects or other historic places and monuments Moveable heritage includes books documents moveable artworks machines clothing and other artifacts that are considered worthy of preservation for the future These include objects significant to the archaeology architecture science or technology of a specified culture Aspects and disciplines of the preservation and conservation of tangible culture include Museology Archival science Conservation cultural heritage Art conservation Archaeological conservation Architectural conservation Film preservation Phonograph record preservation Digital preservation Intangible culture Intangible cultural heritage consists of nonphysical aspects of a particular culture more often maintained by social customs during a specific period in history The concept includes the ways and means of behavior in a society and the often formal rules for operating in a particular cultural climate These include social values and traditions customs and practices aesthetic and spiritual beliefs artistic expression language and other aspects of human activity The significance of physical artifacts can be interpreted as an act against the backdrop of socioeconomic political ethnic religious and philosophical values of a particular group of people Naturally intangible cultural heritage is more difficult to preserve than physical objects Aspects of the preservation and conservation of cultural intangibles include folklore oral history language preservation Natural heritage Natural heritage is also an important part of a societys heritage encompassing the countryside and natural environment including flora and fauna scientifically known as biodiversity as well as geological elements including mineralogical geomorphological paleontological etc scientifically known as geodiversity These kinds of heritage sites often serve as an important component in a countrys tourist industry attracting many visitors from abroad as well as locally Heritage can also include cultural landscapes natural features that may have cultural attributes Aspects of the preservation and conservation of natural heritage include Rare breeds conservation Heirloom plants Digital heritage Digital heritage is made up of computerbased materials such as texts databases images sounds and software being retained for future generations Digital heritage includes physical objects such as documents which have been digitized for retention and artifacts which are born digital ie originally created digitally and having no physical form Protection of cultural heritage History There have been examples of respect for the cultural assets of enemies since ancient times The roots of todays legal situation for the precise protection of cultural heritage also lie in some of the regulations of Austrias ruler Maria Theresa 1717 1780 and the demands of the Congress of Vienna 181415 not to remove works of art from their place of origin in the war The 1863 Lieber code a military legal code governing the wartime conduct of the Union Army also set rules for the protection of cultural heritage The process continued at the end of the 19th century when in 1874 in Brussels at least a draft international agreement on the laws and customs of war was agreed 25 years later in 1899 an international peace conference was held in the Netherlands on the initiative of Tsar Nicholas II of Russia with the aim of revising the declaration which was never ratified and adopting a convention The Hague Conventions of 1899 and 1907 also significantly advanced international law and laid down the principle of the immunity of cultural property Three decades later in 1935 the preamble to the Treaty on the Protection of Artistic and Scientific Institutions Roerich Pact was formulated On the initiative of UNESCO the Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict was signed in 1954 Protection of cultural heritage or protection of cultural goods refers to all measures aimed at protecting cultural property against damage destruction theft embezzlement or other loss The term monument protection is also used for immovable cultural property Protection of cultural heritage relates in particular to the prevention of robbery digs at archaeological sites the looting or destruction of cultural sites and the theft of works of art from churches and museums all over the world and basically measures regarding the conservation and general access to our common cultural heritage Legal protection of cultural heritage comprises a number of international agreements and national laws There is a close partnership between the UN United Nations peacekeeping UNESCO the International Committee of the Red Cross and Blue Shield International The protection of cultural heritage should also preserve the particularly sensitive cultural memory the growing cultural diversity and the economic basis of a state a municipality or a region Whereby there is also a connection between cultural user disruption or cultural heritage and the cause of flight But only through fundamental cooperation including the military units and the planning staff with the locals can the protection of world heritage sites archaeological finds exhibits and archaeological sites from destruction looting and robbery be implemented sustainably The founding president of Blue Shield International Karl von Habsburg summed it up with the words Without the local community and without the local participants that would be completely impossible The ethics and rationale of cultural preservation Objects are a part of the study of human history because they provide a concrete basis for ideas and can validate them Their preservation demonstrates a recognition of the necessity of the past and of the things that tell its story In The Past is a Foreign Country David Lowenthal observes that preserved objects also validate memories While digital acquisition techniques can provide a technological solution that is able to acquire the shape and the appearance of artifacts with unprecedented precision in human history the actuality of the object as opposed to a reproduction draws people in and gives them a literal way of touching the past This poses a danger as places and things are damaged by the hands of tourists the light required to display them and other risks of making an object known and available The reality of this risk reinforces the fact that all artifacts are in a constant state of chemical transformation so that what is considered to be preserved is actually changing it is never as it once was Similarly changing is the value each generation may place on the past and on the artifacts that link it to the past The equality or inseparability of cultural preservation and the protection of human life has been argued by several agencies and writers for example former French president Franois Hollande stated in 2016Our responsibility is to save lives and also to save the stones there is no choice to be made because today both are destroyed Classical civilizations especially Indian have attributed supreme importance to the preservation of tradition Its central idea was that social institutions scientific knowledge and technological applications need to use a heritage as a resource Using contemporary language we could say that ancient Indians considered as social resources both economic assets like natural resources and their exploitation structure and factors promoting social integration like institutions for the preservation of knowledge and for the maintenance of civil order Ethics considered that what had been inherited should not be consumed but should be handed over possibly enriched to successive generations This was a moral imperative for all except in the final life stage of sannyasa What one generation considers cultural heritage may be rejected by the next generation only to be revived by a subsequent generation World heritage movement Significant was the Convention Concerning the Protection of World Cultural and Natural Heritage that was adopted by the General Conference of UNESCO in 1972 As of 2011 there are 936 World Heritage Sites 725 cultural 183 natural and 28 mixed properties in 153 countries Each of these sites is considered important to the international community The underwater cultural heritage is protected by the UNESCO Convention on the Protection of the Underwater Cultural Heritage This convention is a legal instrument helping state parties to improve the protection of their underwater cultural heritage In addition UNESCO has begun designating masterpieces of the Oral and Intangible Heritage of Humanity The Committee on Economic Social and Cultural Rights sitting as part of the United Nations Economic and Social Council with article 15 of its Covenant had sought to instill the principles under which cultural heritage is protected as part of a basic human right Key international documents and bodies include Athens Charter 1931 Roerich Pact 1935 Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict 1954 with a definition of cultural heritage item adopted by some national law Venice Charter 1964 Barcelona Charter 2002 regarding maritime vessel preservation ICOMOS The Blue Shield a network of committees of dedicated individuals across the world that is committed to the protection of the worlds cultural property and is concerned with the protection of cultural and natural heritage tangible and intangible in the event of armed conflict natural or humanmade disaster International Institute for Conservation The US Government Accountability Office issued a report describing some of the United States cultural property protection efforts National and regional heritage movements Much of heritage preservation work is done at the national regional or local levels of society Various national and regional regimes include Australia Burra Charter Heritage Overlay in Victoria Australia Bosnia KONS Brazil National Institute of Historic and Artistic Heritage Canada Heritage conservation in Canada Chile National Monuments Council Chile China State Administration of Cultural Heritage Egypt Supreme Council of Antiquities Estonia Ministry of Culture Estonia National Heritage Board Estonia Ghana Ghanas material cultural heritage Honduras Secretary of State for Culture Arts and Sports Hong Kong Heritage conservation in Hong Kong India Ministry of Culture India National Archives of India Archaeological Survey of India Anthropological Survey of India Culture of India Indian National Trust for Art and Cultural Heritage National Museum Institute of the History of Art Conservation and Museology List of World Heritage Sites in India Indian Heritage Cities Network Mysore Heritage structures in Hyderabad Iran Cultural Heritage Handcrafts and Tourism Organization Japan Cultural Properties of Japan Kenya National Museums of Kenya International Inventories Programme North Macedonia Institute for Protection of Cultural Monuments Malaysia The National Heritage Act Namibia National Heritage Council of Namibia National Monuments Council New Zealand New Zealand Historic Places Trust Pakistan Lahore Museum of Art and Cultural History Lok Virsa Heritage Museum National Museum of Pakistan Pakistan Monument and Heritage Museum Philippines National Commission for Culture and the Arts National Historical Commission of the Philippines Poland National Ossoliski Institute Serbia Immovable Cultural Heritage of Exceptional Importance Immovable Cultural Heritage of Great Importance South Africa South African Heritage Resources Agency Provincial heritage resources authorities Amafa aKwaZuluNatali Heritage Western Cape Northern Cape Heritage Resources Authority National Monuments Council Historical Monuments Commission United Kingdom Conservation in the United Kingdom English Heritage English Heritage Archive National Trust Cadw Northern Ireland Environment Agency Historic Environment Scotland National Trust for Scotland United States of America National Park Service National Register of Historic Places National Historic Site United States List of national memorials of the United States National Military Park Zambia National Heritage Conservation Commission National Museums Board Zimbabwe National Monuments of Zimbabwe Issues in cultural heritage Broad philosophical technical and political issues and dimensions of cultural heritage include Cultural heritage repatriation Cultural heritage management Cultural property law Heritage tourism Virtual heritage Sustainable preservation Climate change and World Heritage Management of cultural heritage Issues in cultural heritage management include Exhibition of cultural heritage objects Radiography of cultural objects Storage of cultural heritage objects Collections maintenance Disaster preparedness Cultural heritage digital preservation Ancient archaeological artefacts and archaeological sites are naturally prone to damage due to their age and environmental conditions Also there have been tragic occurrences of unexpected humanmade disasters such as in the cases of a fire that took place in the 200 years old National Museum of Brazil and the UNESCO World Heritage Site of the Notre Dame Cathedral in Paris Therefore there is a growing need to digitize cultural heritage in order to preserve them in the face of potential calamities such as climate change natural disaster poor policy or inadequate infrastructure For example the Library of Congress has started to digitize its collections in a special program called the National Digital Library Program The Smithsonian has also been actively digitizing its collection with the release of the Smithsonian X 3D Explorer allowing anyone to engage with the digitized versions of the museums millions of artifacts of which only two percent are on display 3D scanning devices have become a practical reality in the field of heritage preservation 3D scanners can produce a highprecision digital reference model that not only digitizes condition but also provides a 3D virtual model for replication The high cost and relative complexity of 3D scanning technologies have made it quite impractical for many heritage institutions in the past but this is changing as technology advances and its relative costs are decreasing to reach a level where even mobile based scanning applications can be used to create a virtual museum There is still a low level of digital archiving of archaeological data obtained via excavation even in the UK where the lead digital archive for archaeology the Archaeology Data Service was established in the 1990s Across the globe countries are at different stages of dealing with digital archaeological archives all dealing with differences in statutory requirements legal ownership of archives and infrastructure See also Antiquarian Architectural Heritage Collecting Heritage film International Council on Monuments and Sites Values heritage Digital methods in preservation DigiCULT ERPANET Intellectual property issues in cultural heritage IPinCH MICHAEL webportal References Further reading Michael Falser Cultural Heritage as Civilizing Mission From Decay to Recovery Heidelberg New York Springer 2015 ISBN 9783319136387 Michael Falser Monica Juneja eds Archaeologizing Heritage Transcultural Entanglements between Local Social Practices and Global Virtual Realities Heidelberg New York Springer 2013 ISBN 9783642358708 FiankanBokonga Catherine 17 October 2017 A historic resolution to protect cultural heritage UNESCO Retrieved 3 August 2021 Ann Marie Sullivan Cultural Heritage New Media A Future for the Past 15 J MARSHALL REV INTELL PROP L 604 2016 httpsrepositoryjmlseducgiviewcontentcgiarticle1392contextripl Barbara T Hoffman Art and cultural heritage law policy and practice Cambridge University Press 2006 Leila A Amineddoleh Protecting Cultural Heritage by Strictly Scrutinizing Museum Acquisitions Fordham Intellectual Property Media Entertainment Law Journal Vol 24 No 3 Available at httpsssrncomabstract2467100 Paolo Davide Farah Riccardo Tremolada Desirability of Commodification of Intangible Cultural Heritage The Unsatisfying Role of IPRs in TRANSNATIONAL DISPUTE MANAGEMENT Special Issues The New Frontiers of Cultural Law Intangible Heritage Disputes Volume 11 Issue 2 March 2014 ISSN 18754120 Available at httpsssrncomabstract2472339 Paolo Davide Farah Riccardo Tremolada Intellectual Property Rights Human Rights and Intangible Cultural Heritage Journal of Intellectual Property Law Issue 2 Part I June 2014 ISSN 0035614X Giuffr pp 2147 Available at httpsssrncomabstract2472388 Nora Lafi Building and Destroying Authenticity in Aleppo Heritage between Conservation Transformation Destruction and ReInvention in Christoph Bernhardt Martin Sabrow Achim Saupe Gebaute Geschichte Historische Authentizitt im Stadtraum Wallstein pp206228 2017 Dallen J Timothy and Gyan P Nyaupane Cultural heritage and tourism in the developing world a regional perspective Taylor Francis 2009 Peter Probst Osogbo and the Art of Heritage Monuments Deities and Money Indiana University Press 2011 Constantine Sandis ed Cultural Heritage Ethics Between Theory and Practice Open Book Publishers 2014 Zuckermann Ghilad et al ENGAGING A Guide to Interacting Respectfully and Reciprocally with Aboriginal and Torres Strait Islander People and their Arts Practices and Intellectual Property Australian Government Indigenous Culture Support 2015 Walters Diana Laven Daniel Davis Peter 2017 Heritage Peacebuilding Suffolk UK Boydell Press ISBN 9781783272167 Archived from the original on 31 March 2017 Retrieved 31 March 2017 Kocj E Midzy mainstremem a undergroundem Dziedzictwo regionalne w kulturze europejskiej odkrywanie znacze w Dziedzictwo kulturowe w regionach europejskich Odkrywanie ochrona i reinterpretacja Seria wydawnicza Studia nad dziedzictwem i pamici kulturow tom I Krakw 2019 red Ewa Kocj Tomasz Kosiek Joanna Szulborskaukaszewicz pp 1035 Dziedzictwo kulturowe w regionach europejskich Odkrywanie ochrona i reinterpretacja Seria wydawnicza Studia nad dziedzictwem i pamici kulturow tom I red Ewa Kocj Tomasz Kosiek Joanna Szulborskaukaszewicz Krakw 2019 p 300 HudsonWard A Widholm J R Scott W Eds 2023 Cultural Heritage and the Campus Community Academic Libraries and Museums in Collaboration ACRL External links Cultural heritage policy history and resources Getty Museum list of major international cultural heritage documents charters and treaties UNESCO World Heritage Centre Official website of the United Nations organization for cultural heritage International Council on Monuments and Sites International Council of Museums International Centre for the Study of the Preservation and Restoration of Cultural Property Cultural routes and landscapes a common heritage of Europe English and French language EPOCH European Research Network on Excellence in Processing Open Cultural Heritage Peace Palace Library Research Guide Archived 12 September 2016 at the Wayback Machine National Council for Preservation Education Ddalo Open source management system for Cultural heritage Cultural heritage travel guide from Wikivoyage Central European University CEU UNESCO UIS_cultural heritage Heritage for Peace",
  },
  {
    title: "World Heritage Sites",
    originalContent:
      "World Heritage Sites are landmarks and areas with legal protection under an international treaty administered by UNESCO for having cultural historical or scientific significance The sites are judged to contain cultural and natural heritage around the world considered to be of outstanding value to humanity To be selected a World Heritage Site is nominated by its host country and determined by the UNESCOs World Heritage Committee to be a unique landmark which is geographically and historically identifiable having a special cultural or physical significance and to be under a sufficient system of legal protection For example World Heritage Sites might be ancient ruins or historical structures buildings cities deserts forests islands lakes monuments mountains or wilderness areas A World Heritage Site may signify a remarkable accomplishment of humankind and serve as evidence of our intellectual history on the planet or it might be a place of great natural beauty As of July 2024 a total of 1223 World Heritage Sites 952 cultural 231 natural and 40 mixed cultural and natural properties exist across 168 countries With 60 selected areas Italy is the country with the most sites followed by China with 59 and Germany with 54 The sites are intended for practical conservation for posterity which otherwise would be subject to risk from human or animal trespassing unmonitored uncontrolled or unrestricted access or threat from local administrative negligence Sites are demarcated by UNESCO as protected zones The World Heritage Sites list is maintained by the international World Heritage Program administered by the UNESCO World Heritage Committee composed of 21 states parties that are elected by the United Nations General Assembly and advised by reviews of international panels of experts in natural or cultural history and education The Program catalogues names and conserves sites of outstanding cultural or natural importance to the common culture and heritage of humankind The programme began with the Convention Concerning the Protection of the World Cultural and Natural Heritage which was adopted by the General Conference of UNESCO on 16 November 1972 Since then 196 states have ratified the convention making it one of the most widely recognised international agreements and the worlds most popular cultural programme History Origin In 1954 the government of Egypt decided to build the new Aswan High Dam whose resulting future reservoir would eventually inundate a large stretch of the Nile valley containing cultural treasures of ancient Egypt and ancient Nubia In 1959 the governments of Egypt and Sudan requested the United Nations Educational Scientific and Cultural Organization UNESCO to assist them to protect and rescue the endangered monuments and sites In 1960 the DirectorGeneral of UNESCO launched the International Campaign to Save the Monuments of Nubia This resulted in the excavation and recording of hundreds of sites the recovery of thousands of objects as well as the salvage and relocation to higher ground of several important temples The most famous of these are the temple complexes of Abu Simbel and Philae The campaign ended in 1980 and was considered a success To thank countries which especially contributed to the campaigns success Egypt donated four temples the Temple of Dendur was moved to the Metropolitan Museum of Art in New York City the Temple of Debod to the Parque del Oeste in Madrid the Temple of Taffeh to the Rijksmuseum van Oudheden in Leiden and the Temple of Ellesyia to Museo Egizio in TurinThe project cost US80 million equivalent to 29583 million in 2023 about 40 million of which was collected from 50 countries The projects success led to other safeguarding campaigns such as saving Venice and its lagoon in Italy the ruins of Mohenjodaro in Pakistan and the Borobodur Temple Compounds in Indonesia Together with the International Council on Monuments and Sites UNESCO then initiated a draft convention to protect cultural heritage Convention and background The convention the signed document of international agreement guiding the work of the World Heritage Committee was developed over a sevenyear period 19651972 The United States initiated the idea of safeguarding places of high cultural or natural importance A White House conference in 1965 called for a World Heritage Trust to preserve the worlds superb natural and scenic areas and historic sites for the present and the future of the entire world citizenry The International Union for Conservation of Nature developed similar proposals in 1968 which were presented in 1972 at the United Nations Conference on the Human Environment in Stockholm Under the World Heritage Committee signatory countries are required to produce and submit periodic data reporting providing the committee with an overview of each participating nations implementation of the World Heritage Convention and a snapshot of current conditions at World Heritage properties Based on the draft convention that UNESCO had initiated a single text was eventually agreed upon by all parties and the Convention Concerning the Protection of the World Cultural and Natural Heritage was adopted by the General Conference of UNESCO on 16 November 1972 The convention came into force on 17 December 1975 As of November 2024 it has been ratified by 196 states 192 UN member states two UN observer states the Holy See and the State of Palestine and two states in free association with New Zealand the Cook Islands and Niue Only one UN member state Liechtenstein has not ratified the convention Objectives By assigning places as World Heritage Sites UNESCO wants to help preserve them for future generations Its motivation is that heritage is our legacy from the past what we live with today and that both cultural and natural heritage are irreplaceable sources of life and inspiration UNESCOs mission with respect to World Heritage consists of eight sub targets These include encouraging the commitment of countries and local population to World Heritage conservation in various ways providing emergency assistance for sites in danger offering technical assistance and professional training and supporting States Parties public awarenessbuilding activities Being listed as a World Heritage Site can positively affect the site its environment and interactions between them A listed site gains international recognition and legal protection and can obtain funds from among others the World Heritage Fund to facilitate its conservation under certain conditions UNESCO reckons the restorations of the following four sites among its success stories Angkor in Cambodia the Old City of Dubrovnik in Croatia the Wieliczka Salt Mine near Krakw in Poland and the Ngorongoro Conservation Area in Tanzania Additionally the local population around a site may benefit from significantly increased tourism revenue When there are significant interactions between people and the natural environment these can be recognised as cultural landscapes Nomination process A country must first identify its significant cultural and natural sites in a document known as the Tentative List Next it can place sites selected from that list into a Nomination File which is evaluated by the International Council on Monuments and Sites and the World Conservation Union A country may not nominate sites that have not been first included on its Tentative List The two international bodies make recommendations to the World Heritage Committee for new designations The Committee meets once a year to determine which nominated properties to add to the World Heritage List sometimes it defers its decision or requests more information from the country that nominated the site There are ten selection criteria a site must meet at least one to be included on the list Selection criteria Until 2004 there were six sets of criteria for cultural heritage and four for natural heritage In 2005 UNESCO modified these and now has one set of ten criteria Nominated sites must be of outstanding universal value and must meet at least one of the ten criteria Cultural To represent a masterpiece of human creative genius To exhibit an important interchange of human values over a span of time or within a cultural area of the world on developments in architecture or technology monumental arts townplanning or landscape design To bear a unique or at least exceptional testimony to a cultural tradition or to a civilization which is living or which has disappeared To be an outstanding example of a type of building architectural or technological ensemble or landscape which illustrates a significant stages in human history To be an outstanding example of a traditional human settlement landuse or seause which is representative of a culture or cultures or human interaction with the environment especially when it has become vulnerable under the impact of irreversible change To be directly or tangibly associated with events or living traditions with ideas or with beliefs with artistic and literary works of outstanding universal significance Natural To contain superlative natural phenomena or areas of exceptional natural beauty and aesthetic importance To be outstanding examples representing major stages of earths history including the record of life significant ongoing geological processes in the development of landforms or significant geomorphic or physiographic features To be outstanding examples representing significant ongoing ecological and biological processes in the evolution and development of terrestrial fresh water coastal and marine ecosystems and communities of plants and animals To contain the most important and significant natural habitats for insitu conservation of biological diversity including those containing threatened species of outstanding universal value from the point of view of science or conservation Extensions and other modifications A country may request to extend or reduce the boundaries modify the official name or change the selection criteria of one of its already listed sites Any proposal for a significant boundary change or to modify the sites selection criteria must be submitted as if it were a new nomination including first placing it on the Tentative List and then onto the Nomination File A request for a minor boundary change one that does not have a significant impact on the extent of the property or affect its outstanding universal value is also evaluated by the advisory bodies before being sent to the committee Such proposals can be rejected by either the advisory bodies or the Committee if they judge it to be a significant change instead of a minor one Proposals to change a sites official name are sent directly to the committee Endangerment A site may be added to the List of World Heritage in Danger if conditions threaten the characteristics for which the landmark or area was inscribed on the World Heritage List Such problems may involve armed conflict and war natural disasters pollution poaching or uncontrolled urbanisation or human development This danger list is intended to increase international awareness of the threats and to encourage counteractive measures Threats to a site can be either proven imminent threats or potential dangers that could have adverse effects on a site The state of conservation for each site on the danger list is reviewed yearly after this the Committee may request additional measures delete the property from the list if the threats have ceased or consider deletion from both the List of World Heritage in Danger and the World Heritage List Only three sites have ever been delisted the Arabian Oryx Sanctuary in Oman the Dresden Elbe Valley in Germany and the Liverpool Maritime Mercantile City in the United Kingdom The Arabian Oryx Sanctuary was directly delisted in 2007 instead of first being put on the danger list after the Omani government decided to reduce the protected areas size by 90 The Dresden Elbe Valley was first placed on the danger list in 2006 when the World Heritage Committee decided that plans to construct the Waldschlsschen Bridge would significantly alter the valleys landscape In response the Dresden City Council attempted to stop the bridges construction However after several court decisions allowed the building of the bridge to proceed the valley was removed from the World Heritage List in 2009 Liverpools World Heritage status was revoked in July 2021 following developments Liverpool Waters and BramleyMoore Dock Stadium on the northern docks of the World Heritage site leading to the irreversible loss of attributes on the site The first global assessment to quantitatively measure threats to Natural World Heritage Sites found that 63 of sites have been damaged by increasing human pressures including encroaching roads agriculture infrastructure and settlements over the last two decades These activities endanger Natural World Heritage Sites and could compromise their unique values Of the Natural World Heritage Sites that contain forest 91 experienced some loss since 2000 Many of them are more threatened than previously thought and require immediate conservation action The destruction of cultural assets and identityestablishing sites is one of the primary goals of modern asymmetrical warfare Terrorists rebels and mercenary armies deliberately smash archaeological sites sacred and secular monuments and loot libraries archives and museums The UN United Nations peacekeeping and UNESCO in cooperation with Blue Shield International are active in preventing such acts No strike lists are also created to protect cultural assets from air strikes The founding president of Blue Shield International Karl von Habsburg summed it up with the words Without the local community and without the local participants that would be completely impossible Criticism The UNESCOadministered project has attracted criticism This was caused by perceived underrepresentation of heritage sites outside Europe disputed decisions on site selection and adverse impact of mass tourism on sites unable to manage rapid growth in visitor numbers A large lobbying industry has grown around the awards because World Heritage listing can significantly increase tourism returns Site listing bids are often lengthy and costly putting poorer countries at a disadvantage Eritreas efforts to promote Asmara are one example In 2016 the Australian government was reported to have successfully lobbied for the World Heritage Site Great Barrier Reef conservation efforts to be removed from a UNESCO report titled World Heritage and Tourism in a Changing Climate The Australian governments actions involving considerable expense for lobbying and visits for diplomats were in response to their concern about the negative impact that an at risk label could have on tourism revenue at a previously designated UNESCO World Heritage Site In 2021 international scientists recommended UNESCO to put the Great Barrier Reef on the endangered list as global climate change had caused a further negative state of the corals and water quality Again the Australian government campaigned against this and in July 2021 the World Heritage Committee made up of diplomatic representatives of 21 countries ignored UNESCOs assessment based on studies of scientists that the reef was clearly in danger from climate change and so should be placed on the list According to environmental protection groups this decision was a victory for cynical lobbying and Australia as custodians of the worlds biggest coral reef was now on probation Several listed locations such as Casco Viejo in Panama and Hi An in Vietnam have struggled to strike a balance between the economic benefits of catering to greatly increased visitor numbers after the recognition and preserving the original culture and local communities Another criticism is that there is a homogeneity to these sites which contain similar styles visitor centres etc meaning that a lot of the individuality of these sites has been removed to become more attractive to tourists Anthropologist Jasper Chalcraft said that World Heritage recognition often ignores contemporary local usage of certain sites This leads to conflicts on the local level which can result in the site being damaged Rock art under world heritage protection at the Tadrart Acacus in Libya have occasionally been intentionally destroyed Chalcraft links this destruction to Libyan national authorities prioritizing World Heritage status over local sensibilities by limiting access to the sites without consulting with the local population UNESCO has also been criticized for alleged geographic bias racism and colourism in world heritage inscription A major chunk of all world heritage inscriptions are located in regions whose populations generally have lighter skin including Europe East Asia and North America Statistics The World Heritage Committee has divided the world into five geographic regions Africa Arab states Asia and the Pacific Europe and North America and Latin America and the Caribbean Russia and the Caucasus states are classified as European while Mexico and the Caribbean are classified as belonging to the Latin America and the Caribbean region The UNESCO geographic regions also give greater emphasis on administrative rather than geographic associations Hence Gough Island located in the South Atlantic is part of the Europe and North America region because the British government nominated the site The table below includes a breakdown of the sites according to these regions and their classification as of July 2024 Countries with 15 or more sites This overview lists the 23 countries with 15 or more World Heritage Sites See also GoUNESCO initiative to promote awareness and provide tools for laypersons to engage with heritage Index of conservation articles Lists of World Heritage Sites Former UNESCO World Heritage Sites Memory of the World Programme UNESCO Intangible Cultural Heritage Lists Ramsar Convention international agreement on wetlands recognition Notes References Footnotes Bibliography External links UNESCO World Heritage portal Official website in English and French The World Heritage List Official searchable list of all Inscribed Properties KML file of the World Heritage List Official KML version of the list for Google Earth and NASA Worldwind UNESCO Information System on the State of Conservation of World Heritage properties Searchable online tool with over 3400 reports on World Heritage Sites Official overview of the World Heritage Forest Program Convention Concerning the Protection of the World Cultural and Natural Heritage Official 1972 Convention Text in seven languages The 1972 Convention at LawReforg Fully indexed and crosslinked with other documents Protected Planet View all Natural World Heritage Sites in the World Database on Protected Areas World Heritage Site Smithsonian Ocean Portal UNESCO chair in ICT to develop and promote sustainable tourism in World Heritage Sites UNESCO World Heritage Sites showcased in Google Arts Culture",
  },
  {
    title: "Roman mythology",
    originalContent:
      "Roman mythology is the body of myths of ancient Rome as represented in the literature and visual arts of the Romans and is a form of Roman folklore Roman mythology may also refer to the modern study of these representations and to the subject matter as represented in the literature and art of other cultures in any period Roman mythology draws from the mythology of the Italic peoples and shares mythemes with ProtoIndoEuropean mythology The Romans usually treated their traditional narratives as historical even when these have miraculous or supernatural elements The stories are often concerned with politics and morality and how an individuals personal integrity relates to his or her responsibility to the community or Roman state Heroism is an important theme When the stories illuminate Roman religious practices they are more concerned with ritual augury and institutions than with theology or cosmogony Roman mythology also draws on Greek mythology primarily during the Hellenistic period of Greek influence and through the Roman conquest of Greece via the artistic imitation of Greek literary models by Roman authors The Romans identified their own gods with those of the ancient Greeks and reinterpreted myths about Greek deities under the names of their Roman counterparts The influence of Greek mythology likely began as early as Romes protohistory Classical mythology is the amalgamated tradition of Greek and Roman mythologies as disseminated especially by Latin literature in Europe throughout the Middle Ages into the Renaissance and up to presentday uses of myths in fiction and movies The interpretations of Greek myths by the Romans often had a greater influence on narrative and pictorial representations of myths than Greek sources In particular the versions of Greek myths in Ovids Metamorphoses written during the reign of Augustus came to be regarded as canonical Nature of Roman myth Because ritual played the central role in Roman religion that myth did for the Greeks it is sometimes doubted that the Romans had much of a native mythology This perception is a product of Romanticism and the classical scholarship of the 19th century which valued Greek civilization as more authentically creative From the Renaissance to the 18th century however Roman myths were an inspiration particularly for European painting The Roman tradition is rich in historical myths or legends concerning the foundation and rise of the city These narratives focus on human actors with only occasional intervention from deities but a pervasive sense of divinely ordered destiny In Romes earliest period history and myth have a mutual and complementary relationship As T P Wiseman notes The Roman stories still matter as they mattered to Dante in 1300 and Shakespeare in 1600 and the founding fathers of the United States in 1776 What does it take to be a free citizen Can a superpower still be a republic How does wellmeaning authority turn into murderous tyranny Major sources for Roman myth include the Aeneid of Virgil and the first few books of Livys history as well as Dionysiuss Roman Antiquities Other important sources are the Fasti of Ovid a sixbook poem structured by the Roman religious calendar and the fourth book of elegies by Propertius Scenes from Roman myth also appear in Roman wall painting coins and sculpture particularly reliefs Founding myths The Aeneid and Livys early history are the best extant sources for Romes founding myths Material from Greek heroic legend was grafted onto this native stock at an early date The Trojan prince Aeneas was cast as husband of Lavinia daughter of King Latinus patronymical ancestor of the Latini and therefore through a convoluted revisionist genealogy as forebear of Romulus and Remus By extension the Trojans were adopted as the mythical ancestors of the Roman people Other myths The characteristic myths of Rome are often political or moral that is they deal with the development of Roman government in accordance with divine law as expressed by Roman religion and with demonstrations of the individuals adherence to moral expectations mos maiorum or failures to do so Rape of the Sabine women explaining the importance of the Sabines in the formation of Roman culture and the growth of Rome through conflict and alliance Numa Pompilius the Sabine second king of Rome who consorted with the nymph Egeria and established many of Romes legal and religious institutions Servius Tullius the sixth king of Rome whose mysterious origins were freely mythologized and who was said to have been the lover of the goddess Fortuna The Tarpeian Rock and why it was used for the execution of traitors Lucretia whose selfsacrifice prompted the overthrow of the early Roman monarchy and led to the establishment of the Republic Cloelia a Roman woman taken hostage by Lars Porsena who escaped but after negotiations returned voluntarily to save others and preserve the peace treaty Horatius at the bridge on the importance of individual valor Mucius Scaevola who thrust his right hand into the fire to prove his loyalty to Rome Caeculus and the founding of Praeneste Manlius and the geese about divine intervention at the Gallic siege of Rome Stories pertaining to the Nonae Caprotinae and Poplifugia festivals Coriolanus a story of politics and morality The Etruscan city of Corythus as the cradle of Trojan and Italian civilization The arrival of the Great Mother Cybele in Rome Religion and myth Narratives of divine activity played a more important role in the system of Greek religious belief than among the Romans for whom ritual and cultus were primary Although Roman religion was not based on scriptures and their exegesis priestly literature was one of the earliest written forms of Latin prose The books libri and commentaries commentarii of the College of Pontiffs and of the augurs contained religious procedures prayers and rulings and opinions on points of religious law Although at least some of this archived material was available for consultation by the Roman senate it was often occultum genus litterarum an arcane form of literature to which by definition only priests had access Prophecies pertaining to world history and to Romes destiny turn up fortuitously at critical junctures in history discovered suddenly in the nebulous Sibylline books which Tarquin the Proud according to legend purchased in the late 6th century BC from the Cumaean Sibyl Some aspects of archaic Roman religion survived in the lost theological works of the 1stcentury BC scholar Varro known through other classical and Christian authors Although traditional Roman religion was conservative in ritual rather than dogmatic in doctrine the meaning of the rituals they perpetuated could be adapted expanded and reinterpreted by accretions of myths etiologies commentary and the influences of other cultures in response to social change The earliest pantheon included Janus Vesta and the socalled Archaic Triad of Jupiter Mars and Quirinus whose three patrician flamens were of the highest order According to tradition Numa Pompilius the Sabine second king of Rome founded Roman religion Numa was believed to have had as his consort and adviser a Roman goddess or nymph of fountains and of prophecy Egeria The Etruscaninfluenced Capitoline Triad of Jupiter Juno and Minerva later became central to official religion replacing the Archaic Triad an unusual example within IndoEuropean religion of a supreme triad formed of two female deities and only one male The cult of Diana became established on the Aventine Hill but the most famous Roman manifestation of this goddess may be Diana Nemorensis owing to the attention paid to her cult by JG Frazer in the mythographic classic The Golden Bough What modern scholars call the Aventine Triad Ceres Liber and Libera developed in association with the rise of plebeians to positions of wealth and influence The gods represented distinctly the practical needs of daily life and the Romans scrupulously accorded them the appropriate rites and offerings Early Roman divinities included a host of specialist gods whose names were invoked in the carrying out of various specific activities Fragments of old ritual accompanying such acts as plowing or sowing reveal that at every stage of the operation a separate deity was invoked the name of each deity being regularly derived from the verb for the operation Tutelary deities were particularly important in ancient Rome Thus Janus and Vesta guarded the door and hearth the Lares protected the field and house Pales the pasture Saturn the sowing Ceres the growth of the grain Pomona the fruit and Consus and Ops the harvest Jupiter the ruler of the gods was honored for the aid his rains might give to the farms and vineyards In his more encompassing character he was considered through his weapon of lightning the director of human activity Owing to his widespread domain the Romans regarded him as their protector in their military activities beyond the borders of their own community Prominent in early times were the gods Mars and Quirinus who were often identified with each other Mars was a god of both war and agriculture he was honored in March and October Quirinus was the patron of the armed community in time of peace The 19thcentury scholar Georg Wissowa thought that the Romans distinguished two classes of gods the di indigetes and the di novensides or novensiles the indigetes were the original gods of the Roman state their names and nature indicated by the titles of the earliest priests and by the fixed festivals of the calendar with 30 such gods honored by special festivals the novensides were later divinities whose cults were introduced to the city in the historical period usually at a known date and in response to a specific crisis or felt need Arnaldo Momigliano and others however have argued that this distinction cannot be maintained During the war with Hannibal any distinction between indigenous and immigrant gods begins to fade and the Romans embraced diverse gods from various cultures as a sign of strength and universal divine favor Foreign gods The absorption of neighboring local gods took place as the Roman state conquered neighboring territories The Romans commonly granted the local gods of a conquered territory the same honors as the earlier gods of the Roman state religion In addition to Castor and Pollux the conquered settlements in Italy seem to have contributed to the Roman pantheon Diana Minerva Hercules Venus and deities of lesser rank some of whom were Italic divinities others originally derived from the Greek culture of Magna Graecia In 203 BC Rome imported the cult object embodying Cybele from Pessinus in Phrygia and welcomed its arrival with due ceremony Both Lucretius and Catullus poets contemporary in the mid1st century BC offer disapproving glimpses of Cybeles wildly ecstatic cult In some instances deities of an enemy power were formally invited through the ritual of evocatio to take up their abode in new sanctuaries at Rome Communities of foreigners peregrini and former slaves libertini continued their own religious practices within the city In this way Mithras came to Rome and his popularity within the Roman army spread his cult as far afield as Roman Britain The important Roman deities were eventually identified with the more anthropomorphic Greek gods and goddesses and assumed many of their attributes and myths Astronomy Many astronomical objects are named after Roman deities like the planets Mercury Venus Mars Jupiter Saturn and Neptune In Roman and Greek mythology Jupiter places his son born by a mortal woman the infant Hercules on Junos breast while she is asleep so the baby will drink her divine milk and thus become immortal an act which would endow the baby with godlike qualities When Juno woke and realized that she was breastfeeding an unknown infant she pushed him away some of her milk spills and the spurting milk became the Milky Way In another version of the myth the abandoned Hercules is given by Minerva to Juno for feeding but Hercules forcefulness causes Minerva to rip him from her breast in pain The milk that squirts out forms the Milky Way See also List of Ovids Metamorphoses characters List of Roman deities Roman Polytheistic Reconstructionism Pillar of YzeuressurCreuse References Sources Beard Mary 1993 Looking Harder for Roman Myth Dumzil Declamation and the Problems of Definition In Mythos in Mythenloser Gesellschaft Das Paradigma Roms Edited by Fritz Graf 4464 Stuttgart Germany Teubner Braund David and Christopher Gill eds 2003 Myth History and Culture in Republican Rome Studies in Honour of T P Wiseman Exeter UK Univ of Exeter Press Cameron Alan 2004 Greek Mythography in the Roman World Oxford Oxford Univ Press Dumzil Georges 1996 Archaic Roman Religion Rev ed Translated by Philip Krapp Baltimore Johns Hopkins Univ Press Fox Matthew 2011 The Myth of Rome In A Companion to Greek Mythology Blackwell Companions to the Ancient World Literature and CultureEdited by Ken Dowden and Niall Livingstone Chichester Malden MA WileyBlackwell Gardner Jane F 1993 Roman Myths The Legendary Past Austin Univ of Texas Press Grandazzi Alexandre 1997 The Foundation of Rome Myth and History Translated by Jane Marie Todd Ithaca NY Cornell Univ Press Hall Edith 2013 Pantomime Visualising Myth in the Roman Empire In Performance in Greek and Roman Theatre Edited by George Harrison and George William Mallory 451743 Leiden Boston Brill Miller Paul Allen 2013 Mythology and the Abject in Imperial Satire In Classical Myth and Psychoanalysis Ancient and Modern Stories of the Self Edited by Vanda Zajko and Ellen OGorman 213230 Oxford New York Oxford University Press Newby Zahra 2012 The Aesthetics of Violence Myth and Danger in Roman Domestic Landscapes Classical Antiquity 312 349389 Wiseman T P 2004 The Myths of Rome Exeter Univ of Exeter Press Woodard Roger D 2013 Myth Ritual and the Warrior in Roman and IndoEuropean Antiquity Cambridge New York Cambridge University Press External links Lexicon Iconographicum Mythologiae Classicae LIMC 19811999 ArtemisVerlag 9 volumes Supplementum 2009 Artemis_Verlag LIMCFrance LIMC Databases Dedicated to GraecoRoman Mythology and its Iconography",
  },
  {
    title: "Norse mythology",
    originalContent:
      "Norse Nordic or Scandinavian mythology is the body of myths belonging to the North Germanic peoples stemming from Old Norse religion and continuing after the Christianization of Scandinavia as the Nordic folklore of the modern period The northernmost extension of Germanic mythology and stemming from ProtoGermanic folklore Norse mythology consists of tales of various deities beings and heroes derived from numerous sources from both before and after the pagan period including medieval manuscripts archaeological representations and folk tradition The source texts mention numerous gods such as the thundergod Thor the ravenflanked god Odin the goddess Freyja and numerous other deities Most of the surviving mythology centers on the plights of the gods and their interaction with several other beings such as humanity and the jtnar beings who may be friends lovers foes or family members of the gods The cosmos in Norse mythology consists of Nine Worlds that flank a central sacred tree Yggdrasil Units of time and elements of the cosmology are personified as deities or beings Various forms of a creation myth are recounted where the world is created from the flesh of the primordial being Ymir and the first two humans are Ask and Embla These worlds are foretold to be reborn after the events of Ragnark when an immense battle occurs between the gods and their enemies and the world is enveloped in flames only to be reborn anew There the surviving gods will meet and the land will be fertile and green and two humans will repopulate the world Norse mythology has been the subject of scholarly discourse since the 17th century when key texts attracted the attention of the intellectual circles of Europe By way of comparative mythology and historical linguistics scholars have identified elements of Germanic mythology reaching as far back as ProtoIndoEuropean mythology During the modern period the Romanticist Viking revival reawoke an interest in the subject matter and references to Norse mythology may now be found throughout modern popular culture The myths have further been revived in a religious context among adherents of Germanic Neopaganism Terminology The historical religion of the Norse people is commonly referred to as Norse mythology Other terms are Scandinavian mythology North Germanic mythology or Nordic mythology Sources Norse mythology is primarily attested in dialects of Old Norse a North Germanic language spoken by the Scandinavian people during the European Middle Ages and the ancestor of modern Scandinavian languages The majority of these Old Norse texts were created in Iceland where the oral tradition stemming from the preChristian inhabitants of the island was collected and recorded in manuscripts This occurred primarily in the 13th century These texts include the Prose Edda composed in the 13th century by the Icelandic scholar lawspeaker and historian Snorri Sturluson and the Poetic Edda a collection of poems from earlier traditional material anonymously compiled in the 13th century The Prose Edda was composed as a prose manual for producing skaldic poetrytraditional Old Norse poetry composed by skalds Originally composed and transmitted orally skaldic poetry utilizes alliterative verse kennings and several metrical forms The Prose Edda presents numerous examples of works by various skalds from before and after the Christianization process and also frequently refers back to the poems found in the Poetic Edda The Poetic Edda consists almost entirely of poems with some prose narrative added and this poetryEddic poetryutilizes fewer kennings In comparison to skaldic poetry Eddic poetry is relatively unadorned The Prose Edda features layers of euhemerization a process in which deities and supernatural beings are presented as having been either actual magicwielding human beings who have been deified in time or beings demonized by way of Christian mythology Texts such as Heimskringla composed in the 13th century by Snorri and Gesta Danorum composed in Latin by Saxo Grammaticus in Denmark in the 12th century are the results of heavy amounts of euhemerization Numerous additional texts such as the sagas provide further information The saga corpus consists of thousands of tales recorded in Old Norse ranging from Icelandic family histories Sagas of Icelanders to Migration period tales mentioning historic figures such as Attila the Hun legendary sagas Objects and monuments such as the Rk runestone and the Kvinneby amulet feature runic inscriptionstexts written in the runic alphabet the indigenous alphabet of the Germanic peoplesthat mention figures and events from Norse mythology Objects from the archaeological record may also be interpreted as depictions of subjects from Norse mythology such as amulets of the god Thors hammer Mjlnir found among pagan burials and small silver female figures interpreted as valkyries or dsir beings associated with war fate or ancestor cults By way of historical linguistics and comparative mythology comparisons to other attested branches of Germanic mythology such as the Old High German Merseburg Incantations may also lend insight Wider comparisons to the mythology of other IndoEuropean peoples by scholars has resulted in the potential reconstruction of far earlier myths Only a tiny amount of poems and tales survive of the many mythical tales and poems that are presumed to have existed during the Middle Ages Viking Age Migration Period and before Later sources reaching into the modern period such as a medieval charm recorded as used by the Norwegian woman Ragnhild Tregagsconvicted of witchcraft in Norway in the 14th centuryand spells found in the 17th century Icelandic Galdrabk grimoire also sometimes make references to Norse mythology Other traces such as place names bearing the names of gods may provide further information about deities such as a potential association between deities based on the placement of locations bearing their names their local popularity and associations with geological features Mythology Gods and other beings Central to accounts of Norse mythology are the plights of the gods and their interaction with various other beings such as with the jtnar who may be friends lovers foes or family members of the gods Numerous gods are mentioned in the source texts As evidenced by records of personal names and place names the most popular god among the Scandinavians during the Viking Age was Thor the thunder god who is portrayed as unrelentingly pursuing his foes his mountaincrushing thunderous hammer Mjlnir in hand In the mythology Thor lays waste to numerous jtnar who are foes to the gods or humanity and is wed to the beautiful goldenhaired goddess Sif The god Odin is also frequently mentioned in surviving texts Oneeyed wolf and ravenflanked with a spear in hand Odin pursues knowledge throughout the nine realms In an act of selfsacrifice Odin is described as having hanged himself upsidedown for nine days and nights on the cosmological tree Yggdrasil to gain knowledge of the runic alphabet which he passed on to humanity and is associated closely with death wisdom and poetry Odin is portrayed as the ruler of Asgard and leader of the Aesir Odins wife is the powerful goddess Frigg who can see the future but tells no one and together they have a beloved son Baldr After a series of dreams had by Baldr of his impending death his death is engineered by Loki and Baldr thereafter resides in Hel a realm ruled over by an entity of the same name Odin must share half of his share of the dead with a powerful goddess Freyja She is beautiful sensual wears a feathered cloak and practices seir She rides to battle to choose among the slain and brings her chosen to her afterlife field Flkvangr Freyja weeps for her missing husband r and seeks after him in faraway lands Freyjas brother the god Freyr is also frequently mentioned in surviving texts and in his association with the weather royalty human sexuality and agriculture brings peace and pleasure to humanity Deeply lovesick after catching sight of the beautiful jtunn Gerr Freyr seeks and wins her love yet at the price of his future doom Their father is the powerful god Njrr Njrr is strongly associated with ships and seafaring and so also wealth and prosperity Freyja and Freyrs mother is Njrrs unnamed sister her name is unprovided in the source material However there is more information about his pairing with the skiing and hunting goddess Skai Their relationship is illfated as Skai cannot stand to be away from her beloved mountains nor Njrr from the seashore Together Freyja Freyr and Njrr form a portion of gods known as the Vanir While the Aesir and the Vanir retain distinct identification they came together as the result of the AesirVanir War While they receive less mention numerous other gods and goddesses appear in the source material For a list of these deities see List of Germanic deities Some of the gods heard less of include the applebearing goddess Iunn and her husband the skaldic god Bragi the goldtoothed god Heimdallr born of nine mothers the ancient god Tr who lost his right hand while binding the great wolf Fenrir and the goddess Gefjon who formed modernday Zealand Denmark Various beings outside of the gods are mentioned Elves and dwarfs are commonly mentioned and appear to be connected but their attributes are vague and the relation between the two is ambiguous Elves are described as radiant and beautiful whereas dwarfs often act as earthen smiths A group of beings variously described as jtnar thursar and trolls in English these are all often glossed as giants frequently appear These beings may either aid deter or take their place among the gods The Norns dsir and aforementioned valkyries also receive frequent mention While their functions and roles may overlap and differ all are collective female beings associated with fate Cosmology In Norse cosmology all beings live in Nine Worlds that center around the cosmological tree Yggdrasil The gods inhabit the heavenly realm of Asgard whereas humanity inhabits Midgard a region in the center of the cosmos Outside of the gods humanity and the jtnar these Nine Worlds are inhabited by beings such as elves and dwarfs Travel between the worlds is frequently recounted in the myths where the gods and other beings may interact directly with humanity Numerous creatures live on Yggdrasil such as the insulting messenger squirrel Ratatoskr and the perching hawk Verflnir The tree itself has three major roots and at the base of one of these roots live the Norns female entities associated with fate Elements of the cosmos are personified such as the Sun Sl a goddess the Moon Mni a god and Earth Jr a goddess as well as units of time such as day Dagr a god and night Ntt a jtunn The afterlife is a complex matter in Norse mythology The dead may go to the murky realm of Hela realm ruled over by a female being of the same name may be ferried away by valkyries to Odins martial hall Valhalla or may be chosen by the goddess Freyja to dwell in her field Flkvangr The goddess Rn may claim those that die at sea and the goddess Gefjon is said to be attended by virgins upon their death Texts also make reference to reincarnation Time itself is presented between cyclic and linear and some scholars have argued that cyclic time was the original format for the mythology Various forms of a cosmological creation story are provided in Icelandic sources and references to a future destruction and rebirth of the worldRagnarokare frequently mentioned in some texts Humanity According to the Prose Edda and the Poetic Edda poem Vlusp the first human couple consisted of Ask and Embla driftwood found by a trio of gods and imbued with life in the form of three gifts After the cataclysm of Ragnarok this process is mirrored in the survival of two humans from a wood Lf and Lfrasir From these two humankind is foretold to repopulate the new and green earth See also Alliterative verse Family tree of the Norse gods List of Germanic deities List of valkyrie names in Norse mythology Greek mythology Roman mythology The horse in Nordic mythology References General sources Further reading General secondary works Abram Christopher 2011 Myths of the Pagan North the Gods of the Norsemen London Continuum ISBN 9781847252470 Aalsteinsson Jn Hnefill 1998 A Piece of Horse Liver Myth Ritual and Folklore in Old Icelandic Sources translated by Terry Gunnell Joan TurvillePetre Reykjavk Flagsvsindastofnun ISBN 9979542640 Andrn Anders Jennbert Kristina Raudvere Catharina editors 2006 Old Norse Religion in LongTerm Perspectives Origins Changes and Interactions Lund Nordic Academic Press ISBN 918911681X Branston Brian 1980 Gods of the North London Thames and Hudson Revised from an earlier hardback edition of 1955 ISBN 0500271771 Christiansen Eric 2002 The Norsemen in the Viking Age Malden Mass Blackwell ISBN 1405149647 Clunies Ross Margaret 1994 Prolonged Echoes Old Norse Myths in Medieval Northern Society vol 1 The Myths Odense Odense Univ Press ISBN 8778380081 Davidson H R Ellis 1964 Gods and Myths of Northern Europe Baltimore Penguin New edition 1990 by Penguin Books ISBN 0140136274 Several runestones Davidson H R Ellis 1969 Scandinavian Mythology London New York Hamlyn ISBN 0872260410 Reissued 1996 as Viking and Norse Mythology New York Barnes and Noble Davidson H R Ellis 1988 Myths and Symbols in Pagan Europe Syracuse NY Syracuse Univ Press ISBN 0815624387 Davidson H R Ellis 1993 The Lost Beliefs of Northern Europe London New York Routledge ISBN 0415049377 de Vries Jan Altgermanische Religionsgeschichte 2 vols 2nd ed Grundriss der germanischen Philologie 1213 Berlin W de Gruyter DuBois Thomas A 1999 Nordic Religions in the Viking Age Philadelphia Univ Pennsylvania Press ISBN 0812217144 Dumzil Georges 1973 Gods of the Ancient Northmen Ed trans Einar Haugen Berkeley University of California Press ISBN 0520035070 Grimm Jacob 1888 Teutonic Mythology 4 vols Trans S Stallybras London Reprinted 2003 by Kessinger ISBN 0766177424 ISBN 0766177432 ISBN 0766177440 ISBN 0766177459 Reprinted 2004 Dover Publications ISBN 0486436152 4 vols ISBN 0486435466 ISBN 0486435474 ISBN 0486435482 ISBN 0486435490 Lindow John 1988 Scandinavian Mythology An Annotated Bibliography Garland Folklore Bibliographies 13 New York Garland ISBN 0824091736 Lindow John 2001 Norse Mythology A Guide to the Gods Heroes Rituals and Beliefs Oxford Oxford University Press ISBN 0195153820 A dictionary of Norse mythology Mirachandra 2006 Treasure of Norse Mythology Volume I ISBN 9783922800996 Motz Lotte 1996 The King the Champion and the Sorcerer A Study in Germanic Myth Wien Fassbaender ISBN 3900538573 ODonoghue Heather 2007 From Asgard to Valhalla the remarkable history of the Norse myths London I B Tauris ISBN 1845113578 Orchard Andy 1997 Cassells Dictionary of Norse Myth and Legend London Cassell ISBN 0304363855 Page R I 1990 Norse Myths The Legendary Past London British Museum and Austin University of Texas Press ISBN 0292755465 Price Neil S 2002 The Viking Way Religion and War in Late Iron Age Scandinavia Uppsala Dissertation Dept Archaeology Ancient History ISBN 9150616269 Simek Rudolf 1993 Dictionary of Northern Mythology Trans Angela Hall Cambridge D S Brewer ISBN 0859913694 New edition 2000 ISBN 0859915131 Simrock Karl Joseph 18531855 Handbuch der deutschen Mythologie Svanberg Fredrik 2003 Decolonizing the Viking Age Stockholm Almqvist Wiksell ISBN 9122020063 v 1 ISBN 9122020071 v 2 TurvillePetre E O Gabriel 1964 Myth and Religion of the North The Religion of Ancient Scandinavia London Weidenfeld Nicolson Reprinted 1975 Westport CN Greenwood Press ISBN 0837174201 Romanticism Anderson Rasmus 1875 Norse Mythology or The Religion of Our Forefathers Chicago SC Griggs Guerber H A 1909 Myths of the Norsemen From the Eddas and Sagas London George G Harrap Reprinted 1992 Mineola NY Dover ISBN 0486273482 Keary A E 1909 The Heroes of Asgard New York Macmillan Company Reprinted 1982 by Smithmark Pub ISBN 0831744758 Reprinted 1979 by Pan Macmillan ISBN 0333078020 Mable Hamilton Wright 1901 Norse Stories Retold from the Eddas Mead and Company Reprinted 1999 New York Hippocrene Books ISBN 0781807700 Mackenzie Donald A 1912 Teutonic Myth and Legend New York W H Wise Co 1934 Reprinted 2003 by University Press of the Pacific ISBN 1410207404 Rydberg Viktor 1889 Teutonic Mythology trans Rasmus B Anderson London Swan Sonnenschein Co Reprinted 2001 Elibron Classics ISBN 1402193912 Reprinted 2004 Kessinger Publishing Company ISBN 0766188914 Modern retellings Bradish Sarah Powers 1900 Old Norse stories New York American Book Company Internet Archive Colum Padraic 1920 The Children of Odin The Book of Northern Myths illustrated by Willy Pogny New York Macmillan Reprinted 2004 by Aladdin ISBN 0689868855 CrossleyHolland Kevin 1981 The Norse Myths New York Pantheon Books ISBN 0394748468 Also released as The Penguin Book of Norse Myths Gods of the Vikings Harmondsworth Penguin ISBN 0140258698 dAulaire Ingri and Edgar 1967 dAulaires Book of Norse Myths New York New York Review of Books Munch Peter Andreas 1927 Norse Mythology Legends of Gods and Heroes Scandinavian Classics Trans Sigurd Bernhard Hustvedt 1963 New York AmericanScandinavian Foundation ISBN 0404045383 Gaiman Neil 2017 Norse Mythology WW Norton Company ISBN 039360909X Syran Nora Louise 2000 Einars Ragnarok External links Media related to Norse mythology at Wikimedia Commons",
  },
  {
    title: "Hindu mythology",
    originalContent:
      "Hindu mythology is the body of myths attributed to and espoused by the adherents of the Hindu religion found in Hindu texts such as the Vedas the itihasa the epics of the Mahabharata and Ramayana the Puranas and mythological stories specific to a particular ethnolinguistic group like the Tamil Periya Puranam and Divya Prabandham and the Mangal Kavya of Bengal Hindu myths are also found in widely translated popular texts such as the fables of the Panchatantra and the Hitopadesha as well as in Southeast Asian texts Meaning of myth Myth is a genre of folklore or theology consisting primarily of narratives that play a fundamental role in a society such as foundational tales or origin myths For folklorists historians philosophers or theologians this is very different from the use of myth simply indicating that something is not true Instead the truth value of a myth is not a defining criterion Hindu myths can be found in the Vedas the itihasa Ramayana and Mahabharata and the major Puranas Other sources include the Bengali literature such as MangalKvya and the Tamil literature such as Divya Prabandham Tirumurai and the Five Great Epics These narratives play a crucial role in the Hindu tradition and are considered real and significant within their cultural and spiritual context offering profound insights into the beliefs and values of Hinduism Origins and development Indus Valley Civilisation According to Joseph Campbell the Indus Valley 26001900 BCE may have left traces in the beliefs and traditions of Hinduism Artefacts have revealed motifs that are also employed and revered by Hindus today such as primary male deities worshipped by a ruling elite mother goddesses nature spirits snake worship as well as the reverence of other theriomorphic animalshaped beings These themes would be maintained by the Dravidian folk religion even after the decline of its parent civilisation around 1800 BCE Vedic Period A major factor in the development of Hinduism was the Vedic religion The IndoAryan migration brought their distinct beliefs to the Indian subcontinent where the Vedas were composed around 1500 BCE The IndoAryans Vedic pantheon of deities included the chief god Indra the sun deity Surya Ushas as well as Agni Brahmanical Period This period saw the composition of commentaries referred to as the Brahmanas Upanishad Period According to Williams from 900 to 600 BCE the protests of the populace against sacrifices made towards the Vedic gods and rebellions against the Brahmin class led to the embrace of reform by the latter and the composition of the fourth Veda and the Vedanta texts About half of the Upanishads were mystical and unitive speaking of experiencing the divine as the one ekam while the other half promoted devotion to one or more deities New gods and goddesses were celebrated and devotional practices began to be introduced Sramanic movements Elements such as those emerging from Buddhism and Jainism made their heteroprax contributions to later Hindu mythology such as temples indoor shrines and rituals modeled after service to a divine king Renunciate traditions contributed elements that questioned sacrifices and the killing of animals and promoted asceticism and vegetarianism All of these themes would be incorporated by the Brahmin classes into the later Hindu synthesis which developed in response to the sramanic movements between ca 500300 BCE and 500 CE and also found their way into Hindu mythology Epic Period The era from 400 BCE to 400 CE was the period of the compilation of Indias great epics the Mahabharata and Ramayana These were central manifestations of the newly developing Hindu synthesis contributing to a specific Hindu mythology emphasising divine action on earth in Vishnus incarnations and other divine manifestations The lore of the devas and the asuras expanded Epic mythology foreshadowed the rich polytheism of the next two periods The Mahabharata contained two appendices that were extremely important sources for later mythological development the Bhagavad Gta and the Harivamsa Puranic Period According to Williams the mythology of the Puranas can be broken into three periods 300500 5001000 10001800 or the whole period may simply be referred to as the Hindu Middle Ages This age saw the composition of the major Puranic texts of the faith along with the rise of sectarianism with followers amassing around the cults of Vishnu Shiva or Devi The three denominations within this period help locate in time historical developments within the sectarian communities the rise and decline of Tantrism and its influence on mainstream mythology the tendencies in Puranic mythologising of subordinating Vedic gods and past heroes to everincreasing moral weaknesses going on to be identified as a period of exuberant polytheism However this was also accompanied with the belief in monotheism the idea that all paths lead to the Ultimate Reality Brahman Tantric Period According to Williams during the Tantric period from 900 to 1600 CE the mythology of Tantra and Shaktism revived and enriched blood sacrifice and the pursuit of pleasure as central themes Tantras stories differed radically in meaning from those of epic mythology which favored devotion asceticism and duty There was either a revival or emphasis that was placed on the shakti or the cosmic energy of goddesses a concept that had emerged during the Indus Valley Civilisation Modern Period In the contemporary era the mythologies of the dominant traditions of Vaishnavism Shaivism and Shaktism prevail Several myths were found or invented to make tribals or former outcastes Hindus and bring them within the cultural whole of a reconstructed Hindu mythological community Mythical themes and types Academic studies of mythology often define mythology as deeply valued stories that explain a societys existence and world order those narratives of a societys creation the societys origins and foundations their gods their original heroes mankinds connection to the divine and their narratives of eschatology what happens in the afterlife This is a very general outline of some of the basic sacred stories with those themes In its broadest academic sense the word myth simply means a traditional story However many scholars restrict the term myth to sacred stories Folklorists often go further defining myths as tales believed as true usually sacred set in the distant past or other worlds or parts of the world and with extrahuman inhuman or heroic characters In classical Greek muthos from which the English word myth derives meant story narrative Hindu mythology does not often have a consistent monolithic structure The same myth typically appears in various versions and can be represented differently across different regional and socioreligious traditions Many of these legends evolve across these texts where the character names change or the story is embellished with greater details According to Suthren Hirst these myths have been given a complex range of interpretations While according to Doniger OFlaherty the central message and moral values remain the same They have been modified by various philosophical schools over time and are taken to have deeper often symbolic meaning Cosmology Deities Pantheism Brahman The Ultimate Reality Vaishnavism Vishnucentric Vishnu The God of Preservation Lakshmi The Goddess of Prosperity Dashavatara Ten incarnations of Vishnu chiefly Krishna and Rama Shaivism Shivacentric Shiva The God of Destruction Parvati The Goddess of Power Ganesha The God of Auspiciousness Kartikeya The God of Victory and War Shaktism Goddesscentric Mahadevi Supreme Goddess Saraswati Goddess of Wisdom Lakshmi Goddess of Prosperity Parvati Goddess of Power Durga Goddess of War Kali Goddess of time and destruction Henotheism and Polytheism Brahma The God of Creation Vishnu The God of Preservation Shiva The God of Destruction Indra The King of the Devas and Svarga Saraswati The Goddess of Wisdom Lakshmi The Goddess of Prosperity Parvati The Goddess of Power Ganesha The God of Auspiciousness Krishna The God of love and protection Radha The goddess of love chief consort of Krishna Rukmini The first queen consort and principal wife of Krishna Satyabhama The third queen consort of Krishna Yamuna one of the main sacred river goddesses in Hinduism and the fourth queen consort of Krishna Bhudevi Goddess of the Earth Kartikeya Murugan God of Victory and War Rama The seventh incarnation of Vishnu Kali A terrible aspect of Parvati Durga A principal aspect of Mahadevi Ashvins Twin gods of medicine Agni God of Fire Rudra God of the storm Shakti Personification of power Vayu God of the wind Surya God of the Sun Varuna God of the oceans Lakshmana Younger Brother of Rama Hanuman Highest devotee of Rama Sita Consort of Rama and incarnation of Lakshmi Sati An incarnation of the goddess Shakti Kubera God of Wealth Parshurama The sixth incarnation of Vishnu Yama God of Death and the Underworld Chandra God of the Moon Balarama incarnation of Shesha and in some traditions an avatar of Vishnu Prajapati Creator deity Kalki Prophesied final incarnation of Vishnu Dashavatara Ten Incarnations of Vishnu Narada Divine sage messenger of gods Sundaravalli Daughter of Vishnu consort of Murugan Devasena Daughter of Vishnu consort of Murugan Kamadeva The God of love and desire Rati The Goddess of love and desire Shani Divine Personification of the planet saturn Dravidian folk religion Indigenous Dravidian faith Mariamman Mother goddess Ayyanar Guardian deity Ayyappan God of Dharma Connections to other belief systems Hinduism shares mythemes with Buddhism Jainism and Sikhism See also Notes Citations General sources Further reading Bhairav J Furcifer Rakesh Khanna 2020 Ghosts Monsters and Demons of India Blaft Publications Private Limited ISBN 9789380636467 OCLC 1259298225 Brockington J L 1998 The Sanskrit Epics BRILL Academic ISBN 9004102604 Buitenen J A B van Dimmitt Cornelia 1978 Classical Hindu Mythology A Reader in the Sanskrit Puranas Philadelphia Temple University Press ISBN 0877221227 Campbell Joseph 2003 Myths of Light Eastern Metaphors of the Eternal Novato Calif New World Library ISBN 1577314034 Dalal Roshen 2010 Hinduism An Alphabetical Guide Penguin Books India ISBN 9780143414216 Dallapiccola Anna L 2002 Dictionary of Hindu Lore and Legend ISBN 0500510881 Dimitrova Stefania 2017 The Day of Brahma The Myths of IndiaEpics of Human Destiny AlphaOmega ISBN 9789549694277 Dowson John 1888 A Classical Dictionary of Hindu Mythology and Religion Geography History and Literature London Trubner Co Krishna Nanditha 2009 The Book of Vishnu Penguin Books India ISBN 9780143067627 Krishna Nanditha 2010 Sacred Animals of India Penguin Books India ISBN 9780143066194 Macdonell Arthur Anthony 1995 Vedic Mythology Delhi Motilal Banarsidass ISBN 8120811135 Pattanaik Devdutt 2003 Indian Mythology Tales Symbols and Rituals from the Heart of the Subcontinent Inner TraditionsBear Company ISBN 0892818700 Rao T A Gopinatha 1914 Elements of Hindu Iconography Vol 1 Part I Madras Law Printing House Walker Benjamin 1968 Hindu World An Encyclopedic Survey of Hinduism London Allen Unwin Wilkins W J 1882 Hindu Mythology Vedic and Purnic Thacker Spink Co Goldberg Philip American Veda Harmony Books 2010 External links Clay Sanskrit Library publishes classical Indian literature including the Mahabharata and Ramayana with facingpage text and translation Also offers searchable corpus and downloadable materials Sanskrit Documents Collection Documents in ITX format of Upanishads Stotras etc Hindu Mythology Stories from Ancient India",
  },
  {
    title: "Enlightenment",
    originalContent:
      "Enlightenment or enlighten may refer to Age of Enlightenment Age of Enlightenment period in Western intellectual history from the late 17th to late 18th century centered in France but also encompassing alphabetically by country or culture England Midlands Enlightenment period in 18thcentury England Greece Modern Greek Enlightenment an 18thcentury national revival and educational movement in Greece Italy Italian Enlightenment period in 18thcentury Italy Jewish Haskalah Jewish Enlightenment movement among European Jews in the late 18th century Poland Enlightenment in Poland ideas of the Age of Enlightenment in Poland Russia Russian Enlightenment 18thcentury period of active government encouragement of proliferation of arts and sciences in Russia Scotland Scottish Enlightenment period in 18thcentury Scotland Spain Enlightenment in Spain came to Spain with a new dynasty the Bourbons subsequent reform and enlightened despotism USA American Enlightenment intellectual culture of the British North American colonies and the early United States Arab Enlightenment or Nahda late 19th to early 20th century Religion Enlightenment in Buddhism translation of the term bodhi awakening Moksha Moksha Jainism Kevala jnana awakened knowledge in Jainism Divine illumination Computing Enlightenment window manager an X Window System window manager Enlighten radiosity engine code to do realtime calculation of indirect lighting radiosity in video Enlightenment Foundation Libraries a set of graphics libraries Events Enlighten Canberra an annual arts and cultural festival in Canberra Australia Enlightenment the main artistic performance in the 2012 Summer Paralympics opening ceremony Film and television Enlightenment Doctor Who a 1983 Doctor Who serial Music Enlightenment Van Morrison album 1990 Enlightenment McCoy Tyner album 1973 Enlightenment Van Morrison song 1990 Enlightenment soundtrack album the soundtrack of the 2012 Summer Paralympics opening ceremony Other uses Ionian Enlightenment the origin of ancient Greek advances in philosophy and science Dark Enlightenment an antidemocratic and reactionary movement that broadly rejects egalitarianism and Whig historiography Enlightenment Intensive a group retreat designed to enable a spiritual enlightenment Enlightenment Movement Afghanistan a Hazara grassroots civil disobedience group created in Afghanistan in 2016 Project Enlightenment an educational program See also CounterEnlightenment a term used by some 20th century commentators to describe contemporary reasoned opposition to the Age of Enlightenment All pages with titles beginning with Enlighten Enlightened disambiguation Illumination disambiguation",
  },
  {
    title: "Wormhole",
    originalContent:
      "A wormhole is a hypothetical structure which connects disparate points in spacetime It may be visualized as a tunnel with two ends at separate points in spacetime ie different locations different points in time or both Wormholes are based on a special solution of the Einstein field equations Specifically they are a transcendental bijection of the spacetime continuum an asymptotic projection of the CalabiYau manifold manifesting itself in antide Sitter space Wormholes are consistent with the general theory of relativity but whether they actually exist is unknown Many scientists postulate that wormholes are merely projections of a fourth spatial dimension analogous to how a twodimensional 2D being could experience only part of a threedimensional 3D object A wellknown analogy of such constructs is provided by the Klein bottle displaying a hole when rendered in three dimensions but not in four or higher dimensions In 1995 Matt Visser suggested there may be many wormholes in the universe if cosmic strings with negative mass were generated in the early universe Some physicists such as Kip Thorne have suggested how to make wormholes artificially Visualization technique For a simplified notion of a wormhole space can be visualized as a twodimensional surface In this case a wormhole would appear as a hole in that surface lead into a 3D tube the inside surface of a cylinder then reemerge at another location on the 2D surface with a hole similar to the entrance An actual wormhole would be analogous to this but with the spatial dimensions raised by one For example instead of circular holes on a 2Dimensional plane the entry and exit points could be visualized as spherical holes in 3D space leading into a fourdimensional tube similar to a spherinder Another way to imagine wormholes is to take a sheet of paper and draw two somewhat distant points on one side of the paper The sheet of paper represents a plane in the spacetime continuum and the two points represent a distance to be traveled but theoretically a wormhole could connect these two points by folding that plane ie the paper so the points are touching In this way it would be much easier to traverse the distance since the two points are now touching Terminology In 1928 German mathematician philosopher and theoretical physicist Hermann Weyl proposed a wormhole hypothesis of matter in connection with mass analysis of electromagnetic field energy however he did not use the term wormhole he spoke of onedimensional tubes instead American theoretical physicist John Archibald Wheeler inspired by Weyls work coined the term wormhole in a 1957 paper he wrote with Charles W Misner This analysis forces one to consider situations where there is a net flux of lines of force through what topologists would call a handle of the multiplyconnected space and what physicists might perhaps be excused for more vividly terming a wormhole Modern definitions Wormholes have been defined both geometrically and topologically From a topological point of view an intrauniverse wormhole a wormhole between two points in the same universe is a compact region of spacetime whose boundary is topologically trivial but whose interior is not simply connected Formalizing this idea leads to definitions such as the following taken from Matt Vissers Lorentzian Wormholes 1996 If a Minkowski spacetime contains a compact region and if the topology of is of the form S where is a threemanifold of the nontrivial topology whose boundary has the topology of the form S2 and if furthermore the hypersurfaces are all spacelike then the region contains a quasipermanent intrauniverse wormhole Geometrically wormholes can be described as regions of spacetime that constrain the incremental deformation of closed surfaces For example in Enrico Rodrigos The Physics of Stargates a wormhole is defined informally as a region of spacetime containing a world tube the time evolution of a closed surface that cannot be continuously deformed shrunk to a world line the time evolution of a point or observer Development Schwarzschild wormholes The first type of wormhole solution discovered was the Schwarzschild wormhole which would be present in the Schwarzschild metric describing an eternal black hole but it was found that it would collapse too quickly for anything to cross from one end to the other Wormholes that could be crossed in both directions known as traversable wormholes were thought to be possible only if exotic matter with negative energy density could be used to stabilize them However physicists later reported that microscopic traversable wormholes may be possible and not require any exotic matter instead requiring only electrically charged fermionic matter with small enough mass that it cannot collapse into a charged black hole While such wormholes if possible may be limited to transfers of information humanly traversable wormholes may exist if reality can broadly be described by the RandallSundrum model 2 a branebased theory consistent with string theory EinsteinRosen bridges EinsteinRosen bridges also known as ER bridges named after Albert Einstein and Nathan Rosen are connections between areas of space that can be modeled as vacuum solutions to the Einstein field equations and that are now understood to be intrinsic parts of the maximally extended version of the Schwarzschild metric describing an eternal black hole with no charge and no rotation Here maximally extended refers to the idea that the spacetime should not have any edges it should be possible to continue this path arbitrarily far into the particles future or past for any possible trajectory of a freefalling particle following a geodesic in the spacetime In order to satisfy this requirement it turns out that in addition to the black hole interior region that particles enter when they fall through the event horizon from the outside there must be a separate white hole interior region that allows us to extrapolate the trajectories of particles that an outside observer sees rising up away from the event horizon And just as there are two separate interior regions of the maximally extended spacetime there are also two separate exterior regions sometimes called two different universes with the second universe allowing us to extrapolate some possible particle trajectories in the two interior regions This means that the interior black hole region can contain a mix of particles that fell in from either universe and thus an observer who fell in from one universe might be able to see the light that fell in from the other one and likewise particles from the interior white hole region can escape into either universe All four regions can be seen in a spacetime diagram that uses KruskalSzekeres coordinates In this spacetime it is possible to come up with coordinate systems such that if a hypersurface of constant time a set of points that all have the same time coordinate such that every point on the surface has a spacelike separation giving what is called a spacelike surface is picked and an embedding diagram drawn depicting the curvature of space at that time the embedding diagram will look like a tube connecting the two exterior regions known as an EinsteinRosen bridge The Schwarzschild metric describes an idealized black hole that exists eternally from the perspective of external observers a more realistic black hole that forms at some particular time from a collapsing star would require a different metric When the infalling stellar matter is added to a diagram of a black holes geography it removes the part of the diagram corresponding to the white hole interior region along with the part of the diagram corresponding to the other universe The EinsteinRosen bridge was discovered by Ludwig Flamm in 1916 a few months after Schwarzschild published his solution and was rediscovered by Albert Einstein and his colleague Nathan Rosen who published their result in 1935 However in 1962 John Archibald Wheeler and Robert W Fuller published a paper showing that this type of wormhole is unstable if it connects two parts of the same universe and that it will pinch off too quickly for light or any particle moving slower than light that falls in from one exterior region to make it to the other exterior region According to general relativity the gravitational collapse of a sufficiently compact mass forms a singular Schwarzschild black hole In the EinsteinCartanSciamaKibble theory of gravity however it forms a regular EinsteinRosen bridge This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part the torsion tensor as a dynamic variable Torsion naturally accounts for the quantummechanical intrinsic angular momentum spin of matter The minimal coupling between torsion and Dirac spinors generates a repulsive spinspin interaction that is significant in fermionic matter at extremely high densities Such an interaction prevents the formation of a gravitational singularity eg a black hole Instead the collapsing matter reaches an enormous but finite density and rebounds forming the other side of the bridge Although Schwarzschild wormholes are not traversable in both directions their existence inspired Kip Thorne to imagine traversable wormholes created by holding the throat of a Schwarzschild wormhole open with exotic matter material that has negative massenergy Other nontraversable wormholes include Lorentzian wormholes first proposed by John Archibald Wheeler in 1957 wormholes creating a spacetime foam in a general relativistic spacetime manifold depicted by a Lorentzian manifold and Euclidean wormholes named after Euclidean manifold a structure of Riemannian manifold Traversable wormholes The Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary matter vacuum energy and it has been shown theoretically that quantum field theory allows states where energy can be arbitrarily negative at a given point Many physicists such as Stephen Hawking Kip Thorne and others argued that such effects might make it possible to stabilize a traversable wormhole The only known natural process that is theoretically predicted to form a wormhole in the context of general relativity and quantum mechanics was put forth by Juan Maldacena and Leonard Susskind in their ER EPR conjecture The quantum foam hypothesis is sometimes used to suggest that tiny wormholes might appear and disappear spontaneously at the Planck scale 494496 and stable versions of such wormholes have been suggested as dark matter candidates It has also been proposed that if a tiny wormhole held open by a negative mass cosmic string had appeared around the time of the Big Bang it could have been inflated to macroscopic size by cosmic inflation Lorentzian traversable wormholes would allow travel in both directions from one part of the universe to another part of that same universe very quickly or would allow travel from one universe to another The possibility of traversable wormholes in general relativity was first demonstrated in a 1973 paper by Homer Ellis and independently in a 1973 paper by K A Bronnikov Ellis analyzed the topology and the geodesics of the Ellis drainhole showing it to be geodesically complete horizonless singularityfree and fully traversable in both directions The drainhole is a solution manifold of Einsteins field equations for a vacuum spacetime modified by inclusion of a scalar field minimally coupled to the Ricci tensor with antiorthodox polarity negative instead of positive Ellis specifically rejected referring to the scalar field as exotic because of the antiorthodox coupling finding arguments for doing so unpersuasive The solution depends on two parameters m which fixes the strength of its gravitational field and n which determines the curvature of its spatial cross sections When m is set equal to 0 the drainholes gravitational field vanishes What is left is the Ellis wormhole a nongravitating purely geometric traversable wormhole Kip Thorne and his graduate student Mike Morris independently discovered in 1988 the Ellis wormhole and argued for its use as a tool for teaching general relativity For this reason the type of traversable wormhole they proposed held open by a spherical shell of exotic matter is also known as a MorrisThorne wormhole Later other types of traversable wormholes were discovered as allowable solutions to the equations of general relativity including a variety analyzed in a 1989 paper by Matt Visser in which a path through the wormhole can be made where the traversing path does not pass through a region of exotic matter However in the pure GaussBonnet gravity a modification to general relativity involving extra spatial dimensions which is sometimes studied in the context of brane cosmology exotic matter is not needed in order for wormholes to existthey can exist even with no matter A type held open by negative mass cosmic strings was put forth by Visser in collaboration with Cramer et al in which it was proposed that such wormholes could have been naturally created in the early universe Wormholes connect two points in spacetime which means that they would in principle allow travel in time as well as in space In 1988 Morris Thorne and Yurtsever worked out how to convert a wormhole traversing space into one traversing time by accelerating one of its two mouths However according to general relativity it would not be possible to use a wormhole to travel back to a time earlier than when the wormhole was first converted into a time machine Until this time it could not have been noticed or have been used 504 Raychaudhuris theorem and exotic matter To see why exotic matter is required consider an incoming light front traveling along geodesics which then crosses the wormhole and reexpands on the other side The expansion goes from negative to positive As the wormhole neck is of finite size we would not expect caustics to develop at least within the vicinity of the neck According to the optical Raychaudhuris theorem this requires a violation of the averaged null energy condition Quantum effects such as the Casimir effect cannot violate the averaged null energy condition in any neighborhood of space with zero curvature but calculations in semiclassical gravity suggest that quantum effects may be able to violate this condition in curved spacetime Although it was hoped recently that quantum effects could not violate an achronal version of the averaged null energy condition violations have nevertheless been found so it remains an open possibility that quantum effects might be used to support a wormhole Modified general relativity In some hypotheses where general relativity is modified it is possible to have a wormhole that does not collapse without having to resort to exotic matter For example this is possible with R2 gravity a form of fR gravity Fasterthanlight travel The impossibility of fasterthanlight relative speed applies only locally Wormholes might allow effective superluminal fasterthanlight travel by ensuring that the speed of light is not exceeded locally at any time While traveling through a wormhole subluminal slowerthanlight speeds are used If two points are connected by a wormhole whose length is shorter than the distance between them outside the wormhole the time taken to traverse it could be less than the time it would take a light beam to make the journey if it took a path through the space outside the wormhole However a light beam traveling through the same wormhole would beat the traveler Time travel If traversable wormholes exist they might allow time travel A proposed timetravel machine using a traversable wormhole might hypothetically work in the following way One end of the wormhole is accelerated to some significant fraction of the speed of light perhaps with some advanced propulsion system and then brought back to the point of origin Alternatively another way is to take one entrance of the wormhole and move it to within the gravitational field of an object that has higher gravity than the other entrance and then return it to a position near the other entrance For both these methods time dilation causes the end of the wormhole that has been moved to have aged less or become younger than the stationary end as seen by an external observer however time connects differently through the wormhole than outside it so that synchronized clocks at either end of the wormhole will always remain synchronized as seen by an observer passing through the wormhole no matter how the two ends move around 502 This means that an observer entering the younger end would exit the older end at a time when it was the same age as the younger end effectively going back in time as seen by an observer from the outside One significant limitation of such a time machine is that it is only possible to go as far back in time as the initial creation of the machine 503 it is more of a path through time rather than it is a device that itself moves through time and it would not allow the technology itself to be moved backward in time According to current theories on the nature of wormholes construction of a traversable wormhole would require the existence of a substance with negative energy often referred to as exotic matter More technically the wormhole spacetime requires a distribution of energy that violates various energy conditions such as the null energy condition along with the weak strong and dominant energy conditions However it is known that quantum effects can lead to small measurable violations of the null energy condition 101 and many physicists believe that the required negative energy may actually be possible due to the Casimir effect in quantum physics Although early calculations suggested a very large amount of negative energy would be required later calculations showed that the amount of negative energy can be made arbitrarily small In 1993 Matt Visser argued that the two mouths of a wormhole with such an induced clock difference could not be brought together without inducing quantum field and gravitational effects that would either make the wormhole collapse or the two mouths repel each other or otherwise prevent information from passing through the wormhole Because of this the two mouths could not be brought close enough for causality violation to take place However in a 1997 paper Visser hypothesized that a complex Roman ring named after Tom Roman configuration of an N number of wormholes arranged in a symmetric polygon could still act as a time machine although he concludes that this is more likely a flaw in classical quantum gravity theory rather than proof that causality violation is possible Interuniversal travel A possible resolution to the paradoxes resulting from wormholeenabled time travel rests on the manyworlds interpretation of quantum mechanics In 1991 David Deutsch showed that quantum theory is fully consistent in the sense that the socalled density matrix can be made free of discontinuities in spacetimes with closed timelike curves However later it was shown that such a model of closed timelike curves can have internal inconsistencies as it will lead to strange phenomena like distinguishing nonorthogonal quantum states and distinguishing proper and improper mixture Accordingly the destructive positive feedback loop of virtual particles circulating through a wormhole time machine a result indicated by semiclassical calculations is averted A particle returning from the future does not return to its universe of origination but to a parallel universe This suggests that a wormhole time machine with an exceedingly short time jump is a theoretical bridge between contemporaneous parallel universes Because a wormhole timemachine introduces a type of nonlinearity into quantum theory this sort of communication between parallel universes is consistent with Joseph Polchinskis proposal of an Everett phone named after Hugh Everett in Steven Weinbergs formulation of nonlinear quantum mechanics The possibility of communication between parallel universes has been dubbed interuniversal travel Wormhole can also be depicted in a Penrose diagram of a Schwarzschild black hole In the Penrose diagram an object traveling faster than light will cross the black hole and will emerge from another end into a different space time or universe This will be an interuniversal wormhole Metrics Theories of wormhole metrics describe the spacetime geometry of a wormhole and serve as theoretical models for time travel An example of a traversable wormhole metric is the following first presented by Ellis see Ellis wormhole as a special case of the Ellis drainhole One type of nontraversable wormhole metric is the Schwarzschild solution see the first diagram The original EinsteinRosen bridge was described in an article published in July 1935 For the Schwarzschild spherically symmetric static solution where d s displaystyle ds is the proper time and c 1 displaystyle c1 If one replaces r displaystyle r with u displaystyle u according to u 2 r 2 m displaystyle u2r2m The fourdimensional space is described mathematically by two congruent parts or sheets corresponding to u 0 displaystyle u0 and u 0 displaystyle u0 which are joined by a hyperplane r 2 m displaystyle r2m or u 0 displaystyle u0 in which g displaystyle g vanishes We call such a connection between the two sheets a bridge For the combined field gravity and electricity Einstein and Rosen derived the following Schwarzschild static spherically symmetric solution where displaystyle varepsilon is the electric charge The field equations without denominators in the case when m 0 displaystyle m0 can be written In order to eliminate singularities if one replaces r displaystyle r by u displaystyle u according to the equation and with m 0 displaystyle m0 one obtains The solution is free from singularities for all finite points in the space of the two sheets In fiction Wormholes are a common element in science fiction because they allow interstellar intergalactic and sometimes even interuniversal travel within human lifetime scales In fiction wormholes have also served as a method for time travel See also Notes References Citations Sources External links What exactly is a wormhole Have wormholes been proven to exist or are they still theoretical answered by Richard F Holman William A Hiscock and Matt Visser Why wormholes by Matt Visser October 1996 Wormholes in General Relativity by Soshichi Uchii at the Wayback Machine archived February 22 2012 Questions and Answers about Wormholes A comprehensive wormhole FAQ by Enrico Rodrigo Large Hadron Collider Theory on how the collider could create a small wormhole possibly allowing time travel into the past animation that simulates traversing a wormhole Renderings and animations of a MorrisThorne wormhole NASAs current theory on wormhole creation",
  },
  {
    title: "Teleportation",
    originalContent:
      "Teleportation is the hypothetical transfer of matter or energy from one point to another without traversing the physical space between them It is a common subject in science fiction and fantasy literature Teleportation is often paired with time travel being that the traveling between the two points takes an unknown period of time sometimes being immediate An apport is a similar phenomenon featured in parapsychology and spiritualism There is no known physical mechanism that would allow for teleportation Some scientific papers and media articles describe quantum teleportation a scheme for quantum information transfer which does not allow for fasterthanlight communication Etymology The use of the term teleport to describe the hypothetical movement of material objects between one place and another without physically traversing the distance between them has been documented as early as 1878 American writer Charles Fort is credited with having coined the word teleportation in 1931 to describe the strange disappearances and appearances of anomalies which he suggested may be connected As in the earlier usage he joined the Greek prefix tele meaning remote to the root of the Latin verb portare meaning to carry Forts first formal use of the word occurred in the second chapter of his 1931 book Lo Mostly in this book I shall specialize upon indications that there exists a transportory force that I shall call Teleportation I shall be accused of having assembled lies yarns hoaxes and superstitions To some degree I think so myself To some degree I do not I offer the data Cultural references Fiction Teleportation is a common subject in science fiction literature film video games and television The use of matter transmitters in science fiction originated at least as early as the 19th century An early example of scientific teleportation as opposed to magical or spiritual teleportation is found in the 1897 novel To Venus in Five Seconds by Fred T Jane Janes protagonist is transported from a strangemachinerycontaining gazebo on Earth to planet Venus hence the title The earliest recorded story of a matter transmitter was Edward Page Mitchells The Man Without a Body in 1877 The Catholic Saint Padre Pio has documented miracles of Bilocation including a vision received by Pope John Paul II This phenomenon has also been reported throughout church history as in the New Testament with Jesus Christ where he was taken to a mountaintop and tempted by Satan Live performance Teleportation illusions have featured in live performances throughout history often under the fiction of miracles psychic phenomenon or magic The cups and balls trick has been performed since 3 BC and can involve balls vanishing reappearing teleporting and transposing objects in two locations interchanging places A common trick of closeup magic is the apparent teleportation of a small object such as a marked playing card which can involve sleightofhand misdirection and pickpocketing Magic shows were popular entertainments at fairs in the 18th century and moved into permanent theatres in the mid19th century Theatres provided greater control of the environment and viewing angles for more elaborate illusions and teleportation tricks grew in scale and ambition To increase audience excitement the teleportation illusion could be conducted under the theme of a predicament escape Magic shows achieved widespread success during the Golden Age of Magic in the late 19th and early 20th centuries Quantum teleportation Quantum teleportation is distinct from regular teleportation as it does not transfer matter from one place to another but rather transmits the quantum information necessary to prepare a microscopic target system in the same quantum state as the source system The scheme was named quantum teleportation because certain properties of the source system are recreated in the target system without any apparent quantum information carrier propagating between the two In 1993 Bennett et al proposed that a quantum state of a particle could be transferred to another distant particle without moving the two particles at all This is called quantum state teleportation There are many following theoretical and experimental papers published In 2008 M Hotta proposed that it may be possible to teleport energy by exploiting quantum energy fluctuations of an entangled vacuum state of a quantum field In 2023 zero temperature quantum energy teleportation was observed and recorded by Kazuki Ikeda for the firsttime across microscopic distances using IBM superconducting computers that are used for quantum computing In 2014 researcher Ronald Hanson and colleagues from the Technical University Delft in the Netherlands demonstrated the teleportation of information between two entangled quantumbits three metres apart A generalization of quantum mechanics suggests particles could be teleport from one place to another This is called particle teleportation With this concept superconductivity can be viewed as the teleportation of some electrons in the superconductor and superfluidity as the teleportation of some of the atoms in the cellular tube Further analysis shows that the teleportation time increases with the square root of mass and longer teleportation times require sustained quantum coherence While particle teleportation may be feasible for an electron a proton may not be feasible Philosophy Philosopher Derek Parfit used teleportation in his teletransportation paradox See also References Further reading David Darling 2005 Teleportation The Impossible Leap John Wiley Sons ISBN 9780471715450 Lawrence M Krauss 1995 The Physics of Star Trek Basic Books ISBN 9780465002047 Eric W Davis 2004 Teleportation Physics Study Air Force Research Laboratory AFRLPREDTR20030034 Bernd Thaller 2005 Advanced Visual Quantum Mechanics Springer 433 Classical teleportation is impossible pp 170171 ISBN 9780387271279 Will Human Teleportation Ever Be Possible Human teleportation is far more impractical than we thought Archived 16 October 2015 at the Wayback Machine Y Wei 2016 How to teleport a particle rather than a state Phys Rev E 93 066103",
  },
  {
    title: "Parallel universe",
    originalContent:
      "Parallel universe may refer to Science Manyworlds interpretation of quantum mechanics which implies the existence of parallel universes Multiverse the sum of all universes eg everything that exists Philosophy Possible world a construct in metaphysics to bring rigor to talk of logical possibility Modal realism an account of possible worlds according to which they are all just as real as the actual world Extended modal realism the view that all worlds possible as well as impossible are as real as the actual world Arts and media Parallel universes in fiction a hypothetical selfcontained plane of existence coexisting with ones own Alternate history a genre of fiction in which historical events differ from reality Alternative universe fan fiction fiction by fan authors that departs from the fictional universe of the source work Literature film and television Parallel Universe Red Dwarf a 1988 TV episode Parallel Universes film a 2001 British TV documentary Mirror Universe Star Trek a fictional parallel universe in the Star Trek franchise Music Parallel Universe 4hero album or the title song 1994 Parallel Universe Garnet Crow album 2010 Parallel Universe an album by Plain White Ts 2018 Parallel Universe song by the Red Hot Chili Peppers 1999 Parallel Dimensions album by Perseo Miranda 2008 See also Metaverse a collective virtual shared space Alternate reality disambiguation Multiverse disambiguation Otherworld Parallel World disambiguation",
  },
  {
    title: "Singularity",
    originalContent:
      "Singularity or singular point may refer to Science technology and mathematics Mathematics Mathematical singularity a point at which a given mathematical object is not defined or not wellbehaved for example infinite or not differentiable Geometry Singular point of a curve where the curve is not given by a smooth embedding of a parameter Singular point of an algebraic variety a point where an algebraic variety is not locally flat Rational singularity Natural sciences Singularity system theory in dynamical and social systems a context in which a small change can cause a large effect Gravitational singularity in general relativity a point in which gravity is so intense that spacetime itself becomes illdefined Initial singularity a hypothesized singularity of infinite density before quantum fluctuations caused the Big Bang and subsequent inflation that created the Universe PenroseHawking singularity theorems in general relativity theory theorems about how gravitation produces singularities such as in black holes PrandtlGlauert singularity the point at which a sudden drop in air pressure occurs Singularity climate a weather phenomenon associated with a specific calendar date Van Hove singularity in the density of states of a material Technology Singularity operating system an experimental operating system developed by Microsoft Research Mechanical singularity a position or configuration of a mechanism or a machine where the subsequent behavior cannot be predicted Singularity software a container technology that does not require root permissions to run Futurology Technological singularity a hypothetical point in time when technological growth becomes uncontrollable and irreversible Arts and entertainment Film and television Singularity a firstseason episode of Disneys So Weird Singularity Star Trek Enterprise a secondseason episode of Star Trek Enterprise The Singularity Agents of SHIELD a thirdseason episode of Agents of SHIELD Singularity a firstseason episode of Stargate SG1 The Singularity film a 2012 documentary about the technological singularity Singularity 2017 film American science fiction film starring John Cusack Singularity the working title for The Lovers starring Josh Hartnett and Bipasha Basu Godzilla Singular Point a 2021 Japanese anime surrounding Godzilla Literature Singularity Sleator novel a 1985 sciencefiction novel by William Sleator Singularity DeSmedt novel a 2004 novel by Bill DeSmedt Singularity audio drama a 2005 Doctor Who audio drama Singularity 7 a graphic novel by Ben Templesmith The Singularity Is Near a 2005 book by Ray Kurzweil on the technological singularity The Singularity Is Nearer a 2024 sequel to Ray Kurzweils 2005 book Singularity comics the Marvel Comics character The Singularity Anne Milano Appels 2024 English translation of Dino Buzzatis 1960 novel Il grande ritratto Larger than Life Music Albums Singularity Joe Morris album 2001 Singularity Peter Hammill album 2006 Singularity Mae album 2007 Singularity Robby Krieger album 2010 Singularity Northlane album 2013 Singularity Jon Hopkins album 2018 Singularity a 2014 EP by Lemaitre The Singularity Phase I Neohumanity 2014 album by Swedish metal band Scar Symmetry Singularity Lead album 2020 album Songs Singularity song performed by V of the South Korean group BTS from their 2018 album Love Yourself Tear Singularity by Believer on the 1993 album Dimensions Singularity by Caligulas Horse on the 2011 album Moments from Ephemeral City Singularity by Textures on the 2011 album Dualism Singularity by Born of Osiris on the 2011 album The Discovery Singularity by Steve Aoki Angger Dimas featuring My Name Is Kay from Its the End of the World as We Know It 2013 Singularity by Tesseract on the 2013 album Altered State Singularity by New Order on the 2015 album Music Complete Singularity by Devin Townsend on the 2019 album Empath People Singularity stage name of Greys Kyle Trewartha Video games Singularity video game a 2010 video game developed by Raven Software Endgame Singularity a 2005 video game Organizations Singularity University a California Benefit Corporation part thinktank part business incubator based on Ray Kurzweils theory of technological singularity Machine Intelligence Research Institute MIRI formerly The Singularity Institute for Artificial Intelligence SIAI Singularity Summit its annual conference See also Singular disambiguation",
  },
  {
    title: "Epistemic justification",
    originalContent:
      "Justification also called epistemic justification is a property of beliefs that fulfill certain norms about what a person should believe Epistemologists often identify justification as a component of knowledge distinguishing it from mere true opinion They study the reasons why someone holds a belief Epistemologists are concerned with various features of belief which include the ideas of warrant a proper justification for holding a belief knowledge rationality and probability among others Debates surrounding epistemic justification often involve the structure of justification including whether there are foundational justified beliefs or whether mere coherence is sufficient for a system of beliefs to qualify as justified Another major subject of debate is the sources of justification which might include perceptual experience the evidence of the senses reason and authoritative testimony among others Justification and knowledge Justification involves the reasons why someone holds a belief that one should hold based on ones current evidence Justification is a property of beliefs insofar as they are held blamelessly In other words a justified belief is a belief that a person is entitled to hold Many philosophers from Plato onward have treated justified true belief JTB as constituting knowledge It is particularly associated with a theory discussed in his dialogues Meno and Theaetetus While in fact Plato seems to disavow justified true belief as constituting knowledge at the end of Theaetetus the claim that Plato unquestioningly accepted this view of knowledge stuck until the proposal of the Gettier problem The subject of justification has played a major role in the value of knowledge as justified true belief Some contemporary epistemologists such as Jonathan Kvanvig assert that justification isnt necessary in getting to the truth and avoiding errors Kvanvig attempts to show that knowledge is no more valuable than true belief and in the process dismissed the necessity of justification due to justification not being connected to the truth Conceptions of justification William P Alston identifies two conceptions of justification 1516 One conception is deontological justification which holds that justification evaluates the obligation and responsibility of a person having only true beliefs This conception implies for instance that a person who has made his best effort but is incapable of concluding the correct belief from his evidence is still justified The deontological conception of justification corresponds to epistemic internalism Another conception is truthconducive justification which holds that justification is based on having sufficient evidence or reasons that entails that the belief is at least likely to be true The truthconductive conception of justification corresponds to epistemic externalism Theories of justification There are several different views as to what entails justification mostly focusing on the question How sure do we need to be that our beliefs correspond to the actual world Different theories of justification require different conditions before a belief can be considered justified Theories of justification generally include other aspects of epistemology such as defining knowledge Notable theories of justification include Foundationalism Basic beliefs justify other nonbasic beliefs Epistemic coherentism Beliefs are justified if they cohere with other beliefs a person holds each belief is justified if it coheres with the overall system of beliefs Infinitism Beliefs are justified by infinite chains of reasons Foundherentism Both fallible foundations and coherence are components of justificationproposed by Susan Haack Internalism and externalism The believer must be able to justify a belief through internal knowledge internalism or outside sources of knowledge externalism Reformed epistemology Beliefs are warranted by proper cognitive functionproposed by Alvin Plantinga Evidentialism Beliefs depend solely on the evidence for them Reliabilism A belief is justified if it is the result of a reliable process Infallibilism Knowledge is incompatible with the possibility of being wrong Fallibilism Claims can be accepted even though they cannot be conclusively proven or justified Nonjustificationism Knowledge is produced by attacking claims and refuting them instead of justifying them Skepticism Knowledge is impossible or undecidable Criticism of theories of justification Robert Fogelin claims to detect a suspicious resemblance between the theories of justification and Agrippas five modes leading to the suspension of belief He concludes that the modern proponents have made no significant progress in responding to the ancient modes of Pyrrhonian skepticism William P Alston criticizes the very idea of a theory of justification He claims There isnt any unique epistemically crucial property of beliefs picked out by justified Epistemologists who suppose the contrary have been chasing a willothewisp What has really been happening is this Different epistemologists have been emphasizing concentrating on pushing different epistemic desiderata different features of belief that are positively valuable from the standpoint of the aims of cognition 22 See also Dream argument Regress argument epistemology Mnchhausen trilemma References External links Stanford Encyclopedia of Philosophy entry on Foundationalist Theories of Epistemic Justification Stanford Encyclopedia of Philosophy entry on Epistemology 2 What is Justification Stanford Encyclopedia of Philosophy entry on Internalist vs Externalist Conceptions of Epistemic Justification Stanford Encyclopedia of Philosophy entry on Coherentist Theories of Epistemic Justification Internet Encyclopedia of Philosophy Internet Encyclopedia of Philosophy entry on Epistemic Justification Internet Encyclopedia of Philosophy entry on Epistemic Entitlement Internet Encyclopedia of Philosophy entry on Internalism and Externalism in Epistemology Internet Encyclopedia of Philosophy entry on Epistemic Consequentialism Internet Encyclopedia of Philosophy entry on Coherentism in Epistemology Internet Encyclopedia of Philosophy entry on Contextualism in Epistemology Internet Encyclopedia of Philosophy entry on KnowledgeFirst Theories of Justification",
  },
  {
    title: "Deontology",
    originalContent:
      "In moral philosophy deontological ethics or deontology from Greek obligation duty study is the normative ethical theory that the morality of an action should be based on whether that action itself is right or wrong under a series of rules and principles rather than based on the consequences of the action It is sometimes described as duty obligation or rulebased ethics Deontological ethics is commonly contrasted to consequentialism utilitarianism virtue ethics and pragmatic ethics In this terminology action is more important than the consequences The term deontological was first used to describe the current specialised definition by C D Broad in his 1930 book Five Types of Ethical Theory Older usage of the term goes back to Jeremy Bentham who coined it prior to 1816 as a synonym of dicastic or censorial ethics ie ethics based on judgement The more general sense of the word is retained in French especially in the term code de dontologie ethical code in the context of professional ethics Depending on the system of deontological ethics under consideration a moral obligation may arise from an external or internal source such as a set of rules inherent to the universe ethical naturalism religious law or a set of personal or cultural values any of which may be in conflict with personal desires Deontological philosophies There are numerous formulations of deontological ethics Kantianism Immanuel Kants theory of ethics is considered deontological for several different reasons First Kant argues that in order to act in the morally right way people must act from duty Pflicht Second Kant argued that it was not the consequences of actions that make them right or wrong but the motives of the person who carries out the action Kants first argument begins with the premise that the highest good must be both good in itself and good without qualification Something is good in itself when it is intrinsically good and is good without qualification when the addition of that thing never makes a situation ethically worse Kant then argues that those things that are usually thought to be good such as intelligence perseverance and pleasure fail to be either intrinsically good or good without qualification Pleasure for example appears not to be good without qualification because when people take pleasure in watching someone suffer this seems to make the situation ethically worse He concludes that there is only one thing that is truly good Nothing in the worldindeed nothing even beyond the worldcan possibly be conceived which could be called good without qualification except a good willKant then argues that the consequences of an act of willing cannot be used to determine that the person has a good will good consequences could arise by accident from an action that was motivated by a desire to cause harm to an innocent person and bad consequences could arise from an action that was wellmotivated Instead he claims a person has a good will when they act out of respect for the moral law People act out of respect for the moral law when they act in some way because they have a duty to do so Thus the only thing that is truly good in itself is a good will and a good will is only good when the willer chooses to do something because it is that persons duty ie out of respect for the law He defines respect as the concept of a worth which thwarts my selflove Kants three significant formulations of the categorical imperative a way of evaluating motivations for action are Act only according to that maxim by which you can also will that it would become a universal law Act in such a way that you always treat humanity whether in your own person or in the person of any other never simply as a means but always at the same time as an end Every rational being must so act as if he were through his maxim always a legislating member in a universal kingdom of ends Kant argued that the only absolutely good thing is a good will and so the single determining factor of whether an action is morally right is the will or motive of the person doing it If they are acting on a bad maximeg I will liethen their action is wrong even if some good consequences come of it In his essay On a Supposed Right to Lie Because of Philanthropic Concerns arguing against the position of Benjamin Constant Des ractions politiques Kant states thatHence a lie defined merely as an intentionally untruthful declaration to another man does not require the additional condition that it must do harm to another as jurists require in their definition mendacium est falsiloquium in praeiudicium alterius For a lie always harms another if not some human being then it nevertheless does harm to humanity in general inasmuch as it vitiates the very source of right Rechtsquelle All practical principles of right must contain rigorous truth This is because such exceptions would destroy the universality on account of which alone they bear the name of principles Divine command theory Although not all deontologists are religious some believe in the divine command theory which is actually a cluster of related theories that essentially state that an action is right if God has decreed that it is right According to English philosopher Ralph Cudworth William of Ockham Ren Descartes and 18thcentury Calvinists all accepted various versions of this moral theory as they all held that moral obligations arise from Gods commands The divine command theory is a form of deontology because according to it the rightness of any action depends upon that action being performed because it is a duty not because of any good consequences arising from that action If God commands people not to work on Sabbath then people act rightly if they do not work on Sabbath because God has commanded that they do not do so If they do not work on Sabbath because they are lazy then their action is not truly speaking right even though the actual physical action performed is the same If God commands not to covet a neighbours goods this theory holds that it would be immoral to do so even if coveting provides the beneficial outcome of a drive to succeed or do well One thing that clearly distinguishes Kantian deontologism from divine command deontology is that Kantianism maintains that man as a rational being makes the moral law universal whereas divine command maintains that God makes the moral law universal Rosss deontological pluralism W D Ross objects to Kants monistic deontology which bases ethics in only one foundational principle the categorical imperative He contends that there is a plurality 7 although this number is seen to vary to interpretation of prima facie duties determining what is right xii These duties are identified by W D Ross the duty of fidelity to keep promises and to tell the truth the duty of reparation to make amends for wrongful acts the duty of gratitude to return kindnesses received the duty of noninjury not to hurt others the duty of beneficence to promote the maximum of aggregate good the duty of selfimprovement to improve ones own condition the duty of justice to distribute benefits and burdens equably 215 One problem the deontological pluralist has to face is that cases can arise where the demands of one duty violate another duty socalled moral dilemmas For example there are cases where it is necessary to break a promise in order to relieve someones distress 28 Ross makes use of the distinction between prima facie duties and absolute duty to solve this problem 28 The duties listed above are prima facie duties moral actions that are required unless a greater obligation trumps them they are general principles whose validity is selfevident to morally mature personsThey are factors that do not take all considerations into account Absolute duty on the other hand is particular to one specific situation taking everything into account and has to be judged on a casebycase basis It is absolute duty that determines which acts are right or wrong Contemporary deontology Contemporary deontologists ie scholars born in the first half of the 20th century include Jzef Maria Bocheski Thomas Nagel T M Scanlon and Roger Scruton Bocheski 1965 makes a distinction between deontic and epistemic authority A typical example of epistemic authority in Bocheskis usage would be the relation of a teacher to her students A teacher has epistemic authority when making declarative sentences that the student presumes is reliable knowledge and appropriate but feels no obligation to accept or obey An example of deontic authority would be the relation between an employer and her employee An employer has deontic authority in the act of issuing an order that the employee is obliged to accept and obey regardless of its reliability or appropriateness Scruton 2017 in his book On Human Nature is critical of consequentialism and similar ethical theories such as hedonism and utilitarianism instead proposing a deontological ethical approach He implies that proportional duty and obligation are essential components of the ways in which we decide to act and he defends natural law against opposing theories He also expresses admiration for virtue ethics and believes that the two ethical theories are not as is frequently portrayed mutually exclusive Deontology and consequentialism Principle of permissible harm Frances Kamms Principle of Permissible Harm 1996 is an effort to derive a deontological constraint that coheres with our considered case judgments while also relying heavily on Kants categorical imperative The principle states that one may harm in order to save more if and only if the harm is an effect or an aspect of the greater good itself This principle is meant to address what Kamm feels are most peoples considered case judgments many of which involve deontological intuitions For instance Kamm argues that we believe it would be impermissible to kill one person to harvest his organs in order to save the lives of five others Yet we think it is morally permissible to divert a runaway trolley that would otherwise kill five innocent immobile people onto a sidetrack where only one innocent and immobile person will be killed Kamm believes the Principle of Permissible Harm explains the moral difference between these and other cases and more importantly expresses a constraint telling us exactly when we may not act to bring about good endssuch as in the organ harvesting case In 2007 Kamm published Intricate Ethics a book that presents a new theory the Doctrine of Productive Purity that incorporates aspects of her Principle of Permissible Harm Like the Principle the Doctrine of Productive Purity is an attempt to provide a deontological prescription for determining the circumstances in which people are permitted to act in a way that harms others Reconciling deontology with consequentialism Various attempts have been made to reconcile deontology with consequentialism Threshold deontology holds that rules ought to govern up to a point despite adverse consequences but when the consequences become so dire that they cross a stipulated threshold consequentialism takes over Theories put forth by Thomas Nagel and Michael S Moore attempt to reconcile deontology with consequentialism by assigning each a jurisdiction Iain Kings 2008 book How to Make Good Decisions and Be Right All the Time uses quasirealism and a modified form of utilitarianism to develop deontological principles that are compatible with ethics based on virtues and consequences King develops a hierarchy of principles to link his metaethics which is more inclined towards consequentialism with the deontological conclusions he presents in his book Secular deontology Intuitionbased deontology is a concept within secular ethics A classical example of literature on secular ethics is the Kural text authored by the ancient Tamil Indian philosopher Valluvar It can be argued that some concepts from deontological ethics date back to this text Concerning ethical intuitionism 20th century philosopher CD Broad coined the term deontological ethics to refer to the normative doctrines associated with intuitionism leaving the phrase ethical intuitionism free to refer to the epistemological doctrines See also References Bibliography Beauchamp Tom L 1991 Philosophical Ethics An Introduction to Moral Philosophy 2nd ed New York McGraw Hill Broad C D 1930 Five Types of Ethical Theory New York Harcourt Brace and Co Flew Antony 1979 Consequentialism In A Dictionary of Philosophy 2nd ed New York St Martins Kamm Frances M 1996 Morality Mortality Vol II Rights Duties and Status New York Oxford University Press 2007 Intricate Ethics Rights Responsibilities and Permissible Harm Oxford Oxford University Press ISBN 9780195189698 9780195345902 Kant Immanuel 1964 Groundwork of the Metaphysic of Morals Harper and Row Publishers Inc ISBN 9780061311598 Lgislation thique et dontologie Bruxelles Editions de Boeck Universit 2011 Karine BREHAUX ISBN 9782843715587 Olson Robert G 1967 Deontological Ethics In The Encyclopedia of Philosophy edited by P Edwards London Collier Macmillan Ross W D 1930 The Right and the Good Oxford Clarendon Press Salzman Todd A 1995 Deontology and Teleology An Investigation of the Normative Debate in Roman Catholic Moral Theology University Press Waller Bruce N 2005 Consider Ethics Theory Readings and Contemporary Issues New York Pearson Longman Wierenga Edward 1983 A Defensible Divine Command Theory Nos 173387407 External links Kantian Ethics Summary A concise summary of the key details of Kants deontology Freedom and the Boundary of Morals Lecture 22 from Stephen Palmquists book The Tree of Philosophy fourth edition 2000 Deontology and Ethical Ends Stanford Encyclopedia of Philosophy entry on Special Obligations Log in to ePortfoliosFedUni ePortfoliosFedUni Deontology framework ethics",
  },
  {
    title: "Cognitivism",
    originalContent:
      "Cognitivism may refer to Cognitivism ethics the philosophical view that ethical sentences express propositions and are capable of being true or false Cognitivism psychology a psychological approach that argues that mental function can be understood as the internal manipulation of symbols Cognitivism aesthetics a view that cognitive psychology can help understand art and the response to it Anecdotal cognitivism a psychological methodology for interpreting animal behavior in terms of mental states See also Cognition the study of the human mind Cognitive anthropology Cognitive science Computationalism Philosophy of mind Situated cognition Sociocognitive Symbol grounding",
  },
  {
    title: "Constructivism",
    originalContent:
      "Constructivism may refer to Art and architecture Constructivism art an early 20thcentury artistic movement that extols art as a practice for social purposes Constructivist architecture an architectural movement in the Soviet Union in the 1920s and 1930s British Constructivists a group of British artists who were active between 1951 to 1955 Education Constructivism philosophy of education a theory about the nature of learning that focuses on how humans make meaning from their experiences Constructivism in science education Constructivist teaching methods based on constructivist learning theory Mathematics Constructivism philosophy of mathematics a logic for founding mathematics that accepts only objects that can be effectively constructed Constructivist set theory Constructivist type theory Philosophy Constructivism philosophy of mathematics a philosophical view that asserts the necessity of constructing a mathematical object to prove that it exists Constructivism philosophy of education a theory that suggests that learners do not passively acquire knowledge through direct instruction instead they construct their understanding through experiences and social interaction integrating new information with their existing knowledge Constructivism philosophy of science a philosophical view maintaining that science consists of mental constructs created as the result of measuring the natural world Moral constructivism or ethical constructivism the view that moral facts are constructed rather than discovered Political and social sciences Constructivism international relations a theory that stresses the socially constructed character of international relations Constructivism ethnic politics a theory that ethnic identities are not unchanging entities and that political developments can shape which identities get activated Constructivist institutionalism Social constructivism the view that human development is socially situated and knowledge is constructed through interaction with others Psychology Constructivism psychological school a psychological approach that assumes that human knowledge is active and constructive See also Constructionism disambiguation Constructive theology Constructive empiricism Deconstructivism a movement of postmodern architecture from the 1980s Neuroconstructivism Transactionalism",
  },
  {
    title: "Dualism",
    originalContent:
      "Dualism most commonly refers to Mindbody dualism a philosophical view which holds that mental phenomena are at least in certain respects not physical phenomena or that the mind and the body are distinct and separable from one another Property dualism a view in the philosophy of mind and metaphysics which holds that although the world is composed of just one kind of substancethe physical kindthere exist two distinct kinds of properties physical properties and mental properties Cosmological dualism the theological or spiritual view that there are only two fundamental concepts such as good and evil and that these two concepts are in every way opposed to one another Dualism may also refer to Dualism cybernetics systems or problems in which an intelligent adversary attempts to exploit the weaknesses of the investigator Dualism Indian philosophy the belief held by certain schools of Indian philosophy that reality is fundamentally composed of two parts Dualism politics the separation of powers between the cabinet and parliament Dualism in medieval politics opposition to hierocracy medieval Epistemological dualism the epistemological question of whether the world we see around us is the real world itself or merely an internal perceptual copy of that world generated by neural processes in our brain Ethical dualism the attribution of good solely to one group of people and evil to another Monism and dualism in international law a principle in contending that international and domestic law are distinct systems of law and that international law only applies to the extent that it does not conflict with domestic law Soul dualism the belief that a person has two or more kinds of souls Media Dualism album a 2011 album by Dutch metal band Textures Dualist album a 2011 album by Taken by Cars Dualism a novel by Bill DeSmedt See also",
  },
  {
    title: "Monism",
    originalContent:
      "Monism attributes oneness or singleness Greek to a concept such as to existence Various kinds of monism can be distinguished Priority monism states that all existing things go back to a source that is distinct from them eg in Neoplatonism everything is derived from The One In this view only the One is ontologically fundamental or prior to everything else Existence monism posits that strictly speaking there exists only a single thing the universe which can only be artificially and arbitrarily divided into many things Substance monism asserts that a variety of existing things can be explained in terms of a single reality or substance Substance monism posits that only one kind of substance exists although many things may be made up of this substance eg matter or mind Dualaspect monism is the view that the mental and the physical are two aspects of or perspectives on the same substance Neutral monism believes the fundamental nature of reality to be neither mental nor physical in other words it is neutral Definitions There are two sorts of definitions for monism The wide definition a philosophy is monistic if it postulates unity of the origin of all things all existing things return to a source that is distinct from them The restricted definition this requires not only unity of origin but also unity of substance and essence Although the term monism is derived from Western philosophy to typify positions in the mindbody problem it has also been used to typify religious traditions In modern Hinduism the term absolute monism has been applied to Advaita Vedanta though Philip Renard points out that this may be a Western interpretation bypassing the intuitive understanding of a nondual reality It is more generally categorized by scholars as a form of absolute nondualism History Material monism can be traced back to the preSocratic philosophers who sought to understand the arche or basic principle of the universe in terms of different material causes These included Thales who argued that the basis of everything was water Anaximenes who claimed it was air and Heraclitus who believed it to be fire Later Parmenides described the world as One which could not change in any way Zeno of Elea defended this view of everything being a single entity through his paradoxes which aim to show the existence of time motion and space to be illusionary Baruch Spinoza argued that God or Nature Deus sive Natura is the only substance of the universe which can be referred to as either God or Nature the two being interchangeable This is because GodNature has all the possible attributes and no two substances can share an attribute which means there can be no other substances than GodNature Monism has been discussed thoroughly in Indian philosophy and Vedanta throughout their history starting as early as the Rig Veda The term monism was introduced in the 18th century by Christian von Wolff in his work Logic 1728 to designate types of philosophical thought in which the attempt was made to eliminate the dichotomy of body and mind and explain all phenomena by one unifying principle or as manifestations of a single substance The mindbody problem in philosophy examines the relationship between mind and matter and in particular the relationship between consciousness and the brain The problem was addressed by Ren Descartes in the 17th century resulting in Cartesian dualism and by preAristotelian philosophers in Avicennian philosophy and in earlier Asian and more specifically Indian traditions Monism was later also applied to the theory of absolute identity set forth by Hegel and Schelling Thereafter the term was more broadly used for any theory postulating a unifying principle The opponent thesis of dualism also was broadened to include pluralism According to Urmson as a result of this extended use the term is systematically ambiguous According to Jonathan Schaffer monism lost popularity due to the emergence of analytic philosophy in the early twentieth century which revolted against the neoHegelians Rudolf Carnap and A J Ayer who were strong proponents of positivism ridiculed the whole question as incoherent mysticism The mindbody problem has reemerged in social psychology and related fields with the interest in mindbody interaction and the rejection of Cartesian mindbody dualism in the identity thesis a modern form of monism Monism is also still relevant to the philosophy of mind where various positions are defended Types Different types of monism include Substance monism the view that the apparent plurality of substances is due to different states or appearances of a single substance Attributive monism the view that whatever the number of substances they are of a single ultimate kind Epistemological monism where ultimately everything that can be thought observed and engaged shares one conceptual system of interaction however complex Partial monism within a given realm of being however many there may be there is only one substance Existence monism the view that there is only one concrete object token The One or the Monad Priority monism the whole is prior to its parts or the world has parts but the parts are dependent fragments of an integrated whole Property monism the view that all properties are of a single type eg only physical properties exist Genus monism the doctrine that there is a highest category eg being Views contrasting with monism are Metaphysical dualism which asserts that there are two ultimately irreconcilable substances or realities such as Good and Evil for example Gnosticism and Manichaeism Metaphysical pluralism which asserts three or more fundamental substances or realities Metaphysical nihilism negates any of the above categories substances properties concrete objects etc Monism in modern philosophy of mind can be divided into three broad categories Certain positions do not fit easily into the above categories such as functionalism anomalous monism and reflexive monism Moreover they do not define the meaning of real Monistic philosophers PreSocratic While the lack of information makes it difficult in some cases to be sure of the details the following preSocratic philosophers thought in monistic terms Thales Water Anaximander Apeiron meaning the undefined infinite Reality is some one thing but we cannot know what Anaximenes of Miletus Air Heraclitus Change symbolized by fire in that everything is in constant flux Parmenides Being or Reality is an unmoving perfect sphere unchanging undivided PostSocrates Neopythagorians such as Apollonius of Tyana centered their cosmologies on the Monad or One Stoics taught that there is only one substance identified as God Middle Platonism under such works as those by Numenius taught that the Universe emanates from the Monad or One Neoplatonism is monistic Plotinus taught that there was an ineffable transcendent god The One of which subsequent realities were emanations From The One emanates the Divine Mind Nous the Cosmic Soul Psyche and the World Cosmos Modern Monistic neuroscientists Gyrgy Buzski Francis Crick Karl Friston Eric Kandel Mark Solms Rodolfo Llinas Ivan Pavlov Roger Sperry Religion Pantheism Pantheism is the belief that everything composes an allencompassing immanent God or that the universe or nature is identical with divinity Pantheists thus do or do not believe in a personal or anthropomorphic god but believe that interpretations of the term differ Pantheism was popularized in the modern era as both a theology and philosophy based on the work of the 17thcentury philosopher Baruch Spinoza whose Ethics was an answer to Descartes famous dualist theory that the body and spirit are separate Spinoza held that the two are the same and this monism is a fundamental quality of his philosophy He was described as a Godintoxicated man and used the word God to describe the unity of all substance Although the term pantheism was not coined until after his death Spinoza is regarded as its most celebrated advocate H P Owen claimed that Pantheists are monists they believe that there is only one Being and that all other forms of reality are either modes or appearances of it or identical with it Pantheism is closely related to monism as pantheists too believe all of reality is one substance called Universe God or Nature Panentheism a slightly different concept is explained below in the next section Some of the most famous pantheists are the Stoics Giordano Bruno and Spinoza Panentheism Panentheism from Greek pn all en in and thes God allinGod is a belief system that posits that the divine be it a monotheistic God polytheistic gods or an eternal cosmic animating force interpenetrates every part of nature but is not one with nature Panentheism differentiates itself from pantheism which holds that the divine is synonymous with the universe In panentheism there are two types of substance pan the universe and God The universe and the divine are not ontologically equivalent God is viewed as the eternal animating force within the universe In some forms of panentheism the cosmos exists within God who in turn transcends pervades or is in the cosmos While pantheism asserts that All is God panentheism claims that God animates all of the universe and also transcends the universe In addition some forms indicate that the universe is contained within God like in the Judaic concept of Tzimtzum Much Hindu thought is highly characterized by panentheism and pantheism Paul Tillich has argued for such a concept within Christian theology as has liberal biblical scholar Marcus Borg and mystical theologian Matthew Fox an Episcopal priest Pandeism Pandeism or pandeism from Ancient Greek romanized pan lit all and Latin deus meaning god in the sense of deism is a term describing beliefs coherently incorporating or mixing logically reconcilable elements of pantheism that God or a metaphysically equivalent creator deity is identical to Nature and classical deism that the creatorgod who designed the universe no longer exists in a status where it can be reached and can instead be confirmed only by reason It is therefore most particularly the belief that the creator of the universe actually became the universe and so ceased to exist as a separate entity Through this synergy pandeism claims to answer primary objections to deism why would God create and then not interact with the universe and to pantheism how did the universe originate and what is its purpose Indian and East Asian religions Characteristics The central problem in Asian religious philosophy is not the bodymind problem but the search for an unchanging Real or Absolute beyond the world of appearances and changing phenomena and the search for liberation from dukkha and the liberation from the cycle of rebirth In Hinduism substanceontology prevails seeing Brahman as the unchanging real beyond the world of appearances In Buddhism process ontology is prevalent seeing reality as empty of an unchanging essence Characteristic for various Asian philosophy technology and religions is the discernment of levels of truth an emphasis on intuitiveexperiential understanding of the Absolute such as jnana bodhi and jianxing Chinese and the technology of yin and yang used within East Asian medicine with an emphasis on the integration of these levels of truth and its understanding Hinduism Vedanta Vedanta is the inquiry into and systematisation of the Vedas and Upanishads to harmonise the various and contrasting ideas that can be found in those texts Within Vedanta different schools exist Vishishtadvaita qualified monism is from the school of Ramanuja Shuddhadvaita inessence monism is the school of Vallabha Dvaitadvaita differential monism is a school founded by Nimbarka Achintya Bheda Abheda a school of Vedanta founded by Chaitanya Mahaprabhu representing the philosophy of inconceivable oneness and difference It can be understood as an integration of the strict dualist dvaita theology of Madhvacharya and the qualified monism vishishtadvaita of Ramanuja Modern Hinduism The colonisation of India by the British had a major impact on Hindu society In response leading Hindu intellectuals started to study western culture and philosophy integrating several western notions into Hinduism This modernised Hinduism at its turn has gained popularity in the west A major role was played in the 19th century by Swami Vivekananda in the revival of Hinduism and the spread of Advaita Vedanta to the west via the Ramakrishna Mission His interpretation of Advaita Vedanta has been called NeoVedanta In Advaita Shankara suggests meditation and Nirvikalpa Samadhi are means to gain knowledge of the already existing unity of Brahman and Atman not the highest goal itself Yoga is a meditative exercise of withdrawal from the particular and identification with the universal leading to contemplation of oneself as the most universal namely Consciousness This approach is different from the classical Yoga of complete thought suppression Vivekananda according to Gavin Flood was a figure of great importance in the development of a modern Hindu selfunderstanding and in formulating the Wests view of Hinduism Central to his philosophy is the idea that the divine exists in all beings that all human beings can achieve union with this innate divinity and that seeing this divine as the essence of others will further love and social harmony According to Vivekananda there is an essential unity to Hinduism which underlies the diversity of its many forms According to Flood Vivekanandas view of Hinduism is the most common among Hindus today This monism according to Flood is at the foundation of earlier Upanishads to theosophy in the later Vedanta tradition and in modern NeoHinduism Buddhism According to the Pli Canon both pluralism nnatta and monism ekatta are speculative views A Theravada commentary notes that the former is similar to or associated with nihilism ucchdavda and the latter is similar to or associated with eternalism sassatavada Levels of truth Within Buddhism a rich variety of philosophical and pedagogical models can be found Various schools of Buddhism discern levels of truth The Two truths doctrine of the Madhyamaka The Three Natures of the Yogacara EssenceFunction or Absoluterelative in Chinese and Korean Buddhism The Trikayaformule consisting of The Dharmakya or Truth body which embodies the very principle of enlightenment and knows no limits or boundaries The Sambhogakya or body of mutual enjoyment which is a body of bliss or clear light manifestation The Nirmakya or created body which manifests in time and space The Prajnaparamitasutras and Madhyamaka emphasize the nonduality of form and emptiness form is emptiness emptiness is form as the heart sutra says In Chinese Buddhism this was understood to mean that ultimate reality is not a transcendental realm but equal to the daily world of relative reality This idea was wellsituated for the existing Chinese culture which emphasized the mundane world and society But this does not tell how the absolute is present in the relative world To deny the duality of samsara and nirvana as the Perfection of Wisdom does or to demonstrate logically the error of dichotomizing conceptualization as Nagarjuna does is not to address the question of the relationship between samsara and nirvana or in more philosophical terms between phenomenal and ultimate reality What then is the relationship between these two realms This question is answered in such schemata as the Five Ranks of Tozan the Oxherding Pictures and Hakuins Four ways of knowing Sikhism Sikhism complies with the concept of Absolute Monism Sikh philosophy advocates that all that our senses comprehend is an illusion God is the ultimate reality Forms being subject to time shall pass away Gods Reality alone is eternal and abiding The thought is that Atma soul is born from and a reflection of ParamAtma Supreme Soul and will again merge into it in the words of the fifth guru of Sikhs Guru Arjan just as water merges back into the water God and Soul are fundamentally the same identical in the same way as Fire and its sparks Atam meh Ram Ram meh Atam which means The Ultimate Eternal reality resides in the Soul and the Soul is contained in Him As from one stream millions of waves arise and yet the waves made of water again become water in the same way all souls have sprung from the Universal Being and would blend again into it Abrahamic faiths Judaism Jewish thought considers God as separate from all physical created things and as existing outside of time According to Maimonides God is an incorporeal being that caused all other existence According to Maimonides to admit corporeality to God is tantamount to admitting complexity to God which is a contradiction to God as the first cause and constitutes heresy While Hasidic mystics considered the existence of the physical world a contradiction to Gods simpleness Maimonides saw no contradiction According to Hasidic thought particularly as propounded by the 18th century early 19thcentury founder of Chabad Shneur Zalman of Liadi God is held to be immanent within creation for two interrelated reasons A very strong Jewish belief is that the Divine lifeforce which brings the universe into existence must constantly be present were this lifeforce to forsake the universe for even one brief moment it would revert to a state of utter nothingness as before the creation Simultaneously Judaism holds as axiomatic that God is an absolute unity and that he is perfectly simple thus if his sustaining power is within nature then his essence is also within nature The Vilna Gaon was very much against this philosophy for he felt that it would lead to pantheism and heresy According to some this is the main reason for the Gaons ban on Chasidism Christianity Creatorcreature distinction Christians maintain that God created the universe ex nihilo and not from his own substance so that the creator is not to be confused with creation but rather transcends it There is a movement of Christian Panentheism Rejection of radical dualism In On Free Choice of the Will Augustine argued in the context of the problem of evil that evil is not the opposite of good but rather merely the absence of good something that does not have existence in itself Likewise C S Lewis described evil as a parasite in Mere Christianity as he viewed evil as something that cannot exist without good to provide it with existence Lewis went on to argue against dualism from the basis of moral absolutism and rejected the dualistic notion that God and Satan are opposites arguing instead that God has no equal hence no opposite Lewis rather viewed Satan as the opposite of Michael the archangel Due to this Lewis instead argued for a more limited type of dualism Other theologians such as Greg Boyd have argued in more depth that the Biblical authors held a limited dualism meaning that God and Satan do engage in real battle but only due to free will given by God for the duration that God allows Mormonism Latter Day Saint theology also expresses a form of dualaspect monism via materialism and eternalism claiming that creation was ex materia as opposed to ex nihilo in conventional Christianity as expressed by Parley Pratt and echoed in view by the movements founder Joseph Smith making no distinction between the spiritual and the material these being not just similarly eternal but ultimately two manifestations of the same reality or substance Parley Pratt implies a vitalism paired with evolutionary adaptation noting these eternal selfexisting elements possess in themselves certain inherent properties or attributes in a greater or less degree or in other words they possess intelligence adapted to their several spheres Parley Pratts view is also similar to Gottfried Leibnizs monadology which holds that reality consists of mind atoms that are living centers of force Brigham Young anticipates a protomentality of elementary particles with his vitalist view there is life in all matter throughout the vast extent of all the eternities it is in the rock the sand the dust in water air the gases and in short in every description and organization of matter whether it be solid liquid or gaseous particle operating with particle The LDS conception of matter is essentially dynamic rather than static if indeed it is not a kind of living energy and that it is subject at least to the rule of intelligence John A Widstoe held a similar more vitalist view that Life is nothing more than matter in motion that therefore all matter possess a kind of life Matter is intelligence hence everything in the universe is alive However Widstoe resisted outright affirming a belief in panpsychism Islam Quran Vincent Cornell argues that the Quran provides a monist image of God by describing reality as a unified whole with God being a single concept that would describe or ascribe all existing things But most argue that Abrahamic religious scriptures especially the Quran see creation and God as two separate existences It explains that everything has been created by God and is under his control but at the same time distinguishes creation as being dependent on the existence of God Sufism Some Sufi mystics advocate monism One of the most notable being the 13thcentury Persian poet Rumi 12071273 in his didactic poem Masnavi espoused monism Rumi says in the Masnavi In the shop for Unity wahdat anything that you see there except the One is an idol Other Sufi mystics however such as Ahmad Sirhindi upheld dualistic Monotheism the separation of God and the Universe The most influential of the Islamic monists was the Sufi philosopher Ibn Arabi 11651240 He developed the concept of unity of being Arabic wadat alwujd which some argue is a monistic philosophy Born in alAndalus he made an enormous impact on the Muslim world where he was crowned the great Master In the centuries following his death his ideas became increasingly controversial Ahmad Sirhindi criticised monistic understanding of unity of being advocating the dualisticcompatible unity of witness Arabic wahdat ashshuhud maintaining separation of creator and creation Later Shah Waliullah Dehlawi reconciled the two ideas maintaining that their differences are semantic differences arguing that the universal existence which is different in creation to creator and the divine essence are different and that the universal existence emanates in a nonplatonic sense from the divine essence and that the relationship between them is similar to the relationship between the number four and a number being even Shiism The doctrine of wadat alwujd also enjoys considerable following in the rationalist philosophy of Twelver Shiism with the most famous modernday adherent being Ruhollah Khomeini Bah Faith Although the teachings of the Bah Faith have a strong emphasis on social and ethical issues there exist a number of foundational texts that have been described as mystical Some of these include statements of a monist nature eg The Seven Valleys and the Hidden Words The differences between dualist and monist views are reconciled by the teaching that these opposing viewpoints are caused by differences in the observers themselves not in that which is observed This is not a higher truthlower truth position God is unknowable For man it is impossible to acquire any direct knowledge of God or the Absolute because any knowledge that one has is relative See also Cosmic pluralism Dialectical monism Henosis Holism Indefinite monism Neoplatonism Material monism Monadology Monistic idealism Ontological pluralism Realistic monism Sikhism Taoism Univocity of being Wuji Notes References Sources Further reading External links Jonathan Schaeffer Monism In Zalta Edward N ed Stanford Encyclopedia of Philosophy Monism at PhilPapers Monism at the Indiana Philosophy Ontology Project Catholic Encyclopedia Monism Hinduisms Online Lexicon search for Monism The Monist",
  },
  {
    title: "Materialism",
    originalContent:
      "Materialism is a form of philosophical monism which holds that matter is the fundamental substance in nature and that all things including mental states and consciousness are results of material interactions of material things According to philosophical materialism mind and consciousness are caused by physical processes such as the neurochemistry of the human brain and nervous system without which they cannot exist Materialism directly contrasts with monistic idealism according to which consciousness is the fundamental substance of nature Materialism is closely related to physicalismthe view that all that exists is ultimately physical Philosophical physicalism has evolved from materialism with the theories of the physical sciences to incorporate forms of physicality in addition to ordinary matter eg spacetime physical energies and forces and exotic matter Thus some prefer the term physicalism to materialism while others use the terms as if they were synonymous Discoveries of neural correlates between consciousness and the brain are taken as empirical support for materialism but some philosophers of mind find that association fallacious or consider it compatible with nonmaterialist ideas Alternative philosophies opposed or alternative to materialism or physicalism include idealism pluralism dualism panpsychism and other forms of monism Epicureanism is a philosophy of materialism from classical antiquity that was a major forerunner of modern science Though ostensibly a deist Epicurus affirmed the literal existence of the Greek gods in either some type of celestial heaven cognate from which they ruled the universe if not on a literal Mount Olympus and his philosophy promulgated atomism while Platonism taught roughly the opposite despite Platos teaching of Zeus as God Overview Materialism belongs to the class of monist ontology and is thus different from ontological theories based on dualism or pluralism For singular explanations of the phenomenal reality materialism is in contrast to idealism neutral monism and spiritualism It can also contrast with phenomenalism vitalism and dualaspect monism Its materiality can in some ways be linked to the concept of determinism as espoused by Enlightenment thinkers Despite the large number of philosophical schools and their nuances all philosophies are said to fall into one of two primary categories defined in contrast to each other idealism and materialisma The basic proposition of these two categories pertains to the nature of reality the primary difference between them is how they answer two fundamental questionswhat reality consists of and how it originated To idealists spirit or mind or the objects of mind ideas are primary and matter secondary To materialists matter is primary and mind or spirit or ideas are secondarythe product of matter acting upon matter The materialist view is perhaps best understood in its opposition to the doctrines of immaterial substance applied to the mind historically by Ren Descartes by itself materialism says nothing about how material substance should be characterized In practice it is frequently assimilated to one variety of physicalism or another Modern philosophical materialists extend the definition of other scientifically observable entities such as energy forces and the spacetime continuum some philosophers such as Mary Midgley suggest that the concept of matter is elusive and poorly defined During the 19th century Karl Marx and Friedrich Engels extended the concept of materialism to elaborate a materialist conception of history centered on the roughly empirical world of human activity practice including labor and the institutions created reproduced or destroyed by that activity They also developed dialectical materialism by taking Hegelian dialectics stripping them of their idealist aspects and fusing them with materialism see Modern philosophy Nonreductive materialism Materialism is often associated with reductionism according to which the objects or phenomena individuated at one level of description if they are genuine must be explicable in terms of the objects or phenomena at some other level of descriptiontypically at a more reduced level Nonreductive materialism explicitly rejects this notion taking the material constitution of all particulars to be consistent with the existence of real objects properties or phenomena not explicable in the terms canonically used for the basic material constituents Jerry Fodor held this view according to which empirical laws and explanations in special sciences like psychology or geology are invisible from the perspective of basic physics History Early history Before Common Era Materialism developed possibly independently in several geographically separated regions of Eurasia during what Karl Jaspers termed the Axial Age c 800200 BC In ancient Indian philosophy materialism developed around 600 BC with the works of Ajita Kesakambali Payasi Kanada and the proponents of the Crvka school of philosophy Kanada became one of the early proponents of atomism The NyayaVaisesika school c 600100 BC developed one of the earliest forms of atomism although their proofs of God and their positing that consciousness was not material precludes labelling them as materialists Buddhist atomism and the Jaina school continued the atomic tradition Ancient Greek atomists like Leucippus Democritus and Epicurus prefigure later materialists The Latin poem De Rerum Natura by Lucretius 99 c 55 BC reflects the mechanistic philosophy of Democritus and Epicurus According to this view all that exists is matter and void and all phenomena result from different motions and conglomerations of base material particles called atoms literally indivisibles De Rerum Natura provides mechanistic explanations for phenomena such as erosion evaporation wind and sound Famous principles like nothing can touch body but body first appeared in Lucretiuss work Democritus and Epicurus did not espouse a monist ontology instead espousing the ontological separation of matter and space ie that space is another kind of being Early Common Era Wang Chong 27 c 100 AD was a Chinese thinker of the early Common Era said to be a materialist Later Indian materialist Jayaraashi Bhatta 6th century in his work Tattvopaplavasimha The Upsetting of All Principles refuted the Nyya Stra epistemology The materialistic Crvka philosophy appears to have died out some time after 1400 when Madhavacharya compiled Sarvadaranasamgraha A Digest of All Philosophies in the 14th century he had no Crvka or Lokyata text to quote from or refer to In early 12thcentury alAndalus Arabian philosopher Ibn Tufail aka Abubacer discussed materialism in his philosophical novel Hayy ibn Yaqdhan Philosophus Autodidactus while vaguely foreshadowing historical materialism Modern philosophy In France Pierre Gassendi 15921665 represented the materialist tradition in opposition to the attempts of Ren Descartes 15961650 to provide the natural sciences with dualist foundations There followed the materialist and atheist abb Jean Meslier 16641729 along with the French materialists Julien Offray de La Mettrie 17091751 Denis Diderot 17131784 tienne Bonnot de Condillac 17141780 Claude Adrien Helvtius 17151771 GermanFrench Baron dHolbach 17231789 and other French Enlightenment thinkers In England materialism was developed in the philosophies of Francis Bacon 15611626 Thomas Hobbes 15881679 and John Locke 16321704 Scottish Enlightenment philosopher David Hume 17111776 became one of the most important materialist philosophers in the 18th century John Walking Stewart 17471822 believed matter has a moral dimension which had a major impact on the philosophical poetry of William Wordsworth 17701850 In late modern philosophy German atheist anthropologist Ludwig Feuerbach signaled a new turn in materialism in his 1841 book The Essence of Christianity which presented a humanist account of religion as the outward projection of mans inward nature Feuerbach introduced anthropological materialism a version of materialism that views materialist anthropology as the universal science Feuerbachs variety of materialism heavily influenced Karl Marx who in the late 19th century elaborated the concept of historical materialismthe basis for what Marx and Friedrich Engels outlined as scientific socialism The materialist conception of history starts from the proposition that the production of the means to support human life and next to production the exchange of things produced is the basis of all social structure that in every society that has appeared in history the manner in which wealth is distributed and society divided into classes or orders is dependent upon what is produced how it is produced and how the products are exchanged From this point of view the final causes of all social changes and political revolutions are to be sought not in mens brains not in mens better insights into eternal truth and justice but in changes in the modes of production and exchange They are to be sought not in the philosophy but in the economics of each particular epoch Through his Dialectics of Nature 1883 Engels later developed a materialist dialectic philosophy of nature a worldview that Georgi Plekhanov the father of Russian Marxism called dialectical materialism In early 20thcentury Russian philosophy Vladimir Lenin further developed dialectical materialism in his 1909 book Materialism and Empiriocriticism which connects his opponents political conceptions to their antimaterialist philosophies A more naturalistoriented materialist school of thought that developed in the mid19th century was German materialism which included Ludwig Bchner 18241899 the Dutchborn Jacob Moleschott 18221893 and Carl Vogt 18171895 even though they had different views on core issues such as the evolution and the origins of life Contemporary history Analytic philosophy Contemporary analytic philosophers eg Daniel Dennett Willard Van Orman Quine Donald Davidson and Jerry Fodor operate within a broadly physicalist or scientific materialist framework producing rival accounts of how best to accommodate the mind including functionalism anomalous monism and identity theory Scientific materialism is often synonymous with and has typically been described as a reductive materialism In the early 21st century Paul and Patricia Churchland advocated a radically contrasting position at least in regard to certain hypotheses eliminative materialism Eliminative materialism holds that some mental phenomena simply do not exist at all and that talk of such phenomena reflects a spurious folk psychology and introspection illusion A materialist of this variety might believe that a concept like belief has no basis in fact eg the way folk science speaks of demoncaused illnesses With reductive materialism at one end of a continuum our theories will reduce to facts and eliminative materialism at the other certain theories will need to be eliminated in light of new facts revisionary materialism is somewhere in the middle Continental philosophy Contemporary continental philosopher Gilles Deleuze has attempted to rework and strengthen classical materialist ideas Contemporary theorists such as Manuel DeLanda working with this reinvigorated materialism have come to be classified as new materialists New materialism has become its own subfield with courses on it at major universities as well as numerous conferences edited collections and monographs devoted to it Jane Bennetts 2010 book Vibrant Matter has been particularly instrumental in bringing theories of monist ontology and vitalism back into a critical theoretical fold dominated by poststructuralist theories of language and discourse Scholars such as Mel Y Chen and Zakiyyah Iman Jackson have critiqued this body of new materialist literature for neglecting to consider the materiality of race and gender in particular Mtis scholar Zoe Todd as well as Mohawk Bear Clan Six Nations and Anishinaabe scholar Vanessa Watts query the colonial orientation of the race for a new materialism Watts in particular describes the tendency to regard matter as a subject of feminist or philosophical care as a tendency too invested in the reanimation of a Eurocentric tradition of inquiry at the expense of an Indigenous ethic of responsibility Other scholars such as Helene Vosters echo their concerns and have questioned whether there is anything particularly new about new materialism as Indigenous and other animist ontologies have attested to what might be called the vibrancy of matter for centuries Others such as Thomas Nail have critiqued vitalist versions of new materialism for depoliticizing flat ontology and being ahistorical Quentin Meillassoux proposed speculative materialism a postKantian return to David Hume also based on materialist ideas Defining matter The nature and definition of matterlike other key concepts in science and philosophyhave occasioned much debate Is there a single kind of matter hyle that everything is made of or are there multiple kinds Is matter a continuous substance capable of expressing multiple forms hylomorphism or a number of discrete unchanging constituents atomism Does matter have intrinsic properties substance theory or lack them prima materia One challenge to the conventional concept of matter as tangible stuff came with the rise of field physics in the 19th century Relativity shows that matter and energy including the spatially distributed energy of fields are interchangeable This enables the ontological view that energy is prima materia and matter is one of its forms In contrast the Standard Model of particle physics uses quantum field theory to describe all interactions On this view it could be said that fields are prima materia and the energy is a property of the field According to the dominant cosmological model the LambdaCDM model less than 5 of the universes energy density is made up of the matter the Standard Model describes and most of the universe is composed of dark matter and dark energy with little agreement among scientists about what these are made of With the advent of quantum physics some scientists believed the concept of matter had merely changed while others believed the conventional position could no longer be maintained Werner Heisenberg said The ontology of materialism rested upon the illusion that the kind of existence the direct actuality of the world around us can be extrapolated into the atomic range This extrapolation however is impossibleatoms are not things The concept of matter has changed in response to new scientific discoveries Thus materialism has no definite content independent of the particular theory of matter on which it is based According to Noam Chomsky any property can be considered material if one defines matter such that it has that property The philosophical materialist Gustavo Bueno uses a more precise term than matter the stroma In Materialism and EmpirioCriticism Lenin argues that the truth of dialectical materialism is unrelated to any particular understanding of matter To him such changes actually confirm the dialectical form of materialism Physicalism George Stack distinguishes between materialism and physicalism In the twentieth century physicalism has emerged out of positivism Physicalism restricts meaningful statements to physical bodies or processes that are verifiable or in principle verifiable It is an empirical hypothesis that is subject to revision and hence lacks the dogmatic stance of classical materialism Herbert Feigl defended physicalism in the United States and consistently held that mental states are brain states and that mental terms have the same referent as physical terms The twentieth century has witnessed many materialist theories of the mental and much debate surrounding them But not all conceptions of physicalism are tied to verificationist theories of meaning or direct realist accounts of perception Rather physicalists believe that no element of reality is missing from the mathematical formalism of our best description of the world Materialist physicalists also believe that the formalism describes fields of insentience In other words the intrinsic nature of the physical is nonexperiential Religious and spiritual views Christianity Hinduism and Transcendental Club Most Hindus and transcendentalists regard all matter as an illusion or maya blinding humans from the truth Transcendental experiences like the perception of Brahman are considered to destroy the illusion Criticism and alternatives From contemporary physicists Rudolf Peierls a physicist who played a major role in the Manhattan Project rejected materialism The premise that you can describe in terms of physics the whole function of a human being including knowledge and consciousness is untenable There is still something missing Erwin Schrdinger said Consciousness cannot be accounted for in physical terms For consciousness is absolutely fundamental It cannot be accounted for in terms of anything else Werner Heisenberg wrote The ontology of materialism rested upon the illusion that the kind of existence the direct actuality of the world around us can be extrapolated into the atomic range This extrapolation however is impossible Atoms are not things Quantum mechanics Some 20thcentury physicists eg Eugene Wigner and Henry Stapp and some modern physicists and science writers eg Stephen Barr Paul Davies and John Gribbin have argued that materialism is flawed due to certain recent findings in physics such as quantum mechanics and chaos theory According to Gribbin and Davies 1991 Then came our Quantum theory which totally transformed our image of matter The old assumption that the microscopic world of atoms was simply a scaleddown version of the everyday world had to be abandoned Newtons deterministic machine was replaced by a shadowy and paradoxical conjunction of waves and particles governed by the laws of chance rather than the rigid rules of causality An extension of the quantum theory goes beyond even this it paints a picture in which solid matter dissolves away to be replaced by weird excitations and vibrations of invisible field energy Quantum physics undermines materialism because it reveals that matter has far less substance than we might believe But another development goes even further by demolishing Newtons image of matter as inert lumps This development is the theory of chaos which has recently gained widespread attention Digital physics The objections of Davies and Gribbin are shared by proponents of digital physics who view information rather than matter as fundamental The physicist and proponent of digital physics John Archibald Wheeler wrote all matter and all things physical are informationtheoretic in origin and this is a participatory universe Some founders of quantum theory such as Max Planck shared their objections He wrote As a man who has devoted his whole life to the most clear headed science to the study of matter I can tell you as a result of my research about atoms this much There is no matter as such All matter originates and exists only by virtue of a force which brings the particle of an atom to vibration and holds this most minute solar system of the atom together We must assume behind this force the existence of a conscious and intelligent Mind This Mind is the matrix of all matter James Jeans concurred with Planck saying The Universe begins to look more like a great thought than like a great machine Mind no longer appears to be an accidental intruder into the realm of matter Philosophical objections In the Critique of Pure Reason Immanuel Kant argued against materialism in defending his transcendental idealism as well as offering arguments against subjective idealism and mindbody dualism But Kant argues that change and time require an enduring substrate Postmodernpoststructuralist thinkers also express skepticism about any allencompassing metaphysical scheme Philosopher Mary Midgley argues that materialism is a selfrefuting idea at least in its eliminative materialist form Varieties of idealism Arguments for idealism such as those of Hegel and Berkeley often take the form of an argument against materialism indeed Berkeleys idealism was called immaterialism Now matter can be argued to be redundant as in bundle theory and mindindependent properties can in turn be reduced to subjective percepts Berkeley gives an example of the latter by pointing out that it is impossible to gather direct evidence of matter as there is no direct experience of matter all that is experienced is perception whether internal or external As such matters existence can only be inferred from the apparent perceived stability of perceptions it finds absolutely no evidence in direct experience If matter and energy are seen as necessary to explain the physical world but incapable of explaining mind dualism results Emergence holism and process philosophy seek to ameliorate the perceived shortcomings of traditional especially mechanistic materialism without abandoning materialism entirely Materialism as methodology Some critics object to materialism as part of an overly skeptical narrow or reductivist approach to theorizing rather than to the ontological claim that matter is the only substance Particle physicist and Anglican theologian John Polkinghorne objects to what he calls promissory materialismclaims that materialistic science will eventually succeed in explaining phenomena it has not so far been able to explain Polkinghorne prefers dualaspect monism to materialism Some scientific materialists have been criticized for failing to provide clear definitions of matter leaving the term materialism without any definite meaning Noam Chomsky states that since the concept of matter may be affected by new scientific discoveries as has happened in the past scientific materialists are being dogmatic in assuming the opposite See also Notes References Further reading External links Materialism Encyclopdia Britannica Vol 17 11th ed 1911 Stanford Encyclopedia Physicalism Eliminative Materialism Philosophical Materialism by Richard C Vitzthum from infidelsorg Dictionary of the Philosophy of Mind on Materialism from the University of Waterloo A new theory of ideomaterialism being a synthesis of idealism and materialism",
  },
  {
    title: "Intelligence",
    originalContent:
      "Intelligence has been defined in many ways the capacity for abstraction logic understanding selfawareness learning emotional knowledge reasoning planning creativity critical thinking and problemsolving It can be described as the ability to perceive or infer information and to retain it as knowledge to be applied to adaptive behaviors within an environment or context The term rose to prominence during the early 1900s Most psychologists believe that intelligence can be divided into various domains or competencies Intelligence has been longstudied in humans and across numerous disciplines It has also been observed in the cognition of nonhuman animals Some researchers have suggested that plants exhibit forms of intelligence though this remains controversial Intelligence in computers or other machines is called artificial intelligence Etymology The word intelligence derives from the Latin nouns intelligentia or intellctus which in turn stem from the verb intelligere to comprehend or perceive In the Middle Ages the word intellectus became the scholarly technical term for understanding and a translation for the Greek philosophical term nous This term however was strongly linked to the metaphysical and cosmological theories of teleological scholasticism including theories of the immortality of the soul and the concept of the active intellect also known as the active intelligence This approach to the study of nature was strongly rejected by early modern philosophers such as Francis Bacon Thomas Hobbes John Locke and David Hume all of whom preferred understanding in place of intellectus or intelligence in their English philosophical works Hobbes for example in his Latin De Corpore used intellectus intelligit translated in the English version as the understanding understandeth as a typical example of a logical absurdity Intelligence has therefore become less common in English language philosophy but it has later been taken up with the scholastic theories that it now implies in more contemporary psychology Definitions There is controversy over how to define intelligence Scholars describe its constituent abilities in various ways and differ in the degree to which they conceive of intelligence as quantifiable A consensus report called Intelligence Knowns and Unknowns published in 1995 by the Board of Scientific Affairs of the American Psychological Association states Individuals differ from one another in their ability to understand complex ideas to adapt effectively to the environment to learn from experience to engage in various forms of reasoning to overcome obstacles by taking thought Although these individual differences can be substantial they are never entirely consistent a given persons intellectual performance will vary on different occasions in different domains as judged by different criteria Concepts of intelligence are attempts to clarify and organize this complex set of phenomena Although considerable clarity has been achieved in some areas no such conceptualization has yet answered all the important questions and none commands universal assent Indeed when two dozen prominent theorists were recently asked to define intelligence they gave two dozen somewhat different definitions Psychologists and learning researchers also have suggested definitions of intelligence such as the following Human Human intelligence is the intellectual power of humans which is marked by complex cognitive feats and high levels of motivation and selfawareness Intelligence enables humans to remember descriptions of things and use those descriptions in future behaviors It gives humans the cognitive abilities to learn form concepts understand and reason including the capacities to recognize patterns innovate plan solve problems and employ language to communicate These cognitive abilities can be organized into frameworks like fluid vs crystallized and the Unified CattellHornCarroll model which contains abilities like fluid reasoning perceptual speed verbal abilities and others Intelligence is different from learning Learning refers to the act of retaining facts and information or abilities and being able to recall them for future use Intelligence on the other hand is the cognitive ability of someone to perform these and other processes Intelligence quotient IQ There have been various attempts to quantify intelligence via psychometric testing Prominent among these are the various Intelligence Quotient IQ tests which were first developed in the early 20th century to screen children for intellectual disability Over time IQ tests became more pervasive being used to screen immigrants military recruits and job applicants As the tests became more popular belief that IQ tests measure a fundamental and unchanging attribute that all humans possess became widespread An influential theory that promoted the idea that IQ measures a fundamental quality possessed by every person is the theory of General Intelligence or g factor The g factor is a construct that summarizes the correlations observed between an individuals scores on a range of cognitive tests Today most psychologists agree that IQ measures at least some aspects of human intelligence particularly the ability to thrive in an academic context However many psychologists question the validity of IQ tests as a measure of intelligence as a whole There is debate about the heritability of IQ that is what proportion of differences in IQ test performance between individuals are explained by genetic or environmental factors The scientific consensus is that genetics does not explain average differences in IQ test performance between racial groups Emotional Emotional intelligence is thought to be the ability to convey emotion to others in an understandable way as well as to read the emotions of others accurately Some theories imply that a heightened emotional intelligence could also lead to faster generating and processing of emotions in addition to the accuracy In addition higher emotional intelligence is thought to help us manage emotions which is beneficial for our problemsolving skills Emotional intelligence is important to our mental health and has ties to social intelligence Social Social intelligence is the ability to understand the social cues and motivations of others and oneself in social situations It is thought to be distinct to other types of intelligence but has relations to emotional intelligence Social intelligence has coincided with other studies that focus on how we make judgements of others the accuracy with which we do so and why people would be viewed as having positive or negative social character There is debate as to whether or not these studies and social intelligence come from the same theories or if there is a distinction between them and they are generally thought to be of two different schools of thought Moral Moral intelligence is the capacity to understand right from wrong and to behave based on the value that is believed to be right It is considered a distinct form of intelligence independent to both emotional and cognitive intelligence Book smart and street smart Concepts of book smarts and street smart are contrasting views based on the premise that some people have knowledge gained through academic study but may lack the experience to sensibly apply that knowledge while others have knowledge gained through practical experience but may lack accurate information usually gained through study by which to effectively apply that knowledge Artificial intelligence researcher Hector Levesque has noted that Given the importance of learning through text in our own personal lives and in our culture it is perhaps surprising how utterly dismissive we tend to be of it It is sometimes derided as being merely book knowledge and having it is being book smart In contrast knowledge acquired through direct experience and apprenticeship is called street knowledge and having it is being street smart Nonhuman animal Although humans have been the primary focus of intelligence researchers scientists have also attempted to investigate animal intelligence or more broadly animal cognition These researchers are interested in studying both mental ability in a particular species and comparing abilities between species They study various measures of problem solving as well as numerical and verbal reasoning abilities Some challenges include defining intelligence so it has the same meaning across species and operationalizing a measure that accurately compares mental ability across species and contexts Wolfgang Khlers research on the intelligence of apes is an example of research in this area as is Stanley Corens book The Intelligence of Dogs Nonhuman animals particularly noted and studied for their intelligence include chimpanzees bonobos notably the languageusing Kanzi and other great apes dolphins elephants and to some extent parrots rats and ravens Cephalopod intelligence provides an important comparative study Cephalopods appear to exhibit characteristics of significant intelligence yet their nervous systems differ radically from those of backboned animals Vertebrates such as mammals birds reptiles and fish have shown a fairly high degree of intellect that varies according to each species The same is true with arthropods g factor in nonhumans Evidence of a general factor of intelligence has been observed in nonhuman animals First described in humans the g factor has since been identified in a number of nonhuman species Cognitive ability and intelligence cannot be measured using the same largely verbally dependent scales developed for humans Instead intelligence is measured using a variety of interactive and observational tools focusing on innovation habit reversal social learning and responses to novelty Studies have shown that g is responsible for 47 of the individual variance in cognitive ability measures in primates and between 55 and 60 of the variance in mice Locurto Locurto These values are similar to the accepted variance in IQ explained by g in humans 4050 Plant It has been argued that plants should also be classified as intelligent based on their ability to sense and model external and internal environments and adjust their morphology physiology and phenotype accordingly to ensure selfpreservation and reproduction A counter argument is that intelligence is commonly understood to involve the creation and use of persistent memories as opposed to computation that does not involve learning If this is accepted as definitive of intelligence then it includes the artificial intelligence of robots capable of machine learning but excludes those purely autonomic sensereaction responses that can be observed in many plants Plants are not limited to automated sensorymotor responses however they are capable of discriminating positive and negative experiences and of learning registering memories from their past experiences They are also capable of communication accurately computing their circumstances using sophisticated costbenefit analysis and taking tightly controlled actions to mitigate and control the diverse environmental stressors Artificial Scholars studying artificial intelligence have proposed definitions of intelligence that include the intelligence demonstrated by machines Some of these definitions are meant to be general enough to encompass human and other animal intelligence as well An intelligent agent can be defined as a system that perceives its environment and takes actions which maximize its chances of success Kaplan and Haenlein define artificial intelligence as a systems ability to correctly interpret external data to learn from such data and to use those learnings to achieve specific goals and tasks through flexible adaptation Progress in artificial intelligence can be demonstrated in benchmarks ranging from games to practical tasks such as protein folding Existing AI lags humans in terms of general intelligence which is sometimes defined as the capacity to learn how to carry out a huge range of tasks Mathematician Olle Hggstrm defines intelligence in terms of optimization power an agents capacity for efficient crossdomain optimization of the world according to the agents preferences or more simply the ability to steer the future into regions of possibility ranked high in a preference ordering In this optimization framework Deep Blue has the power to steer a chessboards future into a subspace of possibility which it labels as winning despite attempts by Garry Kasparov to steer the future elsewhere Hutter and Legg after surveying the literature define intelligence as an agents ability to achieve goals in a wide range of environments While cognitive ability is sometimes measured as a onedimensional parameter it could also be represented as a hypersurface in a multidimensional space to compare systems that are good at different intellectual tasks Some skeptics believe that there is no meaningful way to define intelligence aside from just pointing to ourselves See also References Further reading Gleick James The Fate of Free Will review of Kevin J Mitchell Free Agents How Evolution Gave Us Free Will Princeton University Press 2023 333 pp The New York Review of Books vol LXXI no 1 18 January 2024 pp 2728 30 Agency is what distinguishes us from machines For biological creatures reason and purpose come from acting in the world and experiencing the consequences Artificial intelligences disembodied strangers to blood sweat and tears have no occasion for that p 30 HughesCastleberry Kenna A Murder Mystery Puzzle The literary puzzle Cains Jawbone which has stumped humans for decades reveals the limitations of naturallanguageprocessing algorithms Scientific American vol 329 no 4 November 2023 pp 8182 This murder mystery competition has revealed that although NLP naturallanguage processing models are capable of incredible feats their abilities are very much limited by the amount of context they receive This could cause difficulties for researchers who hope to use them to do things such as analyze ancient languages In some cases there are few historical records on longgone civilizations to serve as training data for such a purpose p 82 Immerwahr Daniel Your Lying Eyes People now use AI to generate fake videos indistinguishable from real ones How much does it matter The New Yorker 20 November 2023 pp 5459 If by deepfakes we mean realistic videos produced using artificial intelligence that actually deceive people then they barely exist The fakes arent deep and the deeps arent fake AIgenerated videos are not in general operating in our media as counterfeited evidence Their role better resembles that of cartoons especially smutty ones p 59 Press Eyal In Front of Their Faces Does facialrecognition technology lead police to ignore contradictory evidence The New Yorker 20 November 2023 pp 2026 Roivainen Eka AIs IQ ChatGPT aced a standard intelligence test but showed that intelligence cannot be measured by IQ alone Scientific American vol 329 no 1 JulyAugust 2023 p 7 Despite its high IQ ChatGPT fails at tasks that require real humanlike reasoning or an understanding of the physical and social world ChatGPT seemed unable to reason logically and tried to rely on its vast database of facts derived from online texts Cukier Kenneth Ready for Robots How to Think about the Future of AI Foreign Affairs vol 98 no 4 JulyAugust 2019 pp 19298 George Dyson historian of computing writes in what might be called Dysons Law that Any system simple enough to be understandable will not be complicated enough to behave intelligently while any system complicated enough to behave intelligently will be too complicated to understand p 197 Computer scientist Alex Pentland writes Current AI machinelearning algorithms are at their core dead simple stupid They work but they work by brute force p 198 Domingos Pedro Our Digital Doubles AI will serve our species not control it Scientific American vol 319 no 3 September 2018 pp 8893 AIs are like autistic savants and will remain so for the foreseeable future AIs lack common sense and can easily make errors that a human never would They are also liable to take our instructions too literally giving us precisely what we asked for instead of what we actually wanted p 93 Marcus Gary Am I Human Researchers need new ways to distinguish artificial intelligence from the natural kind Scientific American vol 316 no 3 March 2017 pp 6163 Marcus points out a so far insuperable stumbling block to artificial intelligence an incapacity for reliable disambiguation Virtually every sentence that people generate is ambiguous often in multiple ways Our brain is so good at comprehending language that we do not usually notice A prominent example is the pronoun disambiguation problem PDP a machine has no way of determining to whom or what a pronoun in a sentencesuch as he she or itrefers Sternberg Robert J Kaufman Scott Barry eds 2011 The Cambridge Handbook of Intelligence Cambridge Cambridge University Press doi1010179781108770422 ISBN 9780521739115 S2CID 241027150 Mackintosh N J 2011 IQ and Human Intelligence second ed Oxford Oxford University Press ISBN 9780199585595 Flynn James R 2009 What Is Intelligence Beyond the Flynn Effect expanded paperback ed Cambridge Cambridge University Press ISBN 9780521741477 Lay summary in C Shalizi 27 April 2009 What Is Intelligence Beyond the Flynn Effect University of Michigan Review Archived from the original on 14 June 2010 Stanovich Keith 2009 What Intelligence Tests Miss The Psychology of Rational Thought New Haven CT Yale University Press ISBN 9780300123852 Lay summary in Jamie Hale What Intelligence Tests Miss Psych Central Review Archived from the original on 24 December 2013 Blakeslee Sandra Hawkins Jeff 2004 On intelligence New York Times Books ISBN 9780805074567 OCLC 55510125 Bock Gregory Goode Jamie Webb Kate eds 2000 The Nature of Intelligence Novartis Foundation Symposium 233 Vol 233 Chichester Wiley doi1010020470870850 ISBN 9780471494348 Lay summary in William D Casebeer 30 November 2001 The Nature of Intelligence Mental Help Review Archived from the original on 26 May 2013 Wolman Benjamin B ed 1985 Handbook of Intelligence consulting editors Douglas K Detterman Alan S Kaufman Joseph D Matarazzo New York Wiley ISBN 9780471897385 Terman Lewis Madison Merrill Maude A 1937 Measuring intelligence A guide to the administration of the new revised StanfordBinet tests of intelligence Riverside textbooks in education Boston MA Houghton Mifflin OCLC 964301 Binet Alfred Simon Th 1916 The development of intelligence in children The BinetSimon Scale Publications of the Training School at Vineland New Jersey Department of Research No 11 E S Kite Trans Baltimore Williams Wilkins p 1 Retrieved 18 July 2010 External links Intelligence on In Our Time at the BBC History of Influences in the Development of Intelligence Theory and Testing Archived 11 November 2007 at the Wayback Machine Developed by Jonathan Plucker at Indiana University The Limits of Intelligence The laws of physics may well prevent the human brain from evolving into an ever more powerful thinking machine By Douglas Fox in Scientific American 14 June 2011 A Collection of Definitions of Intelligence",
  },
  {
    title: "Learning theory",
    originalContent:
      "Learning theory may refer to Education Learning theory education the process of how humans learn Connectivism Educational philosophies an academic field that examines the definitions goals and meaning of education or of specific educational philosophies Behaviorism philosophy of education Cognitivism philosophy of education Constructivism philosophy of education Humanism philosophy of education Elearning theory a cognitive science of effective multimedia elearning Instructional theory Social cognitive theory Social learning theory Computer science Algorithmic learning theory a branch of computational learning theory Sometimes also referred to as algorithmic inductive inference Computational learning theory a mathematical theory to analyze machine learning algorithms Online machine learning the process of teaching a machine Statistical learning theory",
  },
  {
    title: "Depression",
    originalContent:
      "Depression may refer to Mental health Depression mood a state of low mood and aversion to activity Mood disorders characterized by depression are commonly referred to as simply depression including Major depressive disorder also known as clinical depression Bipolar disorder also known as manic depression Dysthymia also known as persistent depressive disorder Economics Economic depression a sustained longterm downturn in economic activity in one or more economies Great Depression a severe economic depression during the 1930s commonly referred to as simply the Depression Long Depression an economic depression during 187396 known at the time as the Great Depression Biology Depression kinesiology an anatomical term of motion refers to downward movement the opposite of elevation Depression physiology a reduction in a biological variable or the function of an organ Central nervous system depression physiological depression of the central nervous system that can result in loss of consciousness Earth science Depression geology a landform sunken or depressed below the surrounding area Depression weather an area of low atmospheric pressure characterized by rain and unstable weather",
  },
  {
    title: "Counseling",
    originalContent:
      "Counseling is the professional guidance of the individual by utilizing psychological methods especially in collecting case history data using various techniques of the personal interview and testing interests and aptitudes This is a list of counseling topics Therapeutic modalities Common areas See also List of psychotherapies Outline of communication Outline of psychology Outline of sociology Subfields of sociology Outline of self Psychopharmacology References",
  },
  {
    title: "Family therapy",
    originalContent:
      "Family therapy also referred to as family counseling family systems therapy marriage and family therapy couple and family therapy is a branch of psychotherapy focused on families and couples in intimate relationships to nurture change and development It tends to view change in terms of the systems of interaction between family members The different schools of family therapy have in common a belief that regardless of the origin of the problem and regardless of whether the clients consider it an individual or family issue involving families in solutions often benefits clients This involvement of families is commonly accomplished by their direct participation in the therapy session The skills of the family therapist thus include the ability to influence conversations in a way that catalyses the strengths wisdom and support of the wider system In the fields early years many clinicians defined the family in a narrow traditional manner usually including parents and children As the field has evolved the concept of the family is more commonly defined in terms of strongly supportive longterm roles and relationships between people who may or may not be related by blood or marriage The conceptual frameworks developed by family therapists especially those of family systems theorists have been applied to a wide range of human behavior including organisational dynamics and the study of greatness History and theoretical frameworks Formal interventions with families to help individuals and families experiencing various kinds of problems have been a part of many cultures probably throughout history These interventions have sometimes involved formal procedures or rituals and often included the extended family as well as nonkin members of the community see for example Hooponopono Following the emergence of specialization in various societies these interventions were often conducted by particular members of a community for example a chief priest physician and so on usually as an ancillary function Family therapy as a distinct professional practice within Western cultures can be argued to have had its origins in the social work movements of the 19th century in the United Kingdom and the United States As a branch of psychotherapy its roots can be traced somewhat later to the early 20th century with the emergence of the child guidance movement and marriage counseling The formal development of family therapy dates from the 1940s and early 1950s with the founding in 1942 of the American Association of Marriage Counselors the precursor of the AAMFT and through the work of various independent clinicians and groups in the United Kingdom John Bowlby at the Tavistock Clinic the United States Donald deAvila Jackson John Elderkin Bell Nathan Ackerman Christian Midelfort Theodore Lidz Lyman Wynne Murray Bowen Carl Whitaker Virginia Satir Ivan BoszormenyiNagy and in Hungary DLP Liebermann who began seeing family members together for observation or therapy sessions There was initially a strong influence from psychoanalysis most of the early founders of the field had psychoanalytic backgrounds and social psychiatry and later from learning theory and behavior therapy and significantly these clinicians began to articulate various theories about the nature and functioning of the family as an entity that was more than a mere aggregation of individuals The movement received an important boost starting in the early 1950s through the work of anthropologist Gregory Bateson and colleagues Jay Haley Donald D Jackson John Weakland William Fry and later Virginia Satir Ivan BoszormenyiNagy Paul Watzlawick and others at Palo Alto in the United States who introduced ideas from cybernetics and general systems theory into social psychology and psychotherapy focusing in particular on the role of communication see Bateson Project This approach eschewed the traditional focus on individual psychology and historical factors that involve socalled linear causation and content and emphasized instead feedback and homeostatic mechanisms and rules in hereandnow interactions socalled circular causation and process that were thought to maintain or exacerbate problems whatever the original causes See also systems psychology and systemic therapy This group was also influenced significantly by the work of US psychiatrist hypnotherapist and brief therapist Milton H Erickson especially his innovative use of strategies for change such as paradoxical directives The members of the Bateson Project like the founders of a number of other schools of family therapy including Carl Whitaker Murray Bowen and Ivan BoszormenyiNagy had a particular interest in the possible psychosocial causes and treatment of schizophrenia especially in terms of the putative meaning and function of signs and symptoms within the family system The research of psychiatrists and psychoanalysts Lyman Wynne and Theodore Lidz on communication deviance and roles eg pseudomutuality pseudohostility schism and skew in families of people with schizophrenia also became influential with systemscommunicationsoriented theorists and therapists A related theme applying to dysfunction and psychopathology more generally was that of the identified patient or presenting problem as a manifestation of or surrogate for the familys or even societys problems See also double bind family nexus By the mid1960s a number of distinct schools of family therapy had emerged From those groups that were most strongly influenced by cybernetics and systems theory there came MRI Brief Therapy and slightly later strategic therapy Salvador Minuchins structural family therapy and the Milan systems model Partly in reaction to some aspects of these systemic models came the experiential approaches of Virginia Satir and Carl Whitaker which downplayed theoretical constructs and emphasized subjective experience and unexpressed feelings including the subconscious authentic communication spontaneity creativity total therapist engagement and often included the extended family Concurrently and somewhat independently there emerged the various intergenerational therapies of Murray Bowen Ivan BoszormenyiNagy James Framo and Norman Paul which present different theories about the intergenerational transmission of health and dysfunction but which all deal usually with at least three generations of a family in person or conceptually either directly in therapy sessions or via homework journeys home etc Psychodynamic family therapy which more than any other school of family therapy deals directly with individual psychology and the unconscious in the context of current relationships continued to develop through a number of groups that were influenced by the ideas and methods of Nathan Ackerman and also by the British School of Object Relations and John Bowlbys work on attachment Multiplefamily group therapy a precursor of psychoeducational family intervention emerged in part as a pragmatic alternative form of intervention especially as an adjunct to the treatment of serious mental disorders with a significant biological basis such as schizophrenia and represented something of a conceptual challenge to some of the systemic and thus potentially familyblaming paradigms of pathogenesis that were implicit in many of the dominant models of family therapy The late 1960s and early 1970s saw the development of network therapy which bears some resemblance to traditional practices such as Hooponopono by Ross Speck and Carolyn Attneave and the emergence of behavioral marital therapy renamed behavioral couples therapy in the 1990s and behavioral family therapy as models in their own right By the late 1970s the weight of clinical experience especially in relation to the treatment of serious mental disorders had led to some revision of a number of the original models and a moderation of some of the earlier stridency and theoretical purism There were the beginnings of a general softening of the strict demarcations between schools with moves toward rapprochement integration and eclecticism although there was nevertheless some hardening of positions within some schools These trends were reflected in and influenced by lively debates within the field and critiques from various sources including feminism and postmodernism that reflected in part the cultural and political tenor of the times and which foreshadowed the emergence in the 1980s and 1990s of the various postsystems constructivist and social constructionist approaches While there was still debate within the field about whether or to what degree the systemicconstructivist and medicalbiological paradigms were necessarily antithetical to each other see also Antipsychiatry Biopsychosocial model there was a growing willingness and tendency on the part of family therapists to work in multimodal clinical partnerships with other members of the helping and medical professions From the mid1980s to the present the field has been marked by a diversity of approaches that partly reflect the original schools but which also draw on other theories and methods from individual psychotherapy and elsewhere these approaches and sources include brief therapy structural therapy constructivist approaches eg Milan systems postMilancollaborativeconversational reflective Bring forthism approach eg Dr Karl Tomms IPscope model and Interventive interviewing solutionfocused therapy narrative therapy a range of cognitive and behavioral approaches psychodynamic and object relations approaches attachment and emotionally focused therapy intergenerational approaches network therapy and multisystemic therapy MST Multicultural intercultural and integrative approaches are being developed with Vincenzo Di Nicola weaving a synthesis of family therapy and transcultural psychiatry in his model of cultural family therapy A Stranger in the Family Culture Families and Therapy Many practitioners claim to be eclectic using techniques from several areas depending upon their own inclinations andor the needs of the clients and there is a growing movement toward a single generic family therapy that seeks to incorporate the best of the accumulated knowledge in the field and which can be adapted to many different contexts however there are still a significant number of therapists who adhere more or less strictly to a particular or limited number of approaches The Liberation Based Healing framework for family therapy offers a complete paradigm shift for working with families while addressing the intersections of race class gender identity sexual orientation and other sociopolitical identity markers This theoretical approach and praxis is informed by critical pedagogy feminism critical race theory and decolonizing theory This framework necessitates an understanding of the ways colonization cisheteronormativity patriarchy white supremacy and other systems of domination impact individuals families and communities and centers the need to disrupt the status quo in how power operates Traditional Western models of family therapy have historically ignored these dimensions and when white male privilege has been critiqued largely by feminist theory practitioners it has often been to the benefit of middleclass white womens experiences While an understanding of intersectionality is of particular significance in working with families with violence a liberatory framework examines how power privilege and oppression operate within and across all relationships Liberatory practices are based on the principles of critical consciousness Accountability and Empowerment These principles guide not only the content of the therapeutic work with clients but also the supervisory and training process of therapists Dr Rhea Almeida developed the cultural context model as a way to operationalize these concepts into practice through the integration of culture circles sponsors and a socioeducational process within the therapeutic work Ideas and methods from family therapy have been influential in psychotherapy generally a survey of over 2500 US therapists in 2006 revealed that of the 10 most influential therapists of the previous quartercentury three were prominent family therapists and that the marital and family systems model was the second most utilized model after cognitive behavioral therapy Techniques Family therapy uses a range of counseling and other techniques including Structural therapy identifies and reorders the organisation of the family system Strategic therapy looks at patterns of interactions between family members SystemicMilan therapy focuses on belief systems Narrative therapy restorying of dominant problemsaturated narrative emphasis on context separation of the problem from the person Transgenerational therapy transgenerational transmission of unhelpful patterns of belief and behaviour IPscope model and Interventive Interviewing Communication theory Psychoeducation Psychotherapy Relationship counseling Relationship education Systemic coaching Systems theory Reality therapy the genogram The number of sessions depends on the situation but the average is 520 sessions A family therapist usually meets several members of the family at the same time This has the advantage of making differences between the ways family members perceive mutual relations as well as interaction patterns in the session apparent both for the therapist and the family These patterns frequently mirror habitual interaction patterns at home even though the therapist is now incorporated into the family system Therapy interventions usually focus on relationship patterns rather than on analyzing impulses of the unconscious mind or early childhood trauma of individuals as a Freudian therapist would do although some schools of family therapy for example psychodynamic and intergenerational do consider such individual and historical factors thus embracing both linear and circular causation and they may use instruments such as the genogram to help to elucidate the patterns of relationship across generations The distinctive feature of family therapy is its perspective and analytical framework rather than the number of people present at a therapy session Specifically family therapists are relational therapists They are generally more interested in what goes on between individuals rather than within one or more individuals although some family therapists in particular those who identify as psychodynamic object relations intergenerational or experiential family therapists EFTs tend to be as interested in individuals as in the systems those individuals and their relationships constitute Depending on the conflicts at issue and the progress of therapy to date a therapist may focus on analyzing specific previous instances of conflict as by reviewing a past incident and suggesting alternative ways family members might have responded to one another during it or instead proceed directly to addressing the sources of conflict at a more abstract level as by pointing out patterns of interaction that the family might have not noticed Family therapists tend to be more interested in the maintenance andor solving of problems rather than in trying to identify a single cause Some families may perceive causeeffect analyses as attempts to allocate blame to one or more individuals with the effect that for many families a focus on causation is of little or no clinical utility It is important to note that a circular way of problem evaluation is used as opposed to a linear route Using this method families can be helped by finding patterns of behaviour what the causes are and what can be done to better their situation Evidence base Family therapy has an evolving evidence base A summary of current evidence is available via the UKs Association of Family Therapy Evaluation and outcome studies can also be found on the Family Therapy and Systemic Research Centre website The website also includes quantitative and qualitative research studies of many aspects of family therapy According to a 2004 French government study conducted by French Institute of Health and Medical Research family and couples therapy was the second most effective therapy after Cognitive behavioral therapy The study used metaanalysis of over a hundred secondary studies to find some level of effectiveness that was either proven or presumed to exist Of the treatments studied family therapy was presumed or proven effective at treating schizophrenia bipolar disorder anorexia and alcohol dependency Concerns and criticism In a 1999 address to the Coalition of Marriage Family and Couples Education conference in Washington DC University of Minnesota Professor William Doherty said I take no joy in being a whistle blower but its time I am a committed marriage and family therapist having practiced this form of therapy since 1977 I train marriage and family therapists I believe that marriage therapy can be very helpful in the hands of therapists who are committed to the profession and the practice But there are a lot of problems out there with the practice of therapy a lot of problems Doherty suggested questions prospective clients should ask a therapist before beginning treatment Can you describe your background and training in marital therapy What is your attitude toward salvaging a troubled marriage versus helping couples break up What is your approach when one partner is seriously considering ending the marriage and the other wants to save it What percentage of your practice is marital therapy Of the couples you treat what percentage would you say work out enough of their problems to stay married with a reasonable amount of satisfaction with the relationship What percentage break up while they are seeing you What percentage do not improve What do you think makes the differences in these results Licensing and degrees Family therapy practitioners come from a range of professional backgrounds and some are specifically qualified or licensedregistered in family therapy licensing is not required in some jurisdictions and requirements vary from place to place In the United Kingdom family therapists will have a prior relevant professional training in one of the helping professions usually psychologists psychotherapists or counselors who have done further training in family therapy either a diploma or an MSc In the United States there is a specific degree and license as a marriage and family therapist however psychologists nurses psychotherapists social workers or counselors and other licensed mental health professionals may practice family therapy In the UK family therapists who have completed a fouryear qualifying programme of study MSc are eligible to register with the professional body the Association of Family Therapy AFT and with the UK Council for Psychotherapy UKCP A masters degree is required to work as a Marriage and Family Therapist MFT in some American states Most commonly MFTs will first earn a MS or MA degree in marriage and family therapy counseling psychology family studies or social work After graduation prospective MFTs work as interns under the supervision of a licensed professional and are referred to as an MFTi Prior to 1999 in California counselors who specialized in this area were called Marriage Family and Child Counselors Today they are known as Marriage and Family Therapists MFT and work variously in private practice in clinical settings such as hospitals institutions or counseling organizations Marriage and family therapists in the United States and Canada often seek degrees from accredited Masters or Doctoral programs recognized by the Commission on Accreditation for Marriage and Family Therapy Education COAMFTE a division of the American Association of Marriage and Family Therapy Requirements vary but in most states about 3000 hours of supervised work as an intern are needed to sit for a licensing exam MFTs must be licensed by the state to practice Only after completing their education and internship and passing the state licensing exam can a person call themselves a Marital and Family Therapist and work unsupervised License restrictions can vary considerably from state to state Contact information about licensing boards in the United States are provided by the Association of Marital and Family Regulatory Boards There have been concerns raised within the profession about the fact that specialist training in couples therapy as distinct from family therapy in general is not required to gain a license as an MFT or membership of the main professional body the AAMFT Values and ethics Since issues of interpersonal conflict power control values and ethics are often more pronounced in relationship therapy than in individual therapy there has been debate within the profession about the different values that are implicit in the various theoretical models of therapy and the role of the therapists own values in the therapeutic process and how prospective clients should best go about finding a therapist whose values and objectives are most consistent with their own An early paper on ethics in family therapy written by Vincenzo Di Nicola in consultation with a bioethicist asked basic questions about whether strategic interventions mean what they say and if it is ethical to invent opinions offered to families about the treatment process such as statements saying that half of the treatment team believes one thing and half believes another Specific issues that have emerged have included an increasing questioning of the longstanding notion of therapeutic neutrality a concern with questions of justice and selfdetermination connectedness and independence functioning versus authenticity and questions about the degree of the therapists promarriagefamily versus proindividual commitment The American Association for Marriage and Family Therapy requires members to adhere to a code of ethics including a commitment to continue therapeutic relationships only so long as it is reasonably clear that clients are benefiting from the relationship Founders and key influences Some key developers of family therapy are Summary of theories and techniques references Journals Australian and New Zealand Journal of Family Therapy Contemporary Family Therapy Family Process Family Relations Family Relations Interdisciplinary Journal of Applied Family Studies ISSN 01976664 Journal of Family Therapy Marriage Fitness Murmurations Journal of Transformative Systemic Practice Sexual and Relationship Therapy Journal of Marital Family Therapy Families Systems and Health See also Footnotes Further reading Deborah Weinstein The Pathological Family Postwar America and the Rise of Family Therapy Ithaca NY Cornell University Press 2013 Satir V Banmen J Gerber J Gomori M 1991 The Satir Model Family Therapy and Beyond Palo Alto CA Science and Behavior Books The Systemic Thinking and Practice Series Routledge Gehring T M Debry M Smith P K Eds 2016 The Family System Test FAST Theory and application Hove BrunnerRoutledge",
  },
  {
    title: "Child therapy",
    originalContent:
      "Child psychotherapy or mental health interventions for children refers to the psychological treatment of various mental disorders diagnosed in children and adolescents The therapeutic techniques developed for younger age ranges specialize in prioritizing the relationship between the child and the therapist The goal of maintaining positive therapistclient relationships is typically achieved using therapeutic conversations and can take place with the client alone or through engagement with family members The term psychotherapy includes the implementation of educational and psychoanalytic support for the client and is effective in problemsolving emotional regulation and encouraging prosocial behaviors as children develop positive changes to their current mindsets Terms describing childfocused treatments may vary from one part of the world to another with particular differences in the use of such terms as therapy child psychotherapy or child analysis Evolution of child psychotherapy Child Psychotherapy has developed varied approaches over the last century Two distinct historic pathways can be identified for presentday provision in Western Europe and in the United States one through the Child Guidance Movement the other stemming from adult psychiatry or psychological medicine which evolved a separate child psychiatry specialism The separation of child and adult psychology The attempt to create a unified method of child mental health care led to the increase of child guidance clinics in England throughout the midtwentieth century The spread of clinics across Europe coincided with the absence of hospital care as the lack of distinction between child and adult psychiatry prevented further analysis of child diagnosis and treatment The first Chair of Child Psychiatry officially coined the term Child and Adolescent Psychiatry in 1973 but it was not until the DSMIII where a full list of distinct child psychiatric disorders were mentioned Psychoanalytic child psychotherapy Psychoanalytic psychotherapy with infants children and adolescents is mainly delivered by people qualified specifically in psychoanalytic child psychotherapy or by trainees under supervision from a specialist in childfocused treatment Recent evidence covering 34 research papers nine of which were randomized controlled trials showed psychoanalytic psychotherapy to be particularly effective for children with the following conditions depression anxiety and behavior disorders personality disorders learning difficulties eating disorders developmental issues Furthermore followup research shows that in psychoanalytic psychotherapy therapeutic improvements continue well beyond the termination of the therapy itself This has been termed a sleeper effect In the UK psychoanalytic psychotherapy is recommended by NICE as an evidencebased treatment for trauma from sexual abuse and severe depression in adolescents following the IMPACT study Evidencebased child and adolescent psychiatry There are various therapeutic assessments to address mental health concerns among children and adolescents Some approaches are backed by strong scientific evidence while some are not Some research suggests that it is the quality of the relationship with the therapist rather than the particular form of therapeutic intervention that is the strongest factor in helping change develop Parentinfant psychotherapy If the normal course of secure attachment between parent and infant is disrupted parentinfant psychotherapy is a catchall term to describe psychotherapies that either aim to restore this bond or to work with vulnerable parents to overcome disruption and prevent further occurrence Examples of this kind of therapy include Watch Wait Wonder and psychoanalytic parentinfant psychotherapy Many of these techniques require a threeway relationship between the parent child and therapist During therapy sessions the parent may express his or her thoughts and feelings which are based on a combination of factors including The parents experiences as a child The parents expectations and hopes for the childs future The relationships the parent has with other people The therapists role is as an observer and an interpreter of the interaction between the infant and the parent He might share some of his thoughts about the behavior of the child with the parent and by doing so offering the parent an alternative way of experiencing the child This technique helps the parent to resolve issues with his or her own infancyexperiences in order to restore secure attachment with the infant And it helps lower the risk for psychopathological developments of the child in the future Group art therapy Group art therapy gives the child a safe environment to access their emotions through a creative medium in the presence of a therapist This nonverbal therapeutic practice alleviates the stress that a child may feel when trying to find the words to express themselves thus it helps rebuild social skills and gain trust in others Studies have also found that this practice can alleviate selfharm engagement This method of psychotherapy has been found particularly helpful for children who exhibit any of the following Autism Aspergers Anxiety and behavior disorders Group art therapy has eight subcategories of specific mechanisms of change Among them are As a form of expression to reveal whats inside As a way of becoming aware of oneself a way to form a narrative of life integrative activation of the brain through experience a form of exploration andor reflection the specifics of the art materialstechniques offered in art therapy as a form to practice andor learn skills art therapy as an easily accessible positive and safe intervention by the use of art materials By bundling together these specific groups the general groups are as follows art therapy as a form of group process the therapeutic alliance in art therapy Within this approach three types of behaviors can be exhibited by the therapist nondirective directive and eclectic Nondirective refers to a following behavior in which the therapist takes on an attitude of observing selfexploration of emotions rather than facilitation or interpretation Directive attitudes however follow a facilitative pattern by asking specific questions to guide the clients artwork With these two processes in mind eclectic combines them to create a facilitative and lenient approach simultaneously and often utilizes emotion checkins at the start of sessions and emotion checkouts at the end of sessions This approach adopts various psychological elements such as psychoeducational mindfulness psychoanalysis and cognitive analytic theories This article sought to analyze this methods effectiveness on a broad spectrum including the following traumatic events PTSD who have educational needs or disabilities children with medical conditions children with none of the former juvenile offenders Art therapy can be implemented as a holistic therapeutic practice for child cancer patients as well effecting 1 in 285 children in the US 15980 children each year Given the alleviating effects that are addressed by this method children were better able to discuss their needs and emotions to their family members and healthcare team The results of this study conveyed that art therapy lead to improved emotional and mental wellbeing and improved communication skills Parentchild interaction therapy PCIT Parentchild interaction therapy is meant to assist parents whom have children ages 27 years old who are prone to disruptive behaviors and emotional difficulties Parentchild therapy utilizing two stages each possessing their own goals and characteristics to create this approach Beginning with childdirected interaction CDI parents learn skills such as praise verbal reflection imitation behavioral description and enjoyment to achieve the goal of warm and secure parenting styles ParentDirected interaction PDI the second phase seeks to decrease the original disruptive behaviors exhibited by the child Both phases are designed to be coached by the therapist via another room while the parent interacts with their child This review found that certain cultural values may impede or contribute to the progress of this approach Challenges of child psychotherapy Disregarding suppressed behaviors Therapeutic interventions among children and adolescents are subject to specific challenges many of which stem from the reliance of family members as a result of the clients lack of independency at the current stage in their lives Unlike adult psychotherapy it is rare for a client to seek treatment themselves in child psychotherapy The involvement of parents in treatment referral often leads to the frequent disregard suppressed behavioral or emotional problems such as anxiety and depression with the majority of referrals relating to disruptive behaviors Lack of motivation The childparent dynamic in psychotherapy also has the tendency to increase disagreements regarding treatment processes Children may be hesitant to accept the idea of undergoing psychotherapy if they were forced into it by a third party This reluctancy to abide by a psychotherapeutic schedule contributes to the challenge of retaining clients in treatment as 4060 of children and adolescents end up dropping out due to demotivation Problems of reporting styles Many challenges associated with child psychotherapy derive from inefficient reports of client symptoms The methods provided for obtaining information of symptoms typically involve questionnaires and interviews that may affect how the client will answer Important characteristics of symptoms such as duration and intensity may not be reliable if the client omits crucial information out of fear or risk of embarrassment See also British Psychotherapy Foundation Michael Fordham Anna Freud Melanie Klein Michael Rutter Donald Winnicott References External links Association of Child Psychotherapists ACP the professional body for Psychoanalytic Child and Adolescent Psychotherapists in the UK and a core NHS profession",
  },
  {
    title: "Human resources",
    originalContent:
      "Human resources HR is the set of people who make up the workforce of an organization business sector industry or economy A narrower concept is human capital the knowledge and skills which the individuals command Similar terms include manpower labor laborpower or personnel The Human Resources department HR department sometimes just called Human Resource of an organization performs human resource management overseeing various aspects of employment such as compliance with labor law and employment standards interviewing and selection performance management administration of employee benefits organizing of employee files with the required documents for future reference and some aspects of recruitment also known as talent acquisition talent management staff wellbeing and employee offboarding They serve as the link between an organizations management and its employees The duties include planning recruitment and selection process posting job ads evaluating the performance of employees organizing resumes and job applications scheduling interviews and assisting in the process and ensuring background checks Another job is payroll and benefits administration which deals with ensuring vacation and sick time are accounted for reviewing payroll and participating in benefits tasks like claim resolutions reconciling benefits statements and approving invoices for payment Human Resources also coordinates employee relations activities and programs including but not limited to employee counseling The last job is regular maintenance this job makes sure that the current HR files and databases are up to date maintaining employee benefits and employment status and performing payrollbenefitrelated reconciliations Activities A human resources manager can have various functions in a company including to Determine the needs of the staffpersonnel Human resource accounting determine whether to use temporary staff or hire employees to fill these needs Recruit andor interview hires Prepare employee records and personal policies Manage employee payroll benefits and compensation Manage employee relations prepare remote work and hybrid work policy Employee retention talent management Deal with performance issues motivate employees monitor staff wellbeing Mediate disputes Establish organizational culture in the organization Reduce workplace politics ensure equal opportunities reduce discrimination Ensure that human resources practices conform to various regulations and human resource metrics Apply HR software to improve HR effectivity Human Resources can be evaluated empirically using various frameworks such as Stamina Model History Human resource management used to be referred to as personnel administration In the 1920s personnel administration focused mostly on the aspects of hiring evaluating and compensating employees However they did not focus on any employment relationships at an organizational performance level or on the systematic relationships in any parties This led to a lacked unifying paradigm in the field during this period According to an HR Magazine article the first personnel management department started at the National Cash Register Co in 1900 The owner John Henry Patterson organized a personnel department to deal with grievances discharges and safety and information for supervisors on new laws and practices after several strikes and employee lockouts This action was followed by other companies for example Ford had high turnover ratios of 380 percent in 1913 but just one year later the line workers of the company had doubled their daily salaries from 250 to 5 even though 250 was a fair wage at that time This example clearly shows the importance of effective management which leads to a greater outcome of employee satisfaction as well as encouraging employees to work together in order to achieve better business objectives During the 1970s American businesses began experiencing challenges due to the substantial increase in competitive pressures Companies experienced globalization deregulation and rapid technological change which caused the major companies to enhance their strategic planning a process of predicting future changes in a particular environment and focus on ways to promote organizational effectiveness This resulted in developing more jobs and opportunities for people to show their skills which were directed to effectively applying employees toward the fulfillment of individual group and organizational goals Many years later the majorminor of human resource management was created at universities and colleges also known as business administration It consists of all the activities that companies used to ensure the more effective use of employees Now human resources focus on the people side of management There are two real definitions of HRM Human Resource Management one is that it is the process of managing people in organizations in a structured and thorough manner This means that it covers the hiring firing pay and perks and performance management This first definition is the modern and traditional version more like what a personnel manager would have done back in the 1920s The second definition is that HRM circles the ideas of management of people in organizations from a macromanagement perspective like customers and competitors in a marketplace This involves the focus on making the employment relationship fulfilling for both management and employees Some research showed that employees can perform at a much higher rate of productivity when their supervisors and managers paid more attention to them The Father of Human relations Elton Mayo was the first person to reinforce the importance of employee communications cooperation and involvement His studies concluded that sometimes the human factors are more important than physical factors such as quality of lighting and physical workplace conditions As a result individuals often place value more on how they feel For example a rewarding system in Human resource management applied effectively can further encourage employees to achieve their best performance Origins of the terminology Pioneering economist John R Commons mentioned human resource in his 1893 book The Distribution of Wealth but did not elaborate The expression was used during the 1910s to 1930s to promote the idea that human beings are of worth as in human dignity by the early 1950s it meant people as a means to an end for employers Among scholars the first use of the phrase in that sense was in a 1958 report by economist E Wight Bakke In regard to how individuals respond to the changes in a labor market the following must be understood Skills and qualifications as industries move from manual to more managerial professions so does the need for more highly skilled staff If the market is tight ie not enough staff for the jobs employers must compete for employees by offering financial rewards community investment etc Geographical spread how far is the job from the individual The distance to travel to work should be in line with remuneration and the transportation and infrastructure of the area also influence who applies for a position Occupational structure the norms and values of the different careers within an organization Mahoney 1989 developed 3 different types of occupational structure namely craft loyalty to the profession organization career path promotion through the firm and unstructured lowerunskilled workers who work when needed Generational difference different age categories of employees have certain characteristics for example their behavior and their expectations of the organization Criticism of its terminology and role One major concern about considering people as assets or resources is that they will be commoditized objectified and abused Critics of the term human resources would argue that human beings are not commodities or resources but are creative and social beings in a productive enterprise The 2000 revision of ISO 9001 in contrast requires identifying the processes their sequence and interaction and to define and communicate responsibilities and authorities In general heavily unionized nations such as France and Germany have adopted and encouraged such approaches Also in 2001 the International Labour Organization decided to revisit and revise its 1975 Recommendation 150 on Human Resources Development resulting in its Labour is not a commodity principle One view of these trends is that a strong social consensus on political economy and a good social welfare system facilitate labor mobility and tend to make the entire economy more productive as labor can develop skills and experience in various ways and move from one enterprise to another with little controversy or difficulty in adapting Another important controversy regards labor mobility and the broader philosophical issue with the usage of the phrase human resources Governments of developing nations often regard developed nations that encourage immigration or guest workers as appropriating human capital that is more rightfully part of the developing nation and required to further its economic growth Over time the United Nations have come to more generally support the developing nations point of view and have requested significant offsetting foreign aid contributions so that a developing nation losing human capital does not lose the capacity to continue to train new people in trades professions and the arts Some businesses and companies are choosing to rename this department using other terms such as people operations or culture department in order to erase this stigma Development Human resource companies play an important part in developing and making a company or organization at the beginning or making a success at the end due to the labor provided by employees Human resources are intended to show how to have better employment relations in the workforce Also to bring out the best work ethic of the employees and therefore making a move to a better working environment Moreover green human resource development is suggested as a paradigm shift from traditional approaches of human resource companies to bring awareness of ways that expertise can be applied to green practices By integrating the expertise knowledge and competencies of human resource development practitioners with industry practitioners most industries have the potential to be transformed into a sector with ecofriendly and proenvironmental culture Human resources also deals with essential motivators in the workplace such as payroll benefits team morale and workplace harassment Planning Administration and operations used to be the two role areas of HR The strategic planning component came into play as a result of companies recognizing the need to consider HR needs in goals and strategies HR directors commonly sit on company executive teams because of the HR planning function Numbers and types of employees and the evolution of compensation systems are among elements in the planning role Various factors affecting Human Resource planning include organizational structure growth business location demographic changes environmental uncertainties expansion See also References External links Library resources in your library and in other libraries about Human resources",
  },
  {
    title: "Organizational behavior",
    originalContent:
      "Organizational behavior or organisational behaviour see spelling differences is the study of human behavior in organizational settings the interface between human behavior and the organization and the organization itself Organizational behavioral research can be categorized in at least three ways individuals in organizations microlevel work groups mesolevel how organizations behave macrolevel Chester Barnard recognized that individuals behave differently when acting in their organizational role than when acting separately from the organization Organizational behavior researchers study the behavior of individuals primarily in their organizational roles One of the main goals of organizational behavior research is to revitalize organizational theory and develop a better conceptualization of organizational life Relation to industrial and organizational psychology Miner 2006 mentioned that there is a certain arbitrariness in identifying a point at which organizational behavior became established as a distinct discipline p 56 suggesting that it could have emerged in the 1940s or 1950s He also underlined the fact that the industrial psychology division of the American Psychological Association did not add organizational to its name until 1970 long after organizational behavior had clearly come into existence p 56 noting that a similar situation arose in sociology Although there are similarities and differences between the two disciplines there is still confusion around differentiating organizational behavior and organizational psychology History As a multidisciplinary science organizational behavior has been influenced by developments in a number of related disciplines including sociology industrialorganizational psychology and economics The Industrial Revolution is a period from the 1760s where new technologies resulted in the adoption of new manufacturing techniques and increased mechanization In his famous iron cage metaphor Max Weber raised concerns over the reduction in religious and vocational work experiences Weber claimed that the Industrial Revolutions focus on efficiency constrained the worker to a kind of prison and stripped a worker of their individuality The significant social and cultural changes caused by the Industrial Revolution also gave rise to new forms of organization Weber analyzed one of these organizations and came to the conclusion that bureaucracy was an organization that rested on rationallegal principles and maximized technical efficiency A number of organizational behavioral practitioners documented their ideas about management and organization The best known theories today originate from Henri Fayol Chester Barnard and Mary Parker Follet All three of them drew from their experience to develop a model of effective organizational management and each of their theories independently shared a focus on human behavior and motivation One of the first management consultants Frederick Taylor was a 19thcentury engineer who applied an approach known as the scientific management Taylor advocated for maximizing task efficiency through the scientific method The scientific method was further refined by Lillian and Frank Gilbreth who utilized time and motion study to further improve worker efficiency In the early 20th century the idea of Fordism emerged Named after automobile mogul Henry Ford the method relied on the standardization of production through the use of assembly lines This allowed unskilled workers to produce complex products efficiently Sorenson later clarified that Fordism developed independently of Taylor Fordism can be explained as the application of bureaucratic and scientific management principles to whole manufacturing process The success of the scientific method and Fordism resulted in the widespread adoption of these methods In the 1920s the Hawthorne Works Western Electric factory commissioned the first of what was to become known as the Hawthorne Studies These studies initially adhered to the traditional scientific method but also investigated whether workers would be more productive with higher or lower lighting levels The results showed that regardless of lighting levels when workers were being studied productivity increased but when the studies ended worker productivity would return to normal In following experiments Elton Mayo concluded that job performance and the socalled Hawthorne Effect was strongly correlated to social relationships and job content Following the Hawthorne Studies motivation became a focal point in the Organizational behavioral community A range of theories emerged in the 1950s and 1960s and include theories from notable Organizational behavioral researchers such as Frederick Herzberg Abraham Maslow David McClelland Victor Vroom and Douglas McGregor These theories underline employee motivation work performance and job satisfaction Herbert Simons Administrative Behavior introduced a number of important Organizational behavior concepts most notably decisionmaking Simon along with Chester Barnard argued that people make decisions differently inside an organization when compared to their decisions outside of an organization While classical economic theories assume that people are rational decisionmakers Simon argued a contrary point He argued that cognition is limited because of bounded rationality For example decisionmakers often employ satisficing the process of utilizing the first marginally acceptable solution rather than the most optimal solution Simon was awarded the Nobel Prize in Economics for his work on organizational decisionmaking In the 1960s and 1970s the field started to become more quantitative and resource dependent This gave rise to contingency theory institutional theory and organizational ecology Starting in the 1980s cultural explanations of organizations and organizational change became areas of study in concert with fields such as anthropology psychology and sociology Current state of the field Research in and the teaching of Organizational behavior primarily takes place in university management departments in colleges of business Sometimes Organizational Behavioral topics are taught in industrial and organizational psychology graduate programs There have been additional developments in Organizational behavior research and practice Anthropology has become increasingly influential and led to the idea that one can understand firms as communities by introducing concepts such as organizational culture organizational rituals and symbolic acts Leadership studies have also become part of Organizational behavior although a single unifying theory remains elusive Organizational behavioral researchers have shown increased interest in ethics and its importance in an organization Some Organizational behavioral researchers have become interested in the aesthetic sphere of organizations Research methods used A variety of methods are used in organizational behavior many of which are found in other social sciences Quantitative methods Quantitative research allows organizational behavior to be studiedcompared through numerical data A key advantage of quantitative studies is that their efficient examinations of large groups can be studied at lower costs and in less time This form of research studies more of the broad study Statistical methods used in OB research commonly include correlation analysis of variance metaanalysis multilevel modeling multiple regression structural equation modeling and time series analysis Computer simulation Computer simulation is a prominent method in organizational behavior While there are many uses for computer simulation most Organizational behavioral researchers have used computer simulation to understand how organizations or firms operate More recently however researchers have also started to apply computer simulation to understand individual behavior at a microlevel focusing on individual and interpersonal cognition and behavior such as the thought processes and behaviors that make up teamwork Qualitative methods Qualitative research consists of several methods of inquiry that generally do not involve the quantification of variablesThis procedure builds and structure patterns of individual behavior An advantage of qualitative research is that it provides a clearer picture of an organization Qualitative methods can range from the content analysis of interviews or written material to written narratives of observations Meaning that qualitative research goes more in depth of their studies as opposed to the entirety Common methods include ethnography case studies historical methods and interviews Topics Consulting Consultants use principles developed in organizational behavior research to assess clients organizational problems and provide high quality services A robust framework to analyze the consultantclient relationship is key in the success of any consulting engagement Counterproductive work behavior Counterproductive work behavior is employee behavior that harms or intends to harm an organization Decisionmaking Many Organizational behavior researchers embrace the rational planning model Decisionmaking research often focuses on how decisions are ordinarily made normative decisionmaking how thinkers arrive at a particular judgement descriptive decisionmaking and how to improve this decisionmaking descriptive decisionmaking Effects of diversity and inclusion Companies that focus on diversity and inclusion are able to benefit from advantages such as better retention and less intention by staff to quit increased job satisfaction lower levels of stress and job withdrawal higher levels of creativity and innovation as well as less onthejob conflict Diversity or focusing on differences between individuals and groups is of course important organizations that have a culture that values the unique perspectives and contributions of all employees also known as inclusion may be able to move the needle from not engaged to engaged Employee mistreatment There are several types of mistreatments that employees endure in organizations including Abusive supervision bullying incivility and sexual harassment Employees in an organization being mistreated also can suffer work withdrawal Withdrawing from an organization can be in the form of being late not fully participating in work duties or looking for a new job Employees may file grievances in an organization with retrospect to a procedure or policy or mistreatment with human interactions Abusive supervision Abusive supervision is the extent to which a supervisor engages in a pattern of behavior that harms subordinates Bullying Although definitions of workplace bullying vary it involves a repeated pattern of harmful behaviors directed towards an individual In order for a behavior to be termed bullying the individual or individuals doing the harm have to possess either singly or jointly more power on any level than the victim Incivility Workplace incivility consists of lowintensity discourteous and rude behavior and is characterized by an ambiguous intent to harm and the violation of social norms governing appropriate workplace behavior Sexual harassment Sexual harassment is behavior that denigrates or mistreats an individual due to his or her gender often creating an offensive workplace that interferes with job performance Teams Jobrelated attitudes and emotions Organizational behavior deals with employee attitudes and feelings including job satisfaction organizational commitment job involvement and emotional labor Job satisfaction reflects the feelings an employee has about his or her job or facets of the job such as pay or supervision Organizational commitment represents the extent to which employees feel attached to their organization Job involvement is the extent to which an individual identifies with their job and considers it a material component of their selfworth Emotional labor concerns the requirement that an employee display certain emotions such smiling at customers even when the employee does not feel the emotion he or she is required to display Leadership There have been a number of theories that concern leadership Early theories focused on characteristics of leaders while later theories focused on leader behavior and conditions under which leaders can be effective Among these approaches are contingency theory the consideration and initiating structure model leadermember exchange or LMX theory pathgoal theory behavioural modification and transformational leadership theory Contingency theory indicates that good leadership depends on characteristics of the leader and the situation The Ohio State Leadership Studies identified dimensions of leadership known as consideration showing concern and respect for subordinates and initiating structure assigning tasks and setting performance goals LMX theory focuses on exchange relationships between individual supervisorsubordinate pairs Pathgoal theory is a contingency theory linking appropriate leader style to organizational conditions and subordinate personality Transformational leadership theory concerns the behaviors leaders engage in that inspire high levels of motivation and performance in followers The idea of charismatic leadership is part of transformational leadership theory In behavioural modification the leaders reward power ability to give or withhold reward and punishment is the focus and the importance of giving contingent vs noncontingent rewards is emphasized Managerial roles In the late 1960s Henry Mintzberg a graduate student at MIT carefully studied the activities of five executives On the basis of his observations Mintzberg arrived at three categories that subsume managerial roles interpersonal roles decisional roles and informational roles Motivation Retaining talented and successful employees is a key factor for a company to maintain a competitive advantage An environment where people can use their talent effectively can help motivate even the most smart hardworking difficult individuals Building great people relies on engagement through motivation and behavioral practices OReilly C and Pfeffer J 2000 Baron and Greenberg 2008 wrote that motivation involves the set of processes that arouse direct and maintain human behavior toward attaining some goal There are several different theories of motivation relevant to Organizational Behavior including equity theory expectancy theory Maslows hierarchy of needs incentive theory organizational justice theory Herzbergs twofactor theory and Theory X and Theory Y Types of motivation Intrinsic Motivation This behavior happens out of the pure thought of an individuals need Not as compensation This behavior is used out of the pure need of selfmotivation It is the need to prove ones self worth Extrinsic motivation is triggered by external rewards Meaning the need for a reward outside of themselves feeling accomplished This can be brought to them by a pay raise bonuses rewards like gift cards and many other sorts Public Relations Public relations is the practice of managing the communication between the public and the organization therefore public relations is also related to organizational behavior National culture National culture is thought to affect the behavior of individuals in organizations This idea is exemplified by Hofstedes cultural dimensions theory Hofstede surveyed a large number of cultures and identified six dimensions of national cultures that influence the behavior of individuals in organizations These dimensions include power distance individualism vs collectivism uncertainty avoidance masculinity vs femininity longterm orientation vs short term orientation and indulgence vs restraint Organizational behavior policies Organizational behavior policies inside organizations such as employee dating are rules that can be applied to employees with fairness Labor relations leadership diversity and inclusion policies will have more satisfied employees with organizational behavior policies Policy implications are underutilized in organizations But the need for implications is important Organizational citizenship behavior Organizational citizenship behavior is behavior that goes beyond assigned tasks and contributes to the wellbeing of organizations Organizational culture Organizational culture reflects the values and behaviors that are commonly observed in an organization Investigators who pursue this line of research assume that organizations can be characterized by cultural dimensions such as beliefs values rituals symbols and so forth Researchers have developed models for understanding an organizations culture or developed typologies of organizational culture Edgar Schein developed a model for understanding organizational culture He identified three levels of organizational culture a artifacts and behaviors b espoused values and c shared basic assumptions Specific cultures have been related to organizational performance and effectiveness Personality Personality concerns consistent patterns of behavior cognition and emotion in individuals The study of personality in organizations has generally focused on the relation of specific traits to employee performance There has been a particular focus on the Big Five personality traits which refers to five overarching personality traits Occupational stress There are number of ways to characterize occupational stress One way of characterizing it is to term it an imbalance between job demands aspects of the job that require mental or physical effort and resources that help manage the demands Workfamily conflict Chester Barnard recognized that individuals behave differently when acting in their work role than when acting in roles outside their work role Workfamily conflict occurs when the demands of family and work roles are incompatible and the demands of at least one role interfere with the discharge of the demands of the other Organization theory Organization theory is concerned with explaining the workings of an organization as a whole or of many organizations The focus of organizational theory is to understand the structure and processes of organizations and how organizations interact with each other and the larger society Bureaucracy Max Weber argued that bureaucracy involved the application of rationallegal authority to the organization of work making bureaucracy the most technically efficient form of organization Weber enumerated a number of principles of bureaucratic organization including a formal organizational hierarchy management by rules organization by functional specialty selecting people based on their skills and technical qualifications an upfocused to organizations board or shareholders or infocused to the organization itself mission and a purposefully impersonal environment eg applying the same rules and structures to all members of the organization These rules reflect Weberian ideal types and how they are enacted in organizations varies according to local conditions Charles Perrow extended Webers work arguing that all organizations can be understood in terms of bureaucracy and that organizational failures are more often a result of insufficient application of bureaucratic principles Economic theories of organization At least three theories are relevant here theory of the firm transaction cost economics and agency theory Theories pertaining to organizational structures Theories pertaining to organizational structures and dynamics include complexity theory French and Ravens five bases of power hybrid organization theory informal organizational theory resource dependence theory and Mintzbergs organigraph Institutional theory Systems theory The systems framework is also fundamental to organizational theory Organizations are complex goaloriented entities Alexander Bogdanov an early thinker in the field developed his tectology a theory widely considered a precursor of Bertalanffys general systems theory One of the aims of general systems theory was to model human organizations Kurt Lewin a social psychologist was influential in developing a systems perspective with regard to organizations He coined the term systems of ideology partly based on his frustration with behaviorist psychology which he believed to be an obstacle to sustainable work in psychology Niklas Luhmann a sociologist developed a sociological systems theory Organizational ecology Organizational ecology models apply concepts from evolutionary theory to the study of populations of organizations focusing on birth founding growth and change and death firm mortality In this view organizations are selected based on their fit with their operating environment Scientific management Scientific management refers to an approach to management based on principles of engineering It focuses on incentives and other practices empirically shown to improve productivity Contributing disciplines Anthropology Human resources management Industrialorganizational psychology Personality psychology Social psychology Sociology Models InputsProcessesOutputs IPO framework Inputs Inputs are the variables like personality group structure and organization culture that lead to processes These variables set the stage for what will occur in an organization later Processes Processes are actions that individuals groups and organisations engage in as a result of inputs and that lead to certain outcomes Outcomes Outcomes are the key variables that you want to explain or predict and that are affected by some other variables InputsMediatorsOutputsInputs IMOI framework Adding to the IPO model the IMOI framework emphasizes that outputs can also become subsequent inputs creating a cyclical process Journals See also References Further reading Ash MG 1992 Cultural Contexts and Scientific Change in Psychology Kurt Lewin in Iowa American Psychologist 47 2 198207 doi1010370003066x472198 Hatch MJ 2006 Organization Theory Modern symbolic and postmodern perspectives 2nd Ed Oxford University Press ISBN 0199260214 Helge H Sheehan MJ Cooper CL Einarsen S Organisational Effects of Workplace Bullying in Bullying and Harassment in the Workplace Developments in Theory Research and Practice 2010 Jones Ishmael 2008 The Human Factor Inside the CIAs Dysfunctional Intelligence Culture New York Encounter Books ISBN 9781594033827 Richmond Lewis 2000 Work as a Spiritual Practice A Practical Buddhist Approach to Inner Growth and Satisfaction on the Job Broadway Robbins Stephen P 2004 Organizational Behavior Concepts Controversies Applications 4th Ed Prentice Hall ISBN 0131709011 Robbins S P 2003 Organisational behaviour global and Southern African perspectives Cape Town Pearson Education South Africa Salin D Helge H Organizational Causes of Workplace Bullying in Bullying and Harassment in the Workplace Developments in Theory Research and Practice 2010 Scott W Richard 2007 Organizations and Organizing Rational Natural and Open Systems Perspectives Pearson Prentice Hall ISBN 0131958933 Weick Karl E 1979 The Social Psychology of Organizing 2nd Ed McGraw Hill ISBN 0075548089 Simon Herbert A 1997 Administrative Behavior A Study of DecisionMaking Processes in Administrative Organizations 4th ed The Free Press Tompkins Jonathan R 2005 Organization Theory and Public ManagementThompson Wadsworth ISBN 9780534174682 Kanigel R 1997 The One Best Way Frederick Winslow Taylor and the Enigma of Efficiency London Brown and Co Morgan Gareth 1986 Images of Organization Newbury Park CA Sage Publications",
  },
  {
    title: "Financial management",
    originalContent:
      "Financial management is the business function concerned with profitability expenses cash and credit These are often grouped together under the rubric of maximizing the value of the firm for stockholders The discipline is then tasked with the efficient acquisition and deployment of both short and longterm financial resources to ensure the objectives of the enterprise are achieved Financial managers FM are specialized professionals directly reporting to senior management often the financial director FD the function is seen as staff and not line Role Financial management is generally concerned with short term working capital management focusing on current assets and current liabilities and managing fluctuations in foreign currency and product cycles often through hedging The function also entails the efficient and effective daytoday management of funds and thus overlaps treasury management It is also involved with long term strategic financial management focused on ia capital structure management including capital raising capital budgeting capital allocation between business units or products and dividend policy these latter in large corporates being more the domain of corporate finance Specific tasks Profit maximization happens when marginal cost is equal to marginal revenue This is the main objective of financial management Maintaining proper cash flow is a short run objective of financial management It is necessary for operations to pay the daytoday expenses eg raw material electricity bills wages rent etc A good cash flow ensures the survival of company see cashflow forecast Minimization on capital cost in financial management can help operations gain more profit Estimating the requirement of funds Businesses make forecast on funds needed in both short run and long run hence they can improve the efficiency of funding The estimation is based on the budget eg sales budget production budget see budget analyst Determining the capital structure Capital structure is how a firm finances its overall operations and growth by using different sources of funds Once the requirement of funds has estimated the financial manager should decide the mix of debt and equity and also types of debt Relationship with other areas of finance Two areas of finance directly overlap financial management i Managerial finance is the academic branch of finance concerned with the managerial application of financial techniques ii Corporate finance is mainly concerned with the longer term capital budgeting and typically is more relevant to large corporations Investment management also related is the professional asset management of various securities shares bonds and other securitiesassets In the context of financial management the function sits with treasury usually the management of the various shortterm financial legal instruments contractual duties obligations or rights appropriate to the companys cash and liquidity management requirements See Treasury management Functions The term financial management refers to a companys financial strategy while personal finance or financial life management refers to an individuals management strategy A financial planner or personal financial planner is a professional who prepares financial plans here Financial management systems Financial management systems are the software and technology used by organizations to connect store and report on assets income and expenses Here the discipline relies on a range of products from spreadsheets invariably as a starting point and frequently in total through commercial EPM and BI tools often BusinessObjects SAP OBI EE Oracle Cognos IBM and Power BI Microsoft See Financial modeling Accounting for discussion See also Financial management for IT services financial management of IT assets and resources Financial Management Service a bureau of the US Treasury which provides financial services for the government Financial mismanagement Financial risk management Corporate finance FPA Managerial finance References Further reading Lawrence Gitman and Chad J Zutter 2019 Principles of Managerial Finance 14th edition AddisonWesley Publishing ISBN 9780133507690 Clive Marsh 2009 Mastering Financial Management Financial Times Prentice Hall ISBN 9780273724544 James Van Horne and John Wachowicz 2009 Fundamentals of Financial Management 13th ed Pearson Education Limited ISBN 9705614229",
  },
  {
    title: "Investment",
    originalContent:
      "Investment is traditionally defined as the commitment of resources to achieve later benefits If an investment involves money then it can be defined as a commitment of money to receive more money later From a broader viewpoint an investment can be defined as to tailor the pattern of expenditure and receipt of resources to optimise the desirable patterns of these flows When expenditures and receipts are defined in terms of money then the net monetary receipt in a time period is termed cash flow while money received in a series of several time periods is termed cash flow stream In finance the purpose of investing is to generate a return on the invested asset The return may consist of a capital gain profit or loss realised if the investment is sold unrealised capital appreciation or depreciation if yet unsold It may also consist of periodic income such as dividends interest or rental income The return may also include currency gains or losses due to changes in foreign currency exchange rates Investors generally expect higher returns from riskier investments When a lowrisk investment is made the return is also generally low Similarly high risk comes with a chance of high losses Investors particularly novices are often advised to diversify their portfolio Diversification has the statistical effect of reducing overall risk Types of financial investments In modern economies traditional investments include Stocks Business ownership known as equity in publicly traded companies Bonds loans to governments and businesses traded on public markets Cash holding a particular currency whether in anticipation of spending or to take advantage of or hedge against changes in a currency exchange rate Real estate which can be rented to provide ongoing income or resold if it increases in value Alternative investments include Private equity in businesses that are not publicly traded on a stock exchange often involving venture capital funds angel investors or equity crowdfunding Other loans including mortgages Commodities such as precious metals like gold agricultural products like potatoes and energy deliveries like natural gas Collectables including art coins vintage cars postage stamps and wine Carbon offsets and credits Digital entities like cryptocurrency and nonfungible tokens Hedge funds that use sophisticated techniques like Derivatives the value of which is determined by a contract and is derived by calculation from the performance of some other sort of underlying investment these include forwards futures options swaps collateralized debt obligations credit default swaps and Tax Receivable Agreements Leveraged investing which is the investment of borrowed money Short selling which typically uses leverage and derivatives to bet that the value of a stock will decline Investment and risk An investor may bear a risk of loss of some or all of their capital invested Investment differs from arbitrage in which profit is generated without investing capital or bearing risk Savings bear the normally remote risk that the financial provider may default Foreign currency savings also bear foreign exchange risk if the currency of a savings account differs from the account holders home currency then there is the risk that the exchange rate between the two currencies will move unfavourably so that the value of the savings account decreases measured in the account holders home currency Even investing in tangible assets like property has its risk And similar to most risks property buyers can seek to mitigate any potential risk by taking out mortgage and by borrowing at a lower loan to security ratio In contrast with savings investments tend to carry more risk in the form of both a wider variety of risk factors and a greater level of uncertainty Industry to industry volatility is more or less of a risk depending In biotechnology for example investors look for big profits on companies that have small market capitalizations but can be worth hundreds of millions quite quickly The risk is high because approximately 90 of biotechnology products researched do not make it to market due to regulations and the complex demands within pharmacology as the average prescription drug takes 10 years and US25 billion worth of capital History In the medieval Islamic world the qirad was a major financial instrument This was an arrangement between one or more investors and an agent where the investors entrusted capital to an agent who then traded with it in hopes of making a profit Both parties then received a previously settled portion of the profit though the agent was not liable for any losses Many will notice that the qirad is similar to the institution of the commenda later used in western Europe though whether the qirad transformed into the commenda or the two institutions evolved independently cannot be stated with certainty In the early 1900s purchasers of stocks bonds and other securities were described in media academia and commerce as speculators Since the Wall Street crash of 1929 and particularly by the 1950s the term investment had come to denote the more conservative end of the securities spectrum while speculation was applied by financial brokers and their advertising agencies to higher risk securities much in vogue at that time Since the last half of the 20th century the terms speculation and speculator have specifically referred to higher risk ventures Investment strategies Value investing A value investor buys assets that they believe to be undervalued and sells overvalued ones To identify undervalued securities a value investor uses analysis of the financial reports of the issuer to evaluate the security Value investors employ accounting ratios such as earnings per share and sales growth to identify securities trading at prices below their worth Warren Buffett and Benjamin Graham are notable examples of value investors Graham and Dodds seminal work Security Analysis was written in the wake of the Wall Street Crash of 1929 The price to earnings ratio PE or earnings multiple is a particularly significant and recognized fundamental ratio with a function of dividing the share price of the stock by its earnings per share This will provide the value representing the sum investors are prepared to expend for each dollar of company earnings This ratio is an important aspect due to its capacity as measurement for the comparison of valuations of various companies A stock with a lower PE ratio will cost less per share than one with a higher PE taking into account the same level of financial performance therefore it essentially means a low PE is the preferred option An instance in which the price to earnings ratio has a lesser significance is when companies in different industries are compared For example although it is reasonable for a telecommunications stock to show a PE in the low teens in the case of hitech stock a PE in the 40s range is not unusual When making comparisons the PE ratio can give you a refined view of a particular stock valuation For investors paying for each dollar of a companys earnings the PE ratio is a significant indicator but the pricetobook ratio PB is also a reliable indication of how much investors are willing to spend on each dollar of company assets In the process of the PB ratio the share price of a stock is divided by its net assets any intangibles such as goodwill are not taken into account It is a crucial factor of the pricetobook ratio due to it indicating the actual payment for tangible assets and not the more difficult valuation of intangibles Accordingly the PB could be considered a comparatively conservative metric Growth investing Growth investors seek investments they believe are likely to have higher earnings or greater value in the future To identify such stocks growth investors often evaluate measures of current stock value as well as predictions of future financial performance Growth investors seek profits through capital appreciation the gains earned when a stock is sold at a higher price than what it was purchased for The pricetoearnings PE multiple is also used for this type of investment growth stock are likely to have a PE higher than others in its industry According to Investopedia author Troy Segal and US Department of State Fulbright fintech research awardee Julius Mansa growth investing is best suited for investors who prefer relatively shorter investment horizons higher risks and are not seeking immediate cash flow through dividends Some investors attribute the introduction of the growth investing strategy to investment banker Thomas Rowe Price Jr who tested and popularized the method in 1950 by introducing his mutual fund the T Rowe Price Growth Stock Fund Price asserted that investors could reap high returns by investing in companies that are wellmanaged in fertile fields A new form of investing that seems to have caught the attention of investors is Venture Capital Venture Capital is independently managed dedicated pools of capital that focus on equity or equitylinked investments in privately held high growth companies Momentum investing Momentum investors generally seek to buy stocks that are currently experiencing a shortterm uptrend and they usually sell them once this momentum starts to decrease Stocks or securities purchased for momentum investing are often characterized by demonstrating consistently high returns for the past three to twelve months However in a bear market momentum investing also involves shortselling securities of stocks that are experiencing a downward trend because it is believed that these stocks will continue to decrease in value Essentially momentum investing generally relies on the principle that a consistently uptrending stock will continue to grow while a consistently downtrending stock will continue to fall Economists and financial analysts have not reached a consensus on the effectiveness of using the momentum investing strategy Rather than evaluating a companys operational performance momentum investors instead utilize trend lines moving averages and the Average Directional Index ADX to determine the existence and strength of trends Dollar cost averaging Dollar cost averaging DCA also known in the UK as poundcost averaging is the process of consistently investing a certain amount of money across regular increments of time and the method can be used in conjunction with value investing growth investing momentum investing or other strategies For example an investor who practices dollarcost averaging could choose to invest 200 a month for the next 3 years regardless of the share price of their preferred stocks mutual funds or exchangetraded funds Many investors believe that dollarcost averaging helps minimize shortterm volatility by spreading risk out across time intervals and avoiding market timing Research also shows that DCA can help reduce the total average cost per share in an investment because the method enables the purchase of more shares when their price is lower and less shares when the price is higher However dollarcost averaging is also generally characterized by more brokerage fees which could decrease an investors overall returns The term dollarcost averaging is believed to have first been coined in 1949 by economist and author Benjamin Graham in his book The Intelligent Investor Graham asserted that investors that use DCA are likely to end up with a satisfactory overall price for all their holdings Microinvesting Microinvesting is a type of investment strategy that is designed to make investing regular accessible and affordable especially for those who may not have a lot of money to invest or who are new to investing Intermediaries and collective investments Investments are often made indirectly through intermediary financial institutions These intermediaries include pension funds banks and insurance companies They may pool money received from a number of individual end investors into funds such as investment trusts unit trusts and SICAVs to make largescale investments Each individual investor holds an indirect or direct claim on the assets purchased subject to charges levied by the intermediary which may be large and varied Approaches to investment sometimes referred to in marketing of collective investments include dollar cost averaging and market timing Investment valuation Free cash flow measures the cash a company generates which is available to its debt and equity investors after allowing for reinvestment in working capital and capital expenditure High and rising free cash flow therefore tend to make a company more attractive to investors The debttoequity ratio is an indicator of capital structure A high proportion of debt reflected in a high debttoequity ratio tends to make a companys earnings free cash flow and ultimately the returns to its investors riskier or volatile Investors compare a companys debttoequity ratio with those of other companies in the same industry and examine trends in debttoequity ratios and free cashflow See also References External links",
  },
  {
    title: "Financial planning",
    originalContent:
      "In general usage a financial plan is a comprehensive evaluation of an individuals current pay and future financial state by using current known variables to predict future income asset values and withdrawal plans This often includes a budget which organizes an individuals finances and sometimes includes a series of steps or specific goals for spending and saving in the future This plan allocates future income to various types of expenses such as rent or utilities and also reserves some income for shortterm and longterm savings A financial plan is sometimes referred to as an investment plan but in personal finance a financial plan can focus on other specific areas such as risk management estates college or retirement Context of business In business financial forecast or financial plan can also refer to an projection across a time horizon typically an annual one of income and expenses for a company division or department see Budget Corporate budget More specifically a financial plan can also refer to the three primary financial statements balance sheet income statement and cash flow statement created within a business plan A financial plan can also be an estimation of cash needs and a decision on how to raise the cash such as through borrowing or issuing additional shares in a company Note that the financial plan may then contain prospective financial statements which are similar but different to those of a budget Financial plans are the entire financial accounting overview of a company Complete financial plans contain all periods and transaction types Its a combination of the financial statements which independently only reflect a past present or future state of the company Financial plans are the collection of the historical present and future financial statements for example a historical present costly expense from an operational issue is normally presented prior to the issuance of the prospective financial statements which propose a solution to said operational issue The confusion surrounding the term financial plans might stem from the fact that there are many types of financial statement reports Individually financial statements show either the past present or future financial results More specifically financial statements also only reflect the specific categories which are relevant For instance investing activities are not adequately displayed in a balance sheet A financial plan is a combination of the individual financial statements and reflect all categories of transactions operations expenses investing over time Some periodspecific financial statement examples include pro forma statements historical period and prospective statements current and future period Compilations are a type of service which involves presenting in the form of financial statements information that is the representation of management There are two types of prospective financial statements financial forecasts financial projections and both relate to the currentfuture time period Prospective financial statements are a time periodtype of financial statement which may reflect the currentfuture financial status of a company using three main reportsfinancial statements cash flow statement income statement and balance sheet Prospective financial statements are of two types forecasts and projections Forecasts are based on managements expected financial position results of operations and cash flows Pro Forma statements take previously recorded results the historical financial data and present a whatif whatif a transaction had happened sooner While the common usage of the term financial plan often refers to a formal and defined series of steps or goals there is some technical confusion about what the term financial plan actually means in the industry For example one of the industrys leading professional organizations the Certified Financial Planner Board of Standards lacks any definition for the term financial plan in its Standards of Professional Conduct publication This publication outlines the professional financial planners job and explains the process of financial planning but the term financial plan never appears in the publications text The accounting and finance industries have distinct responsibilities and roles When the products of their work are combined it produces a complete picture a financial plan A financial analyst studies the data and facts regulationsstandards which are processed recorded and presented by accountants Normally finance personnel study the data results meaning what has happened or what might happen and propose a solution to an inefficiency Investors and financial institutions must see both the issue and the solution to make an informed decision Accountants and financial planners are both involved with presenting issues and resolving inefficiencies so together the results and explanation are provided in a financial plan Issues of definition Textbooks used in universities offering financial planningrelated courses also generally do not define the term financial plan For example Sid Mittra Anandi P Sahu and Robert A Crane authors of Practicing Financial Planning for Professionals do not define what a financial plan is but merely defer to the Certified Financial Planner Board of Standards definition of financial planning Planning When drafting a financial plan the company should establish the planning horizon which is the time period of the plan whether it be on a shortterm usually 12 months or longterm two to five years basis Also the individual projects and investment proposals of each operational unit within the company should be totaled and treated as one large project This process is called aggregation See also Capital budgeting Financial planning business Financial plan budget Optimism bias Personal budget Reference class forecasting References External links Prospective Analysis Guidelines for Forecasting Financial Statements Ignacio VelezPareja Joseph Tham 2008 To Plug or Not to Plug that is the Question No Plugs No Circularity A Better Way to Forecast Financial Statements Ignacio VelezPareja 2008 A Step by Step Guide to Construct a Financial Model Without Plugs and Without Circularity for Valuation Purposes Ignacio VelezPareja 2008 LongTerm Financial Statements Forecasting Reinvesting Retained Earnings Sergei Cheremushkin 2008 The Plan for Financial Comfort Phronesis Wealth Management 2023",
  },
  {
    title: "Econometrics",
    originalContent:
      "Econometrics is an application of statistical methods to economic data in order to give empirical content to economic relationships More precisely it is the quantitative analysis of actual economic phenomena based on the concurrent development of theory and observation related by appropriate methods of inference An introductory economics textbook describes econometrics as allowing economists to sift through mountains of data to extract simple relationships Jan Tinbergen is one of the two founding fathers of econometrics The other Ragnar Frisch also coined the term in the sense in which it is used today A basic tool for econometrics is the multiple linear regression model Econometric theory uses statistical theory and mathematical statistics to evaluate and develop econometric methods Econometricians try to find estimators that have desirable statistical properties including unbiasedness efficiency and consistency Applied econometrics uses theoretical econometrics and realworld data for assessing economic theories developing econometric models analysing economic history and forecasting Basic models linear regression A basic tool for econometrics is the multiple linear regression model In modern econometrics other statistical tools are frequently used but linear regression is still the most frequently used starting point for an analysis Estimating a linear regression on two variables can be visualized as fitting a line through data points representing paired values of the independent and dependent variables For example consider Okuns law which relates GDP growth to the unemployment rate This relationship is represented in a linear regression where the change in unemployment rate Unemployment displaystyle Delta textUnemployment is a function of an intercept 0 displaystyle beta _0 a given value of GDP growth multiplied by a slope coefficient 1 displaystyle beta _1 and an error term displaystyle varepsilon Unemployment 0 1 Growth displaystyle Delta textUnemploymentbeta _0beta _1textGrowthvarepsilon The unknown parameters 0 displaystyle beta _0 and 1 displaystyle beta _1 can be estimated Here 0 displaystyle beta _0 is estimated to be 083 and 1 displaystyle beta _1 is estimated to be 177 This means that if GDP growth increased by one percentage point the unemployment rate would be predicted to drop by 177 1 points other things held constant The model could then be tested for statistical significance as to whether an increase in GDP growth is associated with a decrease in the unemployment as hypothesized If the estimate of 1 displaystyle beta _1 were not significantly different from 0 the test would fail to find evidence that changes in the growth rate and unemployment rate were related The variance in a prediction of the dependent variable unemployment as a function of the independent variable GDP growth is given in polynomial least squares Theory Econometric theory uses statistical theory and mathematical statistics to evaluate and develop econometric methods Econometricians try to find estimators that have desirable statistical properties including unbiasedness efficiency and consistency An estimator is unbiased if its expected value is the true value of the parameter it is consistent if it converges to the true value as the sample size gets larger and it is efficient if the estimator has lower standard error than other unbiased estimators for a given sample size Ordinary least squares OLS is often used for estimation since it provides the BLUE or best linear unbiased estimator where best means most efficient unbiased estimator given the GaussMarkov assumptions When these assumptions are violated or other statistical properties are desired other estimation techniques such as maximum likelihood estimation generalized method of moments or generalized least squares are used Estimators that incorporate prior beliefs are advocated by those who favour Bayesian statistics over traditional classical or frequentist approaches Methods Applied econometrics uses theoretical econometrics and realworld data for assessing economic theories developing econometric models analysing economic history and forecasting Econometrics uses standard statistical models to study economic questions but most often these are based on observational data rather than data from controlled experiments In this the design of observational studies in econometrics is similar to the design of studies in other observational disciplines such as astronomy epidemiology sociology and political science Analysis of data from an observational study is guided by the study protocol although exploratory data analysis may be useful for generating new hypotheses Economics often analyses systems of equations and inequalities such as supply and demand hypothesized to be in equilibrium Consequently the field of econometrics has developed methods for identification and estimation of simultaneous equations models These methods are analogous to methods used in other areas of science such as the field of system identification in systems analysis and control theory Such methods may allow researchers to estimate models and investigate their empirical consequences without directly manipulating the system In the absence of evidence from controlled experiments econometricians often seek illuminating natural experiments or apply quasiexperimental methods to draw credible causal inference The methods include regression discontinuity design instrumental variables and differenceindifferences Example A simple example of a relationship in econometrics from the field of labour economics is ln wage 0 1 years of education displaystyle lntextwagebeta _0beta _1textyears of educationvarepsilon This example assumes that the natural logarithm of a persons wage is a linear function of the number of years of education that person has acquired The parameter 1 displaystyle beta _1 measures the increase in the natural log of the wage attributable to one more year of education The term displaystyle varepsilon is a random variable representing all other factors that may have direct influence on wage The econometric goal is to estimate the parameters 0 and 1 displaystyle beta _0mbox and beta _1 under specific assumptions about the random variable displaystyle varepsilon For example if displaystyle varepsilon is uncorrelated with years of education then the equation can be estimated with ordinary least squares If the researcher could randomly assign people to different levels of education the data set thus generated would allow estimation of the effect of changes in years of education on wages In reality those experiments cannot be conducted Instead the econometrician observes the years of education of and the wages paid to people who differ along many dimensions Given this kind of data the estimated coefficient on years of education in the equation above reflects both the effect of education on wages and the effect of other variables on wages if those other variables were correlated with education For example people born in certain places may have higher wages and higher levels of education Unless the econometrician controls for place of birth in the above equation the effect of birthplace on wages may be falsely attributed to the effect of education on wages The most obvious way to control for birthplace is to include a measure of the effect of birthplace in the equation above Exclusion of birthplace together with the assumption that displaystyle epsilon is uncorrelated with education produces a misspecified model Another technique is to include in the equation additional set of measured covariates which are not instrumental variables yet render 1 displaystyle beta _1 identifiable An overview of econometric methods used to study this problem were provided by Card 1999 Journals The main journals that publish work in econometrics are Econometrica which is published by Econometric Society The Review of Economics and Statistics which is over 100 years old The Econometrics Journal which was established by the Royal Economic Society The Journal of Econometrics which also publishes the supplement Annals of Econometrics Econometric Theory which is a theoretical journal The Journal of Applied Econometrics which applies econometrics to a wide various problems Econometric Reviews which includes reviews on econometric books and software as well The Journal of Business Economic Statistics which is published by the American Statistical Association Limitations and criticisms Like other forms of statistical analysis badly specified econometric models may show a spurious relationship where two variables are correlated but causally unrelated In a study of the use of econometrics in major economics journals McCloskey concluded that some economists report pvalues following the Fisherian tradition of tests of significance of point nullhypotheses and neglect concerns of type II errors some economists fail to report estimates of the size of effects apart from statistical significance and to discuss their economic importance She also argues that some economists also fail to use economic reasoning for model selection especially for deciding which variables to include in a regression In some cases economic variables cannot be experimentally manipulated as treatments randomly assigned to subjects In such cases economists rely on observational studies often using data sets with many strongly associated covariates resulting in enormous numbers of models with similar explanatory ability but different covariates and regression estimates Regarding the plurality of models compatible with observational datasets Edward Leamer urged that professionals properly withhold belief until an inference can be shown to be adequately insensitive to the choice of assumptions See also Further reading Econometric Theory book on Wikibooks Giovannini Enrico Understanding Economic Statistics OECD Publishing 2008 ISBN 9789264033122 References External links Journal of Financial Econometrics Econometric Society The Econometrics Journal Econometric Links Teaching Econometrics Index by the Economics Network UK Applied Econometric Association The Society for Financial Econometrics The interview with Clive Granger Nobel winner in 2003 about econometrics",
  },
  {
    title: "Financial analysis",
    originalContent:
      "Financial analysis also known as financial statement analysis accounting analysis or analysis of finance refers to an assessment of the viability stability and profitability of a business subbusiness project or investment It is performed by professionals who prepare reports using ratios and other techniques that make use of information taken from financial statements and other reports These reports are usually presented to top management as one of their bases in making business decisions Financial analysis may determine if a business will Continue or discontinue its main operation or part of its business Make or purchase certain materials in the manufacture of its product Acquire or rentlease certain machineries and equipment in the production of its goods Issue shares or negotiate for a bank loan to increase its working capital Make decisions regarding investing or lending capital Make other decisions that allow management to make an informed selection on various alternatives in the conduct of its business Firmlevel analysis Financial analysts often assess the following elements of a firm Profitability its ability to earn income and sustain growth in both the short and longterm A companys degree of profitability is usually based on the income statement which reports on the companys results of operations Solvency its ability to pay its obligation to creditors and other third parties in the longterm Liquidity its ability to maintain positive cash flow while satisfying immediate obligations Stability the firms ability to remain in business in the long run without having to sustain significant losses in the conduct of its business Assessing a companys stability requires the use of both the income statement and the balance sheet as well as other financial and nonfinancial indicators Both 2 and 3 are based on the companys balance sheet which indicates the financial condition of a business as of a given point in time Techniques Financial analysts often compare financial ratios of solvency profitability growth etc Past Performance Across historical time periods for the same firm the last 5 years for example Future Performance Using historical figures and certain mathematical and statistical techniques including present and future values This extrapolation method is the main source of errors in financial analysis as past statistics can be poor predictors of future prospects Comparative Performance Comparison between similar firms Comparing financial ratios is merely one way of conducting financial analysis Financial analysts can also use percentage analysis which involves reducing a series of figures as a percentage of some base amount For example a group of items can be expressed as a percentage of net income When proportionate changes in the same figure over a given time period expressed as a percentage is known as horizontal analysis Vertical or commonsize analysis reduces all items on a statement to a common size as a percentage of some base value which assists in comparability with other companies of different sizes As a result all Income Statement items are divided by Sales and all Balance Sheet items are divided by Total Assets Another method is comparative analysis This provides a better way to determine trends Comparative analysis presents the same information for two or more time periods and is presented sidebyside to allow for easy analysis Theoretical challenges Financial ratios face several theoretical challenges They say little about the firms prospects in an absolute sense Their insights about relative performance require a reference point from other time periods or similar firms One ratio holds little meaning As indicators ratios can be logically interpreted in at least two ways One can partially overcome this problem by combining several related ratios to paint a more comprehensive picture of the firms performance Seasonal factors may prevent yearend values from being representative A ratios values may be distorted as account balances change from the beginning to the end of an accounting period Use average values for such accounts whenever possible Financial ratios are no more objective than the accounting methods employed Changes in accounting policies or choices can yield drastically different ratio values See also Business valuation Economic base analysis Financial accounting Financial forecast Cash flow forecast Financial modeling Accounting Financial risk management Corporate finance Financial statement Going concern Fundamental basis of financial statements Managerial risk accounting Notes External links SFAF the French Society of Financial Analysts ACIIA Association of Certified International Investment Analysts EFFAS European Federation of Financial Analysts Societies",
  },
  {
    title: "International trade",
    originalContent:
      "International trade is the exchange of capital goods and services across international borders or territories because there is a need or want of goods or services See World economy In most countries such trade represents a significant share of gross domestic product GDP While international trade has existed throughout history for example Uttarapatha Silk Road Amber Road salt roads its economic social and political importance has been on the rise in recent centuries Carrying out trade at an international level is a complex process when compared to domestic trade When trade takes place between two or more states factors like currency government policies economy judicial system laws and markets influence trade To ease and justify the process of trade between countries of different economic standing in the modern era some international economic organizations were formed such as the World Trade Organization These organizations work towards the facilitation and growth of international trade Statistical services of intergovernmental and supranational organizations and governmental statistical agencies publish official statistics on international trade Characteristics of global trade A product that is transferred or sold from a party in one country to a party in another country is an export from the originating country and an import to the country receiving that product Imports and exports are accounted for in a countrys current account in the balance of payments Trading globally may give consumers and countries the opportunity to be exposed to new markets and products Almost every kind of product can be found in the international market for example food clothes spare parts oil jewellery wine stocks currencies and water Services are also traded such as in tourism banking consulting and transportation Advanced technology including transportation globalization industrialization outsourcing and multinational corporations have major impacts on the international trade systems Differences from domestic trade International trade is in principle not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not However in practical terms carrying out trade at an international level is typically a more complex process than domestic trade The main difference is that international trade is typically more costly than domestic trade This is due to the fact that crossborder trade typically incurs additional costs such as explicit tariffs as well as explicit or implicit nontariff barriers such as time costs due to border delays language and cultural differences product safety the legal system and so on Another difference between domestic and international trade is that factors of production such as capital and labor are often more mobile within a country than across countries Thus international trade is mostly restricted to trade in goods and services and only to a lesser extent to trade in capital labour or other factors of production Trade in goods and services can serve as a substitute for trade in factors of production Instead of importing a factor of production a country can import goods that make intensive use of that factor of production and thus embody it An example of this is the import of laborintensive goods by the United States from China Instead of importing Chinese labor the United States imports goods that were produced with Chinese labor One report in 2010 suggested that international trade was increased when a country hosted a network of immigrants but the trade effect was weakened when the immigrants became assimilated into their new country History The history of international trade chronicles notable events that have affected trading among various economies Theories and models There are several models that seek to explain the factors behind international trade the welfare consequences of trade and the pattern of trade Most traded export products Largest countries or regions by total international trade The following table is a list of the 25 largest trading states according to the World Trade Organization in 2021 and 2022 Top traded commodities by value exports Source International Trade Centre Observances In the US the various US Presidents have held observances to promote big and small companies to be more involved with the export and import of goods and services President George W Bush observed World Trade Week on May 18 2001 and May 17 2002 On May 13 2016 President Barack Obama proclaimed May 15 through May 21 2016 World Trade Week 2016 On May 19 2017 President Donald Trump proclaimed May 21 through May 27 2017 World Trade Week 2017 World Trade Week is the third week of May Every year the President declares that week to be World Trade Week International trade versus local production Food security The tradeoffs between local food production and distant food production are controversial with limited studies comparing environmental impact and scientists cautioning that regionally specific environmental impacts should be considered A 2020 study indicated that local food crop production alone cannot meet the demand for most food crops with current production and consumption patterns and the locations of food production at the time of the study for 7289 of the global population and 100 km radiuses as of early 2020 Studies found that food miles are a relatively minor factor for carbon emissions albeit increased food localization may also enable additional more significant environmental benefits such as recycling of energy water and nutrients For specific foods regional differences in harvest seasons may make it more environmentally friendly to import from distant regions than more local production and storage or local production in greenhouses Qualitative differences and economic aspects Qualitative differences between substitutive products of different production regions may exist due to different legal requirements and quality standards or different levels of controllability by local production and governancesystems which may have aspects of security beyond resource security environmental protection product quality and product design and health The process of transforming supply as well as labor rights may differ as well Local production has been reported to increase local employment in many cases A 2018 study claimed that international trade can increase local employment A 2016 study found that local employment and total labor income in both manufacturing and nonmanufacturing were negatively affected by rising exposure to imports Local production in highincome countries rather than distant regions may require higher wages for workers Higher wages incentivize automation which could allow for automated workers time to be reallocated by society and its economic mechanisms or be converted into leisurelike time Specialization production efficiency and regional differences Local production may require knowledge transfer technology transfer and may not be able to compete in efficiency initially with specialized established industries and businesses or in consumer demand without policy measures such as ecotariffs Regional differences may cause specific regions to be more suitable for a specific production thereby increasing the advantages of specific trade over specific local production Forms of local products that are highly localized may not be able to meet the efficiency of more largescale highly consolidated production in terms of efficiency including environmental impact Resource security A systematic and possibly first largescale crosssectoral analysis of water energy and land in security in 189 countries that links total and sectorial consumption to sources showed that countries and sectors are highly exposed to overexploited insecure and degraded such resources with economic globalization having decreased security of global supply chains The 2020 study finds that most countries exhibit greater exposure to resource risks via international trade mainly from remote production sources and that diversifying trading partners is unlikely to help countries and sectors to reduce these or to improve their resource selfsufficiency Illicit trade Illegal gold trade A number of people in Africa including children were using informal or artisanal methods to produce gold While millions were making a livelihood through this smallscale mining governments of Ghana Tanzania and Zambia complained about the increase in illegal production and gold smuggling Sometimes the procedure involved criminal operations and even human and environmental cost Investigative reports based on Africas export data revealed that gold in large quantities is smuggled out of the country through the United Arab Emirates without any taxes being paid to the producing states Analysis also reflected discrepancies in the amount exported from Africa and the total gold imported into the UAE In July 2020 a report by Swissaid highlighted that the Dubaibased precious metal refining firms including Kaloti Jewellery International Group and Trust One Financial Services T1FS received most of their gold from poor African states like Sudan The gold mines in Sudan were seldom under the militias involved in war crimes and human rights abuses The Swissaid report also highlighted that the illicit gold coming into Dubai from Africa is imported in large quantities by the worlds largest refinery in Switzerland Valcambi Another report in March 2022 revealed the contradiction between the lucrative gold trade of West African countries and the illicit dealings Like Sudan Democratic Republic of Congo DRC Ghana and other states discrepancies were recorded between the gold production in Mali and its trade with Dubai UAE The third largest gold exporter in Africa Mali imposed taxes only on the first 50 kg of gold exports per month which allowed several smallscale miners to enjoy tax exemptions and smuggle gold worth millions In 2014 Malis gold production was 458 tonnes while the UAEs gold imports were 599 tonnes See also Lists List of countries by current account balance List of countries by imports List of countries by exports List of countries by merchandise exports List of countries by service exports List of international trade topics References Further reading Nelson Scott Reynolds Oceans of Grain How American Wheat Remade the World 2022 excerpt Linsi Lukas Burgoon Brian Mgge Daniel K 2023 The Problem with Trade Measurement in International Relations International Studies Quarterly 67 2 Sources External links Data Statistics from intergovernmental sources Data on the value of exports and imports and their quantities often broken down by detailed lists of products are available in statistical collections on international trade published by the statistical services of intergovernmental and supranational organisations and national statistical institutes The definitions and methodological concepts applied for the various statistical collections on international trade often differ in terms of definition eg special trade vs general trade and coverage reporting thresholds inclusion of trade in services estimates for smuggled goods and crossborder provision of illegal services Metadata providing information on definitions and methods are often published along with the data United Nations Commodity Trade Database Trade Map trade statistics for international business development WTO Statistics Portal Statistical Portal OECD European Union International Trade in Goods Data Food and Agricultural Trade Data Archived 20100710 at the Wayback Machine by FAO Other external links The MIT Observatory of Economic Complexity The McGill Faculty of Law runs a Regional Trade Agreements Database that contains the text of almost all preferential and regional trade agreements in the world ptasmcgillca Historical documents on international trade available on FRASER St Louis Fed",
  },
  {
    title: "Environmental economics",
    originalContent:
      "Environmental economics is a subfield of economics concerned with environmental issues It has become a widely studied subject due to growing environmental concerns in the twentyfirst century Environmental economics undertakes theoretical or empirical studies of the economic effects of national or local environmental policies around the world Particular issues include the costs and benefits of alternative environmental policies to deal with air pollution water quality toxic substances solid waste and global warming Environmental economics is distinguished from ecological economics in that ecological economics emphasizes the economy as a subsystem of the ecosystem with its focus upon preserving natural capital One survey of German economists found that ecological and environmental economics are different schools of economic thought with ecological economists emphasizing strong sustainability and rejecting the proposition that humanmade physical capital can substitute for natural capital History The modern field of environmental economics has been traced to the 1960s with significant contribution from PostKeynesian economist Paul Davidson who had just completed a management position with the Continental Oil Company Topics and concepts Market failure Central to environmental economics is the concept of market failure Market failure means that markets fail to allocate resources efficiently As stated by Hanley Shogren and White 2007 A market failure occurs when the market does not allocate scarce resources to generate the greatest social welfare A wedge exists between what a private person does given market prices and what society might want him or her to do to protect the environment Such a wedge implies wastefulness or economic inefficiency resources can be reallocated to make at least one person better off without making anyone else worse off This results in a inefficient market that needs to be corrected through avenues such as government intervention Common forms of market failure include externalities nonexcludability and nonrivalry Externality An externality exists when a person makes a choice that affects other people in a way that is not accounted for in the market price An externality can be positive or negative but is usually associated with negative externalities in environmental economics For instance water seepage in residential buildings occurring in upper floors affect the lower floors Another example concerns how the sale of Amazon timber disregards the amount of carbon dioxide released in the cutting Or a firm emitting pollution will typically not take into account the costs that its pollution imposes on others As a result pollution may occur in excess of the socially efficient level which is the level that would exist if the market was required to account for the pollution A classic definition influenced by Kenneth Arrow and James Meade is provided by Heller and Starrett 1976 who define an externality as a situation in which the private economy lacks sufficient incentives to create a potential market in some good and the nonexistence of this market results in losses of Pareto efficiency In economic terminology externalities are examples of market failures in which the unfettered market does not lead to an efficient outcome Common goods and public goods When it is too costly to exclude some people from access to an environmental resource the resource is either called a common property resource when there is rivalry for the resource such that one persons use of the resource reduces others opportunity to use the resource or a public good when use of the resource is nonrivalrous In either case of nonexclusion market allocation is likely to be inefficient These challenges have long been recognized Hardins 1968 concept of the tragedy of the commons popularized the challenges involved in nonexclusion and common property Commons refers to the environmental asset itself common property resource or common pool resource refers to a property right regime that allows for some collective body to devise schemes to exclude others thereby allowing the capture of future benefit streams and openaccess implies no ownership in the sense that property everyone owns nobody owns The basic problem is that if people ignore the scarcity value of the commons they can end up expending too much effort over harvesting a resource eg a fishery Hardin theorizes that in the absence of restrictions users of an openaccess resource will use it more than if they had to pay for it and had exclusive rights leading to environmental degradation See however Ostroms 1990 work on how people using real common property resources have worked to establish selfgoverning rules to reduce the risk of the tragedy of the commons The mitigation of climate change effects is an example of a public good where the social benefits are not reflected completely in the market price Because the personal marginal benefits are less than the social benefits the market underprovides climate change mitigation This is a public good since the risks of climate change are both nonrival and nonexcludable Such efforts are nonrival since climate mitigation provided to one does not reduce the level of mitigation that anyone else enjoys They are nonexcludable actions as they will have global consequences from which no one can be excluded A countrys incentive to invest in carbon abatement is reduced because it can free ride off the efforts of other countries Over a century ago Swedish economist Knut Wicksell 1896 first discussed how public goods can be underprovided by the market because people might conceal their preferences for the good but still enjoy the benefits without paying for them Global biochemical cycles Valuation Assessing the economic value of the environment is a major topic within the field The values of natural resources often are not reflected in prices that markets set and in fact many of them are available at no monetary charge This mismatch frequently causes distortions in pricing of natural assets both overuse of them and underinvestment in them Economic value or tangible benefits of ecosystem services and more generally of natural resources include both use and indirect see the nature section of ecological economics Nonuse values include existence option and bequest values For example some people may value the existence of a diverse set of species regardless of the effect of the loss of a species on ecosystem services The existence of these species may have an option value as there may be the possibility of using it for some human purpose For example certain plants may be researched for drugs Individuals may value the ability to leave a pristine environment for their children Use and indirect use values can often be inferred from revealed behavior such as the cost of taking recreational trips or using hedonic methods in which values are estimated based on observed prices Nonuse values are usually estimated using stated preference methods such as contingent valuation or choice modelling Contingent valuation typically takes the form of surveys in which people are asked how much they would pay to observe and recreate in the environment willingness to pay or their willingness to accept WTA compensation for the destruction of the environmental good Hedonic pricing examines the effect the environment has on economic decisions through housing prices traveling expenses and payments to visit parks State subsidy Almost all governments and states magnify environmental harm by providing various types of subsidies that have the effect of paying companies and other economic actors more to exploit natural resources than to protect them The damage to nature of such public subsidies has been conservatively estimated at 46 trillion US dollars per year Solutions Solutions advocated to correct such externalities include Environmental regulations Under this plan the economic impact has to be estimated by the regulator Usually this is done using costbenefit analysis There is a growing realization that regulations also known as command and control instruments are not so distinct from economic instruments as is commonly asserted by proponents of environmental economics Eg1 regulations are enforced by fines which operate as a form of tax if pollution rises above the threshold prescribed Eg2 pollution must be monitored and laws enforced whether under a pollution tax regime or a regulatory regime The main difference an environmental economist would argue exists between the two methods however is the total cost of the regulation Command and control regulation often applies uniform emissions limits on polluters even though each firm has different costs for emissions reductions ie some firms in this system can abate pollution inexpensively while others can only abate it at high cost Because of this the total abatement in the system comprises some expensive and some inexpensive efforts Consequently modern Command and control regulations are oftentimes designed in a way that addresses these issues by incorporating utility parameters For instance CO2 emission standards for specific manufacturers in the automotive industry are either linked to the average vehicle footprint US system or average vehicle weight EU system of their entire vehicle fleet Environmental economic regulations find the cheapest emission abatement efforts first and then move on to the more expensive methods Eg as said earlier trading in the quota system means a firm only abates pollution if doing so would cost less than paying someone else to make the same reduction This leads to a lower cost for the total abatement effort as a whole Quotas on pollution Often it is advocated that pollution reductions should be achieved by way of tradeable emissions permits which if freely traded may ensure that reductions in pollution are achieved at least cost In theory if such tradeable quotas are allowed then a firm would reduce its own pollution load only if doing so would cost less than paying someone else to make the same reduction ie only if buying tradeable permits from another firms is costlier In practice tradeable permits approaches have had some success such as the USs sulphur dioxide trading program or the EU Emissions Trading Scheme and interest in its application is spreading to other environmental problems Taxes and tariffs on pollution Increasing the costs of polluting will discourage polluting and will provide a dynamic incentive that is the disincentive continues to operate even as pollution levels fall A pollution tax that reduces pollution to the socially optimal level would be set at such a level that pollution occurs only if the benefits to society for example in form of greater production exceeds the costs This concept was introduced by Arthur Pigou a British economist active in the late nineteenth through the midtwentieth century He showed that these externalities occur when markets fail meaning they do not naturally produce the socially optimal amount of a good or service He argued that a tax on the production of paint would encourage the polluting factory to reduce production to the amount best for society as a whole These taxes are known amongst economists as Pigouvian Taxes and they regularly implemented where negative externalities are present Some advocate a major shift from taxation from income and sales taxes to tax on pollution the socalled green tax shift Better defined property rights The Coase Theorem states that assigning property rights will lead to an optimal solution regardless of who receives them if transaction costs are trivial and the number of parties negotiating is limited For example if people living near a factory had a right to clean air and water or the factory had the right to pollute then either the factory could pay those affected by the pollution or the people could pay the factory not to pollute Or citizens could take action themselves as they would if other property rights were violated The US River Keepers Law of the 1880s was an early example giving citizens downstream the right to end pollution upstream themselves if the government itself did not act an early example of bioregional democracy Many markets for pollution rights have been created in the late twentieth centurysee emissions trading According to the Coase Theorem the involved parties will bargain with each other which results in an efficient solution However modern economic theory has shown that the presence of asymmetric information may lead to inefficient bargaining outcomes Specifically Rob 1989 has shown that pollution claim settlements will not lead to the socially optimal outcome when the individuals that will be affected by pollution have learned private information about their disutility already before the negotiations take place Goldlcke and Schmitz 2018 have shown that inefficiencies may also result if the parties learn their private information only after the negotiations provided that the feasible transfer payments are bounded Using cooperative game theory Gonzalez Marciano and Solal 2019 have shown that in social cost problems involving more than three agents the Coase theorem suffers from many counterexamples and that only two types of property rights lead to an optimal solution Accounting for environmental externalities in the final price In fact the worlds largest industries burn about 73 trillion of free natural capital per year Thus the worlds largest industries would hardly be profitable if they had to pay for this destruction of natural capital Trucost has assessed over 100 direct environmental impacts and condensed them into 6 key environmental performance indicators EKPIs The assessment of environmental impacts is derived from different sources academic journals governments studies etc due to the lack of market prices The table below gives an overview of the 5 regional sectors per EKPI with the highest impact on the overall EKPI If companies are allowed to include some of these externalities in their final prices this could undermine the Jevons paradox and provide enough revenue to help companies innovate Relationship to other fields Environmental economics is related to ecological economics but there are differences Most environmental economists have been trained as economists They apply the tools of economics to address environmental problems many of which are related to socalled market failurescircumstances wherein the invisible hand of economics is unreliable Most ecological economists have been trained as ecologists but have expanded the scope of their work to consider the impacts of humans and their economic activity on ecological systems and services and vice versa This field takes as its premise that economics is a strict subfield of ecology Ecological economics is sometimes described as taking a more pluralistic approach to environmental problems and focuses more explicitly on longterm environmental sustainability and issues of scale Environmental economics is viewed as more idealistic in a price system ecological economics as more realistic in its attempts to integrate elements outside of the price system as primary arbiters of decisions These two groups of specialisms sometimes have conflicting views which may be traced to the different philosophical underpinnings Another context in which externalities apply is when globalization permits one player in a market who is unconcerned with biodiversity to undercut prices of another who is creating a race to the bottom in regulations and conservation This in turn may cause loss of natural capital with consequent erosion water purity problems diseases desertification and other outcomes that are not efficient in an economic sense This concern is related to the subfield of sustainable development and its political relation the antiglobalization movement Environmental economics was once distinct from resource economics Natural resource economics as a subfield began when the main concern of researchers was the optimal commercial exploitation of natural resource stocks But resource managers and policymakers eventually began to pay attention to the broader importance of natural resources eg values of fish and trees beyond just their commercial exploitation It is now difficult to distinguish environmental and natural resource economics as separate fields as the two became associated with sustainability Many of the more radical green economists split off to work on an alternate political economy Environmental economics was a major influence on the theories of natural capitalism and environmental finance which could be said to be two subbranches of environmental economics concerned with resource conservation in production and the value of biodiversity to humans respectively The theory of natural capitalism Hawken Lovins Lovins goes further than traditional environmental economics by envisioning a world where natural services are considered on par with physical capital The more radical green economists reject neoclassical economics in favour of a new political economy beyond capitalism or communism that gives a greater emphasis to the interaction of the human economy and the natural environment acknowledging that economy is threefifths of ecology This political group is a proponent of a transition to renewable energy These more radical approaches would imply changes to money supply and likely also a bioregional democracy so that political economic and ecological environmental limits were all aligned and not subject to the arbitrage normally possible under capitalism An emerging subfield of environmental economics studies its intersection with development economics Dubbed envirodevonomics by Michael Greenstone and B Kelsey Jack in their paper Envirodevonomics A Research Agenda for a Young Field the subfield is primarily interested in studying why environmental quality is so poor in developing countries A strategy for better understanding this correlation between a countrys GDP and its environmental quality involves analyzing how many of the central concepts of environmental economics including market failures externalities and willingness to pay may be complicated by the particular problems facing developing countries such as political issues lack of infrastructure or inadequate financing tools among many others In the field of law and economics environmental law is studied from an economic perspective The economic analysis of environmental law studies instruments such as zoning expropriation licensing third party liability safety regulation mandatory insurance and criminal sanctions A book by Michael Faure 2003 surveys this literature Professional bodies The main academic and professional organizations for the discipline of Environmental Economics are the Association of Environmental and Resource Economists AERE and the European Association for Environmental and Resource Economics EAERE The main academic and professional organization for the discipline of Ecological Economics is the International Society for Ecological Economics ISEE The main organization for Green Economics is the Green Economics Institute By country India The Indian government promotes the Bharatiya model of development considered different from western models The Economic Survey for the year 2024 noted that often solutions to address climate change are fuelled by a market society which seeks to substitute the means to achieve overconsumption rather than addressing overconsumption itself The report argued that India needs a different approach and a Bharatiya Model of Development linked to the principles of sustainability and to the Indian philosophy can help See also Hypotheses and theorems Coase theorem Porter hypothesis Notes References Further reading",
  },
  {
    title: "Sports economics",
    originalContent:
      "Sports economics is a discipline of economics focused on its relationship to sports It covers both the ways in which economists can study the distinctive institutions of sports and the ways in which sports can allow economists to research many topics including discrimination and antitrust law The theoretical foundations of the discipline are heavily based on microeconomics As of 2006 about 100 to 120 college professors taught sports economics courses As of 2024 there are a number of important locations where sports economics is taught and researched by a group of faculty This includes Bielefeld Cork Liverpool Reading and Zurich in Europe as well as Michigan and West Virginia in the United States The community normally meets annually with prestigious events including the North American Association of Sports Economists NAASE Conference and the European Sport Economics Association ESEA Conference The beginning of sports economics Simon Rottenberg is credited with likely being the first to pen a scholarly article in the field of sports economics when he wrote his 1956 article on the Uncertainty of Outcome Hypothesis In this article Rottenberg highlights the relationship of attendance at baseball games with things such as price alternate activities how good the team is how large of a market the team is in and so on Importantly Rottenberg made mention of dispersion of games won by team in the league Key components Competitive balance is one of the most important ideas within sports economics This idea in general refers to the comparison of wins between all teams in a league Rottenberg effectively built this seminal idea with his interest in dispersion of games won Related to competitive balance is the understanding of different leagues and different team within those leagues objectives Understanding the ownership structure and motives of front office personnel through their financial read economic decisions will reveal whether a team is looking to only generate profit attempt to win a championship or something entirely different Making sense of human behavior through data is the central idea of economics and certainly applies to Sports Economics as well Sports leagues look to promote competitive balance to make more games appealing to fans to watch both in person and on TV Many European leagues achieve competitive balance through promotion and relegation systems where multiple leagues are intertwined with teams moving between leagues based upon performance Most sports leagues in the United States are standalone league and work towards competitive balance through other measures These can include salary caps and roster size limits Importance Sometimes sports economics is dismissed as a side hobby for number crunchers However the proliferation of globalized sports markets as well as the extreme rise in sports media demonstrates its importance In the United States the Super Bowl regularly commands the attention of millions In the EU over 15 million people are employed in the sports world On a larger scale sports and sporting outlets provide health benefits as well as general satisfaction for a citizenry which are both prominent state concerns the world over Another feature of sports economics is the datarich environment in sports from which economists can apply and investigate common economic models or problems thus contributing to the field of economics at large References",
  },
  {
    title: "Urban economics",
    originalContent:
      "Urban economics is broadly the economic study of urban areas as such it involves using the tools of economics to analyze urban issues such as crime education public transit housing and local government finance More specifically it is a branch of microeconomics that studies the urban spatial structure and the location of households and firms Quigley 2008 Historically much like economics generally urban economics was influenced by multiple schools of thought including original institutional economics and Marxist economics These heterodox economic currents continue to be used in contemporary politicaleconomic analyses of cities But most urban economics today is neoclassical in orientation and centred largely around urban experiences in the Global North This dominant urban economics also influences mainstream media like The Economist Today much urban economic analysis relies on a particular model of urban spatial structure the monocentric city model pioneered in the 1960s by William Alonso Richard Muth and Edwin Mills While most other forms of neoclassical economics do not account for spatial relationships between individuals and organizations urban economics focuses on these spatial relationships to understand the economic motivations underlying the formation functioning and development of cities Since its formulation in 1964 Alonsos monocentric city model of a discshaped Central Business District CBD and the surrounding residential region has served as a starting point for urban economic analysis Monocentricity has weakened over time because of changes in technology particularly faster and cheaper transportation which makes it possible for commuters to live farther from their jobs in the CBD and communications which allow backoffice operations to move out of the CBD Additionally recent research has sought to explain the polycentricity described in Joel Garreaus Edge City Several explanations for polycentric expansion have been proposed and summarized in models that account for factors such as utility gains from lower average land rents and increasing or constant returns due to economies of agglomeration Strange 2008 Introduction Urban economics is rooted in the location theories of von Thnen Alonso Christaller and Lsch that began the process of spatial economic analysis Capello Nijkamp 200434 Economics is the study of the allocation of scarce resources and as all economic phenomena take place within a geographical space urban economics focuses on the allocation of resources across space in relation to urban areas Arnott McMillen 20067 McCann 20011 Other branches of economics ignore the spatial aspects of decision making but urban economics focuses not only on the location decisions of firms but also of cities themselves as cities themselves represent centers of economic activity OSullivan 20031 Many spatial economic topics can be analyzed within either an urban or regional economics framework as some economic phenomena primarily affect localized urban areas while others are felt over much larger regional areas McCann 20013 Arthur OSullivan believes urban economics is divided into six related themes market forces in the development of cities land use within cities urban transportation urban problems and public policy housing and public policy and local government expenditures and taxes OSullivan 20031314 Market forces in the development of cities Market forces in the development of cities relate to how the location decision of firms and households causes the development of cities The nature and behavior of markets depend somewhat on their locations therefore market performance partly depends on geographyMcCann 20011 If a firm locates in a geographically isolated region its market performance will be different than a firm located in a concentrated region The location decisions of both firms and households create cities that differ in size and economic structure When industries cluster like in Silicon Valley in California they create urban areas with dominant firms and distinct economies By looking at location decisions of firms and households the urban economist is able to address why cities develop where they do why some cities are large and others small what causes economic growth and decline and how local governments affect urban growth OSullivan 200314 Because urban economics is concerned with asking questions about the nature and workings of the economy of a city models and techniques developed within the field are primarily designed to analyze phenomena that are confined within the limits of a single city McCann 20012 Land use Looking at land use within metropolitan areas the urban economist seeks to analyze the spatial organization of activities within cities In attempts to explain observed patterns of land use the urban economist examines the intracity location choices of firms and households Considering the spatial organization of activities within cities urban economics addresses questions in terms of what determines the price of land and why those prices vary across space the economic forces that caused the spread of employment from the central core of cities outward identifying landuse controls such as zoning and interpreting how such controls affect the urban economy OSullivan 200314 Economic policy Economic policy is often implemented at the urban level thus economic policy is often tied to urban policy McCann 20013 Urban problems and public policy tie into urban economics as the theme relates urban problems such as poverty or crime to economics by seeking to answer questions with economic guidance For example does the tendency for the poor to live close to one another make them even poorer OSullivan 200315 Transportation and economics Urban transportation is a theme of urban economics because it affects landuse patterns as transportation affects the relative accessibility of different sites Issues that tie urban transportation to urban economics include the deficit that most transit authorities have and efficiency questions about proposed transportation developments such as lightrail OSullivan 200314 Housing and public policy Housing and public policy relate to urban economics as housing is a unique type of commodity Because housing is immobile when a household chooses a dwelling it is also choosing a location Urban economists analyze the location choices of households in conjunction with the market effects of housing policies OSullivan 200315 In analyzing housing policies we make use of market structures eg perfect market structure There are however problems encountered in making this analysis such as funding uncertainty space etc Government expenditures and taxes The final theme of local government expenditures and taxes relates to urban economics as it analyzes the efficiency of the fragmented local governments presiding in metropolitan areas OSullivan 200315 See also References Literature Arnott Richard McMillen Daniel P eds 2006 A Companion to Urban Economics Blackwell Publishing ISBN 1405106298 Capello Roberta Nijkamp Peter eds 2004 Urban Dynamics and Growth Advances in Urban Economics Elsevier Inc McCann Philip 2001 Urban and Regional Economics Oxford University Press ISBN 9780198776451 ObengOdoom Franklin 2016 Reconstructing Urban Economics Towards a Political Economy of the Built Environment Zed Books ISBN 9781783606597 ObengOdoom Franklin 2023 Urban Economics in the Global South A Study of The Economist Urban Challenge Journal vol 34 no 1 pp 107 118 OSullivan Arthur 2003 Urban economics Boston Mass McGrawHillIrwin ISBN 0072487844 Quigley John M 2008 Urban economics The New Palgrave Dictionary of Economics 2nd ed Strange William C 2008 Urban agglomeration The New Palgrave Dictionary of Economics 2nd ed Further reading Garreau Joel Edge City Life on the New Frontier 1992 Anchor ISBN 9780385424349 Goldstein Gerald S Moses Leon N 1973 A Survey of Urban Economics Journal of Economic Literature 11 2 471515 Kahn Matthew Green Cities Urban Growth and the Environment 2006 Brookings ISBN 9780815748168 ObengOdoom Franklin Reconstructing Urban Economics Towards a Political Economy of the Built Environment 2016 Zed ISBN 9781783606597 Stilwell Frank Understanding Cities Regions Spatial Political Economy 1993 ISBN 9780949138880",
  },
  {
    title: "Trade policy",
    originalContent:
      "A commercial policy also referred to as a trade policy or international trade policy is a governments policy governing international trade Commercial policy is an all encompassing term that is used to cover topics which involve international trade Trade policy is often described in terms of a scale between the extremes of free trade no restrictions on trade on one side and protectionism high restrictions to protect local producers on the other A common commercial policy can sometimes be agreed by treaty within a customs union as with the European Unions common commercial policy and in Mercosur A nations commercial policy will include and take into account the policies adopted by that nations government while negotiating international trade There are several factors that can affect a nations commercial policy all of which can affect international trade policies Theories on international trade policy Trade policy has been controversial since the days of mercantilism Economics or political economy has developed in major part as an effort to make clear various effects of trade policies See International trade theory The hottest topic in economic policy is upgrading in Global Value Chains Types and aspects of Commercial policy Regionalism Regionalism or Regional Trade Agreements RTA are trade policies and agreements that are crafted by the nations in a region for the purposes of increasing international trade in the area RTAs have been described by supporters as a means of increasing free trade with the goal of eventually merging into larger either bilateral or multilateral trade deals The more relatively local area of RTAs are useful in resolving trade issues as well without causing gridlock in other trade agreements Critics of RTAs say that they are a hindrance to the negotiation of trade because they can be lopsided or unfairly beneficial to one side over the other sides particularly if some of the participants are nations that are still in development As China was rising in economic power and prominence they turned to regionalism as a strategic method of leveling the playing field with Europe and the United States In 2000 China signed the Bangkok agreement with the Association of Southeast Asian Nations ASEAN to reduce tariffs in the region The signing of the agreement also began the push for a formal Free Trade Agreement between China and ASEAN However strained relations between China and other Asian nations such as Japan have prevented the same level of regional FTAs to be put in place with Northeast Asia Bilateral Free Trade Agreements A bilateral Free Trade Agreement is when two countries agree to exchange goods to promote trade and investments elimination barriers such as tariffs import quotas and export restrains The United States has signed such treaties as the North American Free Trade Agreement in 1994 as well as with Israel in the 1980s Experts who support such free trade agreements argue that these agreements increase competition while offering business the ability to reach larger markets Critics of bilateral agreements claim that a larger nation such as the United States can use these agreements to unfairly push smaller states into much harsher work loads than the World Trade Organization already requires Relations between the European Union and South Korea have led to both parties signing several bilateral agreements regarding trade policy In 2009 South Korea and the EU signed the EUKorea Free Trade Agreement The signing of the agreement created an FTA that is second only to NAFTA in size The agreement held the benefits of increased free trade between the participants in the FTA as well as increased challenge to the United States Preferential Trade Agreements Preferential agreements are trade deals that involve nations making deals with specific countries that can aid the interests of one another as opposed to the nondiscriminatory deals that are pushed by the WTO Nations have been increasingly preferring such deals since the 1950s as they are quicker to show gains for the parties involved in the agreements A common argument that has been made is that it allows businesses to open up markets that would otherwise be considered closed and therefore falls into the free trade idea that most countries will push for Countries that have similar levels of GDP and a higher scope in their economies as well as their relative position to one another and the rest of the world are more likely to have preferential trade agreements PTAs can also be applied to regional areas with unions such as NAFTA the European Union and ASEAN being examples of regional PTAs Those who opposer PTAs argue that these deals have increased the importance of where a product is made so that tariffs can be applied accordingly The certification of a products origin also unfairly holds back smaller countries that have less resources to spend Others argue that PTAs can hinder negotiations of trade disputes and places an emphasis of which country has more power Ways in which commercial policy is affected Tariffs Trade tariffs are a tax that are placed on the import of foreign goods Tariffs increase the price of imports and are usually levied onto the country the goods are being imported from Governments will use tariffs as a way to promote competition within their own country with businesses of the foreign country that wishes to sell their goods or services In some instances a countrys government will use them as a means of protectionism for their own interests In modern history generally starting at the mid20th century the use of tariffs has been largely diminished in favor of the rise of international trade Beginning in 2017 the Trump administration began to impose tariffs on several of nations that were involved in trade deals with the United States The countries targeted by the Trump Tariffs then retaliated with their own tariffs on American goods Import Quotas Import quotas are the limitations of the amount of goods that can be imported into the country from foreign businesses Generally an import quota is set for a specific period of time with one year being the most common metric Some versions of the quotas limits the quantity of specific goods being imported into a country while other versions place the limit on the value of those goods The objectives of quotas can include the protections of a nations interests ensuring a balance of trade so as not to create deficits retaliation to restrictive trade policies of other countries that do business on the international playing field References External links Media related to Commercial policy at Wikimedia Commons",
  },
  {
    title: "Sovereign debt",
    originalContent:
      "A countrys gross government debt also called public debt or sovereign debt is the financial liabilities of the government sector 81 Changes in government debt over time reflect primarily borrowing due to past government deficits A deficit occurs when a governments expenditures exceed revenues 7982 Government debt may be owed to domestic residents as well as to foreign residents If owed to foreign residents that quantity is included in the countrys external debt In 2020 the value of government debt worldwide was 874 US trillion or 99 measured as a share of gross domestic product GDP Government debt accounted for almost 40 of all debt which includes corporate and household debt the highest share since the 1960s The rise in government debt since 2007 is largely attributable to stimulus measures during the Great Recession and the COVID19 recession The ability of government to issue debt has been central to state formation and to state building Public debt has been linked to the rise of democracy private financial markets and modern economic growth Measuring government debt Government debt is typically measured as the gross debt of the general government sector that is in the form of liabilities that are debt instruments 207 A debt instrument is a financial claim that requires payment of interest andor principal by the debtor to the creditor in the future Examples include debt securities such as bonds and bills loans and government employee pension obligations 207 International comparisons usually focus on general government debt because the level of government responsible for programs for example health care differs across countries and the general government comprises central state provincial regional local governments and social security funds 18 s258 s259 The debt of public corporations such as post offices that provide goods or services on a market basis is not included in general government debt following the International Monetary Funds Government Finance Statistics Manual 2014 GFSM which describes recommended methodologies for compiling debt statistics to ensure international comparability 33 s2127 The gross debt of the general government sector is the total liabilities that are debt instruments An alternative debt measure is net debt which is gross debt minus financial assets in the form of debt instruments 208 s7243 Net debt estimates are not always available since some government assets may be difficult to value such as loans made at concessional rates 208209 s7246 Debt can be measured at market value or nominal value As a general rule the GFSM says debt should be valued at market value the value at which the asset could be exchanged for cash 55 s3107 However the nominal value is useful for a debtissuing government as it is the amount that the debtor owes to the creditor 191 ft28 If market and nominal values are not available face value the undiscounted amount of principal to be repaid at maturity 56 is used 208 s7238 A countrys general government debttoGDP ratio is an indicator of its debt burden since GDP measures the value of goods and services produced by an economy during a period usually a year As well debt measured as a percentage of GDP facilitates comparisons across countries of different size The OECD views the general government debttoGDP ratio as a key indicator of the sustainability of government finance Causes of government debt accumulation An important reason governments borrow is to act as an economic shock absorber For example deficit financing can be used to maintain government services during a recession when tax revenues fall and expenses rise for say unemployment benefits Government debt created to cover costs from major shock events can be particularly beneficial Such events would include a major war like World War II a public health emergency like the COVID19 recession or a severe economic downturn as with the Great Recession In the absence of debt financing when revenues decline during a downturn a government would need to raise taxes or reduce spending which would exacerbate the negative event While government borrowing may be desirable at times a deficits bias can arise when there is disagreement among groups in society over government spending To counter deficit bias many countries have adopted balanced budget rules or restrictions on government debt Examples include the debt anchor in Sweden a debt brake in Germany and Switzerland and the European Unions Stability and Growth Pact agreement to maintain a general government gross debt of no more than 60 of GDP Historic benchmarks The ability of government to issue debt has been central to state formation and to state building Public debt has been linked to the rise of democracy private financial markets and modern economic growth For example in the 17th and 18th centuries England established a parliament that included creditors as part of a larger coalition whose authorization had to be secured for the country to borrow or raise taxes This institution improved Englands ability to borrow because lenders were more willing to hold the debt of a state with democratic institutions that would support debt repayment versus a state where the monarch could not be compelled to repay debt As public debt came to be recognized as a safe and liquid investment it could be used as collateral for private loans This created a complementarity between the development of public debt markets and private financial markets Government borrowing to finance public goods such as urban infrastructure has been associated with modern economic growth 6 Written records point to public borrowing as long as two thousand years ago when Greek citystates such as Syracuse borrowed from their citizens 1016 But the founding of the Bank of England in 1694 revolutionised public finance and put an end to defaults such as the Great Stop of the Exchequer of 1672 when Charles II had suspended payments on his bills From then on the British Government would never fail to repay its creditors In the following centuries other countries in Europe and later around the world adopted similar financial institutions to manage their government debt In 1815 at the end of the Napoleonic Wars British government debt reached a peak of more than 200 of GDP nearly 887 million pounds sterling The debt was paid off over 90 years by running primary budget surpluses that is revenues were greater than spending after payment of interest In 1900 the country with the most total debt was France 1086215525 followed by Russia 656000000 then the United Kingdom 628978782 on a percapita basis the highestdebt countries were New Zealand 58 12s per person the Australian colonies 52 13s and Portugal 35 In 2018 global government debt reached the equivalent of 66 trillion or about 80 of global GDP and by 2020 global government debt reached 87US trillion or 99 of global GDP The COVID19 pandemic caused public debt to soar in 2020 particularly in advanced economies that put in place sweeping fiscal measures Impacts of government debt Government debt accumulation may lead to a rising interest rate which can crowd out private investment as governments compete with private firms for limited investment funds Some evidence suggests growth rates are lower for countries with government debt greater than around 80 percent of GDP A World Bank Group report that analyzed debt levels of 100 developed and developing countries from 1980 to 2008 found that debttoGDP ratios above 77 for developed countries 64 for developing countries reduced future annual economic growth by 0017 002 for developing countries percentage points for each percentage point of debt above the threshold Excessive debt levels may make governments more vulnerable to a debt crisis where a country is unable to make payments on its debt and it cannot borrow more Crises can be costly particularly if a debt crisis is combined with a financialbanking crisis which leads to economywide deleveraging As firms sell assets to pay off debt asset prices fall which risks an even greater fall in incomes further depressing tax revenue and requiring governments to drastically cut government services Examples of debt crises include the Latin American debt crisis of the early 1980s and Argentinas debt crisis in 2001 To help avoid a crisis governments may want to maintain a fiscal breathing space Historical experience shows that room to double the level of government debt when needed is an approximate guide Government debt is built up by borrowing when expenditure exceeds revenue so government debt generally creates an intergenerational transfer This is because the beneficiaries of the governments expenditure on goods and services when the debt is created typically differ from the individuals responsible for repaying the debt in the future An alternative view of government debt sometimes called the Ricardian equivalence proposition is that government debt has no impact on the economy if individuals are altruistic and internalize the impact of the debt on future generations According to this proposition while the quantity of government purchases affects the economy debt financing will have the same impact as tax financing because with debt financing individuals will anticipate the future taxes needed to repay the debt and so increase their saving and bequests by the amount of government debt Such higher individual saving means for example that private consumption falls oneforone with the rise in government debt so the interest rate would not rise and private investment is not crowded out Risk Credit Default risk Historically there have been many cases where governments have defaulted on their debts including Spain in the 16th and 17th centuries which nullified its government debt several times the Confederate States of America whose debt was not repaid after the American Civil War and revolutionary Russia after 1917 which refused to accept responsibility for Imperial Russias foreign debt If government debt is issued in a countrys own fiat money it is sometimes considered risk free because the debt and interest can be repaid by money creation However not all governments issue their own currency Examples include subnational governments like municipal provincial and state governments and countries in the eurozone In the Greek governmentdebt crisis one proposed solution was for Greece to leave the eurozone and go back to issuing the drachma although this would have addressed only future debt issuance leaving substantial existing debt denominated in what would then be a foreign currency Debt of a subnational government is generally viewed as less risky for a lender if it is explicitly or implicitly guaranteed by a regional or national level of government When New York City declined into what would have been bankrupt status during the 1970s a bailout came from New York State and the United States national government US state and local government debt is substantial in 2016 their debt amounted to 3 trillion plus another 5 trillion in unfunded liabilities Inflation risk A country that issues its own currency may be at low risk of default in local currency but if a central bank provides finance by buying government bonds sometimes referred to as debt monetization this can lead to price inflation In an extreme case in the 1920s Weimar Germany suffered from hyperinflation when the government used money creation to pay off the national debt following World War I Exchange rate risk While US Treasury bonds denominated in US dollars may be considered riskfree to an American purchaser a foreign investor bears the risk of a fall in the value of the US dollar relative to their home currency A government can issue debt in foreign currency to eliminate exchange rate risk for foreign lenders but that means the borrowing government then bears the exchange rate risk Also by issuing debt in foreign currency a country cannot erode the value of the debt by means of inflation Almost 70 of all debt in a sample of developing countries from 1979 through 2006 was denominated in US dollars Implicit and contingent liabilities Most governments have contingent liabilities which are obligations that do not arise unless a particular event occurs in the future 76 An example of an explicit contingent liability is a public sector loan guarantee where the government is required to make payments only if the debtor defaults 210 s7252 Examples of implicit contingent liabilities include ensuring the payment of future social security pension benefits covering the obligations of subnational governments in the event of a default and spending for natural disaster relief 209210 Explicit contingent liabilities and net implicit social security obligations should be included as memorandum items to a governments balance sheet 69 7677 209212 but they are not included in government debt because they are not contractual obligations 210 s7252 Indeed it is not uncommon for governments to change unilaterally the benefit structure of social security schemes for example eg by changing the circumstances under which the benefits become payable or the amount of the benefit 76 s449 In the US and in many countries there is no money earmarked for future social insurance payments the system is called a payasyougo scheme According to the 2018 annual reports from the trustees for the US Social Security and Medicare trust funds Medicare is facing a 37 trillion unfunded liability over the next 75 years and Social Security is facing a 13 trillion unfunded liability over the same time frame Neither of these amounts are included in the US gross general government debt which in 2024 was 34 trillion In 2010 the European Commission required EU Member Countries to publish their debt information in standardized methodology explicitly including debts that were previously hidden in a number of ways to satisfy minimum requirements on local national and European Stability and Growth Pact level See also Government finance Specific General References External links The IMF Public Financial Management Blog OECD government debt statistics Japans Central Government Debt Riksgldskontoret Swedish national debt office What is Sovereign Debt United States Treasury Bureau of Public Debt The Debt to the Penny and Who Holds It Archived 20110418 at the Wayback Machine Slaying the Dragon of Debt Regional Oral History Office The Bancroft Library University of California Berkeley A historical collection of documents on or referring to government spending and fiscal policy available on FRASER Eisner Robert 1993 Federal Debt In David R Henderson ed Concise Encyclopedia of Economics 1st ed Library of Economics and Liberty OCLC 317650570 50016270 163149563 Governments Borrowing Power DebatedWisdom 3IVIS GmbH Retrieved 29 October 2016 Databases CLYPS dataset on public debt level and composition in Latin America",
  },
  {
    title: "Fiscal policy",
    originalContent:
      "In economics and political science fiscal policy is the use of government revenue collection taxes or tax cuts and expenditure to influence a countrys economy The use of government revenue expenditures to influence macroeconomic variables developed in reaction to the Great Depression of the 1930s when the previous laissezfaire approach to economic management became unworkable Fiscal policy is based on the theories of the British economist John Maynard Keynes whose Keynesian economics theorised that government changes in the levels of taxation and government spending influence aggregate demand and the level of economic activity Fiscal and monetary policy are the key strategies used by a countrys government and central bank to advance its economic objectives The combination of these policies enables these authorities to target inflation and to increase employment In modern economies inflation is conventionally considered healthy in the range of 23 Additionally it is designed to try to keep GDP growth at 23 and the unemployment rate near the natural unemployment rate of 45 This implies that fiscal policy is used to stabilise the economy over the course of the business cycle Changes in the level and composition of taxation and government spending can affect macroeconomic variables including aggregate demand and the level of economic activity saving and investment income distribution allocation of resources Fiscal policy can be distinguished from monetary policy in that fiscal policy deals with taxation and government spending and is often administered by a government department while monetary policy deals with the money supply interest rates and is often administered by a countrys central bank Both fiscal and monetary policies influence a countrys economic performance Monetary or fiscal policy Since the 1970s it became clear that monetary policy performance has some benefits over fiscal policy due to the fact that it reduces political influence as it is set by the central bank to have an expanding economy before the general election politicians might cut the interest rates Additionally fiscal policy can potentially have more supplyside effects on the economy to reduce inflation the measures of increasing taxes and lowering spending would not be preferred so the government might be reluctant to use these Monetary policy is generally quicker to implement as interest rates can be set every month while the decision to increase government spending might take time to figure out which area the money should be spent on The recession of the 2000s decade shows that monetary policy also has certain limitations A liquidity trap occurs when interest rate cuts are insufficient as a demand booster as banks do not want to lend and the consumers are reluctant to increase spending due to negative expectations for the economy Government spending is responsible for creating the demand in the economy and can provide a kickstart to get the economy out of the recession When a deep recession takes place it is not sufficient to rely just on monetary policy to restore the economic equilibrium Each side of these two policies has its differences therefore combining aspects of both policies to deal with economic problems has become a solution that is now used by the US These policies have limited effects however fiscal policy seems to have a greater effect over the longrun period while monetary policy tends to have a shortrun success In 2000 a survey of 298 members of the American Economic Association AEA found that while 84 percent generally agreed with the statement Fiscal policy has a significant stimulative impact on a less than fully employed economy 71 percent also generally agreed with the statement Management of the business cycle should be left to the Federal Reserve activist fiscal policy should be avoided In 2011 a followup survey of 568 AEA members found that the previous consensus about the latter proposition had dissolved and was by then roughly evenly disputed Stances Depending on the state of the economy fiscal policy may reach for different objectives its focus can be to restrict economic growth by mediating inflation or in turn increase economic growth by decreasing taxes encouraging spending on different projects that act as stimuli to economic growth and enabling borrowing and spending The three stances of fiscal policy are the following Neutral fiscal policy is usually undertaken when an economy is in neither a recession nor an expansion The amount of government deficit spending the excess not financed by tax revenue is roughly the same as it has been on average over time so no changes to it are occurring that would have an effect on the level of economic activity Expansionary fiscal policy is used by the government when trying to balance the contraction phase in the business cycle It involves government spending exceeding tax revenue by more than it has tended to and is usually undertaken during recessions Examples of expansionary fiscal policy measures include increased government spending on public works eg building schools and providing the residents of the economy with tax cuts to increase their purchasing power in order to fix a decrease in the demand Contractionary fiscal policy on the other hand is a measure to increase tax rates and decrease government spending It occurs when government deficit spending is lower than usual This has the potential to slow economic growth if inflation which was caused by a significant increase in aggregate demand and the supply of money is excessive By reducing the economys amount of aggregate income the available amount for consumers to spend is also reduced So contractionary fiscal policy measures are employed when unsustainable growth takes place leading to inflation high prices of investment recession and unemployment above the healthy level of 34 However these definitions can be misleading because even with no changes in spending or tax laws at all cyclic fluctuations of the economy cause cyclic fluctuations of tax revenues and of some types of government spending altering the deficit situation these are not considered to be policy changes Therefore for purposes of the above definitions government spending and tax revenue are normally replaced by cyclically adjusted government spending and cyclically adjusted tax revenue Thus for example a government budget that is balanced over the course of the business cycle is considered to represent a neutral and effective fiscal policy stance Methods of fiscal policy funding Governments spend money on a wide variety of things from the military and police to services such as education and health care as well as transfer payments such as welfare benefits This expenditure can be funded in a number of different ways Taxation Seigniorage the benefit from printing money Borrowing money from the population or from abroad Dipping into fiscal reserves Sale of fixed assets eg land Selling equity to the population Borrowing A fiscal deficit is often funded by issuing bonds such as Treasury bills or and giltedged securities but can also be funded by issuing equity Bonds pay interest either for a fixed period or indefinitely that is funded by taxpayers as a whole Equity offers returns on investment interest that can only be realized in discharging a future tax liability by an individual taxpayer If available government revenue is insufficient to support the interest payments on bonds a nation may default on its debts usually to foreign creditors Public debt or borrowing refers to the government borrowing from the public It is impossible for a government to default on its equity since the total returns available to all investors taxpayers are limited at any point by the total current year tax liability of all investors Dipping into prior surpluses A fiscal surplus is often saved for future use and may be invested in either local currency or any financial instrument that may be traded later once resources are needed and the additional debt is not needed Fiscal straitjacket The concept of a fiscal straitjacket is a general economic principle that suggests strict constraints on government spending and public sector borrowing to limit or regulate the budget deficit over a time period Most US states have balanced budget rules that prevent them from running a deficit The United States federal government technically has a legal cap on the total amount of money it can borrow but it is not a meaningful constraint because the cap can be raised as easily as spending can be authorized and the cap is almost always raised before the debt gets that high Economic effects Governments use fiscal policy to influence the level of aggregate demand in the economy so that certain economic goals can be achieved Price stability Full employment Economic growth The Keynesian view of economics suggests that increasing government spending and decreasing the rate of taxes are the best ways to have an influence on aggregate demand stimulate it while decreasing spending and increasing taxes after the economic expansion has already taken place Additionally Keynesians argue that expansionary fiscal policy should be used in times of recession or low economic activity as an essential tool for building the framework for strong economic growth and working towards full employment In theory the resulting deficits would be paid for by an expanded economy during the expansion that would follow this was the reasoning behind the New Deal The ISLM model is another way of understanding the effects of fiscal expansion As the government increases spending there will be a shift in the IS curve up and to the right In the short run this increases the real interest rate which then reduces private investment and increases aggregate demand placing upward pressure on supply To meet the shortrun increase in aggregate demand firms increase fullemployment output The increase in shortrun price levels reduces the money supply which shifts the LM curve back and thus returning the general equilibrium to the original full employment FE level Therefore the ISLM model shows that there will be an overall increase in the price level and real interest rates in the long run due to fiscal expansion Governments can use a budget surplus to do two things to slow the pace of strong economic growth to stabilise prices when inflation is too high Keynesian theory posits that removing spending from the economy will reduce levels of aggregate demand and contract the economy thus stabilizing prices But economists still debate the effectiveness of fiscal stimulus The argument mostly centers on crowding out whether government borrowing leads to higher interest rates that may offset the stimulative impact of spending When the government runs a budget deficit funds will need to come from public borrowing the issue of government bonds overseas borrowing or monetizing the debt When governments fund a deficit with the issuing of government bonds interest rates can increase across the market because government borrowing creates higher demand for credit in the financial markets This decreases aggregate demand for goods and services either partially or entirely offsetting the direct expansionary impact of the deficit spending thus diminishing or eliminating the achievement of the objective of a fiscal stimulus Neoclassical economists generally emphasize crowding out while Keynesians argue that fiscal policy can still be effective especially in a liquidity trap where they argue crowding out is minimal In the classical view expansionary fiscal policy also decreases net exports which has a mitigating effect on national output and income When government borrowing increases interest rates it attracts foreign capital from foreign investors This is because all other things being equal the bonds issued from a country executing expansionary fiscal policy now offer a higher rate of return In other words companies wanting to finance projects must compete with their government for capital so they offer higher rates of return To purchase bonds originating from a certain country foreign investors must obtain that countrys currency Therefore when foreign capital flows into the country undergoing fiscal expansion demand for that countrys currency increases The increased demand in turn causes the currency to appreciate reducing the cost of imports and making exports from that country more expensive to foreigners Consequently exports decrease and imports increase reducing demand from net exports Some economists oppose the discretionary use of fiscal stimulus because of the inside lag the time lag involved in implementing it which is almost inevitably long because of the substantial legislative effort involved Further the outside lag between the time of implementation and the time that most of the effects of the stimulus are felt could mean that the stimulus hits an alreadyrecovering economy and overheats the ensuing h rather than stimulating the economy when it needs it Some economists are concerned about potential inflationary effects driven by increased demand engendered by a fiscal stimulus In theory fiscal stimulus does not cause inflation when it uses resources that would have otherwise been idle For instance if a fiscal stimulus employs a worker who otherwise would have been unemployed there is no inflationary effect however if the stimulus employs a worker who otherwise would have had a job the stimulus is increasing labor demand while labor supply remains fixed leading to wage inflation and therefore price inflation See also References Bibliography Simonsen MH The Econometrics and The State Brasilia University Editor 19601964 Heyne P T Boettke P J Prychitko D L 2002 The Economic Way of Thinking 10th ed Prentice Hall Larch M and J Nogueira Martins 2009 Fiscal Policy Making in the European Union An Assessment of Current Practice and Challenges Routledge Hansen Bent 2003 The Economic Theory of Fiscal Policy Volume 3 Routledge Anderson J E 2005 Fiscal Reform and its FirmLevel Effects in Eastern Europe and Central Asia Working Papers Series wp800 William Davidson Institute at the University of Michigan D Harries Roger Fenton and the Crimean War Schmidt M 2018 A Look at Fiscal and Monetary Policy Dotdash Pettinger T 2017 Difference between monetary and fiscal policy EconomicsHelporg Amadeo K 2018 Fiscal Policy Types Objectives and Tools Dotdash Kramer L 2019 What Is Fiscal Policy Dotdash Macek R Jank J 2015 The Impact of Fiscal Policy on Economic Growth Depending on Institutional Conditions External links Fiscal Policy topic page from Encyclopdia Britannica",
  },
  {
    title: "Inflation targeting",
    originalContent:
      "In macroeconomics inflation targeting is a monetary policy where a central bank follows an explicit target for the inflation rate for the mediumterm and announces this inflation target to the public The assumption is that the best that monetary policy can do to support longterm growth of the economy is to maintain price stability and price stability is achieved by controlling inflation The central bank uses interest rates as its main shortterm monetary instrument An inflationtargeting central bank will raise or lower interest rates based on abovetarget or belowtarget inflation respectively The conventional wisdom is that raising interest rates usually cools the economy to rein in inflation lowering interest rates usually accelerates the economy thereby boosting inflation The first three countries to implement fullyfledged inflation targeting were New Zealand Canada and the United Kingdom in the early 1990s although Germany had adopted many elements of inflation targeting earlier History Early proposals of monetary systems targeting the price level or the inflation rate rather than the exchange rate followed the general crisis of the gold standard after World War I Irving Fisher proposed a compensated dollar system in which the gold content in paper money would vary with the price of goods in terms of gold so that the price level in terms of paper money would stay fixed Fishers proposal was a first attempt to target prices while retaining the automatic functioning of the gold standard In his Tract on Monetary Reform 1923 John Maynard Keynes advocated what we would now call an inflation targeting scheme In the context of sudden inflations and deflations in the international economy right after World War I Keynes recommended a policy of exchangerate flexibility appreciating the currency as a response to international inflation and depreciating it when there are international deflationary forces so that internal prices remained more or less stable Interest in inflation targeting waned during the Bretton Woods era 19441971 as they were inconsistent with the exchange rate pegs that prevailed during three decades after World War II New Zealand Canada United Kingdom Inflation targeting was pioneered in New Zealand in 1990 Canada was the second country to formally adopt inflation targeting in February 1991 The United Kingdom adopted inflation targeting in October 1992 after exiting the European Exchange Rate Mechanism The Bank of Englands Monetary Policy Committee was given sole responsibility in 1998 for setting interest rates to meet the Governments Retail Prices Index RPI inflation target of 25 The target changed to 2 in December 2003 when the Consumer Price Index CPI replaced the Retail Prices Index as the UK Treasurys inflation index If inflation overshoots or undershoots the target by more than 1 the Governor of the Bank of England is required to write a letter to the Chancellor of the Exchequer explaining why and how he will remedy the situation The success of inflation targeting in the United Kingdom has been attributed to the Banks focus on transparency The Bank of England has been a leader in producing innovative ways of communicating information to the public especially through its Inflation Report which have been emulated by many other central banks Inflation targeting then spread to other advanced economies in the 1990s and began to spread to emerging markets beginning in the 2000s European Central Bank Although the ECB does not consider itself to be an inflationtargeting central bank after the inception of the euro in January 1999 the objective of the European Central Bank ECB has been to maintain price stability within the Eurozone The Governing Council of the ECB in October 1998 defined price stability as inflation of under 2 a yearonyear increase in the Harmonised Index of Consumer Prices HICP for the euro area of below 2 and added that price stability was to be maintained over the medium term The Governing Council confirmed this definition in May 2003 following a thorough evaluation of the ECBs monetary policy strategy On that occasion the Governing Council clarified that in the pursuit of price stability it aims to maintain inflation rates below but close to 2 over the medium term Since then the numerical target of 2 has become common for major developed economies including the United States since January 2012 and Japan since January 2013 In 8 July 2021 the ECB changed its inflation target to a symmetrical 2 over the medium term Symmetry in the inflation target means that the Governing Council considers negative and positive deviations of inflation from the target to be equally undesirable Emerging markets In 2000 Frederic S Mishkin concluded that although inflation targeting is not a panacea and may not be appropriate for many emerging market countries it can be a highly useful monetary policy strategy in a number of them Armenia The Central Bank of Armenia CBA announced in 2006 that it will implement an inflation targeting strategy The process of full transition to inflation targeting was supposed to end in 2008 Operational macroeconomic and institutional preconditions for inflation targeting should have been met to ensure a full transition CBA believes that it has managed to meet all the preconditions successfully and should concentrate on building a public trust in the new monetary policy regime A specific model has been developed to estimate CBAs reaction function and the results showed that the inertia of inflation rate and interest rate are most vital in the reaction function This can be an evidence that the announcement of the strategy is a trustworthy commitment There are people who claim that inflation targeting is too restrictive for dealing with positive supply shocks On the other hand the IMF claims that inflation targeting strategy is good for developing economies however it requires a lot of information for forecasting The Central Bank continued to pursue a policy of tightening monetary conditions during the reporting period increasing the policy interest rate by a total of 275 percentage points At the same time about half of the tightening 125 percentage points was carried out in 2022 in March reacting to the high inflation situation formed in the case of unprecedented uncertainties Being constantly hit by external shocks to the national economy over the past three years Armenia is still on the path of recovery thanks to economic management efforts According to the 3year StandBy Arrangement which came to its end on May 16 2022 important structural and institutional reforms have been implemented Those include improvement of tax compliance budget process refinement strengthening the stability of financial sector and most importantly fostering the inflation targeting framework Chile In Chile a 20 inflation rate pushed the Central Bank of Chile to announce at the end of 1990 an inflation objective for the annual inflation rate for the year ending in December 1991 However Chile was not regarded as a fullyfledged inflation targeter until October 1999 According to Pablo Garca Silva member of the board of the Central Bank of Chile this has allowed to attenuate inflation Garca Silva exemplifies this with the limited inflation seen in Chile during the 2002 Brazilian general election and the Great Recession of 20082009 Czech Republic The Czech National Bank CNB is an example of an inflation targeting central bank in a small open economy with a recent history of economic transition and real convergence to its Western European peers Since 2010 the CNB uses 2 percent with a 1pp range around it as the inflation target The CNB places a lot of emphasis on transparency and communication indeed a recent study of more than 100 central banks found the CNB to be among the four most transparent ones In 2012 inflation was expected to fall well below the target leading the CNB to gradually reduce the level of its basic monetary policy instrument the 2week repo rate until the zero lower bound actually 005 percent was reached in late 2012 In light of the threat of a further fall in inflation and possibly even of a protracted period of deflation on 7 November 2013 the CNB declared an immediate commitment to weaken the exchange rate to the level of 27 Czech korunas per 1 euro dayonday weakening by about 5 percent and to keep the exchange rate from getting stronger than this value until at least the end of 2014 later on this was changed to the second half of 2016 The CNB thus decided to use the exchange rate as a supplementary tool to make sure that inflation returns to the 2 percent target level Such a use of the exchange rate as tool within the regime of inflation targeting should not be confused with a fixed exchangerate system or with a currency war United States In a historic shift on 25 January 2012 US Federal Reserve Chairman Ben Bernanke set a 2 target inflation rate bringing the Fed in line with many of the worlds other major central banks Until then the Feds policy committee the Federal Open Market Committee FOMC did not have an explicit inflation target but regularly announced a desired target range for inflation usually between 17 and 2 measured by the personal consumption expenditures price index Prior to adoption of the target some people argued that an inflation target would give the Fed too little flexibility to stabilise growth andor employment in the event of an external economic shock Another criticism was that an explicit target might turn central bankers into what Mervyn King former Governor of the Bank of England had in 1997 colorfully termed inflation nuttersthat is central bankers who concentrate on the inflation target to the detriment of stable growth employment andor exchange rates King went on to help design the Banks inflation targeting policy and asserts that the buffoonery has not actually happened as did Chairman of the US Federal Reserve Ben Bernanke who stated in 2003 that all inflation targeting at the time was of a flexible variety in theory and practice Former Chairman Alan Greenspan as well as other former FOMC members such as Alan Blinder typically agreed with the benefits of inflation targeting but were reluctant to accept the loss of freedom involved Bernanke however was a wellknown advocate In August 2020 the FOMC released a revised Statement on LongerRun Goals and Monetary Policy Strategy The review announced the FED would seek to achieve inflation that averages 2 over time In practice this means that following periods when inflation has been running persistently below 2 percent appropriate monetary policy will likely aim to achieve inflation moderately above 2 percent for some time This way the fed hopes to better anchor longerterm inflation expectations which they say would foster price stability and moderate longterm interest rates and enhance the Committees ability to promote maximum employment in the face of significant economic disturbances Theoretical questions New classical macroeconomics and rational expectations hypothesis can explain how and why inflation targeting works Expectations of firms or the subjective probability distribution of outcomes will be around the prediction of the theory itself the objective probability distribution of those outcomes for the same information set So rational agents expect the most probable outcome to emerge However there is limited success at specifying the relevant model and the full and perfect knowledge of a given macroeconomic system can be regarded as a comfortable presumption at best Knowledge of the relevant model is not feasible even if highlevel econometrical techniques were accessible or adequate identification of the relevant explanatory variables were performed So estimation bias depends on the quantity and quality of information to which the modeller has access In other words estimations are asymptotically unbiased with respect to the exploited information Meanwhile consistency can be interpreted similarly On the basis of asymptotical unbiasedness a moderated version of the rational expectations hypothesis can be suggested in which familiarity with the theoretical parameters is not a requirement for the relevant model An agent with access to sufficiently vast quality information and highlevel methodological skills could specify its own quasirelevant model describing a specific macroeconomic system By increasing the amount of information processed this agent could further reduce its bias If this agent were also focal such as a central bank then other agents would likely accept the proposed model and adjust their expectations accordingly In this way individual expectations become unbiased as much as possible albeit against a background of considerable passivity According to some researches this is the theoretical background of the functionality of inflation targeting regimes Empirical issues Target band size While most inflation targeting countries set their target band at 2 percentage points the band sizes are wideranging across countries and inflation targeters frequently update their target bands Track record Inflation targeting countries track records in maintaining inflation within the central banks target bands differ substantially and financial markets differentiate inflation targeters by behaviors Debate There is some empirical evidence that inflation targeting does what its advocates claim that is making the outcomes if not the process of monetary policy more transparent A 2021 study in the American Political Science Review found that independent central banks with rigid inflation targeting policies produce worse outcomes in banking crises than independent central banks whose policy mandate does not rigidly prioritize inflation Benefits Inflation targeting allows monetary policy to focus on domestic considerations and to respond to shocks to the domestic economy which is not possible under a fixed exchangerate system Also as a result of better inflation control and stability of economic growth investors may more easily factor in likely interest rate changes into their investment decisions Inflation expectations that are better anchored allow monetary authorities to cut policy interest rates countercyclically Transparency is another key benefit of inflation targeting Central banks in developed countries that have successfully implemented inflation targeting tend to maintain regular channels of communication with the public For example the Bank of England pioneered the Inflation Report in 1993 which outlines the banks views about the past and future performance of inflation and monetary policy Although it was not an inflationtargeting country until January 2012 up until then the United States Statement on LongerRun Goals and Monetary Policy Strategy enumerated the benefits of clear communicationit facilitates wellinformed decisionmaking by households and businesses reduces economic and financial uncertainty increases the effectiveness of monetary policy and enhances transparency and accountability which are essential in a democratic society An explicit numerical inflation target increases a central banks accountability and thus it is less likely that the central bank falls prey to the timeinconsistency trap This accountability is especially significant because even countries with weak institutions can build public support for an independent central bank Institutional commitment can also insulate the bank from political pressure to undertake an overly expansionary monetary policy An econometric analysis found that although inflation targeting results in higher economic growth it does not necessarily guarantee stability based on their study of 36 emerging economies from 1979 to 2009 Shortcomings Supporters of a nominal income target criticize the propensity of inflation targeting to neglect output shocks by focusing solely on the price level Adherents of market monetarism led by Scott Sumner argue that in the United States the Federal Reserves mandate is to stabilize both output and the price level and that consequently a nominal income target would better suit the Feds mandate Australian economist John Quiggin who also endorses nominal income targeting stated that it would maintain or enhance the transparency associated with a system based on stated targets while restoring the balance missing from a monetary policy based solely on the goal of price stability Quiggin blamed the late2000s recession on inflation targeting in an economic environment in which low inflation is a drag on growth In practice many central banks conduct flexible inflation targeting where the central bank strives to keep inflation near the target except when such an effort would imply too much output volatility Quiggin also criticized former Fed Chair Alan Greenspan and former European Central Bank President JeanClaude Trichet for ignoring or even applauding the unsustainable bubbles in speculative real estate that produced the crisis and to reacting too slowly as the evidence emerged In a 2012 oped University of Nottingham economist Mohammed Farhaan Iqbal suggested that inflation targeting evidently passed away in September 2008 referencing the 20072008 financial crisis Frankel suggested that central banks that had been relying on inflation targeting had not paid enough attention to assetprice bubbles and also criticized inflation targeting for inappropriate responses to supply shocks and termsoftrade shocks In turn Iqbal suggested that nominal income targeting or productprice targeting would succeed inflation targeting as the dominant monetary policy regime The debate continues and many observers expect that inflation targeting will continue to be the dominant monetary policy regime perhaps after certain modifications Empirically it is not so obvious that inflation targeteers have better inflation control Some economists argue that better institutions increase a countrys chances of successfully targeting inflation John Williams a highranking Federal Reserve official concluded that when gauged by the behavior of inflation since the crisis inflation targeting delivered on its promise In an article written since the COVID19 pandemic critics have pointed out that the Bank of Canadas inflationtargeting has had unintended consequences with persistently low interest rates over the last 12 years fuelling an increase in home prices by encouraging borrowing and contributing to wealth inequalities by supporting higher equity values Choosing a positive zero or negative inflation target Choosing a positive inflation target has at least two drawbacks Over time the compounded effect of small annual price increases will significantly reduce a currencys purchasing power For example successfully hitting a target of 2 each year for 40 years would cause the price of a 100 basket of goods to rise to 22080 This drawback would be minimized or reversed by choosing a zero inflation target or a negative target Vendors must expend resources more frequently to reprice their goods and services This drawback would be minimized by choosing a zero inflation target However policymakers feel the drawbacks are outweighed by the fact that a positive inflation target reduces the chance of an economy falling into a period of deflation Some economists argue that fear of deflation is unfounded citing studies that show inflation is more likely than deflation to cause an economic contraction Andrew Atkeson and Patrick J Kehoe wrote According to standard economic theory deflation is the necessary consequence of optimal monetary policy In 1969 Milton Friedman argued that under the optimal policy the nominal interest rate should be zero and the price level should fall steadily at the real rate of interest Since then Friedmans argument has been confirmed in a formal setting See for example V V Chari Lawrence Christiano and Patrick Kehoe 1996 and Harold Cole and Narayana Kocherlakota 1998 Effectively Friedman was arguing for a negative moderately deflationary inflation target Numerical target The typical numerical target of 2 has come under debate since the period of rapid inflation experienced following the monetary expansion during the COVID19 pandemic Mohamed ElErian has suggested the Federal Reserve raise its inflation target to a stable 3 rate of inflation saying Theres nothing scientific about 2 Variations In contrast to the usual inflation rate targeting Laurence M Ball proposed targeting longrun inflation using a monetary conditions index In his proposal the monetary conditions index is a weighted average of the interest rate and exchange rate It will be easy to put many other things into this monetary conditions index In the constrained discretion framework inflation targeting combines two contradicting monetary policiesa rulebased approach and a discretionary approachas a precise numerical target is given for inflation in the medium term and a response to economic shocks in the short term Some inflation targeters associate this with more economic stability Countries There were 27 countries regarded by the Bank of Englands Centre for Central Banking Studies as fully fledged inflation targeters at the beginning of 2012 Other lists count 26 or 28 countries as of 2010 Since then the United States and Japan have also adopted inflation targets although the Federal Reserve like the European Central Bank does not consider itself to be an inflationtargeting central bank In addition South Korea Bank of Korea and Iceland Central Bank of Iceland and others See also Inflationism Monetarism Nominal income target Output gap Phillips curve Taylor rule References External links Table of Central Bank Inflation Targets",
  },
  {
    title: "Central bank digital currency",
    originalContent:
      "A central bank digital currency CBDC also called digital fiat currency or digital base money is a digital currency issued by a central bank rather than by a commercial bank It is also a liability of the central bank and denominated in the sovereign currency as is the case with physical banknotes and coins The two primary categories of CBDCs are retail and wholesale Retail CBDCs are designed for households and businesses to make payments for everyday transactions whereas wholesale CBDCs are designed for financial institutions and operate similarly to central bank reserves Retail CBDCs can be distributed through various models In the intermediated model the central bank issues the CBDC and manages core infrastructures while financial intermediaries offer customer services The ECB and the Federal Reserve have proposed intermediated CBDCs Alternatively the central bank could either provide the full service or delegate responsibilities further While CBDCs may share some properties with virtual currency and cryptocurrency such as programmability they differ in that a CBDC is issued by a state However most retail CBDC implementations will likely not use any sort of distributed ledger such as a blockchain As of 2023 over 120 different jurisdictions including major economies like the ECB UK and the US were evaluating national digital currencies As it currently stands 9 countries and the 8 islands making up the Eastern Caribbean Currency Union have launched CBDCs 38 countries and Hong Kong have CBDC pilot programmes and 67 countries and 2 currency unions are researching CBDCs In the United States some states have introduced legislation to ban state payments using CBDCs with Florida being the first state to pass such a law citing privacy concerns CBDCs have faced a plethora of criticisms including concerns about privacy and the potential for them to be used as a tool for coercion and control Their implementation could also have a displacement effect on the private sector affecting bank balance sheets and private payment methods necessitating carefully calibrated policies History Although the term CBDC did not become widely used until after 2019 central banks have researched and launched digital currency projects for decades For example Finlands central bank issued the Avant stored value emoney card in the 1990s In 2014 the Chinese central bank began researching the idea of issuing a CBDC Elsewhere the Ecuadorian central bank operated a mobile payment system from 2014 to 2018 In 2021 Australias central bank conducted a proof of concept for a wholesale CBDC using Ethereum to tokenize syndicated loans aiming to automate and secure highvalue transactions in the banking sector Implementation A central bank digital currency would likely be implemented using a database run by the central bank government or approved privatesector entities The database would keep a record with appropriate privacy and cryptographic protections of the amount of money held by every entity such as people and corporations In contrast to cryptocurrency a central bank digital currency would be centrally controlled even if it was on a distributed database and so a blockchain or other distributed ledger would likely not be required or useful even as they were the original inspiration for the concept By March 2024 the central banks of 134 countries accounting for 98 of the worlds GDP were said to be in various stages of evaluating the launch of a national digital currency These included the ECB the UK and the US Chinas digital RMB was the first digital currency to be issued by a major economy Six central banks have launched a CBDC the Central Bank of The Bahamas Sand Dollar the Eastern Caribbean Central Bank DCash the Central Bank of Nigeria eNaira the Bank of Jamaica JamDex Peoples Bank of China Digital renminbi the Reserve Bank of India Digital Rupee and Bank of Russia Digital Ruble The Central Bank of Brazil has been rolling out tests of a digital Brazilian currency Drex since March 2023 The ECBEurozone decided in October 2023 to move forward to the preparation phase for the potential issuance of a digital euro after a twoyear study phase Some states have also issued or have considered issuing cryptocurrencies these include Venezuela Petro and the Marshall Islands Sovereign These cryptocurrencies are often considered with the intent of increasing a states independence from global financial systems such as by reducing dependence on a foreign currency or by evading international sanctions Contrasting attitudes towards digital currencies were demonstrated by developments in the UK and Switzerland in February 2023 The UK Treasury and the Bank of England said a statebacked digital pound was likely to be launched some time after 2025 Two weeks later a Swiss lobby group triggered a national vote on maintaining a sufficient quantity of cash in circulation over fears that electronic payments make it easier for the state to monitor its citizens actions In a comment on the British governments plans the BBCs Faisal Islam said the issue was about access to the data attached to every spending transaction and whether people might choose to trust a global company more than the state The eye here is on maintaining UK monetary sovereignty against upheaval from the likes of Big Tech A major problem with central bank digital currencies is deciding whether the currency should be easily traceable If its traceable the government has more control than it currently does Additionally theres a technical aspect to consider whether CBDCs should be based on tokens or accounts and how much anonymity users should have Characteristics A CBDC is a digital counterpart to fiat money issued by central banks Like paper banknotes it is a means of payment a unit of account and a store of value And like paper currency each unit is uniquely identifiable to prevent counterfeiting CBDC will have implications for commercial banks probably in the field of lowering banks commissions no big customer dataselling ability accumulating the deposits and deposit policies and credit policies due to higher funding costs for banks Digital fiat currency is part of the base money supply together with other forms of the currency As such DFC is a liability of the central bank just as physical currency is It is a digital bearer instrument that can be stored transferred and transmitted by all kinds of digital payment systems and services The validity of the digital fiat currency is independent of the digital payment systems storing and transferring the digital fiat currency Proposals for CBDC implementation often involve the provision of universal bank accounts at the central banks for all citizens Benefits and impacts Governments and central banks are studying CBDCs and their implications for financial inclusion economic growth technology innovation and the efficiency of bank transactions Potential advantages include Technological efficiency instead of relying on intermediaries such as banks and clearing houses money transfers and payments could be made in real time directly from the payer to the payee Being real time has some advantages Reduces risk payment for goods and services often needs to be done in a timely manner and when payment verification is slow merchants usually accept the risk of some payments not succeeding in exchange for faster service to customers When these risks are eliminated with instant payment verifications merchants no longer need to use intermediaries to handle the risk or to absorb the risk cost themselves Reduces complexity merchants will not need to separately keep track of transactions that are slow where the customer claims to have paid but the money has not arrived yet therefore eliminate the waiting queue which could simplify the transaction process from payment to rendition of goodsservices Reduces or eliminates transaction fees current payment systems like Visa Mastercard American Express etc have a fee attached to each transaction and lowering or eliminating these fees could lead to widespread price drops and increased adoption of digital payments Financial inclusion safe money accounts at the central banks could constitute a strong instrument of financial inclusion allowing any legal resident or citizen to be provided with a free or lowcost basic bank account Preventing illicit activity A CBDC makes it feasible for a central bank to keep track of the exact location of every unit of the currency assuming the more probable centralized database form Tax collection It makes tax avoidance and tax evasion much more difficult since it would become impossible to use methods such as offshore banking and unreported employment to hide financial activity from the central bank or government In contrast cryptocurrencies risk undermining effort to crack down on corporate tax avoidance Combating crime It makes it much easier to spot criminal activity by observing financial activity and thus put an end to it Furthermore in cases where criminal activity has already occurred tracking makes it much harder to successfully launder money and it would often be straightforward to instantly reverse a transaction and return money to the victim of the crime Proof of transaction a digital record exists to prove that money changed hands between two parties which avoids problems inherent to cash such as shortchanging cash theft and conflicting testimonies Protection of money as a public utility digital currencies issued by central banks would provide a modern alternative to physical cash whose abolition is currently being envisaged Safety of payments systems A secure and standard interoperable digital payment instrument issued and governed by a Central Bank and used as the national digital payment instruments boosts confidence in privately controlled money systems and increases trust in the entire national payment system while also boosting competition in payment systems Preservation of seigniorage income public digital currency issuance would avoid a predictable reduction of seigniorage income for governments in the event of a disappearance of physical cash Banking competition the provision of free bank accounts at the central bank offering complete safety of money deposits could strengthen competition between banks to attract bank deposits for example by offering once again remunerated sight deposits Monetary policy transmission the issuance of central bank base money through transfers to the public could constitute a new channel for monetary policy transmission ie helicopter money which would allow more direct control of the money supply than indirect tools such as quantitative easing and interest rates and possibly lead the way towards a full reserve banking system In digital Yuan trial in Shenzhen the CBDC was programmed with an expiration date which encouraged spending and discouraged money from sitting in a saving account In the end 90 of vouchers were spent in shops Demurrage could be implemented such as by shaving off fractions of the value on a scheduled basis as a supplement to traditional inflation targets Financial safety CBDC would provide an alternative to fractional reserve banking for daily uses for those who want to avoid all risk of bank runs despite the relative safety provided by deposit insurance Risks Despite having potential advantages CBDCs remain a controversial topic and there are risks associated with their implementation Banking system disintermediation With the ability to provide digital currency directly to its citizens one concern is that depositors would shift out of the banking system Customers may deem the safety liquidity solvency and publicity of CBDCs to be more attractive weakening the balance sheet position of commercial banks In the extreme this could precipitate potential bank runs and thus make banks funding positions weaker However the Bank of England found that if the introduction of CBDC follows a set of core principles the risk of a systemwide run from bank deposits to CBDC is addressed A central bank could also limit the demand of CBDCs by setting a ceiling on the amount of holdings Centralization Since most central bank digital currencies are centralized rather than decentralized like most cryptocurrencies the controllers of the issuance of CBDCs can add or remove money from anyones account with a flip of a switch Digital dollarization A wellrun foreign digital currency could become a replacement for a local currency for the same reasons as those described in dollarization The announcement of Facebooks Libra contributed to the increased attention to CBDCs by central bankers as well as Chinas progress with DCEP to that of several Asian economies Privacy Governments have direct visibility of financial transactions an eagleeyed view on the spending of everyone Digital currency would give a country broad new powers when it comes to surveillance and controlling its population Data from tracing money routes could lead to losing financial privacy if the CBDC implementation does not have adequate privacy protections This could lead to encouraging of selfcensorship deterioration of freedom of expression and association and ultimately to stalling social developments Cybersecurity Cybersecurity is an important risk to any payment infrastructure While CBDCs offer resiliency by providing a new payment method they would also represent a critical infrastructure potentially making them a highvalue target for cyber attacks Government social manipulation Digital currency will simply become an extension of the surveillance state and it could see citizens fined in a split second for behaviors deemed undesirable Dissidents and activists could see their wallets emptied or taken offline Limiting individual freedom Digital currencies could also empower the state to make it impossible to donate to a vocal NGO Limiting or prohibiting purchases of products Digital currency could prohibit a purchase alcohol on a weekday Digital currency is also programmable The government could theoretically give out money that expires within a certain period of time or money that could only be used on certain items which could be used to induce behaviour that the government is seeking See also Bank for International Settlements ENaira Digital renminbi Digital rupee Digital currency mBridge MPesa ECedi Digital euro History of CBDCs by country Central bank References External links Atlantic Council CBDC Tracker",
  },
  {
    title: "World economy",
    originalContent:
      "The world economy or global economy is the economy of all humans in the world referring to the global economic system which includes all economic activities conducted both within and between nations including production consumption economic management work in general financial transactions and trade of goods and services In some contexts the two terms are distinct the international or global economy is measured separately and distinguished from national economies while the world economy is simply an aggregate of the separate countries measurements Beyond the minimum standard concerning value in production use and exchange the definitions representations models and valuations of the world economy vary widely It is inseparable from the geography and ecology of planet Earth It is common to limit questions of the world economy exclusively to human economic activity and the world economy is typically judged in monetary terms even in cases in which there is no efficient market to help valuate certain goods or services or in cases in which a lack of independent research genuine data or government cooperation makes calculating figures difficult Typical examples are illegal drugs and other black market goods which by any standard are a part of the world economy but for which there is by definition no legal market of any kind However even in cases in which there is a clear and efficient market to establish monetary value economists do not typically use the current or official exchange rate to translate the monetary units of this market into a single unit for the world economy since exchange rates typically do not closely reflect worldwide value for example in cases where the volume or price of transactions is closely regulated by the government Rather market valuations in a local currency are typically translated to a single monetary unit using the idea of purchasing power This is the method used below which is used for estimating worldwide economic activity in terms of real United States dollars or euros However the world economy can be evaluated and expressed in many more ways It is unclear for example how many of the worlds 78 billion people as of March 2020 have most of their economic activity reflected in these valuations According to Angus Maddisona distinguished British economistuntil the middle of the 19th century global output was dominated by China and India with the Indian subcontinent being the worlds largest economy from 1 CE to 17 CE Waves of the Industrial Revolution in Western Europe and Northern America shifted the shares to the Western Hemisphere As of 2024 the following 20 countries or collectives have reached an economy of at least US2 trillion by Gross Domestic Product GDP in nominal or Purchasing Power Parity PPP terms Brazil Canada China Egypt France Germany India Indonesia Italy Japan Mexico South Korea Russia Saudi Arabia Spain Turkey the United Kingdom the United States the European Union and the African Union Despite high levels of government investment the global economy decreased by 34 in 2020 in the midst of the COVID19 pandemic an improvement from the World Banks initial prediction of a 52 percent decrease Cities account for 80 of global GDP thus they faced the brunt of this decline The world economy increased again in 2021 with an estimated 55 percent rebound Overview World economy by country groups World economy by continent Current world economic league table of largest economies in the world by GDP and share of global economic growth Twenty largest economies in the world by nominal GDP Twenty largest economies in the world by GDP PPP Statistical indicators Finance GDP GWP gross world product purchasing power parity exchange rates 5938 trillion 2005 est 5148 trillion 2004 23 trillion 2002 The GWP is the combined gross national income of all the countries in the world When calculating the GWP add GDP of all countries Also GWP shows that imports and exports are equal Because imports and exports balance exactly when considering the whole world this also equals the total global gross domestic product GDP According to the World Bank the 2013 nominal GWP was approximately US7559 trillion In 2017 according to the CIAs World Factbook the GWP was around US8027 trillion in nominal terms and totaled approximately 1278 trillion international dollars in terms of purchasing power parity PPP The per capita PPP GWP in 2017 was approximately Int17500 according to the World Factbook GDP GWP gross world product market exchange rates 6069 trillion 2008 The market exchange rates increased from 1990 to 2008 The reason for this increase is the worlds advancement in terms of technology GDP real growth rate The following part shows the GDP growth rate and the expected value after one year Developed Economies A developed country industrialized country more developed country MDC or more economically developed country MEDC is a sovereign state that has a developed economy and advanced technological infrastructure relative to other less industrialized nations Most commonly the criteria for evaluating the degree of economic development are gross domestic product GDP gross national product GNP the per capita income level of industrialization amount of widespread infrastructure and general standard of living Which criteria are to be used and which countries can be classified as being developed are subjects of debate The GDP of the developed countries is predicted to fall from 22 in 2017 to 20 in 2018 due to the fall in dollar value Developing Countries A developing country is a country with a less developed industrial base industries and a low Human Development Index HDI relative to other countries However this definition is not universally agreed upon There is also no clear agreement on which countries fit this category A nations GDP per capita compared with other nations can also be a reference point In general the United Nations accepts any countrys claim of itself being developing The GDP of the developing countries is expected to rise from 43 in 2017 to 46 in 2018 due to political stability in those countries and advancement in technology Least developed countries The least developed countries LDCs is a list of developing countries that according to the United Nations exhibit the lowest indicators of socioeconomic development with the lowest Human Development Index ratings of all countries in the world The concept of LDCs originated in the late 1960s and the first group of LDCs was listed by the UN in its resolution 2768 XXVI of 18 November 1971 This is a group of countries that are expected to improve their GDP from 48 in 2017 to 54 in 2018 The predicted growth is associated advancement in technology and industrialization of those countries for the past decade GDP per capita purchasing power parity 9300 7500 2005 est 8200 6800 92 2003 7900 5000 2002 World median income purchasing power parity 1041 950 1993 GDP composition by sector agriculture 4 industry 32 services 64 2004 est Inflation rate consumer prices In economics inflation is a general rise in the price level in an economy over a period of time resulting in a sustained drop in the purchasing power of money When the general price level rises each unit of currency buys fewer goods and services consequently inflation reflects a reduction in the purchasing power per unit of money a loss of real value in the medium of exchange and unit of account within the economy The opposite of inflation is deflation a sustained decrease in the general price level of goods and services The common measure of inflation is the inflation rate the annualized percentage change in a general price index usually the consumer price index over time national inflation rates vary widely in individual cases from declining prices in Japan to hyperinflation In economics hyperinflation is very high and typically accelerating inflation in several Third World countries 2003 World 26 2017 28 predicted 2018 Developed Economies 1 to 4 typically Developing Countries 5 to 60 typically Least developed countries 114 2017 83 predicted 2018 Derivatives OTC outstanding notional amount 601 trillion Dec 2010 1 Derivatives exchange traded outstanding notional amount 82 trillion June 2011 2 Global debt issuance 5187 trillion 3 trillion 2004 4938 trillion 398 trillion 2003 3938 trillion 2002 Thomson Financial League Tables Global equity issuance 505 billion 450 billion 2004 388 billion 320 billion 2003 319 billion 250 trillion 2002 Thomson Financial League Tables Employment Unemployment rate 87 2009 est 30 2007 est combined unemployment and underemployment in many nonindustrialized countries developed countries typically 412 unemployment Industries Industrial production growth rate 3 2002 est Energy Yearly electricity production 21080878 GWh 2011 est 15850000 GWh 2003 est 14850000 GWh 2001 est Yearly electricity consumption 14280000 GWh 2003 est 13930000 GWh 2001 est Oil production 79650000 bbld 12663000 m3d 2003 est 75460000 barrels per day 11997000 m3d 2001 Oil consumption 80100000 bbld 12730000 m3d 2003 est 76210000 barrels per day 12116000 m3d 2001 Oil proved reserves 1025 trillion barrel 163 km3 39 cu mi 2001 est Natural gas production 3366 km3 808 cu mi 2012 est 2569 km3 616 cu mi 2001 est Natural gas consumption 2556 km3 613 cu mi 2001 est Natural gas proved reserves 161200 km3 38700 cu mi 1 January 2002 Crossborder Yearly exports 124 trillion 1105 trillion 2009 est Exports commodities the whole range of industrial and agricultural goods and services Exports partners US 127 Germany 71 China 62 France 44 Japan 42 UK 41 2008 Yearly imports 1229 trillion 1095 trillion 2009 est Imports commodities the whole range of industrial and agricultural goods and services Imports partners China 103 Germany 86 US 81 Japan 5 2008 Debt external 569 trillion 40 trillion 31 December 2009 est Gift economy Annual international aid Official Development Assistance ODA of 204 billion 2022 Communications Telephones main lines in use 843923500 20074263367600 2008 Telephones mobile cellular 3300000000 Nov 2007 Internet Service Providers ISPs 10350 2000 est Internet users 3079339857 31 December 2014 3 360985492 31 December 2000 Transport Transportation infrastructure worldwide includes Airports Total 41821 2013 Roadways Total 32345165 km 20098354 mi Paved 19403061 km 12056503 mi Unpaved 12942104 km 8041851 mi 2002 Railways Total 1122650 km 697580 mi includes about 190000 to 195000 km 118000 to 121000 mi of electrified routes of which 147760 km 91810 mi are in Europe 24509 km 15229 mi in the Far East 11050 km 6870 mi in Africa 4223 km 2624 mi in South America and 4160 km 2580 mi in North America Military World military expenditure in 2018 estimated to 1822 trillion Military expenditures percent of GDP roughly 2 of gross world product 1999 Science research and development The Royal Society in a 2011 report stated that in terms of number of papers the share of Englishlanguage scientific research papers the United States was first followed by China the UK Germany Japan France and Canada In 2015 research and development constituted an average 22 of the global GDP according to the UNESCO Institute for Statistics Metrics and rankings of innovation include the Bloomberg Innovation Index the Global Innovation Index and the share of Nobel laureates per capita Resources and environment Forests carbon sinks wood ecosystem services Estimated number of trees that are net lost annually as of 2021 10 billion Global annual deforested land in 20152020 10 million hectares Global annual net forest area loss in 20002010 47 million hectares Other land degradation and land and organismsrelated ecosystem disturbances Soils carbon sink ecosystem services food production Soil erosion by water in 2012 almost 36 billion tons based on a high resolution global potential soil erosion model developed in 2017 Estimated annual loss of agricultural productivity due to soil erosion 8 billion US dollars based on the soil erosion data Soil erosion by water in 2015 approximately 43 billion tons according to a 2020 study Environmental impact of pesticides Pesticide use in tonnes of active ingredient in Australia in 2016 ca 62500 tonnes Oceans ecosystem services food production Blue economy Waste and pollution effects of economic mechanisms effects on ecosystem services As of 2018 about 380 million tonnes of plastic is produced worldwide each year From the 1950s up to 2018 an estimated 63 billion tonnes of plastic has been produced worldwide of which an estimated 9 has been recycled and another 12 has been incinerated with the rest reportedly being dumped in landfills or the natural environment Air pollution Number of human deaths caused annually by air pollution worldwide ca 7 million Estimated global annual cost of air pollution 5 trillion Microplastic pollution Estimated accumulated number of microplastic particles in the North Atlantic Ocean in 2014 15 to 51 trillion particles weighing between 93000 and 236000 metric tons Estimated accumulated number of microplastic particles in the North Atlantic Ocean in 2020 3700 microplastics per cubic meter From the scientific perspective economic activities are embedded in a web of dynamic interrelated and interdependent activities that constitute the natural system of Earth Novel application of cybernetics in decisionmaking such as in decisionmaking related to process and productdesign and related laws and direction of human activity such as economic activity may make it easier to control modern ecological problems Historical development One example for a comparable metric other than GDP are the OECD Better Life Index rankings for different aggregative domains The index includes 11 comparable dimensions of wellbeing Housing housing conditions and spendings eg real estate pricing Income household income after taxes and transfers and net financial wealth Jobs earnings job security and unemployment Community quality of social support network Education education and what one gets out of it Environment quality of environment eg environmental health Governance involvement in democracy Health Life Satisfaction level of happiness Safety murder and assault rates Worklife balance Economic studies To promote exports many government agencies publish on the web economic studies by sector and country Among these agencies include the USCS US DoC and FAS USDA in the United States the EDC and AAFC in Canada Ubifrance in France the UKTI in the United Kingdom the HKTDC and JETRO in Asia Austrade and the NZTE in Oceania Through Partnership Agreements the Federation of International Trade Associations publishes studies from several of these agencies USCS FAS AAFC UKTI and HKTDC as well as other nongovernmental organizations on its website globaltradenet See also Regional economies Events Lists References External links OECD Economic Outlook US Bureau of Labor and Statistics Major Economic Indicators IMF World Economic Outlook UN DESA World Economy publications CIA The World Factbook World Career Education for a Global Economy BBC News Special Report Global Economy Guardian Special Report Global Economy World Bank Summary Trade Statistics for World",
  },
  {
    title: "Urban planning",
    originalContent:
      "Urban planning also known as town planning city planning regional planning or rural planning in specific contexts is a technical and political process that is focused on the development and design of land use and the built environment including air water and the infrastructure passing into and out of urban areas such as transportation communications and distribution networks and their accessibility Traditionally urban planning followed a topdown approach in master planning the physical layout of human settlements The primary concern was the public welfare which included considerations of efficiency sanitation protection and use of the environment as well as effects of the master plans on the social and economic activities Over time urban planning has adopted a focus on the social and environmental bottom lines that focus on planning as a tool to improve the health and wellbeing of people maintaining sustainability standards Similarly in the early 21st century Jane Jacobss writings on legal and political perspectives to emphasize the interests of residents businesses and communities effectively influenced urban planners to take into broader consideration of resident experiences and needs while planning Urban planning answers questions about how people will live work and play in a given area and thus guides orderly development in urban suburban and rural areas Although predominantly concerned with the planning of settlements and communities urban planners are also responsible for planning the efficient transportation of goods resources people and waste the distribution of basic necessities such as water and electricity a sense of inclusion and opportunity for people of all kinds culture and needs economic growth or business development improving health and conserving areas of natural environmental significance that actively contributes to reduction in CO2 emissions as well as protecting heritage structures and built environments Since most urban planning teams consist of highly educated individuals that work for city governments recent debates focus on how to involve more community members in city planning processes Urban planning is an interdisciplinary field that includes civil engineering architecture human geography politics social science and design sciences Practitioners of urban planning are concerned with research and analysis strategic thinking engineering architecture urban design public consultation policy recommendations implementation and management It is closely related to the field of urban design and some urban planners provide designs for streets parks buildings and other urban areas Urban planners work with the cognate fields of civil engineering landscape architecture architecture and public administration to achieve strategic policy and sustainability goals Early urban planners were often members of these cognate fields though today urban planning is a separate independent professional discipline The discipline of urban planning is the broader category that includes different subfields such as landuse planning zoning economic development environmental planning and transportation planning Creating the plans requires a thorough understanding of penal codes and zonal codes of planning Another important aspect of urban planning is that the range of urban planning projects include the largescale master planning of empty sites or Greenfield projects as well as smallscale interventions and refurbishments of existing structures buildings and public spaces Pierre Charles LEnfant in Washington DC Daniel Burnham in Chicago Lcio Costa in Braslia and GeorgesEugene Haussmann in Paris planned cities from scratch and Robert Moses and Le Corbusier refurbished and transformed cities and neighborhoods to meet their ideas of urban planning History There is evidence of urban planning and designed communities dating back to the Mesopotamian Indus Valley Minoan and Egyptian civilizations in the third millennium BCE Archaeologists studying the ruins of cities in these areas find paved streets that were laid out at right angles in a grid pattern The idea of a planned out urban area evolved as different civilizations adopted it Beginning in the 8th century BCE Greek city states primarily used orthogonal or gridlike plans Hippodamus of Miletus 498408 BC the ancient Greek architect and urban planner is considered to be the father of European urban planning and the namesake of the Hippodamian plan grid plan of city layout The ancient Romans also used orthogonal plans for their cities City planning in the Roman world was developed for military defense and public convenience The spread of the Roman Empire subsequently spread the ideas of urban planning As the Roman Empire declined these ideas slowly disappeared However many cities in Europe still held onto the planned Roman city center Cities in Europe from the 9th to 14th centuries often grew organically and sometimes chaotically But in the following centuries with the coming of the Renaissance many new cities were enlarged with newly planned extensions From the 15th century on much more is recorded of urban design and the people that were involved In this period theoretical treatises on architecture and urban planning start to appear in which theoretical questions around planning the main lines ensuring plans meet the needs of the given population and so forth are addressed and designs of towns and cities are described and depicted During the Enlightenment period several European rulers ambitiously attempted to redesign capital cities During the Second French Empire Baron GeorgesEugne Haussmann under the direction of Napoleon III redesigned the city of Paris into a more modern capital with long straight wide boulevards Planning and architecture went through a paradigm shift at the turn of the 20th century The industrialized cities of the 19th century grew at a tremendous rate The evils of urban life for the working poor were becoming increasingly evident as a matter of public concern The laissezfaire style of government management of the economy in fashion for most of the Victorian era was starting to give way to a New Liberalism that championed intervention on the part of the poor and disadvantaged Around 1900 theorists began developing urban planning models to mitigate the consequences of the industrial age by providing citizens especially factory workers with healthier environments The following century would therefore be globally dominated by a central planning approach to urban planning not representing an increment in the overall quality of the urban realm At the beginning of the 20th century urban planning began to be recognized as a separate profession The Town and Country Planning Association was founded in 1899 and the first academic course in Great Britain on urban planning was offered by the University of Liverpool in 1909 In the 1920s the ideas of modernism and uniformity began to surface in urban planning and lasted until the 1970s In 1933 Le Corbusier presented the Radiant City a city that grows up in the form of towers as a solution to the problem of pollution and overcrowding But many planners started to believe that the ideas of modernism in urban planning led to higher crime rates and social problems In the second half of the 20th century urban planners gradually shifted their focus to individualism and diversity in urban centers 21st century practices Urban planners studying the effects of increasing congestion in urban areas began to address the externalities the negative impacts caused by induced demand from larger highway systems in western countries such as in the United States The United Nations Department of Economic and Social Affairs predicted in 2018 that around 25 billion more people occupy urban areas by 2050 according to population elements of global migration New planning theories have adopted nontraditional concepts such as Blue Zones and Innovation Districts to incorporate geographic areas within the city that allow for novel business development and the prioritization of infrastructure that would assist with improving the quality of life of citizens by extending their potential lifespan Planning practices have incorporated policy changes to help address anthropogenic human caused climate change London began to charge a congestion charge for cars trying to access already crowded places in the city Cities nowadays stress the importance of public transit and cycling by adopting such policies Theories Planning theory is the body of scientific concepts definitions behavioral relationships and assumptions that define the body of knowledge of urban planning There are eight procedural theories of planning that remain the principal theories of planning procedure today the rationalcomprehensive approach the incremental approach the transactive approach the communicative approach the advocacy approach the equity approach the radical approach and the humanist or phenomenological approach Some other conceptual planning theories include Ebenezer Howards The Three Magnets theory that he envisioned for the future of British settlement also his Garden Cities the Concentric Model Zone also called the Burgess Model by sociologist Ernest Burgess the Radburn Superblock that encourages pedestrian movement the Sector Model and the Multiple Nuclei Model among others Participatory urban planning Participatory planning is an urban planning approach that involves the entire community in the planning process Participatory planning in the United States emerged during the 1960s and 1970s Technical aspects Technical aspects of urban planning involve the application of scientific technical processes considerations and features that are involved in planning for land use urban design natural resources transportation and infrastructure Urban planning includes techniques such as predicting population growth zoning geographic mapping and analysis analyzing park space surveying the water supply identifying transportation patterns recognizing food supply demands allocating healthcare and social services and analyzing the impact of land use In order to predict how cities will develop and estimate the effects of their interventions planners use various models These models can be used to indicate relationships and patterns in demographic geographic and economic data They might deal with shortterm issues such as how people move through cities or longterm issues such as land use and growth One such model is the Geographic Information System GIS that is used to create a model of the existing planning and then to project future impacts on the society economy and environment Building codes and other regulations dovetail with urban planning by governing how cities are constructed and used from the individual level Enforcement methodologies include governmental zoning planning permissions and building codes as well as private easements and restrictive covenants With recent advances in information and communication technologies and the Internet of Things an increasing number of cities are adopting technologies such as crowdsorced mobile phone sensing and machine learning to collect data and extract useful information to help make informed urban planning decisions Urban planners An urban planner is a professional who works in the field of urban planning for the purpose of optimizing the effectiveness of a communitys land use and infrastructure They formulate plans for the development and management of urban and suburban areas They typically analyze land use compatibility as well as economic environmental and social trends In developing any plan for a community whether commercial residential agricultural natural or recreational urban planners must consider a wide array of issues including sustainability existing and potential pollution transport including potential congestion crime land values economic development social equity zoning codes and other legislation The importance of the urban planner is increasing in the 21st century as modern society begins to face issues of increased population growth climate change and unsustainable development An urban planner could be considered a green collar professional Some researchers suggest that urban planners globally work in different planning cultures adapted to their cities and cultures However professionals have identified skills abilities and basic knowledge sets that are common to urban planners across regional and national boundaries Criticisms and debates The school of neoclassical economics argues that planning is unnecessary or even harmful as it market efficiency allows for effective land use A pluralist strain of political thinking argues in a similar vein that the government should not intrude in the political competition between different interest groups which decides how land is used The traditional justification for urban planning has in response been that the planner does to the city what the engineer or architect does to the home that is make it more amenable to the needs and preferences of its inhabitants The widely adopted consensusbuilding model of planning which seeks to accommodate different preferences within the community has been criticized for being based upon rather than challenging the power structures of the community Instead agonism has been proposed as a framework for urban planning decisionmaking Another debate within the urban planning field is about who is included and excluded in the urban planning decisionmaking process Most urban planning processes use a topdown approach which fails to include the residents of the places where urban planners and city officials are working Sherry Arnsteins ladder of citizen participation is often used by many urban planners and city governments to determine the degree of inclusivity or exclusivity of their urban planning One main source of engagement between city officials and residents are city council meetings that are open to the residents and that welcome public comments Additionally in US there are some federal requirements for citizen participation in governmentfunded infrastructure projects Participatory urban planning has been criticized for contributing to the housing crisis in parts of the world See also References Further reading Pennington Mark 2008 Urban planning In Hamowy Ronald ed The Encyclopedia of Libertarianism Thousand Oaks CA SAGE Cato Institute pp 51718 doi1041359781412965811n316 ISBN 9781412965804 LCCN 2008009151 OCLC 750831024 S2CID 243497795 Knox P L 2020 Better by Design Architecture Urban Planning and the Good City Blacksburg Virginia Tech Publishing DOI httpsdoiorg1021061betterbydesign Paul Waterhouse Raymond Unwin 1912 Old Towns and New Needs also the Town Extension Plan Manchester Victoria University of Manchester OCLC 225676578 Wikidata Q18606907 External links American Planning Association Library guides for urban planning Urban Planning Resources US LibGuides at Arizona State University Urban Planning Research Guides University of California Los Angeles Library Archived from the original on 30 March 2014 Retrieved 21 March 2015 Avery Architectural and Fine Arts Library September 2009 Urban Planning Basic Resources for Research Research Guides New York Columbia University Libraries Archived from the original on 2 April 2015 Retrieved 21 March 2015 Urban and Regional Policy Finding Articles Research Guides US Georgia Tech Archived from the original on 1 June 2022cite web CS1 maint unfit URL link Harvard University Graduate School of Design Urban Planning and Design Research Guides Massachusetts Harvard Library Urban Affairs Planning Topic Guides New York City CUNY Hunter College Libraries Archived from the original on 22 February 2014 Retrieved 21 March 2015 City Planning LibGuides University of Illinois Library at University of Illinois at UrbanaChampaign Archived from the original on 23 August 2015 Urban Studies Planning Research Guides Massachusetts Institute of Technology Libraries Archived from the original on 7 February 2014 Urban and Regional Planning Research Guides US University of Michigan Library Urban Studies Planning Oregon US LibGuides at Portland State University Archived from the original on 2 April 2015 Retrieved 21 March 2015",
  },
  {
    title: "Housing market",
    originalContent:
      "Housing market can refer to The economics of realestate used for residential purposes see Real estate economics Real estate business buying selling or renting real estate land buildings or housing The problem of assigning indivisible items such as houses to people with different preferences such that each person receives a single item see House allocation problem",
  },
  {
    title: "Real estate",
    originalContent:
      "Real estate is property consisting of land and the buildings on it along with its natural resources such as growing crops eg timber minerals or water and wild animals immovable property of this nature an interest vested in this also an item of real property more generally buildings or housing in general In terms of law real relates to land property and is different from personal property while estate means the interest a person has in that land property Real estate is different from personal property which is not permanently attached to the land or comes with the land such as vehicles boats jewelry furniture tools and the rolling stock of a farm and farm animals In the United States the transfer owning or acquisition of real estate can be through business corporations individuals nonprofit corporations fiduciaries or any legal entity as seen within the law of each US state History of real estate The natural right of a person to own property as a concept can be seen as having roots in Roman law as well as Greek philosophy The profession of appraisal can be seen as beginning in England during the 1500s as agricultural needs required land clearing and land preparation Textbooks on the subject of surveying began to be written and the term surveying was used in England while the term appraising was more used in North America Natural law which can be seen as universal law was discussed among writers of the 15th and 16th century as it pertained to property theory and the interstate relations dealing with foreign investments and the protection of citizens private property abroad Natural law can be seen as having an influence in Emerich de Vattels 1758 treatise The Law of Nations which conceptualized the idea of private property One of the largest initial real estate deals in history known as the Louisiana Purchase happened in 1803 when the Louisiana Purchase Treaty was signed This treaty paved the way for western expansion and made the US the owners of the Louisiana Territory as the land was bought from France for fifteen million making each acre roughly 4 cents The oldest real estate brokerage firm was established in 1855 in Chicago Illinois and was initially known as L D Olmsted Co but is now known as Baird Warner In 1908 the National Association of Realtors was founded in Chicago and in 1916 the name was changed to the National Association of Real Estate Boards and this was also when the term realtor was coined to identify real estate professionals The stock market crash of 1929 and the Great Depression in the US caused a major drop in real estate worth and prices and ultimately resulted in depreciation of 50 for the four years after 1929 Housing financing in the US was greatly affected by the Banking Act of 1933 and the National Housing Act in 1934 because it allowed for mortgage insurance for home buyers and this system was implemented by the Federal Deposit Insurance as well as the Federal Housing Administration In 1938 an amendment was made to the National Housing Act and Fannie Mae a government agency was established to serve as a secondary market for mortgages and to give lenders more money in order for new homes to be funded Title VIII of the Civil Rights Act in the US which is also known as the Fair Housing Act was put into place in 1968 and dealt with the incorporation of African Americans into neighborhoods as the issues of discrimination were analyzed with the renting buying and financing of homes Internet real estate as a concept began with the first appearance of real estate platforms on the World Wide Web www and occurred in 1999 Residential real estate Residential real estate may contain either a single family or multifamily structure that is available for occupation or for nonbusiness purposes Residences can be classified by and how they are connected to neighbouring residences and land Different types of housing tenure can be used for the same physical type For example connected residences might be owned by a single entity and leased out or owned separately with an agreement covering the relationship between units and common areas and concerns According to the Congressional Research Service in 2021 65 of homes in the US are owned by the occupier Major categories Attached multiunit dwellings Apartment American English or Flat British English An individual unit in a multiunit building The boundaries of the apartment are generally defined by a perimeter of locked or lockable doors Often seen in multistory apartment buildings Multifamily house Often seen in multistory detached buildings where each floor is a separate apartment or unit Terraced house aka townhouse or rowhouse A number of single or multiunit buildings in a continuous row with shared walls and no intervening space Condominium American English A building or complex similar to apartments owned by individuals Common grounds and common areas within the complex are owned and shared jointly In North America there are townhouse or rowhouse style condominiums as well The British equivalent is a block of flats Housing cooperative aka coop A type of multiple ownership in which the residents of a multiunit housing complex own shares in the cooperative corporation that owns the property giving each resident the right to occupy a specific apartment or unit Majority of housing in Indian metro cities are of these types Tenement A type of building shared by multiple dwellings typically with flats or apartments on each floor and with shared entrance stairway access found in Britain Semidetached dwellings Duplex Two units with one shared wall Detached dwellings Bungalows Splitlevel home Mansions Villas Detached house or singlefamily detached house Cottages Portable dwellings Mobile homes tiny homes or residential caravans A fulltime residence that can be although might not in practice be movable on wheels Houseboats A floating home Tents Usually temporary with roof and walls consisting only of fabriclike material Other categories Chawls Havelis Igloos Huts The size of havelis and chawls is measured in Gaz square yards Quila Marla Beegha and acre See List of house types for a complete listing of housing types and layouts real estate trends for shifts in the market and house or home for more general information Real estate and the environment Real estate can be valued or devalued based on the amount of environmental degradation that has occurred Environmental degradation can cause extreme health and safety risks There is a growing demand for the use of site assessments ESAs when valuing a property for both private and commercial real estate Environmental surveying is made possible by environmental surveyors who examine the environmental factors present within the development of real estate as well as the impacts that development and real estate has on the environment Green development is a concept that has grown since the 1970s with the environmental movement and the World Commission on Environment and Development Green development examines social and environmental impacts with real estate and building There are 3 areas of focus being the environmental responsiveness resource efficiency and the sensitivity of cultural and societal aspects Examples of Green development are green infrastructure LEED conservation development and sustainability developments Real estate in itself has been measured as a contributing factor to the rise in green house gases According to the International Energy Agency real estate in 2019 was responsible for 39 percent of total emissions worldwide and 11 percent of those emissions were due to the manufacturing of materials used in buildings Investment and development Investment in real estate can be categorized by financial risk into core valueadded and opportunistic Real estate development can be less cyclical than real estate investing In markets where land and building prices are rising real estate is often purchased as an investment whether or not the owner intends to use the property Often investment properties are rented out but flipping involves quickly reselling a property sometimes taking advantage of arbitrage or quickly rising value and sometimes after repairs are made that substantially raise the value of the property Luxury real estate is sometimes used as a way to store value especially by wealthy foreigners without any particular attempt to rent it out Some luxury units in London and New York City have been used as a way for corrupt foreign government officials and businesspeople from countries without strong rule of law to launder money or to protect it from seizure Professionals Real estate agent North America Estate agent United Kingdom See also References External links The dictionary definition of real estate at Wiktionary Quotations related to Real estate at Wikiquote",
  },
  {
    title: "Civil engineering",
    originalContent:
      "Civil engineering is a professional engineering discipline that deals with the design construction and maintenance of the physical and naturally built environment including public works such as roads bridges canals dams airports sewage systems pipelines structural components of buildings and railways Civil engineering is traditionally broken into a number of subdisciplines It is considered the secondoldest engineering discipline after military engineering and it is defined to distinguish nonmilitary engineering from military engineering Civil engineering can take place in the public sector from municipal public works departments through to federal government agencies and in the private sector from locally based firms to Fortune Global 500 companies History Civil engineering as a discipline Civil engineering is the application of physical and scientific principles for solving the problems of society and its history is intricately linked to advances in the understanding of physics and mathematics throughout history Because civil engineering is a broad profession including several specialized subdisciplines its history is linked to knowledge of structures materials science geography geology soils hydrology environmental science mechanics project management and other fields Throughout ancient and medieval history most architectural design and construction was carried out by artisans such as stonemasons and carpenters rising to the role of master builder Knowledge was retained in guilds and seldom supplanted by advances Structures roads and infrastructure that existed were repetitive and increases in scale were incremental One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC including Archimedes principle which underpins our understanding of buoyancy and practical solutions such as Archimedes screw Brahmagupta an Indian mathematician used arithmetic in the 7th century AD based on HinduArabic numerals for excavation volume computations Civil engineering profession Engineering has been an aspect of life since the beginnings of human existence The earliest practice of civil engineering may have commenced between 4000 and 2000 BC in ancient Egypt the Indus Valley civilization and Mesopotamia ancient Iraq when humans started to abandon a nomadic existence creating a need for the construction of shelter During this time transportation became increasingly important leading to the development of the wheel and sailing Until modern times there was no clear distinction between civil engineering and architecture and the term engineer and architect were mainly geographical variations referring to the same occupation and often used interchangeably The constructions of pyramids in Egypt c 27002500 BC constitute some of the first instances of large structure constructions in history Other ancient historic civil engineering constructions include the Qanat water management system in modernday Iran the oldest is older than 3000 years and longer than 71 kilometres 44 mi the Parthenon by Iktinos in Ancient Greece 447438 BC the Appian Way by Roman engineers c 312 BC the Great Wall of China by General Meng Tien under orders from Chin Emperor Shih Huang Ti c 220 BC and the stupas constructed in ancient Sri Lanka like the Jetavanaramaya and the extensive irrigation works in Anuradhapura The Romans developed civil structures throughout their empire including especially aqueducts insulae harbors bridges dams and roads In the 18th century the term civil engineering was coined to incorporate all things civilian as opposed to military engineering In 1747 the first institution for the teaching of civil engineering the cole Nationale des Ponts et Chausses was established in France and more examples followed in other European countries like Spain The first selfproclaimed civil engineer was John Smeaton who constructed the Eddystone Lighthouse In 1771 Smeaton and some of his colleagues formed the Smeatonian Society of Civil Engineers a group of leaders of the profession who met informally over dinner Though there was evidence of some technical meetings it was little more than a social society In 1818 the Institution of Civil Engineers was founded in London and in 1820 the eminent engineer Thomas Telford became its first president The institution received a Royal charter in 1828 formally recognising civil engineering as a profession Its charter defined civil engineering asthe art of directing the great sources of power in nature for the use and convenience of man as the means of production and of traffic in states both for external and internal trade as applied in the construction of roads bridges aqueducts canals river navigation and docks for internal intercourse and exchange and in the construction of ports harbours moles breakwaters and lighthouses and in the art of navigation by artificial power for the purposes of commerce and in the construction and application of machinery and in the drainage of cities and towns Civil engineering education The first private college to teach civil engineering in the United States was Norwich University founded in 1819 by Captain Alden Partridge The first degree in civil engineering in the United States was awarded by Rensselaer Polytechnic Institute in 1835 The first such degree to be awarded to a woman was granted by Cornell University to Nora Stanton Blatch in 1905 In the UK during the early 19th century the division between civil engineering and military engineering served by the Royal Military Academy Woolwich coupled with the demands of the Industrial Revolution spawned new engineering education initiatives the Class of Civil Engineering and Mining was founded at Kings College London in 1838 mainly as a response to the growth of the railway system and the need for more qualified engineers the private College for Civil Engineers in Putney was established in 1839 and the UKs first Chair of Engineering was established at the University of Glasgow in 1840 Education Civil engineers typically possess an academic degree in civil engineering The length of study is three to five years and the completed degree is designated as a bachelor of technology or a bachelor of engineering The curriculum generally includes classes in physics mathematics project management design and specific topics in civil engineering After taking basic courses in most subdisciplines of civil engineering they move on to specialize in one or more subdisciplines at advanced levels While an undergraduate degree BEngBSc normally provides successful students with industryaccredited qualifications some academic institutions offer postgraduate degrees MEngMSc which allow students to further specialize in their particular area of interest Practicing engineers In most countries a bachelors degree in engineering represents the first step towards professional certification and a professional body certifies the degree program After completing a certified degree program the engineer must satisfy a range of requirements including work experience and exam requirements before being certified Once certified the engineer is designated as a professional engineer in the United States Canada and South Africa a chartered engineer in most Commonwealth countries a chartered professional engineer in Australia and New Zealand or a European engineer in most countries of the European Union There are international agreements between relevant professional bodies to allow engineers to practice across national borders The benefits of certification vary depending upon location For example in the United States and Canada only a licensed professional engineer may prepare sign and seal and submit engineering plans and drawings to a public authority for approval or seal engineering work for public and private clients This requirement is enforced under provincial law such as the Engineers Act in Quebec No such legislation has been enacted in other countries including the United Kingdom In Australia state licensing of engineers is limited to the state of Queensland Almost all certifying bodies maintain a code of ethics which all members must abide by Engineers must obey contract law in their contractual relationships with other parties In cases where an engineers work fails they may be subject to the law of tort of negligence and in extreme cases criminal charges An engineers work must also comply with numerous other rules and regulations such as building codes and environmental law Subdisciplines There are a number of subdisciplines within the broad field of civil engineering General civil engineers work closely with surveyors and specialized civil engineers to design grading drainage pavement water supply sewer service dams electric and communications supply General civil engineering is also referred to as site engineering a branch of civil engineering that primarily focuses on converting a tract of land from one usage to another Site engineers spend time visiting project sites meeting with stakeholders and preparing construction plans Civil engineers apply the principles of geotechnical engineering structural engineering environmental engineering transportation engineering and construction engineering to residential commercial industrial and public works projects of all sizes and levels of construction Coastal engineering Coastal engineering is concerned with managing coastal areas In some jurisdictions the terms sea defense and coastal protection mean defense against flooding and erosion respectively Coastal defense is the more traditional term but coastal management has become popular as well Construction engineering Construction engineering involves planning and execution transportation of materials and site development based on hydraulic environmental structural and geotechnical engineering As construction firms tend to have higher business risk than other types of civil engineering firms construction engineers often engage in more businesslike transactions such as drafting and reviewing contracts evaluating logistical operations and monitoring supply prices Earthquake engineering Earthquake engineering involves designing structures to withstand hazardous earthquake exposures Earthquake engineering is a subdiscipline of structural engineering The main objectives of earthquake engineering are to understand interaction of structures on the shaky ground foresee the consequences of possible earthquakes and design construct and maintain structures to perform at earthquake in compliance with building codes Environmental engineering Environmental engineering is the contemporary term for sanitary engineering though sanitary engineering traditionally had not included much of the hazardous waste management and environmental remediation work covered by environmental engineering Public health engineering and environmental health engineering are other terms being used Environmental engineering deals with treatment of chemical biological or thermal wastes purification of water and air and remediation of contaminated sites after waste disposal or accidental contamination Among the topics covered by environmental engineering are pollutant transport water purification waste water treatment air pollution solid waste treatment recycling and hazardous waste management Environmental engineers administer pollution reduction green engineering and industrial ecology Environmental engineers also compile information on environmental consequences of proposed actions Forensic engineering Forensic engineering is the investigation of materials products structures or components that fail or do not operate or function as intended causing personal injury or damage to property The consequences of failure are dealt with by the law of product liability The field also deals with retracing processes and procedures leading to accidents in operation of vehicles or machinery The subject is applied most commonly in civil law cases although it may be of use in criminal law cases Generally the purpose of a Forensic engineering investigation is to locate cause or causes of failure with a view to improve performance or life of a component or to assist a court in determining the facts of an accident It can also involve investigation of intellectual property claims especially patents Geotechnical engineering Geotechnical engineering studies rock and soil supporting civil engineering systems Knowledge from the field of soil science materials science mechanics and hydraulics is applied to safely and economically design foundations retaining walls and other structures Environmental efforts to protect groundwater and safely maintain landfills have spawned a new area of research called geoenvironmental engineering Identification of soil properties presents challenges to geotechnical engineers Boundary conditions are often well defined in other branches of civil engineering but unlike steel or concrete the material properties and behavior of soil are difficult to predict due to its variability and limitation on investigation Furthermore soil exhibits nonlinear stressdependent strength stiffness and dilatancy volume change associated with application of shear stress making studying soil mechanics all the more difficult Geotechnical engineers frequently work with professional geologists Geological Engineering professionals and soil scientists Materials science and engineering Materials science is closely related to civil engineering It studies fundamental characteristics of materials and deals with ceramics such as concrete and mix asphalt concrete strong metals such as aluminum and steel and thermosetting polymers including polymethylmethacrylate PMMA and carbon fibers Materials engineering involves protection and prevention paints and finishes Alloying combines two types of metals to produce another metal with desired properties It incorporates elements of applied physics and chemistry With recent media attention on nanoscience and nanotechnology materials engineering has been at the forefront of academic research It is also an important part of forensic engineering and failure analysis Site development and planning Site development also known as site planning is focused on the planning and development potential of a site as well as addressing possible impacts from permitting issues and environmental challenges Structural engineering Structural engineering is concerned with the structural design and structural analysis of buildings bridges towers flyovers overpasses tunnels off shore structures like oil and gas fields in the sea aerostructure and other structures This involves identifying the loads which act upon a structure and the forces and stresses which arise within that structure due to those loads and then designing the structure to successfully support and resist those loads The loads can be self weight of the structures other dead load live loads moving wheel load wind load earthquake load load from temperature change etc The structural engineer must design structures to be safe for their users and to successfully fulfill the function they are designed for to be serviceable Due to the nature of some loading conditions subdisciplines within structural engineering have emerged including wind engineering and earthquake engineering Design considerations will include strength stiffness and stability of the structure when subjected to loads which may be static such as furniture or selfweight or dynamic such as wind seismic crowd or vehicle loads or transitory such as temporary construction loads or impact Other considerations include cost constructibility safety aesthetics and sustainability Surveying Surveying is the process by which a surveyor measures certain dimensions that occur on or near the surface of the Earth Surveying equipment such as levels and theodolites are used for accurate measurement of angular deviation horizontal vertical and slope distances With computerization electronic distance measurement EDM total stations GPS surveying and laser scanning have to a large extent supplanted traditional instruments Data collected by survey measurement is converted into a graphical representation of the Earths surface in the form of a map This information is then used by civil engineers contractors and realtors to design from build on and trade respectively Elements of a structure must be sized and positioned in relation to each other and to site boundaries and adjacent structures Although surveying is a distinct profession with separate qualifications and licensing arrangements civil engineers are trained in the basics of surveying and mapping as well as geographic information systems Surveyors also lay out the routes of railways tramway tracks highways roads pipelines and streets as well as position other infrastructure such as harbors before construction Land surveying In the United States Canada the United Kingdom and most Commonwealth countries land surveying is considered to be a separate and distinct profession Land surveyors are not considered to be engineers and have their own professional associations and licensing requirements The services of a licensed land surveyor are generally required for boundary surveys to establish the boundaries of a parcel using its legal description and subdivision plans a plot or map based on a survey of a parcel of land with boundary lines drawn inside the larger parcel to indicate the creation of new boundary lines and roads both of which are generally referred to as Cadastral surveying They collect data on important geological features below and on the land Construction surveying Construction surveying is generally performed by specialized technicians Unlike land surveyors the resulting plan does not have legal status Construction surveyors perform the following tasks Surveying existing conditions of the future work site including topography existing buildings and infrastructure and underground infrastructure when possible layout or settingout placing reference points and markers that will guide the construction of new structures such as roads or buildings Verifying the location of structures during construction AsBuilt surveying a survey conducted at the end of the construction project to verify that the work authorized was completed to the specifications set on plans Transportation engineering Transportation engineering is concerned with moving people and goods efficiently safely and in a manner conducive to a vibrant community This involves specifying designing constructing and maintaining transportation infrastructure which includes streets canals highways rail systems airports ports and mass transit It includes areas such as transportation design transportation planning traffic engineering some aspects of urban engineering queueing theory pavement engineering Intelligent Transportation System ITS and infrastructure management Municipal or urban engineering Municipal engineering is concerned with municipal infrastructure This involves specifying designing constructing and maintaining streets sidewalks water supply networks sewers street lighting municipal solid waste management and disposal storage depots for various bulk materials used for maintenance and public works salt sand etc public parks and cycling infrastructure In the case of underground utility networks it may also include the civil portion conduits and access chambers of the local distribution networks of electrical and telecommunications services It can also include the optimization of waste collection and bus service networks Some of these disciplines overlap with other civil engineering specialties however municipal engineering focuses on the coordination of these infrastructure networks and services as they are often built simultaneously and managed by the same municipal authority Municipal engineers may also design the site civil works for large buildings industrial plants or campuses ie access roads parking lots potable water supply treatment or pretreatment of waste water site drainage etc Water resources engineering Water resources engineering is concerned with the collection and management of water as a natural resource As a discipline it therefore combines elements of hydrology environmental science meteorology conservation and resource management This area of civil engineering relates to the prediction and management of both the quality and the quantity of water in both underground aquifers and above ground lakes rivers and streams resources Water resource engineers analyze and model very small to very large areas of the earth to predict the amount and content of water as it flows into through or out of a facility However the actual design of the facility may be left to other engineers Hydraulic engineering concerns the flow and conveyance of fluids principally water This area of civil engineering is intimately related to the design of pipelines water supply network drainage facilities including bridges dams channels culverts levees storm sewers and canals Hydraulic engineers design these facilities using the concepts of fluid pressure fluid statics fluid dynamics and hydraulics among others Civil engineering systems Civil engineering systems is a discipline that promotes using systems thinking to manage complexity and change in civil engineering within its broader public context It posits that the proper development of civil engineering infrastructure requires a holistic coherent understanding of the relationships between all of the crucial factors that contribute to successful projects while at the same time emphasizing the importance of attention to technical detail Its purpose is to help integrate the entire civil engineering project life cycle from conception through planning designing making operating to decommissioning See also Associations References Further reading Blockley David 2014 Structural Engineering a very short introduction New York Oxford University Press ISBN 9780199671939 Chen WF Liew JY Richard eds 2002 The Civil Engineering Handbook CRC Press ISBN 9780849309588 Muir Wood David 2012 Civil Engineering a very short introduction New York Oxford University Press ISBN 9780199578634 Ricketts Jonathan T Loftin M Kent Merritt Frederick S eds 2004 Standard handbook for civil engineers 5 ed McGraw Hill ISBN 9780071364737 External links The Institution of Civil Engineers Civil Engineering Software Database The Institution of Civil Engineering Surveyors Civil engineering classes from MIT OpenCourseWare",
  },
  {
    title: "Structural engineering",
    originalContent:
      "Structural engineering is a subdiscipline of civil engineering in which structural engineers are trained to design the bones and joints that create the form and shape of humanmade structures Structural engineers also must understand and calculate the stability strength rigidity and earthquakesusceptibility of built structures for buildings and nonbuilding structures The structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site They can also be involved in the design of machinery medical equipment and vehicles where structural integrity affects functioning and safety See glossary of structural engineering Structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries Structural engineering design uses a number of relatively simple structural concepts to build complex structural systems Structural engineers are responsible for making creative and efficient use of funds structural elements and materials to achieve these goals History Structural engineering dates back to 2700 BC when the step pyramid for Pharaoh Djoser was built by Imhotep the first engineer in history known by name Pyramids were the most common major structures built by ancient civilizations because the structural form of a pyramid is inherently stable and can be almost infinitely scaled as opposed to most other structural forms which cannot be linearly increased in size in proportion to increased loads The structural stability of the pyramid whilst primarily gained from its shape relies also on the strength of the stone from which it is constructed and its ability to support the weight of the stone above it The limestone blocks were often taken from a quarry near the building site and have a compressive strength from 30 to 250 MPa MPa Pa 106 Therefore the structural strength of the pyramid stems from the material properties of the stones from which it was built rather than the pyramids geometry Throughout ancient and medieval history most architectural design and construction were carried out by artisans such as stonemasons and carpenters rising to the role of master builder No theory of structures existed and understanding of how structures stood up was extremely limited and based almost entirely on empirical evidence of what had worked before and intuition Knowledge was retained by guilds and seldom supplanted by advances Structures were repetitive and increases in scale were incremental No record exists of the first calculations of the strength of structural members or the behavior of structural material but the profession of a structural engineer only really took shape with the Industrial Revolution and the reinvention of concrete see History of Concrete The physical sciences underlying structural engineering began to be understood in the Renaissance and have since developed into computerbased applications pioneered in the 1970s Timeline 14521519 Leonardo da Vinci made many contributions 1638 Galileo Galilei published the book Two New Sciences in which he examined the failure of simple structures 1660 Hookes law by Robert Hooke 1687 Isaac Newton published Philosophi Naturalis Principia Mathematica which contains his laws of motion 1750 EulerBernoulli beam equation 17001782 Daniel Bernoulli introduced the principle of virtual work 17071783 Leonhard Euler developed the theory of buckling of columns 1826 ClaudeLouis Navier published a treatise on the elastic behaviors of structures 1873 Carlo Alberto Castigliano presented his dissertation Intorno ai sistemi elastici which contains his theorem for computing displacement as the partial derivative of the strain energy This theorem includes the method of least work as a special case 1874 Otto Mohr formalized the idea of a statically indeterminate structure 1922 Timoshenko corrects the EulerBernoulli beam equation 1936 Hardy Cross publication of the moment distribution method an important innovation in the design of continuous frames 1941 Alexander Hrennikoff solved the discretization of plane elasticity problems using a lattice framework 1942 Richard Courant divided a domain into finite subregions 1956 J Turner R W Clough H C Martin and L J Topps paper on the Stiffness and Deflection of Complex Structures introduces the name finiteelement method and is widely recognized as the first comprehensive treatment of the method as it is known today Structural failure The history of structural engineering contains many collapses and failures Sometimes this is due to obvious negligence as in the case of the PtionVille school collapse in which Rev Fortin Augustin constructed the building all by himself saying he didnt need an engineer as he had good knowledge of construction following a partial collapse of the threestory schoolhouse that sent neighbors fleeing The final collapse killed 94 people mostly children In other cases structural failures require careful study and the results of these inquiries have resulted in improved practices and a greater understanding of the science of structural engineering Some such studies are the result of forensic engineering investigations where the original engineer seems to have done everything in accordance with the state of the profession and acceptable practice yet a failure still eventuated A famous case of structural knowledge and practice being advanced in this manner can be found in a series of failures involving box girders which collapsed in Australia during the 1970s Theory Structural engineering depends upon a detailed knowledge of applied mechanics materials science and applied mathematics to understand and predict how structures support and resist selfweight and imposed loads To apply the knowledge successfully a structural engineer generally requires detailed knowledge of relevant empirical and theoretical design codes the techniques of structural analysis as well as some knowledge of the corrosion resistance of the materials and structures especially when those structures are exposed to the external environment Since the 1990s specialist software has become available to aid in the design of structures with the functionality to assist in the drawing analyzing and designing of structures with maximum precision examples include AutoCAD StaadPro ETABS Prokon Revit Structure Inducta RCB etc Such software may also take into consideration environmental loads such as earthquakes and winds Profession Structural engineers are responsible for engineering design and structural analysis Entrylevel structural engineers may design the individual structural elements of a structure such as the beams and columns of a building More experienced engineers may be responsible for the structural design and integrity of an entire system such as a building Structural engineers often specialize in particular types of structures such as buildings bridges pipelines industrial tunnels vehicles ships aircraft and spacecraft Structural engineers who specialize in buildings may specialize in particular construction materials such as concrete steel wood masonry alloys and composites Structural engineering has existed since humans first started to construct their structures It became a more defined and formalized profession with the emergence of architecture as a distinct profession from engineering during the industrial revolution in the late 19th century Until then the architect and the structural engineer were usually one and the same thing the master builder Only with the development of specialized knowledge of structural theories that emerged during the 19th and early 20th centuries did the professional structural engineers come into existence The role of a structural engineer today involves a significant understanding of both static and dynamic loading and the structures that are available to resist them The complexity of modern structures often requires a great deal of creativity from the engineer in order to ensure the structures support and resist the loads they are subjected to A structural engineer will typically have a four or fiveyear undergraduate degree followed by a minimum of three years of professional practice before being considered fully qualified Structural engineers are licensed or accredited by different learned societies and regulatory bodies around the world for example the Institution of Structural Engineers in the UK Depending on the degree course they have studied andor the jurisdiction they are seeking licensure in they may be accredited or licensed as just structural engineers or as civil engineers or as both civil and structural engineers Another international organisation is IABSEInternational Association for Bridge and Structural Engineering The aim of that association is to exchange knowledge and to advance the practice of structural engineering worldwide in the service of the profession and society Specializations Building structures Structural building engineering is primarily driven by the creative manipulation of materials and forms and the underlying mathematical and scientific ideas to achieve an end that fulfills its functional requirements and is structurally safe when subjected to all the loads it could reasonably be expected to experience This is subtly different from architectural design which is driven by the creative manipulation of materials and forms mass space volume texture and light to achieve an end which is aesthetic functional and often artistic The structural design for a building must ensure that the building can stand up safely able to function without excessive deflections or movements which may cause fatigue of structural elements cracking or failure of fixtures fittings or partitions or discomfort for occupants It must account for movements and forces due to temperature creep cracking and imposed loads It must also ensure that the design is practically buildable within acceptable manufacturing tolerances of the materials It must allow the architecture to work and the building services to fit within the building and function air conditioning ventilation smoke extract electrics lighting etc The structural design of a modern building can be extremely complex and often requires a large team to complete Structural engineering specialties for buildings include Earthquake engineering Faade engineering Fire engineering Roof engineering Tower engineering Wind engineering Earthquake engineering structures Earthquake engineering structures are those engineered to withstand earthquakes The main objectives of earthquake engineering are to understand the interaction of structures with the shaking ground foresee the consequences of possible earthquakes and design and construct the structures to perform during an earthquake Earthquakeproof structures are not necessarily extremely strong like the El Castillo pyramid at Chichen Itza shown above One important tool of earthquake engineering is base isolation which allows the base of a structure to move freely with the ground Civil engineering structures Civil structural engineering includes all structural engineering related to the built environment It includes The structural engineer is the lead designer on these structures and often the sole designer In the design of structures such as these structural safety is of paramount importance in the UK designs for dams nuclear power stations and bridges must be signed off by a chartered engineer Civil engineering structures are often subjected to very extreme forces such as large variations in temperature dynamic loads such as waves or traffic or high pressures from water or compressed gases They are also often constructed in corrosive environments such as at sea in industrial facilities or below ground resisted and significant deflections of structures The forces which parts of a machine are subjected to can vary significantly and can do so at a great rate The forces which a boat or aircraft are subjected to vary enormously and will do so thousands of times over the structures lifetime The structural design must ensure that such structures can endure such loading for their entire design life without failing These works can require mechanical structural engineering Boilers and pressure vessels Coachworks and carriages Cranes Elevators Escalators Marine vessels and hulls Aerospace structures Aerospace structure types include launch vehicles Atlas Delta Titan missiles ALCM Harpoon Hypersonic vehicles Space Shuttle military aircraft F16 F18 and commercial aircraft Boeing 777 MD11 Aerospace structures typically consist of thin plates with stiffeners for the external surfaces bulkheads and frames to support the shape and fasteners such as welds rivets screws and bolts to hold the components together Nanoscale structures A nanostructure is an object of intermediate size between molecular and microscopic micrometersized structures In describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale Nanotextured surfaces have one dimension on the nanoscale ie only the thickness of the surface of an object is between 01 and 100 nm Nanotubes have two dimensions on the nanoscale ie the diameter of the tube is between 01 and 100 nm its length could be much greater Finally spherical nanoparticles have three dimensions on the nanoscale ie the particle is between 01 and 100 nm in each spatial dimension The terms nanoparticles and ultrafine particles UFP often are used synonymously although UFP can reach into the micrometer range The term nanostructure is often used when referring to magnetic technology Structural engineering for medical science Medical equipment also known as armamentarium is designed to aid in the diagnosis monitoring or treatment of medical conditions There are several basic types diagnostic equipment includes medical imaging machines used to aid in diagnosis equipment includes infusion pumps medical lasers and LASIK surgical machines medical monitors allow medical staff to measure a patients medical state Monitors may measure patient vital signs and other parameters including ECG EEG blood pressure and dissolved gases in the blood diagnostic medical equipment may also be used in the home for certain purposes eg for the control of diabetes mellitus A biomedical equipment technician BMET is a vital component of the healthcare delivery system Employed primarily by hospitals BMETs are the people responsible for maintaining a facilitys medical equipment Structural elements Any structure is essentially made up of only a small number of different types of elements Columns Beams Plates Arches Shells Catenaries Many of these elements can be classified according to form straight plane curve and dimensionality onedimensional twodimensional Columns Columns are elements that carry only axial force compression or both axial force and bending which is technically called a beamcolumn but practically just a column The design of a column must check the axial capacity of the element and the buckling capacity The buckling capacity is the capacity of the element to withstand the propensity to buckle Its capacity depends upon its geometry material and the effective length of the column which depends upon the restraint conditions at the top and bottom of the column The effective length is K l displaystyle Kl where l displaystyle l is the real length of the column and K is the factor dependent on the restraint conditions The capacity of a column to carry axial load depends on the degree of bending it is subjected to and vice versa This is represented on an interaction chart and is a complex nonlinear relationship Beams A beam may be defined as an element in which one dimension is much greater than the other two and the applied loads are usually normal to the main axis of the element Beams and columns are called line elements and are often represented by simple lines in structural modeling cantilevered supported at one end only with a fixed connection simply supported fixed against vertical translation at each end and horizontal translation at one end only and able to rotate at the supports fixed supported in all directions for translation and rotation at each end continuous supported by three or more supports a combination of the above ex supported at one end and in the middle Beams are elements that carry pure bending only Bending causes one part of the section of a beam divided along its length to go into compression and the other part into tension The compression part must be designed to resist buckling and crushing while the tension part must be able to adequately resist the tension Trusses A truss is a structure comprising members and connection points or nodes When members are connected at nodes and forces are applied at nodes members can act in tension or compression Members acting in compression are referred to as compression members or struts while members acting in tension are referred to as tension members or ties Most trusses use gusset plates to connect intersecting elements Gusset plates are relatively flexible and unable to transfer bending moments The connection is usually arranged so that the lines of force in the members are coincident at the joint thus allowing the truss members to act in pure tension or compression Trusses are usually used in largespan structures where it would be uneconomical to use solid beams Plates Plates carry bending in two directions A concrete flat slab is an example of a plate Plates are understood by using continuum mechanics but due to the complexity involved they are most often designed using a codified empirical approach or computer analysis They can also be designed with yield line theory where an assumed collapse mechanism is analyzed to give an upper bound on the collapse load This technique is used in practice but because the method provides an upperbound ie an unsafe prediction of the collapse load for poorly conceived collapse mechanisms great care is needed to ensure that the assumed collapse mechanism is realistic Shells Shells derive their strength from their form and carry forces in compression in two directions A dome is an example of a shell They can be designed by making a hangingchain model which will act as a catenary in pure tension and inverting the form to achieve pure compression Arches Arches carry forces in compression in one direction only which is why it is appropriate to build arches out of masonry They are designed by ensuring that the line of thrust of the force remains within the depth of the arch It is mainly used to increase the bountifulness of any structure Catenaries Catenaries derive their strength from their form and carry transverse forces in pure tension by deflecting just as a tightrope will sag when someone walks on it They are almost always cable or fabric structures A fabric structure acts as a catenary in two directions Materials Structural engineering depends on the knowledge of materials and their properties in order to understand how different materials support and resist loads It also involves a knowledge of Corrosion engineering to avoid for example galvanic coupling of dissimilar materials Common structural materials are Iron wrought iron cast iron Concrete reinforced concrete prestressed concrete Alloy steel stainless steel Masonry Timber hardwood softwood Aluminium Composite materials plywood Other structural materials adobe bamboo carbon fibre fiber reinforced plastic mudbrick roofing materials See also Notes References Hibbeler R C 2010 Structural Analysis PrenticeHall Blank Alan McEvoy Michael Plank Roger 1993 Architecture and Construction in Steel Taylor Francis ISBN 0419176608 Hewson Nigel R 2003 Prestressed Concrete Bridges Design and Construction Thomas Telford ISBN 0727727745 Heyman Jacques 1999 The Science of Structural Engineering Imperial College Press ISBN 1860941893 Hosford William F 2005 Mechanical Behavior of Materials Cambridge University Press ISBN 0521846706 Further reading Blockley David 2014 A Very Short Introduction to Structural Engineering Oxford University Press ISBN 9780199671939 Bradley Robert E Sandifer Charles Edward 2007 Leonhard Euler Life Work and Legacy Elsevier ISBN 0444527281 Chapman Allan 2005 Englands Leornardo Robert Hooke and the Seventeenth Centurys Scientific Revolution CRC Press ISBN 0750309873 Dugas Ren 1988 A History of Mechanics Courier Dover Publications ISBN 0486656322 Feld Jacob Carper Kenneth L 1997 Construction Failure John Wiley Sons ISBN 0471574775 Galilei Galileo translators Crew Henry de Salvio Alfonso 1954 Dialogues Concerning Two New Sciences Courier Dover Publications ISBN 0486600998 Kirby Richard Shelton 1990 Engineering in History Courier Dover Publications ISBN 0486264122 Heyman Jacques 1998 Structural Analysis A Historical Approach Cambridge University Press ISBN 0521622492 Labrum EA 1994 Civil Engineering Heritage Thomas Telford ISBN 072771970X Lewis Peter R 2004 Beautiful Bridge of the Silvery Tay Tempus Mir Ali 2001 Art of the Skyscraper the Genius of Fazlur Khan Rizzoli International Publications ISBN 0847823709 Rozhanskaya Mariam Levinova I S 1996 Statics in Morelon Rgis Rashed Roshdi 1996 Encyclopedia of the History of Arabic Science vol 23 Routledge ISBN 0415020638 Whitbeck Caroline 1998 Ethics in Engineering Practice and Research Cambridge University Press ISBN 0521479444 Hoogenboom PCJ 1998 Discrete Elements and Nonlinearity in Design of Structural Concrete Walls Section 13 Historical Overview of Structural Concrete Modelling ISBN 9090118438 Nedwell PJ Swamy RNed 1994 FerrocementProceedings of the Fifth International Symposium Taylor Francis ISBN 0419197001 External links Structural Engineering Association International National Council of Structural Engineers Associations Structural Engineering Institute an institute of the American Society of Civil Engineers Structurae database of structures Structural Engineering Association International The EN Eurocodes are a series of 10 European Standards EN 1990 EN 1999 providing a common approach for the design of buildings and other civil engineering works and construction products",
  },
  {
    title: "Environmental engineering",
    originalContent:
      "Environmental engineering is a professional engineering discipline related to environmental science It encompasses broad scientific topics like chemistry biology ecology geology hydraulics hydrology microbiology and mathematics to create solutions that will protect and also improve the health of living organisms and improve the quality of the environment Environmental engineering is a subdiscipline of civil engineering and chemical engineering While on the part of civil engineering the Environmental Engineering is focused mainly on Sanitary Engineering Environmental engineering applies scientific and engineering principles to improve and maintain the environment to protect human health protect natures beneficial ecosystems and improve environmentalrelated enhancement of the quality of human life Environmental engineers devise solutions for wastewater management water and air pollution control recycling waste disposal and public health They design municipal water supply and industrial wastewater treatment systems and design plans to prevent waterborne diseases and improve sanitation in urban rural and recreational areas They evaluate hazardouswaste management systems to evaluate the severity of such hazards advise on treatment and containment and develop regulations to prevent mishaps They implement environmental engineering law as in assessing the environmental impact of proposed construction projects Environmental engineers study the effect of technological advances on the environment addressing local and worldwide environmental issues such as acid rain global warming ozone depletion water pollution and air pollution from automobile exhausts and industrial sources Most jurisdictions impose licensing and registration requirements for qualified environmental engineers Etymology The word environmental has its root in the late 19thcentury French word environ verb meaning to encircle or to encompass The word environment was used by Carlyle in 1827 to refer to the aggregate of conditions in which a person or thing lives The meaning shifted again in 1956 when it was used in the ecological sense where Ecology is the branch of science dealing with the relationship of living things to their environment The second part of the phrase environmental engineer originates from Latin roots and was used in the 14th century French as engignour meaning a constructor of military engines such as trebuchets harquebuses longbows cannons catapults ballistas stirrups armour as well as other deadly or bellicose contraptions The word engineer was not used to reference public works until the 16th century and it likely entered the popular vernacular as meaning a contriver of public works during John Smeatons time History Ancient civilizations Environmental engineering is a name for work that has been done since early civilizations as people learned to modify and control the environmental conditions to meet needs As people recognized that their health was related to the quality of their environment they built systems to improve it The ancient Indus Valley Civilization 3300 BCE to 1300 BCE had advanced control over their water resources The public work structures found at various sites in the area include wells public baths water storage tanks a drinking water system and a citywide sewage collection system They also had an early canal irrigation system enabling largescale agriculture From 4000 to 2000 BCE many civilizations had drainage systems and some had sanitation facilities including the Mesopotamian Empire MohenjoDaro Egypt Crete and the Orkney Islands in Scotland The Greeks also had aqueducts and sewer systems that used rain and wastewater to irrigate and fertilize fields The first aqueduct in Rome was constructed in 312 BCE and the Romans continued to construct aqueducts for irrigation and safe urban water supply during droughts They also built an underground sewer system as early as the 7th century BCE that fed into the Tiber River draining marshes to create farmland as well as removing sewage from the city Modern era Very little change was seen from the decline of the Roman Empire until the 19th century where improvements saw increasing efforts focused on public health Modern environmental engineering began in London in the mid19th century when Joseph Bazalgette designed the first major sewerage system following the Great Stink The citys sewer system conveyed raw sewage to the River Thames which also supplied the majority of the citys drinking water leading to an outbreak of cholera The introduction of drinking water treatment and sewage treatment in industrialized countries reduced waterborne diseases from leading causes of death to rarities The field emerged as a separate academic discipline during the middle of the 20th century in response to widespread public concern about water and air pollution and other environmental degradation As society and technology grew more complex they increasingly produced unintended effects on the natural environment One example is the widespread application of the pesticide DDT to control agricultural pests in the years following World War II The story of DDT as vividly told in Rachel Carsons Silent Spring 1962 is considered to be the birth of the modern environmental movement which led to the modern field of environmental engineering Education Many universities offer environmental engineering programs through either the department of civil engineering or chemical engineering and also including electronic projects to develop and balance the environmental conditions Environmental engineers in a civil engineering program often focus on hydrology water resources management bioremediation and water and wastewater treatment plant design Environmental engineers in a chemical engineering program tend to focus on environmental chemistry advanced air and water treatment technologies and separation processes Some subdivisions of environmental engineering include natural resources engineering and agricultural engineering Courses for students fall into a few broad classes Mechanical engineering courses oriented towards designing machines and mechanical systems for environmental use such as water and wastewater treatment facilities pumping stations garbage segregation plants and other mechanical facilities Environmental engineering or environmental systems courses oriented towards a civil engineering approach in which structures and the landscape are constructed to blend with or protect the environment Environmental chemistry sustainable chemistry or environmental chemical engineering courses oriented towards understanding the effects of chemicals in the environment including any mining processes pollutants and also biochemical processes Environmental technology courses oriented towards producing electronic or electrical graduates capable of developing devices and artifacts able to monitor measure model and control environmental impact including monitoring and managing energy generation from renewable sources Curriculum The following topics make up a typical curriculum in environmental engineering Mass and Energy transfer Environmental chemistry Inorganic chemistry Organic Chemistry Nuclear Chemistry Growth models Resource consumption Population growth Economic growth Risk assessment Hazard identification Doseresponse Assessment Exposure assessment Risk characterization Comparative risk analysis Water pollution Water resources and pollutants Oxygen demand Pollutant transport Water and waste water treatment Air pollution Industry transportation commercial and residential emissions Criteria and toxic air pollutants Pollution modelling eg Atmospheric dispersion modeling Pollution control Air pollution and meteorology Global change Greenhouse effect and global temperature Carbon nitrogen and oxygen cycle IPCC emissions scenarios Oceanic changes ocean acidification other effects of global warming on oceans and changes in the stratosphere see Physical impacts of climate change Solid waste management and resource recovery Life cycle assessment Source reduction Collection and transfer operations Recycling Wastetoenergy conversion Landfill Applications Water supply and treatment Environmental engineers evaluate the water balance within a watershed and determine the available water supply the water needed for various needs in that watershed the seasonal cycles of water movement through the watershed and they develop systems to store treat and convey water for various uses Water is treated to achieve water quality objectives for the end uses In the case of a potable water supply water is treated to minimize the risk of infectious disease transmission the risk of noninfectious illness and to create a palatable water flavor Water distribution systems are designed and built to provide adequate water pressure and flow rates to meet various enduser needs such as domestic use fire suppression and irrigation Wastewater treatment There are numerous wastewater treatment technologies A wastewater treatment train can consist of a primary clarifier system to remove solid and floating materials a secondary treatment system consisting of an aeration basin followed by flocculation and sedimentation or an activated sludge system and a secondary clarifier a tertiary biological nitrogen removal system and a final disinfection process The aeration basinactivated sludge system removes organic material by growing bacteria activated sludge The secondary clarifier removes the activated sludge from the water The tertiary system although not always included due to costs is becoming more prevalent to remove nitrogen and phosphorus and to disinfect the water before discharge to a surface water stream or ocean outfall Air pollution management Scientists have developed air pollution dispersion models to evaluate the concentration of a pollutant at a receptor or the impact on overall air quality from vehicle exhausts and industrial flue gas stack emissions To some extent this field overlaps the desire to decrease carbon dioxide and other greenhouse gas emissions from combustion processes Environmental impact assessment and mitigation Environmental engineers apply scientific and engineering principles to evaluate if there are likely to be any adverse impacts to water quality air quality habitat quality flora and fauna agricultural capacity traffic ecology and noise If impacts are expected they then develop mitigation measures to limit or prevent such impacts An example of a mitigation measure would be the creation of wetlands in a nearby location to mitigate the filling in of wetlands necessary for a road development if it is not possible to reroute the road In the United States the practice of environmental assessment was formally initiated on January 1 1970 the effective date of the National Environmental Policy Act NEPA Since that time more than 100 developing and developed nations either have planned specific analogous laws or have adopted procedure used elsewhere NEPA is applicable to all federal agencies in the United States Regulatory agencies Environmental Protection Agency The US Environmental Protection Agency EPA is one of the many agencies that work with environmental engineers to solve critical issues An essential component of EPAs mission is to protect and improve air water and overall environmental quality to avoid or mitigate the consequences of harmful effects See also Associations References Further reading Davis M L and D A Cornwell 2006 Introduction to environmental engineering 4th ed McGrawHill ISBN 9780072424119 National Academies of Sciences Engineering and Medicine 2019 Environmental Engineering for the 21st Century Addressing Grand Challenges Report Washington DC The National Academies Press doi101722625121 ISBN 9780309476522cite report CS1 maint multiple names authors list link",
  },
];
